{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocab from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 8)\n",
      "(4794, 8)\n",
      "(4795, 8)\n"
     ]
    }
   ],
   "source": [
    "data_train_df = pd.read_csv('data/Twitter/hate_twitter/train_clean.csv')\n",
    "\n",
    "# using code from https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "train, val, test = np.split(data_train_df.sample(frac=1, random_state=8),\\\n",
    "    [int(0.7*len(data_train_df)),int(0.85*len(data_train_df))])\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/Twitter/hate_twitter/hate_train.csv')\n",
    "val.to_csv('data/Twitter/hate_twitter/hate_val.csv')\n",
    "test.to_csv('data/Twitter/hate_twitter/hate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/Twitter/hate_twitter/hate_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg omg omg yay found wonderful price segasaturn throwback',\n",
       " 'payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place',\n",
       " 'trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo',\n",
       " 'turn resignation',\n",
       " 'happy bihday hajime hosogai bihday bihday 30']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = list(train_df[train_df['clean_tweet'].notna()]['clean_tweet'])\n",
    "data_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 omg omg omg yay found wonderful price segasaturn throwback',\n",
       " '0 payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place',\n",
       " '0 trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo',\n",
       " '1 turn resignation',\n",
       " '0 happy bihday hajime hosogai bihday bihday 30',\n",
       " '0 im happy glassesgirl asiangirl holiday ecogreenpark fashion casual',\n",
       " '0 whats going world',\n",
       " '0 hour 56pm ibar monday thursday ibar specials hour',\n",
       " '0 im london tomorrow blogtacular full planning prep blogger',\n",
       " '0 happy little squid saying good morning happy wednesday squid smile wednesday',\n",
       " '0 crazy day',\n",
       " '0 upon time driving love freedom concentration blackandwhite photooftheday',\n",
       " '0 friends know well cake bihay critic samba batman honeycake friends',\n",
       " '1 sick preprogrammed hillbots rhetoric enough already unity patriotism love fe',\n",
       " '0 many many returns day',\n",
       " '0 takes ceain kind person scam scammed vulnerable lonely ppl given attn',\n",
       " '0 would steal awork boohiss',\n",
       " '0 gog pusiste closup love',\n",
       " '0 hour walking looking something could buy ended',\n",
       " '0 little brother higher tolerance alcohol lol',\n",
       " '0 stamford attack bull chase leave lot despite fact youre strong source fo',\n",
       " '0 theatre goers buzzing aspirations week',\n",
       " '0 lighttherapy help depression altwaystoheal healthy happy',\n",
       " '0 yummy iam positive affirmation',\n",
       " '0 bizarre tragic',\n",
       " '0 never much feminism fuelled anger recently feminism socialjustice',\n",
       " '0 tips attract good positive energy via energy',\n",
       " '0 seriously im going go hulk mode minute tosser boyfriend hulksmash fuckthisshit',\n",
       " '0 hello link bio adorable fluffy cutedog love instagood photooftheday',\n",
       " '0 want learn desire joy innocence happiness without happiness',\n",
       " '0 another day awesome city called new york riasworld parade harricrishna',\n",
       " '0 enjoying night fun bihday love',\n",
       " '0 23 small businesses closed yrs nyc negatively transforming extremely fast',\n",
       " '0 cant wait tonight next event else coming delhi newdelhi polishgirl foodie meeting',\n",
       " '0 today last teaching day nohampton academy wow daventry hod',\n",
       " '0 everybody blocks shes like annoying mosquito troll',\n",
       " '0 refreshing iam positive affirmation',\n",
       " '0 challenges aaps claim 70 punjabis drugaddicts dares prove',\n",
       " '0 bible quote day sunday bibleverse brave',\n",
       " '0 prairie dog ejrbakker animals wildlife familyprairie dogmother children via geniushowtoc',\n",
       " '0 makes angry hang diy fuckers sick 24hoursinpolice',\n",
       " '0 grow cute',\n",
       " '0 embrace enjoy makes truly eternal value store treasures heaven lectiodivina',\n",
       " '0 405 backed miles idiot decided jump overpass near westminster traffic crazy california',\n",
       " '0 new listings ebay woohoo sell selling buy shop shopping',\n",
       " '0 jumping joy animated graduation stepbystep',\n",
       " '0 lots strawberries appearing garden morning homegrown strawberries',\n",
       " '0 must washed 38 towels today argggg girls indeed lovesex blowjobs massage',\n",
       " '0 tragedy orlando tragic',\n",
       " '1 allahsoil kneejerk reaction assume must isis',\n",
       " '0 well donald trump officially reached bottom clown paying hood chicks promotion',\n",
       " '0 dont respond well compliments quotes latenights ness crying',\n",
       " '1 horribly ads 2016 made us cringe',\n",
       " '0 sad adult tshi swag shioopia quote',\n",
       " '1 quotes women kingfisher girls xxx',\n",
       " '0 im tired alone solitary moment makes want come back home scars emo',\n",
       " '0 today grieve trump seeks fuher recognition votes shameful orlando',\n",
       " '0 okay little sad sometimes travel travelwithboyfriend journey goodlife',\n",
       " '0 happy sunday great day man selfie sunday',\n",
       " '0 nothing ahead sunshine getoutside glowing cantstopsmiling weekend',\n",
       " '0 allwhoretweetswantnewfollowers tfbjp teamfollowback openfollow followback 3271',\n",
       " '0 kat love gibson guitar girls jamaica jammin music jovirockwell katchr',\n",
       " '0 tinker bell naked 2b hardcore chapter',\n",
       " '0 model love take time ur',\n",
       " '0 seems like lifetime ago',\n",
       " '0 late ff gamedev indiedev indiegamedev squad',\n",
       " '0 avicii last tour asia paylife umf umfkorea2016 seoul korea 20160612',\n",
       " '0 little needed make life marcus aurelius quotes',\n",
       " '0 model love take time ur',\n",
       " '0 canada consumer price index mom came 04 expectations 05 may blog silver gold',\n",
       " '0 appletstag selfie smile girl love hair selca summer followme instagood',\n",
       " '0 glad made fantasy team captain euro2016',\n",
       " '0 man home ive never love couple couples relationship',\n",
       " '0 bihday',\n",
       " '0 playing pearl tomorrow norainonthecourse',\n",
       " '0 broken buffalo simulation buffalo take vicinity homes way',\n",
       " '0 youre focused whether contour point makeup parents bought wakeup',\n",
       " '0 silly time god amazing goaldigger beedaboss kokcoutureclothing',\n",
       " '0 fun em2016 croatia game soccer inlovewithcroatia',\n",
       " '0 victorious iam positive affirmation',\n",
       " '0 finished episode season sherlock sign',\n",
       " '0 decided happy good health',\n",
       " '0 happy moments family family myfamily family moments instalike',\n",
       " '0 kids loud reason gotta love em kids dads fathersday instamood smile funny',\n",
       " '0 allow children way better way ever find gandhi',\n",
       " '0 race ladies gentlemen canadiangp last vettel vs hamilton',\n",
       " '0 sunday bird scout time instagood instacool instabird',\n",
       " '0 rip elder brother one believed yunging god rest ur soul death happy 50th bihday 10days sir',\n",
       " '0 periscope rant orlando florida hu',\n",
       " '0 victory nascar race brands hatch',\n",
       " '0 dont think every day good day try missing one cavett robe goodday inspiration',\n",
       " '0 parking meter guy gave hrs free parking today payitforward vancity',\n",
       " '0 yay finally colourpop newfaves makeup lovegettingmail',\n",
       " '0 berry happy breakfast cheers healthy hayley south beach miami beach',\n",
       " '0 happy friday everyone friday summer toronto',\n",
       " '0 model love take time ur',\n",
       " '0 bihday lunch dear friend sammaki lovely afternoon bihday',\n",
       " '0 course hows trip brno hear close brno rummors',\n",
       " '0 find making schools places educationfest',\n",
       " '0 201606190619sunday fathersdaynikenikeairmaxairmaxi',\n",
       " '0 letbno man steal joy',\n",
       " '0 je relate soo badly msaken little knowledge one single group want show anyhow',\n",
       " '0 standing ideas mullahs kill ignorant mullahs pakistan would prosperous',\n",
       " '0 chewing gum gum seat litter blue arriva',\n",
       " '0 game gone tired called expes flapping mouths play game win lose',\n",
       " '0 sabotaged defamed stalked retaliated every level entire life slandered',\n",
       " '0 glad sharing bihday tupac please play thugs mansion tupac bihday tupac',\n",
       " '0 great addition reasons shouldnt potus cant spell',\n",
       " '0 feel horrible watching brother sick',\n",
       " '0 first 2000 views new shis done well youtube youtubegaming',\n",
       " '0 thankful clean sheets thankful positive',\n",
       " '0 got austins car back home safe sound great day obx perfect weather perfect day even got drive',\n",
       " '0 fathers day wonderful fathers day enjoy fullest',\n",
       " '0 anyone know plans race anymore runner tiffany',\n",
       " '0 working song ep musicistherapy music newmusic aist lyrics songwriter think im calling brick brick',\n",
       " '0 po gorilla simulator need adapt environment need tear city material',\n",
       " '0 cant stuff',\n",
       " '0 front street eats presents fathers day great fathers world',\n",
       " '0 seen video england fans singing surrender arent morons aware decommissioned weapons decade ago',\n",
       " '0 pray orlando continues happen',\n",
       " '0 waking hearing 50 people died gay still family orlando',\n",
       " '0 beautiful cleopatra earrings shop link bio fashion hot',\n",
       " '0 friday call weekend friday ill try thursday call text reasonstokillmyself pathetic',\n",
       " '0 anyone else notice suppoers pageant girls years internalize female oppressiv',\n",
       " '0 thats exactly dad cant describe words biasedlaw seprates kids daddy fathersday',\n",
       " '0 senseless',\n",
       " '0 miss mom inshot girls cute summer blur sun fun',\n",
       " '0 146562489121 update social analytics instalike html foss geek igers',\n",
       " '0 yet another example qualified prez winning right',\n",
       " '0 rock brother wat way staing show new abt keshi ali ted spice',\n",
       " '0 angry melbourne street melbourne streetphoto blackandwhite roaming lifesucks',\n",
       " '0 ronnie enjoying sunshine daisy field',\n",
       " '0 flightless bird ironandwine gay gayboys instagram instagay follow heyboo',\n",
       " '0 youre take selfie',\n",
       " '0 know sure lazy lair nobody sends letters ceainly thousands',\n",
       " '0 happy 68th month ngit love lots 09',\n",
       " '0 wishing dad fathersday sundayfunday bestdad dad',\n",
       " '0 advises build happy longlasting marriage longlasting marriage via',\n",
       " '0 home snug bed lovely night home family lucky snug',\n",
       " '1 irritations like newghostbusters haters indisputably intrusively indecent imitations misogynist whiners',\n",
       " '0 saw miller lite beer',\n",
       " '1 arabs preying poor slavelabor',\n",
       " '0 theyre bars rez recreational activities youth',\n",
       " '0 happy announce named game mythic indiedev gamedev announcement',\n",
       " '0 good stuff multivitamins weekend 061116',\n",
       " '0 film bull dominate bull direct whatever want sta',\n",
       " '0 snow white gt open gt sleepy sneezy bashful dm us today',\n",
       " '0 im going see finding dory findingdory fishfriday',\n",
       " '0 ummm still day4 silenttreatment',\n",
       " '0 want performance buy nvidia right',\n",
       " '0 bet make sure nither markamplaura evicted week',\n",
       " '0 jetski fun bora bora bora bora always favorite vacation spot team vacation',\n",
       " '0 know found sad read deaths also people hating blaming heterosexuals',\n",
       " '0 outlining themes wouldbe poems haha',\n",
       " '0 didnt give us update cyberpunk 2077 xboxe3 hope hear something soon',\n",
       " '0 11th year anniversary michael jacksons vindication',\n",
       " '0 still blaming harper guys need professional help wow',\n",
       " '0 love e32016 home uk',\n",
       " '1 might libtard libtard sjw liberal politics',\n",
       " '0 check',\n",
       " '0 big thank making sons favourite fish soup especially eddy',\n",
       " '0 entering new phase life feeling going back student life making hea thumping thumping punediaries sta',\n",
       " '0 sad news mr hockey gordiehowe died 25 yrs red wings second time goals 800 9newsmornings',\n",
       " '0 females smile face shady asf leave room',\n",
       " '0 gets via emotion anger society',\n",
       " '0 life need give good kick im',\n",
       " '1 smoke old year new year time marijuana unleashed ibooks',\n",
       " '0 matter opinion hate place world time teach children respect',\n",
       " '0 eyebrow game weak',\n",
       " '0 check family lawyer food integral family bonding nomnom foodporn food blog',\n",
       " '0 renaissance music stylistic period music historyancient vitorr staup read share',\n",
       " '0 2016 year hooliganism returned football stadiums',\n",
       " '0 stupid word software 100 date yet refuses save macbook deleted entire day urgent work',\n",
       " '0 mom told im make build bear',\n",
       " '0 fathers day',\n",
       " '0 alifuneral turned liberal rally shocker',\n",
       " '0 dont live little live music weekend sunday music livelong',\n",
       " '0 buffalo bull dominate bull direct whatever want',\n",
       " '1 oh fuuuuuuuuuuuuck right',\n",
       " '0 kpop dance choreographer bts cl',\n",
       " '0 evatom thank happy bihday gin bihday newcastle theyorkshirespirit',\n",
       " '0 moment moment life',\n",
       " '0 isis burns alive 19 girls feminist west complain mythical patriarchy wage gap factsoverfeelings altright',\n",
       " '0 craziestpeoples motivation',\n",
       " '0 shilpa shetty turns year oldergtgtgt bihday oneworldnews',\n",
       " '0 nice words nice saturday quote havefuntoday love',\n",
       " '0 together forever prayfororlando hea love design color symbol beautiful quote',\n",
       " '0 fathersday open 10 taking walkins usual see soon',\n",
       " '0 news carson uses word fairness trumpswiddleblackbuddy usedbytrump',\n",
       " '0 today one stop murdering orlandolove god',\n",
       " '0 sundaymorning sunday thinkingaboutyou missingyou missing selfie daphneisbackbitches',\n",
       " '0 face reading emails looking news world',\n",
       " '0 fathers day thehardworking dads fathersday iloveyou',\n",
       " '0 sketching moose lot fun one ilovedrawing animals makes',\n",
       " '0 end always wrong whenyourenotrightinyourhea unwanted used damaged unstable',\n",
       " '0 everyone thinks youre see private',\n",
       " '0 body 2yr old recovered fully disneygatorattack',\n",
       " '0 awww beautiful song chemical romance literally staed cry listening',\n",
       " '0 transformationtuesday im excited abs coming vegansofig',\n",
       " '0 goofy idiot lot goofy dumb americans get nod goofydonnie',\n",
       " '0 ear whispered hea lips kissed soul judy garland',\n",
       " '0 everybody makingsharing memes peaining people losing lives recent events funny youre disgusting',\n",
       " '0 honestly due laziness inactivity ages 1824 election poles online bernie wouldve landslide',\n",
       " '0 pat robeson orlando shooting gods punishment scotus samesex marriage ruling blog silver',\n",
       " '1 multiple types racism america us america',\n",
       " '0 sometimes wonder gone wrong life humans much hate lost humanity orlandoshooting peace',\n",
       " '0 thankful vaccines thankful positive',\n",
       " '0 took first step learning family heritage ordering ancestrydna test',\n",
       " '0 thanks im twitter',\n",
       " '0 fathers day patrick murphy finest man ive ever known miss plenty',\n",
       " '0 bihday surround people love motivate encourage',\n",
       " '0 thankful shoes thankful positive',\n",
       " '0 european monetary union current account sa rose previous 273bto 362b april blog silver',\n",
       " '0 actual fruit id raspberry cuz france id framboise pride yall',\n",
       " '0 epicfail diy donald pretending know kkk davidduke',\n",
       " '0 canceled disneyland fireworks day go waited year see new show disneyland heabroken disneywhy',\n",
       " '0 come across poster cantwait xxx',\n",
       " '0 one ever depressed something pain struggles let us know alive smile keep moving',\n",
       " '0 ready go tedx tedxchelmsford wherehasmyhairgone strategy',\n",
       " '0 thankful therapy thankful positive',\n",
       " '0 mgaenp 118 episodes thought sho stand corrected kahani hamari',\n",
       " '0 youve gotta love millennials micah tyler youtube weekend',\n",
       " '0 finally found way delete old tweets might find useful well deletetweets',\n",
       " '0 fathers day king',\n",
       " '0 mr wings page spread magazine come sad isnt coverboy lol',\n",
       " '1 jeffsessions thinks wont notice redacts decades ove application ag confirmation',\n",
       " '0 medical humor people truths feel better sad true love husband highlights',\n",
       " '0 train hasnt moved im binge tweeting',\n",
       " '0 current mood workingout goals igotthis devoted',\n",
       " '0 congrats ur wish come true working superxcited songs',\n",
       " '0 happy friday everyone friday fun love enjoy weekend healthy fitness progress work hard dontgiveup',\n",
       " '1 expes explain best way deal friends family',\n",
       " '0 praying affected orlando attack matters whether gay matters',\n",
       " '0 fathersday love papa jaan',\n",
       " '0 oh way im going new jersey summer since thats lives summer2016',\n",
       " '0 make conscious decision cant control world control mood',\n",
       " '0 work fakest people ever sodone',\n",
       " '0 beautiful sunday morning fathersday money',\n",
       " '0 gays killed en masse islamic murderer twink blames calling culprit',\n",
       " '0 weeeellll know dont like travelso seen youre closemaybe twice xx',\n",
       " '0 hot blonde happy face covered fresh semen amateur blonde private pics cum facial',\n",
       " '0 never stop suppoing love lotsamplots',\n",
       " '0 bihday ksi keep beast',\n",
       " '0 would rather aiiight',\n",
       " '0 sentiments exactly war minorities war control think',\n",
       " '0 happy bihday day 23 lebron miley jordan im bihday dad cake spos chuckecheese beer presents bihd',\n",
       " '0 thank wednesday',\n",
       " '0 hapoyfathersday kimkardashian wishes kanyewest fathers day cheek',\n",
       " '0 face following successful completion first vo session voiceover aist',\n",
       " '0 blackheathstandard summerfair',\n",
       " '0 quite often scratch surface youre left',\n",
       " '0 time tomorrow pitched camper ready',\n",
       " '0 thankful imagination thankful positive',\n",
       " '0 auld chalkboard architecture please',\n",
       " '0 bangkok thailand',\n",
       " '0 yeah coupon tmobiletuesdays doesnt work guess pizza tonight',\n",
       " '0 finally got nandos tho good',\n",
       " '0 really shitty day far please cheer internet friends crappyday depressed nausea',\n",
       " '0 cameras roll next month ajewinutah indie dreams wow toomanyhashtags',\n",
       " '0 mindsconsole true motivation',\n",
       " '0 happening warcraftmovie warcraft',\n",
       " '0 jpy negative external environment remains suppoive mufg blog silver gold forex',\n",
       " '0 anyone else one family member related marriage thrives back stabbing actual family grrr ikeelu',\n",
       " '0 went fye didnt album betterweather notbetterweather',\n",
       " '1 must heabreaking blacks within find theyre helpless',\n",
       " '1 society overcome possible end racism happens one person time spirituality',\n",
       " '0 waking dream midsummers night perfect bihday beautiful',\n",
       " '0 many people country listen folk music choice never even considered theres another genre',\n",
       " '0 key life thank dallas clayton',\n",
       " '0 got best news grateful cantstopsmiling',\n",
       " '0 bruh people really knowing better still choosing better lmfao',\n",
       " '0 happy fathers day special dads dad daddy fathersday love thebabyboutique dubai',\n",
       " '0 hump day office lovely assistant sunshine florida girls love life homeoffice',\n",
       " '0 weddings impoant celebrate life possibility anne hathaway wedding love',\n",
       " '0 bihday girl mybihday ready 24 1992 gypsy',\n",
       " '0 good saturday life picstitch pic instamood instahub',\n",
       " '0 like phab2pro great small tablet motoz 55 vzw huge mistake toobig sorrybutdoa',\n",
       " '0 begun something new page super daily tarot guidance',\n",
       " '1 familys horrible attack thegreenpalmcottage plettenburgbay westerncape thyini thiza want',\n",
       " '0 looking forward seeing perform 4pm today going amazing',\n",
       " '0 knowledgeable iam positive affirmation',\n",
       " '0 beaming iam positive affirmation',\n",
       " '0 model love take time ur',\n",
       " '0 next session',\n",
       " '0 seasone',\n",
       " '0 thankful palm trees thankful positive',\n",
       " '0 magnettherapy really work altwaystoheal healing healthy',\n",
       " '0 happy friday everyone pokemongo pokemonsunmoon pokemon golduck psyduck pokemonanime anime wateype',\n",
       " '0 try tail us stop butt good time goldenretriever animals',\n",
       " '0 suga hope bts day pay via',\n",
       " '0 anxietyproblems frustrated annoyed many emotions able express',\n",
       " '0 time cast take stage historyhasitseyesonyou',\n",
       " '0 lives taken gay',\n",
       " '0 happy friday folks smirk friday dogs kidding kid goat lsd hippy bumblebaytuna',\n",
       " '0 world ocean day',\n",
       " '0 hea always broken thats okay dont need gave mine love soulmates onehea onelove lovewins',\n",
       " '0 watch udtapunjab know reality bhagwantmann ghuggi tell bjp',\n",
       " '0 cant believe im graduating two weeks eek nervous celebrations',\n",
       " '0 nzdusd rises 07040 eyeing gdt price index fomc statement blog silver gold forex',\n",
       " '0 refugees immts move cause hype hope prosperity poorer poor',\n",
       " '1 hispanic feel like stomping listen retweet boricua',\n",
       " '0 power means netflix',\n",
       " '0 utterly pathetic wish ali around beat person senseless',\n",
       " '0 nba made shooters space longer power around rim going call anything therebetter oldschool',\n",
       " '0 pmqs dealt bloody beer destitute desperate povey',\n",
       " '0 humanity word lost meaning nowadays truth',\n",
       " '0 lawless feckless elitist lying manipulative scheming pandering antiamerican globalists historical',\n",
       " '1 fuck xenophobia drink arabian spiced coffee eat berberian spiced food rainbow diversity',\n",
       " '0 playing sophie hutchings seventeen music song',\n",
       " '0 campers decide work camp imold',\n",
       " '0 get street cred tackling half menu well waiting round two',\n",
       " '0 cried need someone',\n",
       " '1 holiday fun socially relevant podcast celebrates 70 bewitched xmas ep',\n",
       " '0 gulfcoastnightlife hour fri sat 13th street jazz bistro oceansprings',\n",
       " '0 vegas booked',\n",
       " '0 writing editing floats boat',\n",
       " '0 cant remember last time didnt check phone going bed',\n",
       " '0 pls norfolkhour eaaa norfolk polo festival stas tomorrow',\n",
       " '0 could us liat channel orlando mass shooting di snapchat really break hea pic',\n",
       " '0 im sure still suppo right gun wakeupamerica',\n",
       " '1 take action filthy plantation location manager disgraceful pos boycott',\n",
       " '0 time take action save insure invest future visit us',\n",
       " '0 kinda coffee today greatquotes quotes self selfie selfwoh motivation truth',\n",
       " '0 waited frickin 25 minutes chicken messed order charged much hell man',\n",
       " '0 need book asap wow foodporn vegan healthylifestyle recipes',\n",
       " '0 smilemore peeps smilemore behumble behappy notfake peaceful',\n",
       " '0 eurusd hovering 11240 daily peaks blog silver gold forex',\n",
       " '0 gotta love deliver printing client say dont look day 30 40 design print',\n",
       " '0 husband surprises flowers lilies justlikethat oberoi splendor jv',\n",
       " '0 change thoughts change world confident determined optimistic',\n",
       " '0 finally get sent kindof ha',\n",
       " '0 kids loved drums mom couldnt close deal salesmen bengaluru street market wednesday',\n",
       " '0 hello selfie cold',\n",
       " '0 traveling happy family holidays car sunnyday sunny sunday glasses enjoy',\n",
       " '0 watch demo video product development team feeling saasy grateful shedwool shiftwork',\n",
       " '0 lilin happy bihday rp 19000 bihday bihday lilin candle lilinulangtahun',\n",
       " '0 reading book silent cry emotional insperationalwoman',\n",
       " '0 come get 2008 ford mustang usedcars houston texas suv mustangcoupe sale thursday',\n",
       " '0 youre profiling based looks isnt illegal butthu feelthebern',\n",
       " '0 true love true love wedding bride groom tears crying tearsofjoy married',\n",
       " '0 thank thoughts prayers victims',\n",
       " '0 sore feet loving life jack',\n",
       " '0 big weekend ahead auditions tomorrow dancing sunday dialogue singing looking forward guysamp',\n",
       " '0 thoughts prayers deepest condolences people orlando rest america',\n",
       " '0 fan wapo love story see quickly divide theres tragedy orlando',\n",
       " '0 ooh surprise delivery no2 use next month',\n",
       " '0 whats wrong america must step prolife orlando',\n",
       " '0 enjoy weekend weekend friday teacher teach school relax',\n",
       " '0 fathers day supperdad nikonion nikkoreyes indiaclicks mansoon rainy season',\n",
       " '0 wsib refunds way much bk employers greed worse refunds payout workers',\n",
       " '0 saved tule today',\n",
       " '0 sad see many fake warrior fans especially laker fan month ago',\n",
       " '0 everyones talking yeezyboost750 im still daydreaming 350s lololol butforreal yeezyboost350alltheway',\n",
       " '1 go girl maga',\n",
       " '0 gotta donethree lions eng euro2016',\n",
       " '0 serious holiday bluesfirst day back work sucked come home empty house',\n",
       " '0 ill missing champ thegreatest',\n",
       " '0 record bigboobed girls together theyre one roof',\n",
       " '0 bday fathers loved',\n",
       " '0 weeks go im san antonio bound see cant wait see little brother missyou mybrotherskeeper',\n",
       " '0 youre going',\n",
       " '0 ignored confused disappointed many years cant see changesadly tried trust',\n",
       " '0 whats happing orlando really shocking must remember behind death toll someone son daughter',\n",
       " '0 preordered perfect body yoga program',\n",
       " '0 excited get painting done bath vanity area',\n",
       " '0 want sta new job like pls cantkeepwaiting newchapter',\n",
       " '0 never understand girls post drama fight twitter make big fool',\n",
       " '0 last known living 911 search dog dies texas age 16 searchdog bretagne rip',\n",
       " '0 gearing ak1442016',\n",
       " '0 terrible news cant believe hate directed towards lgbt community heabreaking',\n",
       " '0 willy picking night morning mean bites hard wrassling game mean ow',\n",
       " '0 chill time fave guilty pleasure wee vino bliss metime chill watching keeping kardashians',\n",
       " '0 mermaids beautiful love instagood instadaily cute fashion apparel',\n",
       " '1 trump cuck nazi swastika ebay trump donaldtrump republicans kkk whitepower',\n",
       " '0 one biggest aims retreats true happiness',\n",
       " '0 im grateful affirmations',\n",
       " '0 even stitches im still breathing blackandwhite night',\n",
       " '0 love pherell williams little big town',\n",
       " '0 see among others everybody say yeah',\n",
       " '0 view seem campaigning across country today rememberi',\n",
       " '0 podcast',\n",
       " '0 pls come back album together',\n",
       " '0 thankful travel thankful positive',\n",
       " '1 far blog comments claim senad lulic statement wasnt racist join debate lazio',\n",
       " '0 clean solar technologies fly around world using patented panels ht',\n",
       " '0 believe loveyourself newday havetolove goodvibes',\n",
       " '0 audusd low key local calendar coming days lend suppo westpac blog silver gold forex',\n",
       " '0 long cant wait',\n",
       " '0 newyearnewyou bull hill climb reach target complete task survive strong',\n",
       " '0 staed day brief moment yoga breath peace',\n",
       " '0 truthful iam positive affirmation',\n",
       " '0 true lnp cut seniors pension home july 1st 2017',\n",
       " '0 need book mine soon',\n",
       " '0 playing ernie makes airedale bei',\n",
       " '0 neco really cuz niggas know writing still amma keep praying god almight',\n",
       " '0 daily affirmation committed goals success assured abundance prosperity beauty belief mindset selflove',\n",
       " '0 know today going shitty yolo umbrella weather rain badday ohmygod heaven god',\n",
       " '0 enjoy view great nature sea ocean beach plants view tagsforlikes instalike',\n",
       " '0 excited new album taster happiness',\n",
       " '0 thinkers intellectual refugees without home heah thoughts seek asylum minds men rarely given beh',\n",
       " '0 blessed bihday nataliepomanwishing many healthysuccessful yrs classicbeauty',\n",
       " '0 fridayfeeling winery today full bmw conveible valdobbiadene prosecco',\n",
       " '0 creative password ive change many times due nice people',\n",
       " '0 rdailysketch grumpy cat 11th june tardarsauce grumpycat grumpy cat tardar',\n",
       " '0 sneak peek next issue sale tomorrow worldoceansday',\n",
       " '0 happy work conference right mindset leads cultureofdevelopment organizations work mindset',\n",
       " '0 pass way would like friends say said yolo lived rip picoftheday',\n",
       " '0 hope ok think outcome doubtful unbelievable',\n",
       " '0 make money expectedgoodnight server',\n",
       " '0 hey everyone mensfashion menstyle boy boys boyfriend boygay fun gayarab',\n",
       " '0 thankful weekends thankful positive',\n",
       " '0 yay fantastic edinburgh',\n",
       " '0 teamhawthorn ready live link doughdiscoday',\n",
       " '0 least honest thats wild ass shit',\n",
       " '0 us wnyers hero early days truly could wrong team lifted entire region ego',\n",
       " '0 maybe stop talking nations leaders happens house daily',\n",
       " '0 sheer waste two hours watching much noise crap udthapunjab',\n",
       " '0 check new trending funny gif saturday night live yay high five cecily strong bobby moyni',\n",
       " '0 make decision inside magnetize life happine',\n",
       " '0 saw last video know still clarababylegs didnt call make feel better dep',\n",
       " '0 doubt mood attitude completely changes simple workout noattitude ilift runnergirl',\n",
       " '0 special thanks thanks everything staff amazing',\n",
       " '0 travel inshot girls cute summer blur sun fun dog hair beach hot cool',\n",
       " '0 life pushes push back come stronger came peace love',\n",
       " '0 wednesdaywisdomenjoychocolatebeobreakthrough cancerfreebodybeauty',\n",
       " '0 fathersday fathers day buy things happy fathers day',\n",
       " '0 wait minute nobody told im supposed hate sex catholic wife fake hating fakeittilyamakeit',\n",
       " '0 thank got email',\n",
       " '0 time japa amo valentinesday japa araraquara nigth',\n",
       " '0 emailed hours ago reply',\n",
       " '0 lots making happening makerspace week even bigger plans works makeredau',\n",
       " '0 ubie3 ive waited day',\n",
       " '0 well hillary way would bernie sanders burning hell murdered sandy hook kids nevermind nragop',\n",
       " '0 cuandojuegachilemepongo saaaaaaaaa shhhhhhhe iiiiiii vamoschile vamoschilecarajo copaamerica',\n",
       " '0 thanks adding list appreciation followed internetmarketing nbafinals euro2016',\n",
       " '0 enjoy life alex alex day business relax lifestyle feliz felicidad',\n",
       " '0 love',\n",
       " '0 appletstag compleanno toa cake family bday selfie smile girl summer',\n",
       " '0 youre nearly sweethea inspiration nevergiveup youwilldothis comeonerin youramazing loveandhugs',\n",
       " '0 trump coming guns liberal nominee sta muslims end rednecks',\n",
       " '0 girlbestfriends missyou',\n",
       " '0 dubai mall dubai uae',\n",
       " '0 didnt win gift card yall',\n",
       " '0 would love collaborate send us pm fb sta discussion',\n",
       " '0 dont count days make days count romeo westie dog quoteoftheday ht',\n",
       " '0 broads think like low self esteem willing settle anything smallminded',\n",
       " '0 lil bro playing ball year im swagged',\n",
       " '0 yeahhhh come back soon empty nomorewhiskey metalheads',\n",
       " '0 healthy cant get much done life work days feel good jerry west',\n",
       " '0 mood day peace love happiness love life hairstyle tiger',\n",
       " '0 havent seen fringe catalogue yet top list',\n",
       " '0 literally funny laugh smile life rap battle',\n",
       " '0 finally reached 200 followers twitch follower hypu stream tomorrow',\n",
       " '0 currently first stages fully redecorating studio keep eye planned htt',\n",
       " '0 impoant',\n",
       " '0 new shoes training underarmour spaanrace gymlife',\n",
       " '0 place',\n",
       " '0 coming arrowhead elementary 20162017 school year assistantprincipal olentangy',\n",
       " '0 yes thanks trump pointing poll shows losing trumpfail',\n",
       " '0 hey see august somethingtolookforwardto',\n",
       " '0 face pretty much sums mood past couple weeks',\n",
       " '0 thankful pa dance love',\n",
       " '0 miles reach oxford uk annual conference',\n",
       " '1 rushlimbaugh incredibly obama impression via',\n",
       " '0 opening unconditional love lets connect',\n",
       " '0 factsguide heabreaking photo motivation',\n",
       " '0 orange new black comes back days wowwwwww',\n",
       " '0 remember lost empire dreams success goals aim world believeinyou neverstop',\n",
       " '0 cant freaking wait showcase abella cum dangerassfan',\n",
       " '0 well jel ive night town long like years',\n",
       " '0 shopping dress business appointment tomorrow berlin next week',\n",
       " '1 victim islamophobia adam saleh refers black people abeed slaves one sketches',\n",
       " '0 happy bihday mother bihday',\n",
       " '0 different eye contact two guys care another one whos',\n",
       " '0 happy hump day friday horizon ladies humpdaymotivation shopthemint',\n",
       " '0 beach time vallaa beach mxico times',\n",
       " '0 joining us year freedomconference16 register',\n",
       " '0 good night life love latepost goodnight tanjung lesung beach club',\n",
       " '0 cant beat classic osaka colours',\n",
       " '0 wow really appears people trying sell free tickets muhammadali funeral',\n",
       " '0 sukhbir badal announces officebearers eastcoast zone canada',\n",
       " '0 como se fosse um desenho drawing dragqueen female iammyself alone',\n",
       " '0 less weeks away july open spin flowas hulahoops poi fans dragonstaff',\n",
       " '0 hey exploreraw may want check latest updates glastofest',\n",
       " '0 good morning monday',\n",
       " '0 friday tgif skate skateboarding friday fun sun summer hot extreme',\n",
       " '0 fathersday looking something see whats trending community',\n",
       " '0 unmoved doaba bigs pargat sarwanphillaur satinderkariha sulk',\n",
       " '0 would think people would care poor children starvingbut gen dont give shit',\n",
       " '0 fotokuapp funnehanever goodmorning langwalangending wheres nike bag',\n",
       " '0 blackout buffalo simulation buffalo take vicinity homes way',\n",
       " '0 tomhiddleston day',\n",
       " '0 still cant work oven properly hudson home guide',\n",
       " '0 sadly uns commitment pakistan invisible succumbed powerful become fp tool',\n",
       " '0 soon three baby family expecting fathersday life blessedandgrateful',\n",
       " '0 fiancs brother girlfriend moving us hopefully goes well',\n",
       " '0 natureperfection bull hill climb reach target complete task survive str',\n",
       " '0 electronic music bogota colombia puntohost cedm edm dj fashion music',\n",
       " '0 never ever disappoints today crystal hero made face antiherorocks guess dont creamy beer',\n",
       " '0 smile weekend sun beautiful dance leap play summer love remberinglastweek',\n",
       " '0 friday weekendfullofsurprises',\n",
       " '0 concelebrate albanpilgrimage first time',\n",
       " '0 think draw next time saturday',\n",
       " '0 schools summer repost tbt school schoolsout summer summeime',\n",
       " '0 waiting rodthegod rodstewahits cardiff',\n",
       " '0 fewer cookies row rows getting smaller half cookies mampms',\n",
       " '0 finals new gambino life amazing weekend',\n",
       " '0 dont matter gun retweet australia indiedev gamedev indiegogo reddit kickstaer gamedev132',\n",
       " '0 beach day babylovingmyneicemoreandmoreeachdayguudlifeblessedgrateful family',\n",
       " '0 new script new film september film actor actorslife joy work',\n",
       " '0 happy wife happy life',\n",
       " '0 college park md marriott absolutely useless please fix lose rewards member horrible service',\n",
       " '0 ole folks taking shipping early bihday gifts',\n",
       " '0 thank ramadhan2016 ramadhankareem hea love holy month masjid',\n",
       " '0 another day another mass shooting america happened tolerance respect opinions values',\n",
       " '0 cont day dl2016 downloadfestival metal smiling beard goodtimes blog awesome megadeth',\n",
       " '0 thank reddevils make us proud one belirl euro2016 bel',\n",
       " '0 top london exciting way spend last day big new adventure newjob',\n",
       " '0 sent camping trip card dad wnote camping trips sent card',\n",
       " '0 yes received acceptance letter masters back october goodtimes history',\n",
       " '0 contactcenters quality improve fun perks avi ootd result new best',\n",
       " '0 coffee break icons isolated white background',\n",
       " '0 freeshipping worldwide make offer 29 iron planch antique vintage heavy cast iron',\n",
       " '0 referees decision final usga make mockery us open years trot',\n",
       " '1 carl palladino unfit buffalo school board resign',\n",
       " '0 hope getting email signups date',\n",
       " '0 honest make anymore gaming videos pc obs acting like shit nomore obs gaming pc computergaming',\n",
       " '0 sun felizmiercoles thebest yet tocome madrid summer2016 madrid spain',\n",
       " '0 hoe already looking another nigga fuck',\n",
       " '0 healthy action foundational key success pablo picasso',\n",
       " '0 cant wait anymore day left well nt even tht 22 hrs 46 mins still cant control',\n",
       " '0 left house 613am autism testing drove 2amphalf hours happy autism autismawareness',\n",
       " '0 hardcore bands teenagesexvideo free download',\n",
       " '0 hear modernist mags way mods',\n",
       " '0 anyone else notice suppoers pageant girls years internalize female oppressive system',\n",
       " '0 wouldve thought going last thing posted change',\n",
       " '0 great news first parklives bike ride takes place sunday join us',\n",
       " '0 mothersday bull hill climb reach target complete task survive strong exc',\n",
       " '0 whatever prayed time breaking fast granted never refused ampblessedramzan',\n",
       " '0 people make minds abraham lincoln quote personaldevelopment',\n",
       " '0 prayers victims senseless act gunviolence last night orlando prayerchangesthings sundaymorning',\n",
       " '0 beachdaypoolcleavagelgbt funigdaily',\n",
       " '0 syrian girl thought photojournalist holding weapon surrendered',\n",
       " '0 im introve person talk little like aloneso jisub day crjunnieuyendong htt',\n",
       " '0 cant wait graduate taxidermy degree',\n",
       " '0 happy fathersday everyone',\n",
       " '0 happy smile hair model style fashion lips lipstick eyesbrown brunette',\n",
       " '0 babies trend lab cocoa mint changing pad cover bouncingbaby',\n",
       " '0 waiting rodthegod cardiff',\n",
       " '0 waiting come',\n",
       " '1 hope see none inauguration wait wont watching idiot anyway mrminority boycott',\n",
       " '0 sad news thoughts jocox family news parliament mp labourpay nottingham',\n",
       " '0 know love dew drops water sun golfcourse golf sunrise mount tabor',\n",
       " '0 happy work conference right mindset leads cultureofdevelopment organizations work mindset',\n",
       " '0 monday great way sta week mimosas brunch jetsocialite',\n",
       " '1 latest blicqer daily thanks maga',\n",
       " '0 pickin crows playing tn fitness spa june 15th 700 pm open public folkmusic',\n",
       " '0 mashaallah cleared 2nd year 7367 ramzan ki rehmat',\n",
       " '0 fun morning session cutie letthekids kids smile tule redslippers',\n",
       " '0 calm sta save insure invest future visit new site',\n",
       " '0 soloman aswad agreed record verse charity single songforafrica',\n",
       " '0 hello staed taking ur course contagious content',\n",
       " '0 sent cute text reads ignores im',\n",
       " '0 sad literally anyone wants buy plane ticket nyc happen id highly appreciate',\n",
       " '1 montana says racism whitefish montana nazis christmas hanukkah richardspencer andrewanglin',\n",
       " '0 setting account hello twitter myfirsttweet photooftheday amazing beautiful lol',\n",
       " '1 invited help700 actions peace povey sept 1625',\n",
       " '0 saturday breakfast vibes',\n",
       " '0 fathers dayits easy father equally hard real dad',\n",
       " '0 peppa pig birds finger family nursery rhymes lyrics via',\n",
       " '0 never chance vote presidential candidate excited cycle looks different',\n",
       " '0 morning idol love',\n",
       " '0 today blogger beauty makeuplover lipstick mac maccosmetics macchrischang',\n",
       " '0 secret sauce employees productivity leadership management',\n",
       " '0 heres another night laying wide awake cantsleep',\n",
       " '0 smile doesnt always mean person sometimes simply means strong enough face problems',\n",
       " '0 see world differently change thoughts change world smile',\n",
       " '0 next dining devika date announced week',\n",
       " '0 awesome something details soon',\n",
       " '0 yesterday ozen restaurant eatallyoucan',\n",
       " '0 works new hymn satb piano words james montgomery para psalm 72',\n",
       " '0 model love take time ur',\n",
       " '0 fixturereleaseday lfc',\n",
       " '0 first time studio work commuting manhattan depressing line long dig inn sweetgreen lunch',\n",
       " '0 happy fathers day fathers even deadbeat dads',\n",
       " '0 thank much oppounity cherish forever',\n",
       " '0 goodmorning neymar playing goodday felling wish good day',\n",
       " '0 parents toddler drowned neighbors pool charged murder neglect murder',\n",
       " '0 best ways keep employees leadership hr productivity',\n",
       " '0 super fun mess muddy',\n",
       " '0 keep coming til dont remember',\n",
       " '0 need another tattoo cant im per',\n",
       " '0 norman gorilla simulator need adapt environment need tear city material',\n",
       " '0 old mom bihday',\n",
       " '0 new video go give watch youtube haul springhaul newvideo',\n",
       " '0 versatile sew really awesome clients today thankful love hair',\n",
       " '0 bottles eh dug stas telling us already dead deep',\n",
       " '0 146631675197 update social analytics hustle html linux css',\n",
       " '0 omg tell even tho packed lunch tonight',\n",
       " '0 feel personally victimized fact arent coming acu tour ouch heabroken hu scratchemcats',\n",
       " '0 trump teleprompter low energy',\n",
       " '0 prayers orlando hatred world prayersfororlando orlando ness unite prayfororlando',\n",
       " '0 personalised name swirl brackets ro gbp 2800 get shop cool home fun',\n",
       " '0 congratulations niblles forever hamsters',\n",
       " '0 great day girls great weather end summer fun sun girls 2k16',\n",
       " '0 new season hunt stas tonightcant wait season3',\n",
       " '0 food hipstergirl loves food thisiswhyiwonteverbeskinny',\n",
       " '0 caridge couple woh rooting days',\n",
       " '0 mine middle middle school yesterday growingup movingon',\n",
       " '0 im michael hope great time enjoy life',\n",
       " '0 baby find website id sexy404 gt',\n",
       " '0 finally made',\n",
       " '1 absolutely disgusting',\n",
       " '0 need washington state gray healthy mental health depersion washingtonstate',\n",
       " '0 woori energizing tuesday listening rainbow whoo movingup rankings gtlt jisook woori hoho',\n",
       " '0 new vinyl radiohead moon shares pool',\n",
       " '0 absolutely disgusting need euro2016',\n",
       " '0 thank god godisgood blessed godsgrace fitfam fitness healthy athlete',\n",
       " '0 four tet isnt happy soundcloud reaching one million followers soundcloud four tet sai',\n",
       " '1 despise hatred racism causes many black people',\n",
       " '0 else usa many lives orlando',\n",
       " '0 aww yeah good bing bong bing bong',\n",
       " '0 beach bakery theres bike rack talked owner said shes considering bike rack rainierbeach',\n",
       " '0 hoping bullseye archery',\n",
       " '0 thursdaythoughts feel like kinda tired crazyworld beat',\n",
       " '0 ive listening selena kumbia kings way today',\n",
       " '1 preordered sounds really great book',\n",
       " '0 setting graduation tomorrow night stas 6pm exeter cricket club next uni',\n",
       " '0 hippie pretty lt3',\n",
       " '0 springfield bull dominate bull direct whatever want',\n",
       " '0 great sta weekend',\n",
       " '0 divine iam positive affirmation',\n",
       " '1 guess white women arent blame work propaganda alllivesmatter',\n",
       " '0 introduction happy days days',\n",
       " '0 sometimes takes smile smile via',\n",
       " '1 snatch wig james brown head get',\n",
       " '0 love whithout proof',\n",
       " '0 bihday beautiful kajal',\n",
       " '0 sunday day lovely daddys fetes fetesdesperes theday days',\n",
       " '0 fathersday fathers dad children husband wife',\n",
       " '0 swear saw driver sta didnt look like crew e3',\n",
       " '0 thursday much peaceandlove',\n",
       " '0 happy bihday little boy toad bihday 5years lovely dog pet',\n",
       " '0 amazing hard disadvantaged footballers settle team yet called wealthy pay bribes teams',\n",
       " '0 dont worry people change facebook profile pic status tweet empathetic hashtag cdn',\n",
       " '0 7th sahri ramadan stay blessed good morning nice',\n",
       " '0 smile saturday weekendvibes summer vacation smile lovelife friends livingthedream style',\n",
       " '0 thankful enemies thankful positive',\n",
       " '0 world love',\n",
       " '0 palmy aus travel palmbeach straya beautiful life rise intothewild palm',\n",
       " '0 remember hating bitches aint happy happy bitches aint hating ampblessed',\n",
       " '0 live life well teespring hoodies womenteeshis',\n",
       " '0 brother would imstamood instadaily roof bestpeople',\n",
       " '0 cat kitty sandy',\n",
       " '0 thank lol',\n",
       " '0 older people see pic nose remembers smells close impoant sites like',\n",
       " '1 black feel like stomping listen retweet tampa miami',\n",
       " '0 listening country songs long week like version',\n",
       " '0 us lost wonderful dads soon hard day almost fathersday',\n",
       " '0 sharing love others addicting thing world take love granted',\n",
       " '0 100 amazing health benefits cucumbers healthy altwaystoheal',\n",
       " '0 minutes broadways biggest night',\n",
       " '0 two sleeps sitgesgaypride boys',\n",
       " '0 poetry world info nice aqwal brkng news crkt update weather ltand much follow follow',\n",
       " '0 meet kwangsoo oppa yeahhh',\n",
       " '0 bihday rajkuamar sir great success ahead',\n",
       " '0 bihday rg',\n",
       " '0 foods healing body doplants healthy',\n",
       " '1 look latest tweet conman doesnt want unity bigot conman',\n",
       " '0 got tickets see week today happy bachelorette weekend',\n",
       " '0 twitter mentions american folk museum fun cloudy day summer summerinnyc',\n",
       " '0 trump clinton leading americans choose lesser evil namds si bernie sanders',\n",
       " '0 get get get enjoy music today free apps free music',\n",
       " '0 sleep firing magic carpet next stop agrabah',\n",
       " '0 work drives crazy look joythat comes dances inspiringwords mybeaconoflight',\n",
       " '0 nightmares fears truehow false time around feelingbad depressed trust fling affair',\n",
       " '1 bridge appeared denial indigenous first people racism ergo',\n",
       " '1 fo woh texas police chief officer acted rude racist viral video',\n",
       " '1 could systemicracism impossible real acts racism easier stand unfounately',\n",
       " '0 black spindle zen metor one night destiny goodjob',\n",
       " '0 48 hours go go glamping wi twins girlpower friends',\n",
       " '0 cant hide another vip clientcompletely different',\n",
       " '0 get request communalise much anger',\n",
       " '0 thankful family vacations thankful positive',\n",
       " '0 electronic music bogota colombia cedm edm dj fashion music',\n",
       " '0 noninvasive ventilation study really stas tomorrow first review meeting',\n",
       " '0 style bull hill climb reach target complete task survive strong exciteme',\n",
       " '0 mlearning gorilla simulator need adapt environment need tear city mater',\n",
       " '0 lose snap streak 140 days crying',\n",
       " '0 love instagood photooftheday toptags tbt cute beautiful followme follow',\n",
       " '0 happy fathers day love dad fathersday love dad',\n",
       " '0 word complain sunday',\n",
       " '0 hardly see mike mikeperez classreunion woodrowwilson',\n",
       " '0 wow iam positive affirmation',\n",
       " '0 secret sauce happy employees inc j2dw work',\n",
       " '0 weve staed set ground rules 1afilm film project',\n",
       " '0 little babies running home mommy fascist little babies',\n",
       " '0 youre reason made fav poboy hubby even menu today yegfood',\n",
       " '0 seems id good rock rap funny comedy pop hiphop',\n",
       " '0 good friends good food good life bestfriends bff pizza pizzahut',\n",
       " '1 presidents policy israeli settlements obama slammed treacherous antisemite',\n",
       " '0 happy happinesswins persevere positivity goodlife remember happiness',\n",
       " '0 best customer service received rezwan victoria station branch',\n",
       " '1 needs exit political journalism world exit stage left',\n",
       " '0 onerepublic wherever go official video via',\n",
       " '0 fathers day best forever louis baby beutiful freddie',\n",
       " '0 welcome new stylemile members shopping saavoyagermiles',\n",
       " '0 im gonna watch house cards',\n",
       " '0 model love take time ur',\n",
       " '0 woooohooo im going bml16 daunting buying ticket im super learn game meet everyone',\n",
       " '0 happiest iv long time feel weights lifted im dragged anymore newbegginings',\n",
       " '0 passed weekend aunt cathy yeagerlife fun auntcathy love missthem',\n",
       " '0 hate family makes fun lgbt wrong pick want man woman husband wife',\n",
       " '0 first time student first day school goodluck studentlife',\n",
       " '0 lunches bestie ger thai food foodporn love fat first thai food',\n",
       " '0 super duper lovely dance',\n",
       " '0 sentlel offence find self bored sunday afternoons used look forward day ay days',\n",
       " '0 rolls royce wedding weddingday weddings bride love weddinginspiration',\n",
       " '0 peoplefeelvoiceless disenfranchised injustice inequality division vacuum created filled oppounists notinmyname name',\n",
       " '0 rationchallenge fundraising helping educate syrianrefugees love faces thankfulthursday',\n",
       " '0 left miss already',\n",
       " '0 nobody friend unless car',\n",
       " '0 finally found way delete old tweets might find useful well deletetweets',\n",
       " '0 miss girlfriend much feeling little depressed today need friend',\n",
       " '0 fathers day twitter',\n",
       " '0 convince boyfriend',\n",
       " '0 glazed sun',\n",
       " '0 bang trend mobile charging latte charging customers luxury wed',\n",
       " '0 gameshow bull hill climb reach target complete task survive strong excit',\n",
       " '0 looking feel joy join 20 speakers free summit unleashyourjoy',\n",
       " '0 bloody marys fathers day bring dad tomorrow fathersdaydad weloveyou',\n",
       " '0 catch interview tomorrow siriusxm radio 3pm est',\n",
       " '0 looking forwards sneaky peek new extension tomorrow huddersfield',\n",
       " '0 finally suppos leagueoflegends',\n",
       " '0 clients bring presents chocolate galaxychocolate galaxy ms moo make',\n",
       " '0 silentsunday beachbeautifulphotooftheday loveocean',\n",
       " '0 like middle east destabilizes tribal war centuries',\n",
       " '0 john micahow hell people like elected totalpolitician outoftouch disgraceful',\n",
       " '0 playing naoyuki onda temple twilight music song',\n",
       " '0 beautiful lapel badges omalleyclan rally limerick june 2426 irishclan',\n",
       " '0 great one great one mrhockey',\n",
       " '0 fathers day shout fathers around world appreciate love',\n",
       " '0 step day best classof2017 seniors',\n",
       " '0 hello world saturdays breakfast lifestyle igers fashion spoy comfyclothes nike',\n",
       " '0 sotrue home family love money isnt everything blessed',\n",
       " '0 monday go kickass fabulous passion style simplicity xdcct gratitude',\n",
       " '1 trumps allies unstoppable obama via christmaseve',\n",
       " '0 whats yo locs growing months tho doesnt seem',\n",
       " '0 cannot wait go see finding dory findingdory cantwait',\n",
       " '0 meat love taare paris milan tokio fashionfood cuisine italy',\n",
       " '0 choose happiness pharellwilliams ellen lovelife expressyourself askforhelp',\n",
       " '0 aap congress hell bent vitiating communal harmony punjab bjp leaders',\n",
       " '0 crack corn 85x11 bic pen outcast ink sketchbook sketchy sketch fear scarred',\n",
       " '0 space travel save world people living via moon enjoy news',\n",
       " '0 still havent said shit town made orlando',\n",
       " '0 funnyvideos attack bull game 3d really think head empty around city side',\n",
       " '0 wonder dt still demanding rnc book bigger venue gop convention bigly',\n",
       " '0 first hard workout floridabased coach 20c 80 humidity 545am feeling pretty florida runchat sweaty',\n",
       " '0 think hea need bandage love',\n",
       " '0 breathing exercises help relax highly recommend practice',\n",
       " '0 goodmorning everyone feel free fly high sad alone lifeteacheseverything letthelegsgocrazy ht',\n",
       " '0 cant wait get back studio days away getting staed wecantwait theintroduction roxxymontana',\n",
       " '0 first order facebook page staup newbusiness swimwear facebook',\n",
       " '1 many white americans actually believe chance peace sexualpredator altright helm moronsmatter',\n",
       " '0 love city love life beautiful picoftheday lovelovelove us travel',\n",
       " '0 mean okay nofilter snapchat selfie safetyfirst buckleup cute gay gayboy',\n",
       " '0 nice iam positive affirmation',\n",
       " '0 half half available sorry whole milk dumps coffee front us seriously',\n",
       " '0 healthy fear affliction housed imagination may overcome intellect noelcastanza ff twt',\n",
       " '0 rest peace name sakestephen keshi',\n",
       " '0 nervous leaving country month im scared ill get homesick',\n",
       " '0 somehow someway always end third wheeling',\n",
       " '0 things happy people positive grateful people',\n",
       " '0 google shutdown youtube feed',\n",
       " '0 time round bestie bihday',\n",
       " '0 boyfriend moving gtgtgtgt house',\n",
       " '0 shitty video driving lake mono unique limestone lake slowly getting drained like cali',\n",
       " '0 baby love love people instaboy boys',\n",
       " '1 sikh temple vandalised calgary wso condemns act',\n",
       " '0 either would choose relationships lovelife sextips liveahappysexylife xo',\n",
       " '0 everybody wasnt born',\n",
       " '0 look arrived today hoodies sydneycanberra only6weeksto go',\n",
       " '0 woooooaaaahhh delivery feels like christmas cant wait',\n",
       " '1 stunning response plan nominate muslimbrotherhood member lead pay',\n",
       " '1 feminismiscancer feminismisterrorism feminismmuktbharat malevote ignored',\n",
       " '0 today day make change eliminate negativity choose live positive lifestyle instead',\n",
       " '0 beach day bae cocoabeach summer florida life love vibes',\n",
       " '0 life kids sons daughter goodboy goodgirl little littlebrother love family',\n",
       " '0 enjoyed sunny weather churchill cari hang often cuties yeg',\n",
       " '0 glass wine aladdin tv roast lamb dinner cooking better half perfectsunday',\n",
       " '0 live world people complain post things justify laziness instead getting something change',\n",
       " '0 know pretty clear really cant help cant control mouth',\n",
       " '0 like huh love breakup single respect life peaceout missyou',\n",
       " '0 newstar bull dominate bull direct whatever want',\n",
       " '0 sta bizmu',\n",
       " '0 love big dick',\n",
       " '0 happy bday twins bday bestfriend wuatb lovlyfriend woman love life',\n",
       " '1 birmingham friends dont miss special lecture bonhoeffer racism bonhoeffer expe',\n",
       " '0 final two shows amazing lot thank fantastic year shattered',\n",
       " '0 dinner time healthy food mood knubisoft web mobile game development',\n",
       " '0 datenight heabeats sele smile miaminights amctheaters aventura',\n",
       " '0 todays festival outfit edm music love fashion dj',\n",
       " '1 book enduring conviction fred korematsu quest justice humanrights ht',\n",
       " '0 oooh looks amazing',\n",
       " '0 hanging minime daldife lifeofdad kidlife idad thebaddadsclub beardgang',\n",
       " '0 uberrush ondemand delivery network friday',\n",
       " '0 new batch bowt1 went oven soon',\n",
       " '0 python27 concurrency best friends code restructure get concurrency celery developers python',\n",
       " '0 things remarkably people often lifetips',\n",
       " '0 teen girl killed others injured downtown oakland shooting guns mentalillness gangs orlando',\n",
       " '0 homeboy got nothing new level trolling',\n",
       " '0 happy happiness selfrespect respect life lifehacks love diy blog blogger',\n",
       " '0 lip gloss tho photooftheday selfie shinylips',\n",
       " '0 love prawns barbie bbq sunshine pougal albufeira family gay',\n",
       " '0 sure miss old amarillo',\n",
       " '0 samsunggalaxys2 rooster simulation want climb vast expanse mountains reached leakage',\n",
       " '0 huge fan im excited live stream qampa midday via gilbeosullivan',\n",
       " '0 pontoon tuesday turnup yg',\n",
       " '0 busy busy twice back yard best summer venues nyma region tanglewood soon spac aug',\n",
       " '0 waiting beach threeoaks summer sunnyday vitmind sand beautiful',\n",
       " '0 dazzling iam positive affirmation',\n",
       " '0 get posts seen make others happy come join nowlinkup bloggers via',\n",
       " '0 life sho time live exists makes wasting time',\n",
       " '0 route london see coldplay london coldplaywembley',\n",
       " '0 love instagood photooftheday toptags tbt cute beautiful followme follow',\n",
       " '0 let chill',\n",
       " '1 people spit face god',\n",
       " '0 lighter attack bull chase leave lot despite fact youre strong source foo',\n",
       " '0 day tomhiddleston',\n",
       " '0 windows10 split screen browser customcasetab noedit 712',\n",
       " '0 fathers day real caring fathers god bless fathers bless cheers',\n",
       " '0 fathers day fathers dads biological step adopted foster father much love',\n",
       " '0 babe sunday eva love life baby dog instagood animal animals',\n",
       " '0 humpday enjoy newest act simply titled stripper filmed yesterday apply',\n",
       " '0 solidarity orlando solidaritywithorlando praying sendinglove orlando news lgbt',\n",
       " '0 suppo advocate miscegenation genocide',\n",
       " '0 msrlm24 soon sta cry proudfan',\n",
       " '0 seasons want moreeee like 5000 tatianaismybae whatsupwithcophine',\n",
       " '0 gives reason god grace love god fathers day three loving',\n",
       " '0 lovemeinstagoodfollowphotooftheday tagsforlikebeautifulselfieg',\n",
       " '0 ill decide love life friends power',\n",
       " '0 feel sick learnt jo coxs passing hands violent man gun died helping people prayers family',\n",
       " '0 today celebrate lulybtips affirmations',\n",
       " '0 officially teammagenta relieved',\n",
       " '0 check recording youre lonely singsnap karaokelivemusicsad',\n",
       " '0 luke father dahvader luke starwars watch selfie like l4l film laserena',\n",
       " '0 yahoooo finally one plus get marshmallow update',\n",
       " '0 fuji mountfuji japan japan travel travelphotography traveltheworld life mount',\n",
       " '0 almost friday euro2016',\n",
       " '0 thankful sense sight thankful positive',\n",
       " '0 exhausted staying lemans24 adrenaline really pumping ahead finish europeangp f1 lm24',\n",
       " '0 happy doesnt mean forever theres sad sto doesnt mean forever cause sad story happy ending',\n",
       " '0 tiny crown floral photography image imagine aforall',\n",
       " '0 took last antibiotic woo mightbeallergictoants',\n",
       " '0 outside new flat newflat bournemouth flat couple boyfriend topfloor',\n",
       " '0 todays garden love',\n",
       " '0 aww yeah good bing bong bing bong',\n",
       " '0 things ive working week see game come together',\n",
       " '0 people know friends friends never never anything',\n",
       " '0 times running last tickets still available',\n",
       " '0 heading mass today boss head trade show',\n",
       " '0 picking bean 20 minutes',\n",
       " '0 damn orlando',\n",
       " '0 place',\n",
       " '0 love instagood photooftheday toptags tbt cute beautiful followme follow',\n",
       " '0 dang find wifi dowload',\n",
       " '0 woh watching anymore makes wonder many people would attend cheer gospel awards prime time',\n",
       " '0 screening miss sing songs potp shock treatment',\n",
       " '0 en directo en periscope morning gym train lifestyle girl bestpick',\n",
       " '0 amazon rc nitro gas truck hsp 110 car 4wd via toyspotting hsp trucks 4wd play',\n",
       " '0 weekend sta weekendmotivation fridayfeeling tgif',\n",
       " '0 really lmaoo obviously related haha wonder came stupid sometimes though',\n",
       " '0 story escaped prison perfection featured lifecoach',\n",
       " '0 oh natalie poman classic beauty bihday',\n",
       " '0 filed repo one find since friday',\n",
       " '0 guys mv guys jacobisgod',\n",
       " '0 came across today store knac feelinglikeakidagain',\n",
       " '0 dont think ill ever close',\n",
       " '0 honestyhour always break back see ppl straight never vise versa',\n",
       " '0 hey hey lookies found website',\n",
       " '0 life complicated simple happy quote quoteoftheday motivation',\n",
       " '0 shes central plot thoughhhhh',\n",
       " '0 fathersdaymessage father day message buy things happy fathers da',\n",
       " '0 one closest friends moving away',\n",
       " '0 booked tickets march',\n",
       " '0 music time baby baby happiness 29weeks mommyduties family katana pampering',\n",
       " '0 biggest problem actually works',\n",
       " '1 hate crimes watch arabs never seen much hate poor like',\n",
       " '0 spent familytime simple wefie latepost melawanarus untuk',\n",
       " '0 boom packed ready im gonna try sleep',\n",
       " '1 giant version flag rebellion sedition flapping virginia i95n',\n",
       " '0 wedding day marieampmarco ladesirade guadeloupe temoin',\n",
       " '0 another sneak peak weeks little project handcramp caligraphy fineliners',\n",
       " '0 va boston jersey texass summer looking great',\n",
       " '0 thank god another daydailyappreciation wednesday everyone',\n",
       " '0 cant bring read orlandonightclubshooting went offline news breaking wake worst massacre ever',\n",
       " '0 good day yall shoreline mensfashion essentials love',\n",
       " '0 akalis bjp distance udta punjab row vineetjoshi india punjab akalidalbjp',\n",
       " '0 thought youd like pin pinterest wish understands smile',\n",
       " '0 know find incredibly disrespectful taking personal call meeting frustrated',\n",
       " '0 mood rainy days wheres sun dublin ireland sun',\n",
       " '0 people get time work go euros havent gone need long hard look selfs',\n",
       " '0 woot weeks hawaii hawaii2016',\n",
       " '0 said god give whats opposite person hahaha sorry others mine',\n",
       " '0 weeks assembly principal launches end year rewards trip students',\n",
       " '0 terriblethis reminds much selenaa beautiful talented rising star like christina murdered scum',\n",
       " '0 jwu streetdance workshop later classes',\n",
       " '0 herbalremedies anxiety work altwaystoheal healthy healing peace',\n",
       " '0 tragic time4change timeforasaferamericaforall prayfororlandoamppulsevictims prayersforchristinagrimmiefamily',\n",
       " '0 day late dollar sho post blog alliesmommy love follow sho loss life',\n",
       " '0 cameron opinion done nothing talk nation patronise people di',\n",
       " '0 chick posted hot pic lounge chair captionreason cooking grill happy fathers day',\n",
       " '0 im surprised didnt gun usually carry guns every else man',\n",
       " '0 djdemoivegotafeelingng004web1997ukhxint web hardcore 1gabba vk',\n",
       " '0 gaz coombes looking forward tonight rsc bihday',\n",
       " '0 stop robbing innocent customers hard earned aiime guys idea dey managed recharge',\n",
       " '0 hanging favorite humans family sidesandwich friday',\n",
       " '0 carmineryderrr na xxx horny young sexy porn snapshot kinky wet hot slut naughty nude',\n",
       " '0 naturaliam positive affirmation',\n",
       " '0 little really week trump made',\n",
       " '0 morning world together',\n",
       " '0 im scared uuu',\n",
       " '0 pops still happy fathers day fathersday father dad amsniper',\n",
       " '0 sadly case week opinion 2016 mentality outrage',\n",
       " '0 dearest almighty plz eradicate savagery entire worldvery soon collegeofspirituality',\n",
       " '0 tb felt like sharing',\n",
       " '0 arrived training programmeleader scared summer thechallenge',\n",
       " '0 orlando love need world today',\n",
       " '1 bama total hypocrites cheer like hell remain silent confederateflag stance',\n",
       " '0 coldplay tomorrow',\n",
       " '0 sunday hugs',\n",
       " '0 ootdwiwtmetodayfwisfashion swagfringedayoffgoodday love japantokyobeautiful',\n",
       " '0 dining lunchbytheocean moonlight beach',\n",
       " '0 rip coach amodu shuaibu exsuper eagles coach nff technical directorjust days keshis death miss amodu',\n",
       " '0 first day wee walk wake waiting rest counsellors land americamp sh',\n",
       " '1 diisgusted perks offered passengers race male crew members flt 43 vs others arnt',\n",
       " '0 cant wait',\n",
       " '0 kad raya still relevant regardless advanced technology kan tried find musical one takda dan tak banyak piliha',\n",
       " '0 sesalefueeadele pa hello adele longboard love instagood felizdomingo',\n",
       " '0 less year kids lost parents wow may god give strength go',\n",
       " '0 fathersday yes daughters',\n",
       " '0 youre excited sprint cars saturday coworkers work celebrate',\n",
       " '0 looks like le monsieur bob approves new shag rug kitty love likes',\n",
       " '0 saturdayzorro siberian husky abducted suv morning walk newdelhi shame letlive',\n",
       " '0 busty sexy women naked hardcore anthems',\n",
       " '0 im everyone says e3 cringe cuz ur cool irl amirite twitch e3ontwitch e32016 gaming rant',\n",
       " '0 liveme veryangry veryveryangrythe russians onairnow',\n",
       " '0 20160612 disneyland disney love friends movie l4l f4f',\n",
       " '0 please stop violence talk listen respect empathy love orlando',\n",
       " '0 oitnb decided poussey face',\n",
       " '0 todayon saturday decided write accept invitation explore town iambeautiful inlove',\n",
       " '1 cpcldr elxn42 enabled mps make statements great altright work cpc cdnpoli',\n",
       " '0 spent big day bigday weekend ootd blog blogger elegant love saturday fashionblog',\n",
       " '0 categories professionals dread country police doctors killers ripoff godfidence',\n",
       " '0 heres another one andrew fridaynight couple boohoonight boyfriend',\n",
       " '0 trinationodi see empty stadiums westindies',\n",
       " '0 christmas2015 gorilla simulator need adapt environment need tear city',\n",
       " '0 love roses special red ones',\n",
       " '0 raining offer pantaloons shop',\n",
       " '0 ps mobiletrixs allpikegs news cricket infofuny follow p0etry girlno friendship follo',\n",
       " '0 brunomars music music videos make happyhappy',\n",
       " '0 going udtapunjab today hrs frm overwhelmed shanaticforever',\n",
       " '0 playing john burr asking melancholy melancholymusic',\n",
       " '0 add close peepz staycloseofamily lt3 sunday gmikedrezzy',\n",
       " '0 driving selfie snapchat friends besties chillin funtimes fun music beard',\n",
       " '0 thankful time thankful positive',\n",
       " '0 digital world live',\n",
       " '0 ive low crying eyes thats funny read ive complete oppositetotally everything laughing',\n",
       " '0 waiting pm narendra modi ji come allahabad 13 june',\n",
       " '0 thankful holiday sales thankful positive',\n",
       " '0 shes real life esco prostitute dont gaf',\n",
       " '0 makes keep selflove',\n",
       " '0 nude rear naughty naked school girls',\n",
       " '0 contdi thought fab showi hope career continues success would love follow',\n",
       " '0 creative sent thebigscreen weekend cheers',\n",
       " '0 make great day',\n",
       " '0 healthy sta saturday morning glutenfree protein shake fitness training energy juiceplus',\n",
       " '0 adam eve instagood tbt cute follow followme photooftheday tagsforlikes',\n",
       " '0 happy bihday week us cheersbihdayweek friends',\n",
       " '0 pulseshooting agree makes sense lack gun control policy us thats real terror',\n",
       " '0 always find time laugh autofollow followback retweet share follow4follow photo girl cute',\n",
       " '0 spending fathersday special little one ella daddysgirl',\n",
       " '0 loved tonights interview two great guys easy fun need bad news weekend',\n",
       " '0 friday fitfam ready ridiculous foodporn last night',\n",
       " '0 moreno bull dominate bull direct whatever want',\n",
       " '0 thanks shall big shopping spree whattobuy shopping mummybloggers',\n",
       " '0 world stinks death broadcasted truth christinagrimmie muhammadali orlandoshooting',\n",
       " '0 watching snaptraveler stories makes hope fun barcelona',\n",
       " '0 happy pincher dog',\n",
       " '0 nailsdone idgt',\n",
       " '0 look fabulous love htl fashion',\n",
       " '0 real friends smiles appear smile joke',\n",
       " '0 eng wal sticking together diy rus time get back',\n",
       " '0 number likes 50 number retweets number new follows whytwitterisdying makenewfriends reachout',\n",
       " '0 boom toureiffel three day paris pretty dead sporadic selfie',\n",
       " '0 massive thank followers love suppo thankyou followers twitter',\n",
       " '0 reply tweet droughtisrealreel',\n",
       " '0 hometown',\n",
       " '0 50 years thing cleveland purpose doesnt even',\n",
       " '0 name song inshot girls cute summer blur sun fun dog hair beach hot',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_na_train_df = train_df[train_df['clean_tweet'].notna()]\n",
    "data_col = not_na_train_df['label'].astype('str') + \" \" + not_na_train_df['clean_tweet']\n",
    "data_col = list(data_col)\n",
    "data_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Note: much of this code taken from code used Winter 2022 NLP course with Chenhao Tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/Twitter/hate_twitter/')  # Modify the path of `data_dir` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of line 0: omg omg omg yay found wonderful price segasaturn throwback\n",
      "Tokens of line 0: ['omg', 'omg', 'omg', 'yay', 'found', 'wonderful', 'price', 'segasaturn', 'throwback']\n",
      "String of line 1: payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place\n",
      "Tokens of line 1: ['payintheusa', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'place']\n",
      "String of line 2: trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo\n",
      "Tokens of line 2: ['trainhard', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'places', 'lo']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "\n",
    "# create unigram vocab\n",
    "tweet_lens  = []\n",
    "for i, line in enumerate(data_train):\n",
    "    tokens = tokenizer.tokenize(line.strip())\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tweet_lens.append(len(tokens))\n",
    "    counter.update(tokens)\n",
    "    if i < 3:\n",
    "        print(f\"String of line {i}: {line.strip()}\")\n",
    "        print(f\"Tokens of line {i}: {tokens}\")\n",
    "counter = dict(counter)\n",
    "\n",
    "vocab = {}\n",
    "# Populate the vocabulary with words that appear at least 3 times.\n",
    "for word, freq in counter.items():\n",
    "    if freq < 3 and word not in ['<pad>', '<unk>']:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "output_filepath = data_dir.joinpath('unigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify max tweet length\n",
    "np.max(tweet_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of line 0: omg omg omg yay found wonderful price segasaturn throwback\n",
      "Tokens of line 0: ['omg omg', 'omg omg', 'omg yay', 'yay found', 'found wonderful', 'wonderful price', 'price <unk>', '<unk> throwback']\n",
      "String of line 1: payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place\n",
      "Tokens of line 1: ['payintheusa polar', 'polar bear', 'bear climb', 'climb racing', 'racing angry', 'angry polar', 'polar bear', 'bear climb', 'climb racing', 'racing polar', 'polar bear', 'bear living', 'living cold', 'cold place']\n",
      "String of line 2: trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo\n",
      "Tokens of line 2: ['trainhard polar', 'polar bear', 'bear climb', 'climb racing', 'racing angry', 'angry polar', 'polar bear', 'bear climb', 'climb racing', 'racing polar', 'polar bear', 'bear living', 'living cold', 'cold places', 'places lo']\n",
      "Vocab size before frequency filtering: 82684\n",
      "Vocab size after frequency filtering: 7933\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# create bigram vocab\n",
    "for i, line in enumerate(data_train):\n",
    "    tokens = tokenizer.tokenize(line.strip())\n",
    "    # tokens = line.split()\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "    bigram_lst = [tokens[i] + \" \" + tokens[i + 1] for i in range(len(tokens) - 1)]\n",
    "    counter.update([tokens[i] + \" \" + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "    if i < 3:\n",
    "        print(f\"String of line {i}: {line.strip()}\")\n",
    "        print(f\"Tokens of line {i}: {bigram_lst}\")\n",
    "counter = dict(counter)\n",
    "print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "vocab = {}\n",
    "for word, freq in list(counter.items()):\n",
    "    if freq < 3:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = data_dir.joinpath('bigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab, data_dir, feature_field, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Extract and save the vocab and features.\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    splits = ['train','test','val']\n",
    "\n",
    "    gram, mode = feature_name.split('_')\n",
    "    if gram not in ['unigram', 'bigram'] or mode not in ['binary', 'count']:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for split in splits:\n",
    "        datapath = data_dir.joinpath(f'hate_{split}.csv')\n",
    "        print('datapath',datapath)\n",
    "        data_df = pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        features = list(data_df[feature_field])\n",
    "        \n",
    "        sent_lengths = []\n",
    "        values, rows, cols = [], [], []\n",
    "        labels = list(data_df['label'])\n",
    "        print(f\"\\nExtract {gram} {mode} features from {datapath}\")\n",
    "        for i, line in enumerate(features):\n",
    "            if i % 1000 == 1:\n",
    "                print(f\"Processing {i}/{len(features)} row\")\n",
    "            #label = int(line[0])\n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "            tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "            if gram.find('bigram') != -1:\n",
    "                tokens.extend(\n",
    "                    [tokens[i] + ' ' + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "            feature = {}\n",
    "            for tk in tokens:\n",
    "                if tk not in vocab:\n",
    "                    continue\n",
    "                elif mode == 'binary':\n",
    "                    feature[vocab[tk]] = 1\n",
    "                elif mode == 'count':\n",
    "                    feature[vocab[tk]] = feature.get(vocab[tk], 0) + 1\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            for j in feature:\n",
    "                values.append(feature[j])\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            sent_lengths.append(len(tokens))\n",
    "\n",
    "        features = sparse.csr_matrix((values, (rows, cols)),\n",
    "                                     shape=(len(features), len(vocab)))\n",
    "        print(f\"{split} feature matrix shape: {features.shape}\")\n",
    "        output_feature_filepath = data_dir.joinpath(f'{split}_{gram}_{mode}_features.npz')\n",
    "        sparse.save_npz(output_feature_filepath, features)\n",
    "\n",
    "        np_labels = np.asarray(labels)\n",
    "        print(f\"{split} label array shape: {np_labels.shape}\")\n",
    "        output_label_filepath = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        np.savez(output_label_filepath, np_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7562)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7562)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7562)\n",
      "val label array shape: (4790,)\n",
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7562)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7562)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7562)\n",
      "val label array shape: (4790,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7933)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7933)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7933)\n",
      "val label array shape: (4790,)\n",
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7933)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7933)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7933)\n",
      "val label array shape: (4790,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/bigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_count')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str,\n",
    "                                     tune: bool = False) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    tune : `bool`, optional\n",
    "        Whether or not to tune the hyperparameters of the regularization strength\n",
    "        of the model of the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'val','test']\n",
    "    features, labels = {}, {}\n",
    "\n",
    "    for split in splits:\n",
    "        features_path = data_dir.joinpath(f'{split}_{feature_name}_features.npz')\n",
    "        labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        features[split] = sparse.load_npz(features_path)\n",
    "        labels[split] = np.load(labels_path)['arr_0']\n",
    "    best_dev, best_model = 0, None\n",
    "    if tune:\n",
    "        for c in np.linspace(-5, 5, 11):\n",
    "            clf = LogisticRegression(random_state=42,\n",
    "                                     max_iter=100,\n",
    "                                     fit_intercept=False,\n",
    "                                     C=np.exp2(c))\n",
    "            clf.fit(features['train'], labels['train'])\n",
    "            dev_preds = clf.predict(features['val'])\n",
    "            dev_accuracy = accuracy_score(labels['val'], dev_preds)\n",
    "            print(c, dev_accuracy)\n",
    "            if dev_accuracy > best_dev:\n",
    "                best_dev, best_model = dev_accuracy, clf\n",
    "    else:\n",
    "        best_model = LogisticRegression(random_state=42,\n",
    "                                        max_iter=100,\n",
    "                                        fit_intercept=False)\n",
    "        best_model.fit(features['train'], labels['train'])\n",
    "\n",
    "    preds = {\n",
    "        'val': best_model.predict(features['val']),\n",
    "        'test': best_model.predict(features['test'])\n",
    "    }\n",
    "    for splt, splt_preds in preds.items():\n",
    "        print(\"{} accuracy: {:.4f}\".format(splt, accuracy_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} binary recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} macro recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds, \n",
    "                                                                average=\"macro\"))),\n",
    "        print(\"{} macro f1: {:.4f}\".format(\n",
    "            splt, f1_score(labels[splt], splt_preds, average='macro')))\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm bigram binary different from bigram count\n",
    "split = 'val'\n",
    "feature_name='bigram_binary'\n",
    "features = {}\n",
    "features_path = Path(data_dir).joinpath(f'{split}_{feature_name}_features.npz')\n",
    "features[split] = sparse.load_npz(features_path)\n",
    "features[split].toarray()[0:5,18:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 4787, 4788, 4789]),\n",
       " array([18, 18, 18, ..., 18, 18, 18]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices where nonzero\n",
    "np.nonzero(features[split].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [7],\n",
       "       [5],\n",
       "       [1],\n",
       "       [6]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm bigram binary different from bigram count\n",
    "split = 'val'\n",
    "feature_name='bigram_count'\n",
    "features = {}\n",
    "features_path = Path(data_dir).joinpath(f'{split}_{feature_name}_features.npz')\n",
    "#labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "features[split] = sparse.load_npz(features_path)\n",
    "features[split].toarray()[0:5,18:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9559\n",
      "val binary recall: 0.5145\n",
      "val macro recall: 0.7523\n",
      "val macro f1: 0.8016\n",
      "test accuracy: 0.9570\n",
      "test binary recall: 0.5296\n",
      "test macro recall: 0.7586\n",
      "test macro f1: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9547\n",
      "val binary recall: 0.5116\n",
      "val macro recall: 0.7503\n",
      "val macro f1: 0.7973\n",
      "test accuracy: 0.9558\n",
      "test binary recall: 0.5265\n",
      "test macro recall: 0.7565\n",
      "test macro f1: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9282\n",
      "val binary recall: 0.0000\n",
      "val macro recall: 0.5000\n",
      "val macro f1: 0.4814\n",
      "test accuracy: 0.9330\n",
      "test binary recall: 0.0000\n",
      "test macro recall: 0.5000\n",
      "test macro f1: 0.4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9282\n",
      "val binary recall: 0.0000\n",
      "val macro recall: 0.5000\n",
      "val macro f1: 0.4814\n",
      "test accuracy: 0.9330\n",
      "test binary recall: 0.0000\n",
      "test macro recall: 0.5000\n",
      "test macro f1: 0.4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show important weights\n",
    "\n",
    "Code heavily utilizes code from Chenhao Tan's Winter 2022 NLP Course at the University of Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_important_weights(weights, words):\n",
    "    \"\"\"\n",
    "    Print important pairs of weights and words.\n",
    "    # Parameters\n",
    "    weights : `Iterable`, required.\n",
    "        Weights from a learned model.\n",
    "    words : `Iterable`, required.\n",
    "        Word types of the vocabulary.  \n",
    "        It must be true that `len(weights) == len(words)`.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "\n",
    "    def print_pairs(pairs):\n",
    "        for weight, word in pairs:\n",
    "            print(\"{: .4f} | {}\".format(weight, word))\n",
    "\n",
    "    assert len(weights) == len(words)\n",
    "    pairs = list(zip(weights, words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
    "    print(\"Most hateful words:\")\n",
    "    print_pairs(pairs[:10])\n",
    "    print(\"\\nLeast hateful words:\")\n",
    "    print_pairs(reversed(pairs[-10:]))\n",
    "\n",
    "    pairs = list(zip(abs(weights), words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=False)\n",
    "    print(\"\\nMost neutral words:\")\n",
    "    print_pairs(pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9559\n",
      "val binary recall: 0.5145\n",
      "val macro recall: 0.7523\n",
      "val macro f1: 0.8016\n",
      "test accuracy: 0.9570\n",
      "test binary recall: 0.5296\n",
      "test macro recall: 0.7586\n",
      "test macro f1: 0.8000\n",
      "\n",
      "Most hateful words:\n",
      " 4.2309 | allahsoil\n",
      " 3.1589 | racism\n",
      " 2.6945 | misogyny\n",
      " 2.6012 | bigot\n",
      " 2.5492 | 2017\n",
      " 2.5025 | white\n",
      " 2.2742 | latest\n",
      " 2.1515 | racist\n",
      " 2.1493 | treason\n",
      " 2.1267 | misogynist\n",
      "\n",
      "Least hateful words:\n",
      "-3.0948 | bihday\n",
      "-2.6052 | day\n",
      "-2.5491 | orlando\n",
      "-2.3904 | positive\n",
      "-2.3258 | healthy\n",
      "-2.2211 | friday\n",
      "-2.1652 | weekend\n",
      "-2.1620 | hardcore\n",
      "-2.1542 | thankful\n",
      "-2.1484 | days\n",
      "\n",
      "Most neutral words:\n",
      " 0.0000 | <pad>\n",
      " 0.0000 | remains\n",
      " 0.0001 | grandad\n",
      " 0.0001 | awasome\n",
      " 0.0001 | inwoo\n",
      " 0.0002 | selfrespect\n",
      " 0.0003 | titanic\n",
      " 0.0003 | lauren\n",
      " 0.0004 | ebay\n",
      " 0.0005 | systems\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "model_trained: LogisticRegression = fit_and_eval_logistic_regression(\n",
    "    feature_name='unigram_binary', data_dir=Path(data_dir), tune=False)\n",
    "weights = model_trained.coef_[0]\n",
    "vocab = json.load(open(vocab_filepath))\n",
    "print(\"\")\n",
    "print_important_weights(weights=weights, words=vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampled Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41608, 8)\n",
      "(8916, 8)\n",
      "(8916, 8)\n"
     ]
    }
   ],
   "source": [
    "upsampled_train_df = pd.read_csv('data/Twitter/hate_twitter/train_upsampled.csv')\n",
    "\n",
    "# using code from https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "train_up, val_up, test_up = np.split(upsampled_train_df.sample(frac=1, random_state=8),\\\n",
    "    [int(0.7*len(upsampled_train_df)),int(0.85*len(upsampled_train_df))])\n",
    "print(train_up.shape)\n",
    "print(val_up.shape)\n",
    "print(test_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_train.csv')\n",
    "val_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_val.csv')\n",
    "test_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab, data_dir, feature_field, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Extract and save the vocab and features.\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    splits = ['train','test','val']\n",
    "    #splits = ['train']\n",
    "\n",
    "    gram, mode = feature_name.split('_')\n",
    "    if gram not in ['unigram', 'bigram'] or mode not in ['binary', 'count']:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for split in splits:\n",
    "        datapath = data_dir.joinpath(f'hate_upsampled_{split}.csv')\n",
    "        print('datapath',datapath)\n",
    "        data_df = pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        features = list(data_df[feature_field])\n",
    "        \n",
    "        sent_lengths = []\n",
    "        values, rows, cols = [], [], []\n",
    "        labels = list(data_df['label'])\n",
    "        print(f\"\\nExtract {gram} {mode} features from {datapath}\")\n",
    "        for i, line in enumerate(features):\n",
    "            if i % 1000 == 1:\n",
    "                print(f\"Processing {i}/{len(features)} row\")\n",
    "            #label = int(line[0])\n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            # tokens = line[1:].strip().split(  )  # Tokenizing differently affects the results.\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "            tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "            if gram.find('bigram') != -1:\n",
    "                #print('yes bigram')\n",
    "                tokens.extend(\n",
    "                    [tokens[i] + ' ' + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "            feature = {}\n",
    "            for tk in tokens:\n",
    "                if tk not in vocab:\n",
    "                    continue\n",
    "                elif mode == 'binary':\n",
    "                    feature[vocab[tk]] = 1\n",
    "                elif mode == 'count':\n",
    "                    feature[vocab[tk]] = feature.get(vocab[tk], 0) + 1\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            for j in feature:\n",
    "                values.append(feature[j])\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            sent_lengths.append(len(tokens))\n",
    "            #labels.append(label)\n",
    "\n",
    "        features = sparse.csr_matrix((values, (rows, cols)),\n",
    "                                     shape=(len(features), len(vocab)))\n",
    "        print(f\"{split} feature matrix shape: {features.shape}\")\n",
    "        output_feature_filepath = data_dir.joinpath(f'{split}_{gram}_{mode}_upsamp_features.npz')\n",
    "        sparse.save_npz(output_feature_filepath, features)\n",
    "\n",
    "        np_labels = np.asarray(labels)\n",
    "        print(f\"{split} label array shape: {np_labels.shape}\")\n",
    "        output_label_filepath = data_dir.joinpath(f'{split}_upsamp_labels.npz')\n",
    "        np.savez(output_label_filepath, np_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7562)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7562)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7562)\n",
      "val label array shape: (8907,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7562)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7562)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7562)\n",
      "val label array shape: (8907,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7933)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7933)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7933)\n",
      "val label array shape: (8907,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7933)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7933)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7933)\n",
      "val label array shape: (8907,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/bigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str,\n",
    "                                     tune: bool = False) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    tune : `bool`, optional\n",
    "        Whether or not to tune the hyperparameters of the regularization strength\n",
    "        of the model of the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    # Implement logistic regression with scikit-learn.\n",
    "    # Print out the accuracy scores on dev and test data.\n",
    "\n",
    "    splits = ['train', 'val','test']\n",
    "    features, labels = {}, {}\n",
    "\n",
    "    for split in splits:\n",
    "        features_path = data_dir.joinpath(f'{split}_{feature_name}_upsamp_features.npz')\n",
    "        labels_path = data_dir.joinpath(f'{split}_upsamp_labels.npz')\n",
    "        features[split] = sparse.load_npz(features_path)\n",
    "        labels[split] = np.load(labels_path)['arr_0']\n",
    "    best_dev, best_model = 0, None\n",
    "    if tune:\n",
    "        for c in np.linspace(-5, 5, 11):\n",
    "            clf = LogisticRegression(random_state=42,\n",
    "                                     max_iter=100,\n",
    "                                     fit_intercept=False,\n",
    "                                     C=np.exp2(c))\n",
    "            clf.fit(features['train'], labels['train'])\n",
    "            dev_preds = clf.predict(features['val'])\n",
    "            dev_accuracy = accuracy_score(labels['val'], dev_preds)\n",
    "            print(c, dev_accuracy)\n",
    "            if dev_accuracy > best_dev:\n",
    "                best_dev, best_model = dev_accuracy, clf\n",
    "    else:\n",
    "        best_model = LogisticRegression(random_state=42,\n",
    "                                        max_iter=200,\n",
    "                                        fit_intercept=False)\n",
    "        best_model.fit(features['train'], labels['train'])\n",
    "\n",
    "    preds = {\n",
    "        'val': best_model.predict(features['val']),\n",
    "        'test': best_model.predict(features['test'])\n",
    "    }\n",
    "    for splt, splt_preds in preds.items():\n",
    "        print(\"{} accuracy: {:.4f}\".format(splt, accuracy_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} binary recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} macro recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds, \n",
    "                                                                average=\"macro\"))),\n",
    "        print(\"{} macro f1: {:.4f}\".format(\n",
    "            splt, f1_score(labels[splt], splt_preds, average='macro')))\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9650\n",
      "val binary recall: 0.9843\n",
      "val macro recall: 0.9652\n",
      "val macro f1: 0.9650\n",
      "test accuracy: 0.9631\n",
      "test binary recall: 0.9854\n",
      "test macro recall: 0.9630\n",
      "test macro f1: 0.9631\n"
     ]
    }
   ],
   "source": [
    "model = fit_and_eval_logistic_regression(feature_name='unigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9641\n",
      "val binary recall: 0.9847\n",
      "val macro recall: 0.9644\n",
      "val macro f1: 0.9641\n",
      "test accuracy: 0.9630\n",
      "test binary recall: 0.9868\n",
      "test macro recall: 0.9629\n",
      "test macro f1: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.4971\n",
      "val binary recall: 0.9966\n",
      "val macro recall: 0.5040\n",
      "val macro f1: 0.3421\n",
      "test accuracy: 0.5035\n",
      "test binary recall: 0.9951\n",
      "test macro recall: 0.5028\n",
      "test macro f1: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.4971\n",
      "val binary recall: 0.9966\n",
      "val macro recall: 0.5040\n",
      "val macro f1: 0.3421\n",
      "test accuracy: 0.5035\n",
      "test binary recall: 0.9951\n",
      "test macro recall: 0.5028\n",
      "test macro f1: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9650\n",
      "val binary recall: 0.9843\n",
      "val macro recall: 0.9652\n",
      "val macro f1: 0.9650\n",
      "test accuracy: 0.9631\n",
      "test binary recall: 0.9854\n",
      "test macro recall: 0.9630\n",
      "test macro f1: 0.9631\n",
      "\n",
      "Most hateful words:\n",
      " 5.9076 | allahsoil\n",
      " 4.2857 | racism\n",
      " 4.0006 | bigot\n",
      " 3.4879 | equality\n",
      " 3.4789 | white\n",
      " 3.3869 | blacklivesmatter\n",
      " 3.3394 | neighbors\n",
      " 3.2137 | mc\n",
      " 3.2048 | blatantly\n",
      " 3.2015 | shitty\n",
      "\n",
      "Least hateful words:\n",
      "-3.9929 | bihday\n",
      "-3.2502 | orlando\n",
      "-3.1340 | hardcore\n",
      "-2.8810 | thankful\n",
      "-2.7886 | healthy\n",
      "-2.7294 | friday\n",
      "-2.7153 | getting\n",
      "-2.6452 | day\n",
      "-2.6402 | tomorrow\n",
      "-2.5961 | followers\n",
      "\n",
      "Most neutral words:\n",
      " 0.0000 | <pad>\n",
      " 0.0000 | elder\n",
      " 0.0000 | litter\n",
      " 0.0000 | vaccines\n",
      " 0.0000 | devoted\n",
      " 0.0000 | rooting\n",
      " 0.0000 | minime\n",
      " 0.0000 | mornin\n",
      " 0.0000 | ankara\n",
      " 0.0000 | pl\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "model_trained: LogisticRegression = fit_and_eval_logistic_regression(\n",
    "    feature_name='unigram_binary', data_dir=Path(data_dir), tune=False)\n",
    "weights = model_trained.coef_[0]\n",
    "vocab = json.load(open(vocab_filepath))\n",
    "print(\"\")\n",
    "print_important_weights(weights=weights, words=vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
