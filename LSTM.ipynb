{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# wv = api.load('glove-twitter-50')\n",
    "\n",
    "# for index, word in enumerate(wv.index_to_key):\n",
    "#     if index == 10:\n",
    "#         break\n",
    "#     print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/Twitter/hate_twitter/hate_train.csv\")\n",
    "df_test = pd.read_csv(\"data/Twitter/hate_twitter/hate_test.csv\")\n",
    "df_val = pd.read_csv(\"data/Twitter/hate_twitter/hate_val.csv\")\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "df_train = df_train[df_train['clean_tweet'].notna()]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_test = df_test[df_test['clean_tweet'].notna()]\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n",
    "df_val = df_val[df_val['clean_tweet'].notna()]\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = dtokenizer_language = 'english')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_train.txt', 'w') as f:\n",
    "    f.write(df_train['clean_tweet'].str.cat(sep='\\n'))\n",
    "\n",
    "with open('labels_train.txt', 'w') as f:\n",
    "    f.write(df_train['label'].apply(str).str.cat(sep='\\n'))\n",
    "    \n",
    "with open('output_test.txt', 'w') as f:\n",
    "    f.write(df_test['clean_tweet'].str.cat(sep='\\n'))\n",
    "with open('labels_test.txt', 'w') as f:\n",
    "    f.write(df_test['label'].apply(str).str.cat(sep='\\n'))\n",
    "\n",
    "with open('output_val.txt', 'w') as f:\n",
    "    f.write(df_val['clean_tweet'].str.cat(sep='\\n'))\n",
    "with open('labels_val.txt', 'w') as f:\n",
    "    f.write(df_val['label'].apply(str).str.cat(sep='\\n'))\n",
    "    \n",
    "with open('output_train.txt', 'r') as f:\n",
    "    tweets_train = f.read()\n",
    "with open('labels_train.txt', 'r') as f:\n",
    "    labels_train = f.read()\n",
    "\n",
    "with open('output_test.txt', 'r') as f:\n",
    "    tweets_test = f.read()\n",
    "with open('labels_test.txt', 'r') as f:\n",
    "    labels_test = f.read()\n",
    "    \n",
    "with open('output_val.txt', 'r') as f:\n",
    "    tweets_val = f.read()\n",
    "with open('labels_val.txt', 'r') as f:\n",
    "    labels_val = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'vocab' from 'torchtext.vocab' (/Users/sophiamlawer/opt/anaconda3/lib/python3.8/site-packages/torchtext/vocab.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2c59499e8f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'vocab' from 'torchtext.vocab' (/Users/sophiamlawer/opt/anaconda3/lib/python3.8/site-packages/torchtext/vocab.py)"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for text in tweets_int_train:\n",
    "    for word in text:\n",
    "        counter.update(tokenizer(word))\n",
    "vocab_words = vocab(counter, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_index = -1\n",
    "vocab_words.set_default_index(default_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.twitter.27B.zip:  81%|████████  | 1.23G/1.52G [06:29<01:33, 3.15MB/s]   \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06cf095adee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"twitter.27B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# glove = vocab.GloVe('6B',cache=VECTORS_CACHE_DIR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.{}.{}d.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    359\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name=\"twitter.27B\", dim=50)\n",
    "\n",
    "# glove = vocab.GloVe('6B',cache=VECTORS_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab_words.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(tweets_int_train, max_size=100000) \n",
    "LABEL.build_vocab(tweets_int_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({k: TEXT.vocab.stoi[k] for k in list(TEXT.vocab.stoi)[:15]}) \n",
    "print(f\"Most common 15 words in the vocab are: {TEXT.vocab.freqs.most_common(15)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
    "LABEL = LabelField(tensor_type=torch.FloatTensor)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 650\n",
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = loadEmbeddingMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix():\n",
    "        #load different embedding file from Kaggle depending on which embedding \n",
    "        #matrix we are going to experiment with\n",
    "        EMBEDDING_FILE='../input/glove-twitter/glove.twitter.27B.25d.txt'\n",
    "        embed_size = 25\n",
    "\n",
    "        embeddings_index = dict()\n",
    "        #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "        f = open(EMBEDDING_FILE)\n",
    "        for line in f:\n",
    "            #split up line into an indexed array\n",
    "            values = line.split()\n",
    "            #first index is word\n",
    "            word = values[0]\n",
    "            #store the rest of the values in the array as a new array\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs #50 dimensions\n",
    "        f.close()\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device == 'cpu':\n",
    "    VECTORS_CACHE_DIR = '/Users/sophiamlawer/.vector_cache'\n",
    "    # Please change above to your cache\n",
    "else:\n",
    "    VECTORS_CACHE_DIR = './.vector_cache'\n",
    "    # This is the default cache on Colab. Caching may not work\n",
    "    # as expected on Colab.\n",
    "    \n",
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# glove = GloVe(name='6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "unk_token = '<unk>'\n",
    "counter_words = Counter()\n",
    "for (text) in tweets_split:\n",
    "    counter_words.update(text)\n",
    "# print(counter_words)\n",
    "vocab_words = vocab(counter_words, specials=[unk_token]) \n",
    "vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def split_tweet(df):\n",
    "    tweets_split = df.split('\\n')\n",
    "    all_text2 = ' '.join(tweets_split)\n",
    "    # create a list of words\n",
    "    words = all_text2.split()\n",
    "    # Count all the words using Counter Method\n",
    "    count_words = Counter(words)\n",
    "\n",
    "    total_words = len(words)\n",
    "    sorted_words = count_words.most_common(total_words)\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "    return tweets_split, vocab_to_int\n",
    "\n",
    "def tweets_int(tweets_split):\n",
    "    tweet_int =[]\n",
    "    for tweet in tweets_split:\n",
    "        tweet_int.append(tweet)\n",
    "    return tweet_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_split_train, _ = split_tweet(tweets_train)\n",
    "print ('Number of tweets :', len(tweets_split_train))\n",
    "tweets_split_test, _ = split_tweet(tweets_test) \n",
    "tweets_split_val, _ = split_tweet(tweets_val) \n",
    "\n",
    "tweets_int_train = tweets_int(tweets_split_train)\n",
    "tweets_int_test = tweets_int(tweets_split_test) \n",
    "tweets_int_val = tweets_int(tweets_split_val) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_split_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f33029200e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_split_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_split_train' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_split_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "glove = vocab.GloVe('6B',cache=VECTORS_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab_words.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_text2 = ' '.join(tweets_split)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_int = []\n",
    "for tweet in tweets_split:\n",
    "    r = [vocab_to_int[w] for w in tweet.split()]\n",
    "    tweets_int.append(r)\n",
    "print (tweets_int[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tweets_len = [len(x) for x in tweets_int]\n",
    "pd.Series(tweets_len).hist()\n",
    "plt.show()\n",
    "pd.Series(tweets_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tweets_int, seq_length):\n",
    "    ''' \n",
    "    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(tweets_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, tweet in enumerate(tweets_int):\n",
    "        tweet_len = len(tweet)\n",
    "        \n",
    "        if tweet_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-tweet_len))\n",
    "            new = zeroes+tweet\n",
    "        elif tweet_len > seq_length:\n",
    "            new = tweet[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pad_features(tweets_int, 10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.split('\\n')\n",
    "encoded_labels = [1 if label =='1' else 0 for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(df2), torch.from_numpy(encoded_labels))\n",
    "\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(preprocess_data_seq(train_0))\n",
    "valid_data = list(preprocess_data_seq(valid_0))\n",
    "test_data = list(preprocess_data_seq(test_0))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, glove, dropout=0,freeze_glove = False):\n",
    "        ''' Initialize model parameters corresponding to ---\n",
    "            - embedding layer\n",
    "            - linear layer to map from hidden vector to the vocabulary\n",
    "            - optionally, dropout layers.  Dropout layers can be placed after \n",
    "              the embedding layer or/and after the RNN layer. Dropout within\n",
    "              an RNN is only applied when there are two or more num_layers.\n",
    "            - optionally, initialize the model parameters.\n",
    "            \n",
    "            The arguments are:\n",
    "            \n",
    "            vocab_size: size of vocabulary\n",
    "            embedding_dim: size of an embedding vector\n",
    "            hidden_dim: size of hidden/state vector in RNN\n",
    "            num_layers: number of layers in RNN\n",
    "            dropout: dropout probability.\n",
    "            \n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.glove = glove_\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.freeze_glove = freeze_glove\n",
    "        self.word_embeddings = torch.nn.Embedding.from_pretrained(self.glove, self.freeze_glove)\n",
    "        self.rnn = nn.LSTM(self.embed_size, self.hidden_dim, self.n_layers, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden0):\n",
    "        ''' \n",
    "        Run forward propagation for a given minibatch of inputs using\n",
    "        hidden0 as the initial hidden state.\n",
    "\n",
    "        In LSTMs hidden0 = (h_0, c_0). \n",
    "\n",
    "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
    "        Return this as well so that it can be used to initialize the next\n",
    "        batch.\n",
    "        \n",
    "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
    "        the more efficient CrossEntropyLoss.  See \n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
    "        '''\n",
    "        batch_size = input.size(0)\n",
    "        embeds = self.word_embeddings(input)\n",
    "        print(hidden0[0].shape)\n",
    "        lstm_out, hidden_new = self.rnn(embeds, hidden0)\n",
    "#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        print(tag_space.shape)\n",
    "#       sig_out = tag_space.view(batch_size, -1)\n",
    "        tag_space = tag_space[:, -1]\n",
    "        print(tag_space.shape)\n",
    "\n",
    "        return tag_space, hidden_new\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "#         if (train_on_gpu):\n",
    "#             hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "#                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "#         else:\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "    \n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, n_layers, glove_=glove_vector, freeze_glove=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion =nn.BCELoss()\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "# if(train_on_gpu):\n",
    "#     model.cuda()\n",
    "\n",
    "model.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "#         if(train_on_gpu):\n",
    "#             inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        print(output.squeeze().shape, labels.shape)\n",
    "        loss = loss_function(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "#                 if(train_on_gpu):\n",
    "#                     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = model(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, model):\n",
    "    \n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for idx, (text,label) in enumerate(dataloader):\n",
    "        model.zero_grad() #initialize to 0 gradient\n",
    "        out, hidden = model(text, hidden) #picking forward method\n",
    "        loss = loss_function(out.view(-1, vocab_size), label.view(-1)) #calcs the loss\n",
    "        loss.backward() #backpropogation\n",
    "        optimizer.step() #changing the gradient descent\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        total_acc, total_count = 0, 0\n",
    "        for idx, (label, word_idxs) in enumerate(dataloader):\n",
    "            log_probs = model(word_idxs)\n",
    "            total_acc += (log_probs.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 3 # epoch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "accuracies=[]\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_loader, model_glove_fro)\n",
    "    accuracy = get_accuracy(train_loader, model_glove_fro)\n",
    "    accuracies.append(accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
    "    \n",
    "plt.plot(range(1, EPOCHS+1), accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile load_data.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torchtext.legacy import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "def load_dataset(test_sen=None):\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
    "    Field : A class that stores information about the way of preprocessing\n",
    "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
    "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
    "                 will pad each sequence to have a fix length of 200.\n",
    "                 \n",
    "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
    "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
    "                  \n",
    "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
    "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
    "    LABEL = torchtext.data.LabelField(tensor_type=torch.FloatTensor)\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "\n",
    "    '''Alternatively we can also use the default configurations'''\n",
    "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data handling\n",
    "import numpy as np # base of all\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import seaborn as sns  # advance plotting\n",
    "# from wordcloud import WordCloud # to see the words as image\n",
    "import torch # PyTorch for building Networks\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator, Iterator\n",
    "from torchtext import vocab\n",
    "from sklearn.model_selection import train_test_split # split the data into training and testing\n",
    "from sklearn.metrics import accuracy_score # accuracy metric\n",
    "from nltk import word_tokenize # very popular Text processing Library\n",
    "import random # to perform randomisation of tasks\n",
    "from tqdm.notebook import tqdm # for a continuous progress bar style\n",
    "import time # time module \n",
    "import os # import operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"data/Twitter/hate_twitter/\"\n",
    "text_field = Field(tokenize=word_tokenize)\n",
    "label_field = LabelField(dtype=torch.float) \n",
    "fields = [('clean_tweet',text_field),('label',label_field)] \n",
    "train, val, test = TabularDataset.splits(path=OUT_PATH, train='hate_train.csv',validation='hate_val.csv',\n",
    "                                         test='hate_test.csv', \n",
    "                                         format='csv',skip_header=True,fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Type of \"train:\" {type(train)}\\n Length of \"train\": {len(train)}\\n' )\n",
    "i = random.randint(0,len(train)) # generate a random index  within the lenth of train\n",
    "print(f'Keys at index {i} of \"train\": {train[i].__dict__.keys()}\\n')\n",
    "print(\"Contents at random index:\\n\",vars(train.examples[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
