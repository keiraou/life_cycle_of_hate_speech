{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Full.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model\n",
        "\n",
        "This code uses the example of the following tutorials/codes: [Richard Cheng's fake news classification](https://github.com/itsuncheng/fake_news_classification), [Analytics Vidhya's Build First Text Classification](https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/), and our previous class homeworks.\n"
      ],
      "metadata": {
        "id": "NPLlBEsUMTiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F5mcvgvmJw9",
        "outputId": "794eebd9-7f4d-49e9-e401-d7a88868deb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.11.2\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 25.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Collecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x38d08000 @  0x7f3132a43615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ps8NS81Pl_kA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchtext.vocab import GloVe\n",
        "\n",
        "# glove = GloVe(name=\"twitter.27B\", dim=25)\n",
        "\n",
        "# glove.twitter.27B.25d\n",
        "\n",
        "# glove = GloVe(name='6B')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nn86Sr-guRE",
        "outputId": "ccec7134-c9f6-4ae1-f416-30f19c184988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [04:46, 5.31MB/s]                            \n",
            "100%|█████████▉| 1193513/1193514 [00:24<00:00, 49621.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "from itertools import combinations\n",
        "from torchtext import vocab\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "if device == 'cpu':\n",
        "    VECTORS_CACHE_DIR = '/Users/sophiamlawer/.vector_cache'\n",
        "    # Please change above to your cache\n",
        "else:\n",
        "    VECTORS_CACHE_DIR = './.vector_cache'\n",
        "    # This is the default cache on Colab. Caching may not work\n",
        "    # as expected on Colab.\n",
        "\n",
        "\n",
        "\n",
        "glove = vocab.GloVe(name=\"twitter.27B\", dim=100,cache=VECTORS_CACHE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkIj_VgMUKWy",
        "outputId": "134ff7c5-2334-44aa-8862-c362d5e9bab6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./.vector_cache/glove.twitter.27B.zip: 1.52GB [04:46, 5.31MB/s]                            \n",
            "100%|█████████▉| 1193513/1193514 [00:52<00:00, 22930.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Read in data\n",
        "import pandas as pd \n",
        "\n",
        "df_train = pd.read_csv(\"hate_upsampled_train.csv\")\n",
        "df_test = pd.read_csv(\"hate_upsampled_test.csv\")\n",
        "df_val = pd.read_csv(\"hate_upsampled_val.csv\")\n",
        "\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "WzAdrfHXi4-s",
        "outputId": "a8958748-01da-411c-e256-2dcb0f663a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41608, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1     id  label  \\\n",
              "0       44049         15405  15406      0   \n",
              "1       33142          3684   3685      0   \n",
              "2       40964         12093  12094      0   \n",
              "3       53102         25149  25150      0   \n",
              "4       15142         12744  12745      1   \n",
              "\n",
              "                                               tweet  \\\n",
              "0  @user we really are. lmaoo we are obviously re...   \n",
              "1  i think my #hea need a #bandage again ð   #...   \n",
              "2  new phone #xperiaz3+ #xperiaz4 #sony #thebeast...   \n",
              "3    #dinner at my friend's #steak #shop never #d...   \n",
              "4  @user @user @user and sorry if we are ethnical...   \n",
              "\n",
              "                                            hash_tag  \\\n",
              "0                                                 []   \n",
              "1                         ['hea', 'bandage', 'love']   \n",
              "2  ['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...   \n",
              "3  ['dinner', 'steak', 'shop', 'disappoint', 'yum...   \n",
              "4                                                 []   \n",
              "\n",
              "                                         clean_tweet  \\\n",
              "0  really lmaoo obviously related haha wonder cam...   \n",
              "1                        think hea need bandage love   \n",
              "2  new phone xperiaz3 xperiaz4 sony thebeast copp...   \n",
              "3  dinner friends steak shop never disappoint yum...   \n",
              "4  sorry ethnically cleansing east jerusalem beth...   \n",
              "\n",
              "                                     tokenized_tweet  \\\n",
              "0  <user> we really are. lmaoo we are obviously r...   \n",
              "1  i think my <hashtag> hea need a <hashtag> band...   \n",
              "2  new phone <hashtag> xperiaz<number>+ <hashtag>...   \n",
              "3    <hashtag> dinner at my friend's <hashtag> st...   \n",
              "4  <user> <user> <user> and sorry if we are ethni...   \n",
              "\n",
              "                                tokenized_tweet_NLTK  \n",
              "0  @user really lmaoo obviously related haha wond...  \n",
              "1                        think hea need bandage love  \n",
              "2  new phone xperiaz3 xperiaz4 sony thebeast copp...  \n",
              "3  dinner friend's steak shop never disappoint yu...  \n",
              "4  @user @user @user sorry ethnically cleansing e...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f5982de-142a-42de-9aa6-13b171e6fb39\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hash_tag</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tokenized_tweet</th>\n",
              "      <th>tokenized_tweet_NLTK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44049</td>\n",
              "      <td>15405</td>\n",
              "      <td>15406</td>\n",
              "      <td>0</td>\n",
              "      <td>@user we really are. lmaoo we are obviously re...</td>\n",
              "      <td>[]</td>\n",
              "      <td>really lmaoo obviously related haha wonder cam...</td>\n",
              "      <td>&lt;user&gt; we really are. lmaoo we are obviously r...</td>\n",
              "      <td>@user really lmaoo obviously related haha wond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33142</td>\n",
              "      <td>3684</td>\n",
              "      <td>3685</td>\n",
              "      <td>0</td>\n",
              "      <td>i think my #hea need a #bandage again ð   #...</td>\n",
              "      <td>['hea', 'bandage', 'love']</td>\n",
              "      <td>think hea need bandage love</td>\n",
              "      <td>i think my &lt;hashtag&gt; hea need a &lt;hashtag&gt; band...</td>\n",
              "      <td>think hea need bandage love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40964</td>\n",
              "      <td>12093</td>\n",
              "      <td>12094</td>\n",
              "      <td>0</td>\n",
              "      <td>new phone #xperiaz3+ #xperiaz4 #sony #thebeast...</td>\n",
              "      <td>['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...</td>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "      <td>new phone &lt;hashtag&gt; xperiaz&lt;number&gt;+ &lt;hashtag&gt;...</td>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53102</td>\n",
              "      <td>25149</td>\n",
              "      <td>25150</td>\n",
              "      <td>0</td>\n",
              "      <td>#dinner at my friend's #steak #shop never #d...</td>\n",
              "      <td>['dinner', 'steak', 'shop', 'disappoint', 'yum...</td>\n",
              "      <td>dinner friends steak shop never disappoint yum...</td>\n",
              "      <td>&lt;hashtag&gt; dinner at my friend's &lt;hashtag&gt; st...</td>\n",
              "      <td>dinner friend's steak shop never disappoint yu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15142</td>\n",
              "      <td>12744</td>\n",
              "      <td>12745</td>\n",
              "      <td>1</td>\n",
              "      <td>@user @user @user and sorry if we are ethnical...</td>\n",
              "      <td>[]</td>\n",
              "      <td>sorry ethnically cleansing east jerusalem beth...</td>\n",
              "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; and sorry if we are ethni...</td>\n",
              "      <td>@user @user @user sorry ethnically cleansing e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f5982de-142a-42de-9aa6-13b171e6fb39')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f5982de-142a-42de-9aa6-13b171e6fb39 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f5982de-142a-42de-9aa6-13b171e6fb39');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train['clean_tweet'].notna()]\n",
        "df_test = df_test[df_test['clean_tweet'].notna()]\n",
        "df_val = df_val[df_val['clean_tweet'].notna()]\n",
        "\n",
        "df_train[[\"clean_tweet\", \"label\"]].to_csv(\"train.csv\")\n",
        "df_test[[\"clean_tweet\", \"label\"]].to_csv(\"test.csv\")\n",
        "df_val[[\"clean_tweet\", \"label\"]].to_csv(\"val.csv\")"
      ],
      "metadata": {
        "id": "qv7yKipzjE-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[[\"clean_tweet\", \"label\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cDLaOXmBzb3F",
        "outputId": "16dd0f63-a3f9-49ea-a5a9-cf6da08d790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             clean_tweet  label\n",
              "0      really lmaoo obviously related haha wonder cam...      0\n",
              "1                            think hea need bandage love      0\n",
              "2      new phone xperiaz3 xperiaz4 sony thebeast copp...      0\n",
              "3      dinner friends steak shop never disappoint yum...      0\n",
              "4      sorry ethnically cleansing east jerusalem beth...      1\n",
              "...                                                  ...    ...\n",
              "41603      invited help700 actions peace povey sept 1625      1\n",
              "41604         might libtard libtard sjw liberal politics      1\n",
              "41605  people arent protesting trump republican wonth...      1\n",
              "41606          ppl playing statedey playing india future      0\n",
              "41607  tonight presents outdeh rootsandchallis nairob...      0\n",
              "\n",
              "[41560 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7acb17f-582b-4b63-bc13-a33ed3390272\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>really lmaoo obviously related haha wonder cam...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>think hea need bandage love</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dinner friends steak shop never disappoint yum...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sorry ethnically cleansing east jerusalem beth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41603</th>\n",
              "      <td>invited help700 actions peace povey sept 1625</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41604</th>\n",
              "      <td>might libtard libtard sjw liberal politics</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41605</th>\n",
              "      <td>people arent protesting trump republican wonth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41606</th>\n",
              "      <td>ppl playing statedey playing india future</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41607</th>\n",
              "      <td>tonight presents outdeh rootsandchallis nairob...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41560 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7acb17f-582b-4b63-bc13-a33ed3390272')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7acb17f-582b-4b63-bc13-a33ed3390272 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7acb17f-582b-4b63-bc13-a33ed3390272');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try This Model"
      ],
      "metadata": {
        "id": "9eYmi8ZBneKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "TEXT = Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = LabelField(sequential=False, use_vocab=False, dtype = torch.float,batch_first=True)"
      ],
      "metadata": {
        "id": "PsL5vWt6iq68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fields = [('label', LABEL), ('clean_tweet',TEXT)]\n",
        "# fields = [('clean_tweet',TEXT)]\n",
        "fields = [(None, None), ('clean_tweet',TEXT),('label', LABEL)]\n"
      ],
      "metadata": {
        "id": "ItV_PAynjAzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = TabularDataset.splits(path = \"/content/\", train='train.csv',validation='val.csv',\n",
        "                                         test='test.csv', format='csv',\n",
        "                                         fields= fields, skip_header=True)\n",
        "\n",
        "print(vars(train.examples[8]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PvY2OljnhYo",
        "outputId": "e5ab3db6-3c52-4d26-ab0f-241d5f74280b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clean_tweet': ['dear', 'please', 'give', 'us', 'multiracial', 'objective', 'list', 'prove', 'selection'], 'label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train,min_freq=3,vectors = \"glove.twitter.27B.100d\")  \n",
        "# LABEL.build_vocab(train)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "# print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "# print(LABEL.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WQ4ToRVjUO7",
        "outputId": "f64091fb-82ec-4b1e-c1ab-6c032f6fd769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of TEXT vocabulary: 10396\n",
            "[('nt', 3678), ('love', 2114), ('like', 1965), ('trump', 1943), ('day', 1636), ('i', 1609), ('people', 1424), ('libtard', 1392), ('white', 1329), ('new', 1296)]\n",
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7f7c57cad390>>, {'<unk>': 0, '<pad>': 1, 'nt': 2, 'love': 3, 'like': 4, 'trump': 5, 'day': 6, 'i': 7, 'people': 8, 'libtard': 9, 'white': 10, 'new': 11, 'black': 12, 'happy': 13, 'm': 14, 's': 15, 'do': 16, 'racist': 17, 'one': 18, 'time': 19, 'politics': 20, 'us': 21, 'allahsoil': 22, 'ca': 23, 'get': 24, 'good': 25, 'life': 26, 'today': 27, 'feel': 28, 'might': 29, 'liberal': 30, 'see': 31, 'women': 32, 'hate': 33, 'go': 34, 'retweet': 35, 'sjw': 36, 'want': 37, 're': 38, 'positive': 39, 'obama': 40, 'thankful': 41, 'you': 42, 'would': 43, 'racism': 44, 'listen': 45, 'via': 46, 'got': 47, 'bihday': 48, 'work': 49, 'make': 50, 'take': 51, 'great': 52, 'never': 53, 'think': 54, 'really': 55, 'need': 56, 'back': 57, 'america': 58, 'smile': 59, 'way': 60, '2016': 61, 'man': 62, 'world': 63, 'right': 64, 'know': 65, 'miami': 66, 'stomping': 67, 'fathers': 68, 'thanks': 69, 'stop': 70, 'going': 71, 'girl': 72, 'much': 73, 'first': 74, 'say': 75, 'woman': 76, 'race': 77, 'fun': 78, 'video': 79, 'even': 80, 'that': 81, 've': 82, 'healthy': 83, 'best': 84, 'look': 85, 'still': 86, 'sad': 87, 'weekend': 88, 'men': 89, 'year': 90, 'days': 91, 'comments': 92, 'family': 93, 'ever': 94, 'live': 95, 'ur': 96, 'many': 97, 'pay': 98, 'wait': 99, 'friends': 100, 'music': 101, 'sex': 102, 'news': 103, 'summer': 104, 'next': 105, 'beautiful': 106, 'always': 107, 'nothing': 108, 'friday': 109, 'girls': 110, 'makes': 111, 'bull': 112, 'says': 113, 'well': 114, '2017': 115, 'wo': 116, 'morning': 117, 'come': 118, 'free': 119, 'show': 120, 'let': 121, 'media': 122, 'thank': 123, 'latest': 124, 'last': 125, 'find': 126, 'tampa': 127, 'home': 128, 'blm': 129, 'orlando': 130, 'call': 131, 'things': 132, 'week': 133, 'is': 134, 'keep': 135, 'please': 136, 'cute': 137, 'every': 138, 'fathersday': 139, 'hope': 140, 'believe': 141, 'everyone': 142, 'hatred': 143, 'tomorrow': 144, 'years': 145, 'model': 146, 'real': 147, 'sunday': 148, 'school': 149, 'act': 150, 'brexit': 151, 'guy': 152, 'night': 153, 'god': 154, 'fuck': 155, 'police': 156, 'does': 157, 'hispanic': 158, 'blog': 159, 'help': 160, 'history': 161, 'follow': 162, 'happiness': 163, 'done': 164, 'game': 165, 'bigot': 166, 'usa': 167, 'thing': 168, 'finally': 169, 'better': 170, 'another': 171, 'calgary': 172, 'little': 173, 'made': 174, 'person': 175, 'altright': 176, 'sikh': 177, 'word': 178, 'affirmation': 179, 'someone': 180, 'said': 181, 'temple': 182, 'watch': 183, 'daily': 184, 'book': 185, 'could': 186, 'leadership': 187, 'peace': 188, 'condemns': 189, 'he': 190, 'thought': 191, 'vandalised': 192, 'wso': 193, 'dad': 194, 'amazing': 195, 'did': 196, 'selfie': 197, 'teambts': 198, 'president': 199, 'face': 200, 'd': 201, 'malevote': 202, 'suppoers': 203, 'boricua': 204, 'healing': 205, 'team': 206, 'lost': 207, 'give': 208, 'must': 209, 'saying': 210, 'true': 211, 'porn': 212, 'looking': 213, 'tonight': 214, 'americans': 215, 'also': 216, 'yes': 217, 'emiratis': 218, 'maga': 219, 'wish': 220, 'bad': 221, 'they': 222, 'may': 223, 'altwaystoheal': 224, 'bigotry': 225, 'gold': 226, 'iam': 227, 'pa': 228, 'attack': 229, 'proud': 230, 'enough': 231, 'change': 232, 'end': 233, 'read': 234, 'lol': 235, 'misogyny': 236, 'na': 237, 'factory': 238, 'feeling': 239, 'needs': 240, 'food': 241, 'play': 242, 'city': 243, 'ready': 244, 'sea': 245, 'ignored': 246, 'silver': 247, 'old': 248, 'kkk': 249, 'kids': 250, 'sta': 251, 'big': 252, 'feminismiscancer': 253, 'feminismisterrorism': 254, 'feminismmuktbharat': 255, 'country': 256, 'without': 257, 'job': 258, 'long': 259, 'antiracism': 260, 'christmas': 261, 'guys': 262, 'nice': 263, 'around': 264, 'gets': 265, 'getting': 266, 'use': 267, 'coming': 268, 'funny': 269, 'enjoy': 270, 'everything': 271, 'misogynist': 272, 'friend': 273, 'uk': 274, 'win': 275, 'seashepherd': 276, 'left': 277, 'others': 278, 'kind': 279, 'shit': 280, 'are': 281, 'away': 282, 'blacklivesmatter': 283, 'father': 284, 'tweet': 285, 'yet': 286, 'blessed': 287, 'hard': 288, 'wow': 289, 'buffalo': 290, 'hey': 291, 'power': 292, 'matter': 293, 'already': 294, 'equality': 295, 'open': 296, 'forex': 297, 'miss': 298, 'paladino': 299, 'trumps': 300, 'fear': 301, 'ppl': 302, 'shepherd': 303, 'angry': 304, 'hea': 305, 'holiday': 306, 'truth': 307, 'whites': 308, '2016in4words': 309, 'american': 310, 'oh': 311, 'tell': 312, 'found': 313, 'sun': 314, 'cool': 315, 'business': 316, 'hot': 317, 'muslims': 318, 'baby': 319, 'climb': 320, 'lets': 321, 'money': 322, 'muslim': 323, 'nude': 324, 'rest': 325, 'twitter': 326, 'whatever': 327, 'comes': 328, 'hair': 329, 'bear': 330, 'ignorance': 331, 'looks': 332, 'naked': 333, 'awesome': 334, 'dog': 335, 'notmypresident': 336, 'house': 337, 'feminism': 338, 'children': 339, 'saturday': 340, '10': 341, 'gon': 342, 'lot': 343, 'waiting': 344, 'boy': 345, 'jews': 346, 'words': 347, 'education': 348, 'something': 349, 'strong': 350, 'anything': 351, 'cnn': 352, 'excited': 353, 'leave': 354, 'southafrica': 355, 'beach': 356, 'mad': 357, 'ally': 358, 'course': 359, 'gay': 360, 'israel': 361, 'sexy': 362, 'talking': 363, 'two': 364, 'videos': 365, 'carl': 366, 'check': 367, 'color': 368, 'fans': 369, 'islam': 370, 'try': 371, 'weeks': 372, 'instagood': 373, 'polar': 374, 'watching': 375, 'boycott': 376, 'place': 377, 'reading': 378, 'sick': 379, 'yeah': 380, 'antisemitism': 381, 'grateful': 382, 'rape': 383, 'resist': 384, 'trying': 385, 'wedding': 386, 'japan': 387, 'oil': 388, 'post': 389, 'far': 390, 'mean': 391, 'means': 392, 'playing': 393, 'social': 394, 'blacks': 395, 'quote': 396, 'vote': 397, 'forward': 398, 'gop': 399, 'list': 400, 'soon': 401, 'tcot': 402, 'vs': 403, 'dear': 404, 'fucking': 405, 'heres': 406, 'obamas': 407, 'ass': 408, 'fascism': 409, 'flag': 410, 'remarks': 411, 'she': 412, 'buy': 413, 'rip': 414, 'suppo': 415, 'there': 416, 'calls': 417, 'full': 418, 'newyork': 419, 'point': 420, 'times': 421, 'crazy': 422, 'death': 423, 'motivation': 424, 'problem': 425, 'talk': 426, 'wants': 427, 'gorilla': 428, 'ht': 429, 'pretty': 430, 'shows': 431, 'bc': 432, 'direct': 433, 'making': 434, 'michelle': 435, 'towards': 436, 'wrong': 437, 'join': 438, 'kill': 439, 'dead': 440, 'dominate': 441, 'war': 442, 'anyone': 443, 'islamic': 444, 'putinschoice': 445, 'seems': 446, 'single': 447, 'young': 448, 'become': 449, 'mind': 450, 'ill': 451, 'republican': 452, 'travel': 453, 'wall': 454, 'called': 455, 'forget': 456, 'goes': 457, 'hillary': 458, 'monday': 459, 'stay': 460, 'stupid': 461, 'target': 462, 'theresistance': 463, 'xenophobia': 464, 'christians': 465, 'islamophobia': 466, 'joy': 467, 'together': 468, 'tweets': 469, 'joke': 470, 'mother': 471, 'omg': 472, 'lovely': 473, 'working': 474, 'fascist': 475, 'perfect': 476, 'comment': 477, 'mindset': 478, 'quotes': 479, 'worst': 480, 'african': 481, 'health': 482, 'religion': 483, 'boys': 484, 'followme': 485, 'calling': 486, 'came': 487, 'different': 488, 'discrimination': 489, 'disgusting': 490, 'june': 491, 'message': 492, 'ai': 493, 'body': 494, 'couple': 495, 'hear': 496, 'lives': 497, 'teamsuperjunior': 498, 'inspiration': 499, 'nyc': 500, 'terrorism': 501, 'traitor': 502, 'ago': 503, 'campaign': 504, 'cow': 505, 'hateful': 506, 'lgbt': 507, 'ny': 508, 'protesting': 509, 'son': 510, 'speak': 511, 'fashion': 512, 'leftright': 513, 'polarisation': 514, 'uselections2016': 515, 'asian': 516, 'cold': 517, 'living': 518, 'bit': 519, 'class': 520, 'reality': 521, 'sexist': 522, 'super': 523, 'treason': 524, 'what': 525, 'carlpaladino': 526, 'liberals': 527, 'tbt': 528, 'victims': 529, 'agree': 530, 'beauty': 531, 'denial': 532, 'human': 533, 'photooftheday': 534, 'racing': 535, 'since': 536, 'used': 537, 'close': 538, 'hell': 539, 'london': 540, 'running': 541, 'who': 542, 'bing': 543, 'liar': 544, 'nazi': 545, 'put': 546, 'understand': 547, 'da': 548, 'fought': 549, 'hours': 550, 'justice': 551, 'less': 552, 'remember': 553, 'story': 554, 'wishes': 555, 'actually': 556, 'almost': 557, 'auspol': 558, 'bong': 559, 'fact': 560, 'run': 561, 'aicle': 562, 'care': 563, 'loving': 564, 'share': 565, 'song': 566, 'bbc': 567, 'dumb': 568, 'shot': 569, 'though': 570, 'troll': 571, 'else': 572, 'forever': 573, 'head': 574, 'homophobic': 575, 'self': 576, 'small': 577, 'special': 578, 'stories': 579, 'surprise': 580, 'gone': 581, 'poetry': 582, 'shooting': 583, 'sleep': 584, 'cochair': 585, 'dads': 586, 'fuhered': 587, 'happened': 588, 'impoant': 589, 'lying': 590, 'nazis': 591, 'newyear': 592, 'seriously': 593, 'states': 594, 'wonthey': 595, 'wtf': 596, 'hand': 597, 'maybe': 598, 'moment': 599, 'officers': 600, 'todays': 601, 'voted': 602, 'we': 603, 'went': 604, 'abuse': 605, 'conference': 606, 'half': 607, 'movie': 608, 'systemic': 609, 'welcome': 610, 'delete': 611, 'male': 612, 'past': 613, 'photo': 614, 'season': 615, 'street': 616, 'tv': 617, 'age': 618, 'celebrate': 619, 'guess': 620, 'least': 621, 'reason': 622, 'response': 623, 'stopracism': 624, 'wife': 625, 'yay': 626, 'anymore': 627, 'disease': 628, 'euro2016': 629, 'india': 630, 'lady': 631, 'meet': 632, 'needed': 633, 'south': 634, 'damn': 635, 'fired': 636, 'kid': 637, 'lifestyle': 638, 'nationalist': 639, 'telling': 640, 'woh': 641, 'dream': 642, 'ignorant': 643, 'jokes': 644, 'office': 645, 'pic': 646, 'send': 647, 'smh': 648, 'sorry': 649, 'thoughts': 650, 'wonderful': 651, 'yesterday': 652, 'claims': 653, 'disappointed': 654, 'fat': 655, 'lt3': 656, 'months': 657, 'neverump': 658, 'seeing': 659, 'sessions': 660, 'sign': 661, 'stand': 662, 'success': 663, 'africa': 664, 'cat': 665, 'culture': 666, 'football': 667, 'given': 668, 'ok': 669, 'reflections': 670, 'seen': 671, 'ugly': 672, 'apparently': 673, 'despite': 674, 'idea': 675, 'russia': 676, 'staff': 677, 'tear': 678, '12': 679, 'all': 680, 'ask': 681, 'church': 682, 'content': 683, 'dangerous': 684, 'depression': 685, 'extremism': 686, 'government': 687, 'heard': 688, 'high': 689, 'inequality': 690, 'tired': 691, 'whitepeople': 692, 'y': 693, 'ag': 694, 'anc': 695, 'cause': 696, 'fucked': 697, 'future': 698, 'gender': 699, 'green': 700, 'lumpy': 701, 'million': 702, 'present': 703, 'revolution': 704, 'rights': 705, 'sure': 706, 'thursday': 707, 'violence': 708, 'whole': 709, 'environment': 710, 'loved': 711, 'mood': 712, 'respect': 713, 'saw': 714, 'shop': 715, 'state': 716, '50': 717, 'board': 718, 'brown': 719, 'chick': 720, 'coffee': 721, 'community': 722, 'gt': 723, 'homes': 724, 'late': 725, 'lies': 726, 'marijuana': 727, 'blue': 728, 'child': 729, 'date': 730, 'guns': 731, 'htt': 732, 'mom': 733, 'scum': 734, 'till': 735, 'alone': 736, 'eyes': 737, 'hello': 738, 'order': 739, 'racial': 740, 'absolutely': 741, 'dance': 742, 'decades': 743, 'fly': 744, 'rejected': 745, 'accept': 746, 'adapt': 747, 'bed': 748, 'car': 749, 'fake': 750, 'impression': 751, 'leaving': 752, 'meant': 753, 'month': 754, 'picture': 755, 'policebrutality': 756, 'poor': 757, 'shopping': 758, 'simulator': 759, 'transformation': 760, 'wanna': 761, 'youtube': 762, 'ad': 763, 'australia': 764, 'combat': 765, 'donaldtrump': 766, 'freedom': 767, 'officially': 768, 'preorder': 769, 'sexism': 770, 'tech': 771, 'choose': 772, 'die': 773, 'll': 774, 'official': 775, 'potus': 776, 'prevents': 777, 'safe': 778, 'schools': 779, 'shut': 780, 'speech': 781, 'dark': 782, 'deal': 783, 'fitness': 784, 'fool': 785, 'hour': 786, 'land': 787, 'save': 788, 'teen': 789, 'truly': 790, 'wanted': 791, 'daughter': 792, 'early': 793, 'expect': 794, 'fight': 795, 'knew': 796, 'later': 797, 'pick': 798, 'pm': 799, 'policies': 800, 'using': 801, 'wednesday': 802, 'either': 803, 'europe': 804, 'flowers': 805, 'gift': 806, 'hollywood': 807, 'humanity': 808, 'humanrights': 809, 'industry': 810, 'law': 811, 'pig': 812, 'racists': 813, 'service': 814, 'simulation': 815, 'sweet': 816, 'common': 817, 'evil': 818, 'happening': 819, 'inauguration': 820, 'jewish': 821, 'light': 822, 'lose': 823, 'nigga': 824, 'officer': 825, 'paladinos': 826, 'places': 827, 'pussy': 828, 'science': 829, 'spread': 830, 'total': 831, 'york': 832, '1st': 833, 'criticism': 834, 'group': 835, 'issue': 836, 'jeffsessions': 837, 'males': 838, 'mc': 839, 'reverse': 840, 'trip': 841, 'account': 842, 'anti': 843, 'attention': 844, 'brand': 845, 'complete': 846, 'daddy': 847, 'hu': 848, 'idiot': 849, 'originally': 850, 'photos': 851, 'propaganda': 852, 'anxiety': 853, 'case': 854, 'everyday': 855, 'flight': 856, 'horrible': 857, 'isis': 858, 'main': 859, 'rather': 860, 'resignation': 861, 'system': 862, 'texas': 863, 'ya': 864, 'able': 865, 'americas': 866, 'believes': 867, 'bullying': 868, 'cleveland': 869, 'cologne': 870, 'company': 871, 'deep': 872, 'donald': 873, 'especially': 874, 'fo': 875, 'following': 876, 'fraud': 877, 'ibooks': 878, 'immigrants': 879, 'jan': 880, 'mil': 881, 'newswithed': 882, 'perhaps': 883, 'realize': 884, 'slavery': 885, 'smiles': 886, 'sunshine': 887, 'terrorists': 888, 'tho': 889, 'turn': 890, 'unleashed': 891, 'eat': 892, 'empty': 893, 'excellent': 894, 'fire': 895, 'jew': 896, 'moron': 897, 'profiling': 898, 'stas': 899, 'wonder': 900, 'xxx': 901, 'add': 902, 'assault': 903, 'brother': 904, 'leads': 905, 'line': 906, 'missing': 907, 'phillysuppophilly': 908, 'snapchat': 909, 'toptags': 910, 'update': 911, 'winner': 912, 'yo': 913, '20': 914, 'answer': 915, 'bitch': 916, 'experience': 917, 'fakenews': 918, 'memes': 919, 'plz': 920, 'podcast': 921, 'practice': 922, 'rwnj': 923, 'stuff': 924, 'treat': 925, 'university': 926, 'wake': 927, 'yrs': 928, 'action': 929, 'charles': 930, 'dystopian': 931, 'eah': 932, 'fan': 933, 'goodmorning': 934, 'hearing': 935, 'instead': 936, 'lack': 937, 'michelleobama': 938, 'msnbc': 939, 'national': 940, 'promote': 941, 'skin': 942, 'surprised': 943, 'takes': 944, 'within': 945, 'write': 946, '100': 947, 'can': 948, 'fall': 949, 'loves': 950, 'nasty': 951, 'not': 952, 'opinion': 953, 'parents': 954, 'public': 955, 'putinspuppet': 956, 'relax': 957, 'rude': 958, 'voters': 959, 'womens': 960, '642': 961, 'along': 962, 'aww': 963, 'comedy': 964, 'destroy': 965, 'enemies': 966, 'environmental': 967, 'female': 968, 'florida': 969, 'genocide': 970, 'jeff': 971, 'la': 972, 'laugh': 973, 'p2': 974, 'putin': 975, 'speaking': 976, 'students': 977, 'top': 978, 'tuesday': 979, 'useful': 980, 'vacation': 981, 'whiteprivilege': 982, 'allowed': 983, 'build': 984, 'disney': 985, 'episode': 986, 'favorite': 987, 'gym': 988, 'immigration': 989, 'killed': 990, 'landholding': 991, 'opposition': 992, 'piece': 993, 'political': 994, 'povey': 995, 'question': 996, 'smiling': 997, 'star': 998, 'told': 999, 'trumpsamerica': 1000, 'violent': 1001, 'walking': 1002, 'dare': 1003, 'deaths': 1004, 'dick': 1005, 'diversity': 1006, 'election': 1007, 'hardcore': 1008, 'members': 1009, 'nigger': 1010, 'prison': 1011, 'reaction': 1012, 'shame': 1013, 'space': 1014, 'was': 1015, 'arabs': 1016, 'birds': 1017, 'confirmation': 1018, 'customer': 1019, 'deletetweets': 1020, 'denounce': 1021, 'depressed': 1022, 'event': 1023, 'expected': 1024, 'fail': 1025, 'happen': 1026, 'hold': 1027, 'meeting': 1028, 'mine': 1029, 'notice': 1030, 'park': 1031, 'series': 1032, 'side': 1033, 'stands': 1034, 'tale': 1035, 'thrown': 1036, 'wh': 1037, '30': 1038, 'assholes': 1039, 'balls': 1040, 'control': 1041, 'decided': 1042, 'due': 1043, 'followers': 1044, 'force': 1045, 'form': 1046, 'haha': 1047, 'harassment': 1048, 'hatecrime': 1049, 'market': 1050, 'met': 1051, 'millions': 1052, 'minute': 1053, 'name': 1054, 'outrage': 1055, 'picoftheday': 1056, 'prayfororlando': 1057, 'purpose': 1058, 'rapist': 1059, 'red': 1060, 'sometimes': 1061, 'tgif': 1062, 'three': 1063, 'tickets': 1064, 'afternoon': 1065, 'agenda': 1066, 'ahead': 1067, 'alabama': 1068, 'argument': 1069, 'attacks': 1070, 'bday': 1071, 'blame': 1072, 'bring': 1073, 'caused': 1074, 'claim': 1075, 'correct': 1076, 'cry': 1077, 'families': 1078, 'fang': 1079, 'guilty': 1080, 'impossible': 1081, 'judge': 1082, 'keeps': 1083, 'nervous': 1084, 'outside': 1085, 'problems': 1086, 'sharpens': 1087, 'staing': 1088, 'swiftly': 1089, 'trash': 1090, 'view': 1091, 'visit': 1092, 'aampe': 1093, 'acts': 1094, 'animals': 1095, 'arrived': 1096, 'beginning': 1097, 'blicqer': 1098, 'born': 1099, 'cantwait': 1100, 'cheer': 1101, 'crying': 1102, 'except': 1103, 'expats': 1104, 'extremists': 1105, 'feminist': 1106, 'haters': 1107, 'have': 1108, 'move': 1109, 'murdering': 1110, 'nature': 1111, 'openly': 1112, 'organizations': 1113, 'probably': 1114, 'silence': 1115, 'style': 1116, 'upset': 1117, 'vine': 1118, 'whitegenocide': 1119, '70': 1120, '99c99p': 1121, 'actions': 1122, 'biggest': 1123, 'boyfriend': 1124, 'died': 1125, 'difficult': 1126, 'evening': 1127, 'facebook': 1128, 'instagram': 1129, 'killing': 1130, 'paid': 1131, 'photography': 1132, 'politicians': 1133, 'relevant': 1134, 'staed': 1135, 'statement': 1136, 'term': 1137, 'tyler': 1138, 'united': 1139, 'walk': 1140, 'water': 1141, 'wear': 1142, 'acting': 1143, 'adam': 1144, 'advice': 1145, 'allow': 1146, 'app': 1147, 'bigots': 1148, 'chance': 1149, 'cultureofdevelopment': 1150, 'definition': 1151, 'donkey': 1152, 'games': 1153, 'learn': 1154, 'leftist': 1155, 'none': 1156, 'often': 1157, 'pedophilia': 1158, 'pics': 1159, 'prayers': 1160, 'resistance': 1161, 'revenge': 1162, 'stream': 1163, 'suppoing': 1164, 'teach': 1165, 'took': 1166, 'works': 1167, 'xenophobic': 1168, 'zionism': 1169, 'adultery': 1170, 'alive': 1171, 'bout': 1172, 'clearly': 1173, 'cover': 1174, 'crime': 1175, 'definitely': 1176, 'democrats': 1177, 'eu': 1178, 'eve': 1179, 'fair': 1180, 'feels': 1181, 'gun': 1182, 'husband': 1183, 'israeli': 1184, 'kills': 1185, 'language': 1186, 'longer': 1187, 'lunch': 1188, 'ones': 1189, 'palestinian': 1190, 'pizza': 1191, 'pop': 1192, 'semitic': 1193, 'terms': 1194, 'tlot': 1195, 'ways': 1196, 'wishing': 1197, 'audiblechannels': 1198, 'blonde': 1199, 'bought': 1200, 'dinner': 1201, 'dreams': 1202, 'entire': 1203, 'glad': 1204, 'happens': 1205, 'insults': 1206, 'knows': 1207, 'listening': 1208, 'movement': 1209, 'pathetic': 1210, 'pigs': 1211, 'plans': 1212, 'pos': 1213, 'radicalisation': 1214, 'rain': 1215, 'repo': 1216, 'room': 1217, 'stereotype': 1218, 'triggered': 1219, 'whitesupremacy': 1220, 'workers': 1221, '2nd': 1222, 'abt': 1223, 'apaheid': 1224, 'booked': 1225, 'break': 1226, 'breakfast': 1227, 'conce': 1228, 'evidence': 1229, 'finished': 1230, 'happier': 1231, 'homophobia': 1232, 'imagine': 1233, 'internet': 1234, 'learning': 1235, 'lots': 1236, 'low': 1237, 'lucky': 1238, 'mexico': 1239, 'mtv': 1240, 'non': 1241, 'perry': 1242, 'powerful': 1243, 'privilege': 1244, 'promoting': 1245, 'sense': 1246, 'soul': 1247, 'sunny': 1248, 'ta': 1249, 'tragic': 1250, 'ableism': 1251, 'affirmations': 1252, 'amwriting': 1253, 'anywhere': 1254, 'boob': 1255, 'broken': 1256, 'buddy': 1257, 'cake': 1258, 'canada': 1259, 'cop': 1260, 'council': 1261, 'declared': 1262, 'equal': 1263, 'etc': 1264, 'familiar': 1265, 'feelings': 1266, 'hands': 1267, 'hrs': 1268, 'kentucky': 1269, 'leader': 1270, 'moments': 1271, 'names': 1272, 'reach': 1273, 'sending': 1274, 'spark': 1275, 'st': 1276, 'thankyou': 1277, 'traffic': 1278, 'turned': 1279, 'typical': 1280, 'version': 1281, 'vids': 1282, 'yoga': 1283, 'actor': 1284, 'beer': 1285, 'clinton': 1286, 'club': 1287, 'concept': 1288, 'debate': 1289, 'destroyed': 1290, 'drink': 1291, 'easy': 1292, 'ends': 1293, 'everybody': 1294, 'fantastic': 1295, 'final': 1296, 'fresh': 1297, 'garden': 1298, 'innocent': 1299, 'inside': 1300, 'kevin': 1301, 'levels': 1302, 'passed': 1303, 'period': 1304, 'research': 1305, 'scared': 1306, 'society': 1307, 'totally': 1308, 'training': 1309, 'unbelievable': 1310, 'unpresidented': 1311, 'voice': 1312, '24': 1313, 'blacktwitter': 1314, 'chief': 1315, 'clothes': 1316, 'colonialism': 1317, 'corruption': 1318, 'democracy': 1319, 'familys': 1320, 'gave': 1321, 'hang': 1322, 'hatespeech': 1323, 'heal': 1324, 'humans': 1325, 'iconic': 1326, 'indigenous': 1327, 'iq': 1328, 'jealous': 1329, 'joe': 1330, 'lame': 1331, 'lover': 1332, 'luck': 1333, 'married': 1334, 'mr': 1335, 'pray': 1336, 'protest': 1337, 'prove': 1338, 'reasons': 1339, 'rock': 1340, 'set': 1341, 'steal': 1342, 'tony': 1343, 'airpo': 1344, 'anniversary': 1345, 'arabic': 1346, 'beat': 1347, 'dogs': 1348, 'driver': 1349, 'em': 1350, 'friendship': 1351, 'gbp': 1352, 'heabroken': 1353, 'incest': 1354, 'lie': 1355, 'likely': 1356, 'missed': 1357, 'pain': 1358, 'plan': 1359, 'projection': 1360, 'ramadan': 1361, 'resign': 1362, 'sadly': 1363, 'serve': 1364, 'thinking': 1365, 'treasonoustrump': 1366, 'whilst': 1367, '14': 1368, 'ashamed': 1369, 'bless': 1370, 'chill': 1371, 'chinese': 1372, 'christianity': 1373, 'countdown': 1374, 'deplorable': 1375, 'dude': 1376, 'faced': 1377, 'fast': 1378, 'film': 1379, 'finding': 1380, 'fox': 1381, 'ideology': 1382, 'koreans': 1383, 'military': 1384, 'nation': 1385, 'notmypres': 1386, 'online': 1387, 'rally': 1388, 'rant': 1389, 'realized': 1390, 'rebellion': 1391, 'records': 1392, 'sexualpredator': 1393, 'simple': 1394, 'swastika': 1395, 'tears': 1396, 'thinks': 1397, 'value': 1398, 'attitude': 1399, 'behind': 1400, 'beyond': 1401, 'blatant': 1402, 'brain': 1403, 'california': 1404, 'chain': 1405, 'congrats': 1406, 'demo': 1407, 'dj': 1408, 'effect': 1409, 'ethnocentrism': 1410, 'favourite': 1411, 'flights': 1412, 'goals': 1413, 'helping': 1414, 'hopeful': 1415, 'hrc': 1416, 'huge': 1417, 'illegal': 1418, 'injustice': 1419, 'logic': 1420, 'loser': 1421, 'mark': 1422, 'memories': 1423, 'middle': 1424, 'milo': 1425, 'mma': 1426, 'moving': 1427, 'normalizing': 1428, 'number': 1429, 'phone': 1430, 'press': 1431, 'pure': 1432, 'refugees': 1433, 'reveals': 1434, 'scary': 1435, 'shitty': 1436, 'sho': 1437, 'silent': 1438, 'sing': 1439, 'spend': 1440, 'store': 1441, 'stupidity': 1442, 'survive': 1443, 'taking': 1444, 'wars': 1445, 'bag': 1446, 'bar': 1447, 'based': 1448, 'biher': 1449, 'block': 1450, 'blocked': 1451, 'chicago': 1452, 'critics': 1453, 'cut': 1454, 'door': 1455, 'england': 1456, 'english': 1457, 'exciting': 1458, 'germany': 1459, 'global': 1460, 'greed': 1461, 'happyholidays': 1462, 'hated': 1463, 'hero': 1464, 'hit': 1465, 'holidays': 1466, 'hosts': 1467, 'irish': 1468, 'jobs': 1469, 'local': 1470, 'lt': 1471, 'natural': 1472, 'okay': 1473, 'orange': 1474, 'result': 1475, 'role': 1476, 'rondarousey': 1477, 'shown': 1478, 'step': 1479, 'sucks': 1480, 'test': 1481, 'veiled': 1482, 'watched': 1483, 'animal': 1484, 'antiblackness': 1485, 'bitter': 1486, 'blamed': 1487, 'bye': 1488, 'caught': 1489, 'checked': 1490, 'chosen': 1491, 'college': 1492, 'defending': 1493, 'energy': 1494, 'everywhere': 1495, 'george': 1496, 'gives': 1497, 'hill': 1498, 'hunger': 1499, 'jesus': 1500, 'jo': 1501, 'king': 1502, 'lead': 1503, 'looked': 1504, 'mens': 1505, 'nearly': 1506, 'omits': 1507, 'opening': 1508, 'rapeculture': 1509, 'reached': 1510, 'reminder': 1511, 'remove': 1512, 'rough': 1513, 'secret': 1514, 'shiless': 1515, 'slow': 1516, 'spain': 1517, 'stage': 1518, 'staup': 1519, 'tedtalks': 1520, 'terror': 1521, 'thousands': 1522, 'trumpism': 1523, 'turning': 1524, 'vandals': 1525, 'vicinity': 1526, '17': 1527, '25': 1528, 'among': 1529, 'anybody': 1530, 'celebrating': 1531, 'cou': 1532, 'crisis': 1533, 'democraticpay': 1534, 'deserve': 1535, 'educate': 1536, 'ending': 1537, 'hopes': 1538, 'hypocrite': 1539, 'idiots': 1540, 'ignore': 1541, 'july': 1542, 'lake': 1543, 'largest': 1544, 'laws': 1545, 'mast': 1546, 'mexican': 1547, 'minority': 1548, 'opposed': 1549, 'policy': 1550, 'republicans': 1551, 'rid': 1552, 'road': 1553, 'rules': 1554, 'signs': 1555, 'suicide': 1556, 'tool': 1557, 'tragedy': 1558, 'trending': 1559, 'unless': 1560, '11': 1561, 'citizens': 1562, 'dying': 1563, 'facts': 1564, 'finger': 1565, 'foodie': 1566, 'forgot': 1567, 'fred': 1568, 'goodbye': 1569, 'govt': 1570, 'kinda': 1571, 'legend': 1572, 'makeup': 1573, 'marketing': 1574, 'marriage': 1575, 'mass': 1576, 'mountains': 1577, 'profile': 1578, 'proof': 1579, 'rac': 1580, 'release': 1581, 'russian': 1582, 'sale': 1583, 'spos': 1584, 'standing': 1585, 'victim': 1586, 'vile': 1587, 'worse': 1588, '3d': 1589, 'acceptable': 1590, 'ape': 1591, 'billion': 1592, 'bollywood': 1593, 'china': 1594, 'clean': 1595, 'completely': 1596, 'explain': 1597, 'folks': 1598, 'groups': 1599, 'growing': 1600, 'instamood': 1601, 'jcpenny': 1602, 'joseon': 1603, 'ladies': 1604, 'lawofattraction': 1605, 'member': 1606, 'nobody': 1607, 'palestine': 1608, 'progress': 1609, 'record': 1610, 'ris2016': 1611, 'sunset': 1612, 'terrorist': 1613, 'trumpusa': 1614, 'type': 1615, 'unifying': 1616, '15': 1617, 'afraid': 1618, 'anger': 1619, 'banks': 1620, 'bluntly': 1621, 'boots': 1622, 'choice': 1623, 'classism': 1624, 'congress': 1625, 'conservative': 1626, 'criminal': 1627, 'design': 1628, 'expose': 1629, 'feeding': 1630, 'festival': 1631, 'grow': 1632, 'insane': 1633, 'japanese': 1634, 'major': 1635, 'masses': 1636, 'minorities': 1637, 'narrative': 1638, 'niggers': 1639, 'note': 1640, 'noticed': 1641, 'page3': 1642, 'played': 1643, 'producers': 1644, 'progressive': 1645, 'queen': 1646, 'refugeeswelcome': 1647, 'regarding': 1648, 'renowned': 1649, 'rich': 1650, 'second': 1651, 'slut': 1652, 'station': 1653, 'task': 1654, 'train': 1655, 'vegan': 1656, 'viral': 1657, 'virginia': 1658, '600': 1659, 'ale': 1660, 'asians': 1661, 'atlanta': 1662, 'attacked': 1663, 'babies': 1664, 'blur': 1665, 'breaking': 1666, 'bs': 1667, 'btw': 1668, 'bullshit': 1669, 'cdnpoli': 1670, 'collection': 1671, 'detroit': 1672, 'difference': 1673, 'double': 1674, 'ep': 1675, 'exposed': 1676, 'fit': 1677, 'freemilo': 1678, 'genetics': 1679, 'hide': 1680, 'hillaryclinton': 1681, 'horrifying': 1682, 'including': 1683, 'interracial': 1684, 'jihad': 1685, 'joking': 1686, 'laughing': 1687, 'marx': 1688, 'nations': 1689, 'naughty': 1690, 'normal': 1691, 'occurredjust': 1692, 'oitnb': 1693, 'otherwise': 1694, 'possible': 1695, 'posted': 1696, 'prevail': 1697, 'quick': 1698, 'repoed': 1699, 'ride': 1700, 'scrubs': 1701, 'showing': 1702, 'site': 1703, 'slammed': 1704, 'southern': 1705, 'stereotypes': 1706, 'supremacy': 1707, 'syria': 1708, 'tells': 1709, 'tips': 1710, 'tlc': 1711, 'tried': 1712, 'trust': 1713, 'upon': 1714, 'youth': 1715, 'adult': 1716, 'advanced': 1717, 'agreed': 1718, 'aist': 1719, 'anal': 1720, 'antisemitic': 1721, 'area': 1722, 'arrested': 1723, 'asked': 1724, 'australian': 1725, 'boss': 1726, 'boycottdelta': 1727, 'bully': 1728, 'changes': 1729, 'committed': 1730, 'conviction': 1731, 'cud': 1732, 'delta': 1733, 'dem': 1734, 'dies': 1735, 'dory': 1736, 'draw': 1737, 'gb': 1738, 'has': 1739, 'hates': 1740, 'hating': 1741, 'hole': 1742, 'instalike': 1743, 'kindness': 1744, 'lighttherapy': 1745, 'losing': 1746, 'majority': 1747, 'mt': 1748, 'nicola': 1749, 'nose': 1750, 'patriotwatch': 1751, 'paul': 1752, 'pool': 1753, 'promise': 1754, 'puppy': 1755, 'sounds': 1756, 'tinfoilhat': 1757, 'udtapunjab': 1758, 'west': 1759, 'wet': 1760, 'winning': 1761, 'anyway': 1762, 'ash': 1763, 'average': 1764, 'bailed': 1765, 'begin': 1766, 'bernanke': 1767, 'bet': 1768, 'bitches': 1769, 'bread': 1770, 'busy': 1771, 'calm': 1772, 'changed': 1773, 'civil': 1774, 'classic': 1775, 'confused': 1776, 'continue': 1777, 'countries': 1778, 'documented': 1779, 'dumping': 1780, 'enduring': 1781, 'false': 1782, 'feminists': 1783, 'four': 1784, 'fridayfeeling': 1785, 'garbage': 1786, 'ground': 1787, 'harm': 1788, 'hitler': 1789, 'homophobe': 1790, 'hood': 1791, 'indian': 1792, 'instadaily': 1793, 'intellectual': 1794, 'issues': 1795, 'january': 1796, 'korematsu': 1797, 'learned': 1798, 'massive': 1799, 'minutes': 1800, 'nbafinals': 1801, 'noh': 1802, 'offensive': 1803, 'prejudice': 1804, 'pride': 1805, 'quest': 1806, 'quit': 1807, 'representation': 1808, 'return': 1809, 'socialmedia': 1810, 'teacher': 1811, 'twice': 1812, 'ty': 1813, 'valid': 1814, 'voting': 1815, 'wine': 1816, 'zionazis': 1817, '2017in3words': 1818, '60s': 1819, '75': 1820, 'attracted': 1821, 'biden': 1822, 'blk': 1823, 'blogger': 1824, 'camp': 1825, 'cats': 1826, 'celebration': 1827, 'chase': 1828, 'chat': 1829, 'coal': 1830, 'dictator': 1831, 'dudes': 1832, 'eating': 1833, 'enjoying': 1834, 'excuse': 1835, 'figure': 1836, 'france': 1837, 'golf': 1838, 'horny': 1839, 'how': 1840, 'hungry': 1841, 'kiss': 1842, 'leaves': 1843, 'like4like': 1844, 'literally': 1845, 'loose': 1846, 'loss': 1847, 'mmusimaimane': 1848, 'msm': 1849, 'oops': 1850, 'pc': 1851, 'pink': 1852, 'policing': 1853, 'prince': 1854, 'project': 1855, 'salad': 1856, 'slant': 1857, 'songs': 1858, 'straight': 1859, 'thailand': 1860, 'turns': 1861, 'wa': 1862, 'web': 1863, 'whiteness': 1864, 'wtf2016': 1865, 'xians': 1866, 'xmas': 1867, 'xx': 1868, '13': 1869, 'asshole': 1870, 'available': 1871, 'bears': 1872, 'celebrates': 1873, 'chant': 1874, 'clear': 1875, 'commercial': 1876, 'count': 1877, 'create': 1878, 'dancing': 1879, 'deplorables': 1880, 'doom': 1881, 'emotions': 1882, 'endof2016': 1883, 'faces': 1884, 'folx': 1885, 'front': 1886, 'giant': 1887, 'giving': 1888, 'guide': 1889, 'helps': 1890, 'indians': 1891, 'inspirational': 1892, 'journalism': 1893, 'kaepernick': 1894, 'key': 1895, 'kinky': 1896, 'microaggressions': 1897, 'ordered': 1898, 'orlandoshooting': 1899, 'pisses': 1900, 'proved': 1901, 'punjab': 1902, 'rainbow': 1903, 'rational': 1904, 'relationship': 1905, 'rose': 1906, 'scumbag': 1907, 'shy': 1908, 'sister': 1909, 'source': 1910, 'suck': 1911, 'tedatibm': 1912, 'thread': 1913, 'trial': 1914, 'trumpworld': 1915, 'valuechain': 1916, 'vast': 1917, 'wakeup': 1918, 'waste': 1919, 'woke': 1920, 'adventure': 1921, 'assume': 1922, 'bewitched': 1923, 'bike': 1924, 'cancel': 1925, 'cavs': 1926, 'christmaseve': 1927, 'continues': 1928, 'cuz': 1929, 'david': 1930, 'deadliest': 1931, 'decision': 1932, 'dispose': 1933, 'drunk': 1934, 'east': 1935, 'expanse': 1936, 'goodvibes': 1937, 'graduation': 1938, 'incredible': 1939, 'inmates': 1940, 'inshot': 1941, 'interested': 1942, 'killer': 1943, 'kitchen': 1944, 'known': 1945, 'lifelessons': 1946, 'lonely': 1947, 'lovelife': 1948, 'mental': 1949, 'nba': 1950, 'negligence': 1951, 'neighbor': 1952, 'netflix': 1953, 'offer': 1954, 'ohio': 1955, 'oppounity': 1956, 'ove': 1957, 'paris': 1958, 'popular': 1959, 'poster': 1960, 'preach': 1961, 'presidents': 1962, 'ra': 1963, 'radio': 1964, 'ran': 1965, 'reject': 1966, 'ripchristina': 1967, 'rohingya': 1968, 'rooster': 1969, 'santa': 1970, 'sealed': 1971, 'serious': 1972, 'shocking': 1973, 'shoes': 1974, 'sir': 1975, 'slurs': 1976, 'socially': 1977, 'title': 1978, 'town': 1979, 'vegas': 1980, 'versace': 1981, 'vibes': 1982, 'village': 1983, 'weird': 1984, 'whores': 1985, 'workplace': 1986, 'wud': 1987, 'acab': 1988, 'album': 1989, 'although': 1990, 'arrive': 1991, 'blood': 1992, 'censorship': 1993, 'choices': 1994, 'classes': 1995, 'communist': 1996, 'covering': 1997, 'di': 1998, 'disgraceful': 1999, 'douchebag': 2000, 'empire': 2001, 'exactly': 2002, 'former': 2003, 'funding': 2004, 'goaded': 2005, 'gorgeous': 2006, 'held': 2007, 'launch': 2008, 'lesson': 2009, 'link': 2010, 'messi': 2011, 'monster': 2012, 'mostly': 2013, 'movies': 2014, 'multiple': 2015, 'nails': 2016, 'nofilter': 2017, 'planning': 2018, 'platform': 2019, 'princess': 2020, 'rapacious': 2021, 'religions': 2022, 'repeat': 2023, 'restrained': 2024, 'review': 2025, 'ring': 2026, 'riyadh': 2027, 'rooted': 2028, 'roots': 2029, 'saleh': 2030, 'sees': 2031, 'segregation': 2032, 'shoot': 2033, 'sight': 2034, 'sky': 2035, 'sma': 2036, 'someones': 2037, 'tea': 2038, 'teapay': 2039, 'travelled': 2040, 'weather': 2041, 'womenvote': 2042, 'workout': 2043, 'yummy': 2044, 'adl': 2045, 'analytics': 2046, 'apple': 2047, 'arrest': 2048, 'awful': 2049, 'axed': 2050, 'basketball': 2051, 'becoming': 2052, 'blatantly': 2053, 'bot': 2054, 'brilliant': 2055, 'brought': 2056, 'brutality': 2057, 'colleague': 2058, 'controversial': 2059, 'cretin': 2060, 'cult': 2061, 'dems': 2062, 'development': 2063, 'e32016': 2064, 'effective': 2065, 'effo': 2066, 'epic': 2067, 'ethnicity': 2068, 'ff': 2069, 'fucktrump': 2070, 'hi': 2071, 'hugs': 2072, 'imperialism': 2073, 'imperialsm': 2074, 'interest': 2075, 'leaders': 2076, 'meditation': 2077, 'mo': 2078, 'montana': 2079, 'msg': 2080, 'near': 2081, 'oiler': 2082, 'presidentelect': 2083, 'previous': 2084, 'pulse': 2085, 'questions': 2086, 'racis': 2087, 'radicalism': 2088, 'relationships': 2089, 'relaxing': 2090, 'remark': 2091, 'retail': 2092, 'round': 2093, 'route66': 2094, 'serves': 2095, 'shake': 2096, 'shameful': 2097, 'sickening': 2098, 'sides': 2099, 'snowflakes': 2100, 'socialjustice': 2101, 'sold': 2102, 'sparks': 2103, 'sundown': 2104, 'systemicracism': 2105, 'towns': 2106, 'travellers': 2107, 'tweet4taiji': 2108, 'wearing': 2109, 'worked': 2110, 'wout': 2111, 'aande': 2112, 'adamsaleh': 2113, 'africans': 2114, 'ambassador': 2115, 'animation': 2116, 'behappy': 2117, 'bigoted': 2118, 'britain': 2119, 'broke': 2120, 'canucks': 2121, 'challenge': 2122, 'cheers': 2123, 'commercials': 2124, 'communism': 2125, 'congratulations': 2126, 'considering': 2127, 'coverage': 2128, 'cr7': 2129, 'criminals': 2130, 'cutest': 2131, 'dictionary': 2132, 'dr': 2133, 'drinks': 2134, 'emboldened': 2135, 'empower': 2136, 'entering': 2137, 'entry': 2138, 'equate': 2139, 'exclusive': 2140, 'faith': 2141, 'feed': 2142, 'fighting': 2143, 'flower': 2144, 'gov': 2145, 'gweh': 2146, 'ha': 2147, 'heabreaking': 2148, 'ideas': 2149, 'klan': 2150, 'kwanzaa': 2151, 'lecture': 2152, 'likes': 2153, 'lmao': 2154, 'mcdonalds': 2155, 'meme': 2156, 'merrychristmas': 2157, 'misogynistic': 2158, 'motherfucker': 2159, 'mouth': 2160, 'offered': 2161, 'offers': 2162, 'piss': 2163, 'player': 2164, 'positivity': 2165, 'products': 2166, 'protect': 2167, 'psychological': 2168, 'rapechucktodd': 2169, 'received': 2170, 'singing': 2171, 'slander': 2172, 'smoking': 2173, 'stl': 2174, 'stone': 2175, 'stuck': 2176, 'suit': 2177, 'syndrome': 2178, 'ticket': 2179, 'union': 2180, 'wing': 2181, 'worry': 2182, 'writing': 2183, 'wypipo': 2184, 'address': 2185, 'africanamerican': 2186, 'alleged': 2187, 'alllivesmatter': 2188, 'amjoy': 2189, 'anime': 2190, 'anonymous': 2191, 'antiwhite': 2192, 'apes': 2193, 'appeal': 2194, 'appreciate': 2195, 'argued': 2196, 'arms': 2197, 'asia': 2198, 'basket': 2199, 'belgian': 2200, 'beloy': 2201, 'benefits': 2202, 'beware': 2203, 'bill': 2204, 'bonhoeffer': 2205, 'bored': 2206, 'boundaries': 2207, 'brief': 2208, 'chair': 2209, 'character': 2210, 'conflate': 2211, 'conman': 2212, 'credit': 2213, 'cruel': 2214, 'data': 2215, 'diet': 2216, 'divisive': 2217, 'ego': 2218, 'embarrass': 2219, 'essential': 2220, 'events': 2221, 'experiences': 2222, 'fabulous': 2223, 'felony': 2224, 'foxnews': 2225, 'gaslighting': 2226, 'gif': 2227, 'gratitude': 2228, 'greatest': 2229, 'happynewyear': 2230, 'hashtag': 2231, 'heas': 2232, 'hire': 2233, 'hype': 2234, 'incredibly': 2235, 'intolerant': 2236, 'involved': 2237, 'john': 2238, 'journey': 2239, 'judges': 2240, 'kick': 2241, 'lazy': 2242, 'mars': 2243, 'meccalive': 2244, 'mixed': 2245, 'ncaa': 2246, 'ness': 2247, 'page': 2248, 'pictures': 2249, 'postbrexit': 2250, 'protests': 2251, 'realitycheck': 2252, 'recordeven': 2253, 'remain': 2254, 'roll': 2255, 'seats': 2256, 'sell': 2257, 'sharing': 2258, 'sound': 2259, 'spit': 2260, 'study': 2261, 'successful': 2262, 'supposed': 2263, 'teaching': 2264, 'throw': 2265, 'touch': 2266, 'trophy': 2267, 'tuckercarlson': 2268, 'turmoil': 2269, 'ugh': 2270, 'ukraine': 2271, 'unmasking': 2272, 'varying': 2273, 'wales': 2274, 'weasel': 2275, 'whitewashing': 2276, 'wild': 2277, 'workshop': 2278, 'advocate': 2279, 'asses': 2280, 'bama': 2281, 'bride': 2282, 'bringing': 2283, 'bus': 2284, 'career': 2285, 'catch': 2286, 'chicks': 2287, 'childish': 2288, 'christina': 2289, 'cops': 2290, 'customers': 2291, 'deny': 2292, 'destruction': 2293, 'dismantle': 2294, 'drawing': 2295, 'drive': 2296, 'edinburgh': 2297, 'eh': 2298, 'european': 2299, 'fp': 2300, 'frustrated': 2301, 'geographic': 2302, 'german': 2303, 'governments': 2304, 'greatness': 2305, 'guardian': 2306, 'highlycontested': 2307, 'honestly': 2308, 'hopefully': 2309, 'interview': 2310, 'keeping': 2311, 'manager': 2312, 'modern': 2313, 'newyearseve': 2314, 'nigeria': 2315, 'offical': 2316, 'paying': 2317, 'performance': 2318, 'prez': 2319, 'program': 2320, 'released': 2321, 'repugnant': 2322, 'restaurant': 2323, 'rhetoric': 2324, 'richardspencer': 2325, 'row': 2326, 'saved': 2327, 'sept': 2328, 'shallow': 2329, 'somebody': 2330, 'soros': 2331, 'spring': 2332, 'stillwithher': 2333, 'stlouis': 2334, 'stunning': 2335, 'terrible': 2336, 'terry': 2337, 'thug': 2338, 'til': 2339, 'weapon': 2340, 'were': 2341, 'whitesupremacist': 2342, 'wins': 2343, 'wisdom': 2344, 'yup': 2345, 'yur': 2346, '13th': 2347, '142017': 2348, '911': 2349, 'abusive': 2350, 'aicles': 2351, 'allies': 2352, 'amandanunes': 2353, 'amrica': 2354, 'antiamerican': 2355, 'antisemite': 2356, 'bestie': 2357, 'bluelivesmatter': 2358, 'books': 2359, 'capitalism': 2360, 'ceain': 2361, 'charge': 2362, 'charging': 2363, 'client': 2364, 'clients': 2365, 'coward': 2366, 'crack': 2367, 'craziness': 2368, 'current': 2369, 'danger': 2370, 'de': 2371, 'definitions': 2372, 'e3': 2373, 'emotional': 2374, 'empathy': 2375, 'enteainment': 2376, 'euref': 2377, 'fell': 2378, 'finish': 2379, 'fl': 2380, 'foodporn': 2381, 'gotten': 2382, 'hotel': 2383, 'humpday': 2384, 'hv': 2385, 'icymi': 2386, 'instructed': 2387, 'invited': 2388, 'lebron': 2389, 'manchester': 2390, 'milk': 2391, 'millennials': 2392, 'misogynists': 2393, 'motivate': 2394, 'moved': 2395, 'murdered': 2396, 'neck': 2397, 'netanyahu': 2398, 'newshow': 2399, 'newyear2017': 2400, 'obstruction': 2401, 'overused': 2402, 'painting': 2403, 'passion': 2404, 'peaceful': 2405, 'posts': 2406, 'preordered': 2407, 'priyankachopra': 2408, 'professor': 2409, 'pueorico': 2410, 'quantico': 2411, 'racially': 2412, 'raygun': 2413, 'recent': 2414, 'reinforced': 2415, 'relatives': 2416, 'replies': 2417, 'repoer': 2418, 'request': 2419, 'retard': 2420, 'risk': 2421, 'september': 2422, 'shocked': 2423, 'shootings': 2424, 'sit': 2425, 'sites': 2426, 'snapshot': 2427, 'sourcenation': 2428, 'stem': 2429, 'stronger': 2430, 'student': 2431, 'subjected': 2432, 'suppoer': 2433, 'talent': 2434, 'thependulum': 2435, 'thieves': 2436, 'thin': 2437, 'tour': 2438, 'tweeted': 2439, 'uncle': 2440, 'unfounately': 2441, 'unity': 2442, 'vehicle': 2443, 'website': 2444, 'whitenationalist': 2445, 'winter': 2446, 'yr': 2447, '2000': 2448, '2008': 2449, '21': 2450, 'abe': 2451, 'ads': 2452, 'afp': 2453, 'ageism': 2454, 'alt': 2455, 'badly': 2456, 'barack': 2457, 'biblical': 2458, 'biherism': 2459, 'brokers': 2460, 'building': 2461, 'card': 2462, 'chaplin': 2463, 'conflates': 2464, 'cuomo': 2465, 'democratic': 2466, 'describe': 2467, 'dress': 2468, 'driving': 2469, 'ebay': 2470, 'elect': 2471, 'embrace': 2472, 'eugenics': 2473, 'exist': 2474, 'findingdory': 2475, 'flapping': 2476, 'general': 2477, 'girlfriend': 2478, 'goodnight': 2479, 'headlines': 2480, 'humiliated': 2481, 'i95n': 2482, 'incessant': 2483, 'inclusivity': 2484, 'info': 2485, 'invasion': 2486, 'leakage': 2487, 'littering': 2488, 'location': 2489, 'lord': 2490, 'lyrics': 2491, 'master': 2492, 'mi': 2493, 'michigan': 2494, 'mistake': 2495, 'mustread': 2496, 'myanmar': 2497, 'neutrality': 2498, 'nonsensical': 2499, 'nytimes': 2500, 'occur': 2501, 'outlets': 2502, 'packing': 2503, 'patience': 2504, 'praying': 2505, 'presented': 2506, 'psa': 2507, 'quiet': 2508, 'rates': 2509, 'reveal': 2510, 'rise': 2511, 'rsf': 2512, 'sarcasm': 2513, 'screaming': 2514, 'sedition': 2515, 'shine': 2516, 'speaks': 2517, 'squad': 2518, 'sums': 2519, 'treats': 2520, 'uae': 2521, 'unstoppable': 2522, 'warriors': 2523, 'weak': 2524, 'wrote': 2525, 'yorkers': 2526, 'zero': 2527, '148': 2528, '5th': 2529, 'accusations': 2530, 'added': 2531, 'announces': 2532, 'august': 2533, 'austin': 2534, 'base': 2535, 'bashing': 2536, 'basis': 2537, 'behavior': 2538, 'bestfriend': 2539, 'box': 2540, 'brings': 2541, 'bro': 2542, 'cameron': 2543, 'cc': 2544, 'childhood': 2545, 'chose': 2546, 'classy': 2547, 'clothing': 2548, 'connecting': 2549, 'counting': 2550, 'creative': 2551, 'crowd': 2552, 'cultism': 2553, 'dc': 2554, 'dec': 2555, 'disgrace': 2556, 'disrespect': 2557, 'districts': 2558, 'dollar': 2559, 'easier': 2560, 'economic': 2561, 'educational': 2562, 'eff': 2563, 'elections': 2564, 'erased': 2565, 'establishment': 2566, 'et': 2567, 'exit': 2568, 'fab': 2569, 'fellow': 2570, 'flying': 2571, 'gamedev': 2572, 'graduated': 2573, 'grew': 2574, 'heading': 2575, 'heaven': 2576, 'hypocrisy': 2577, 'ice': 2578, 'inflame': 2579, 'information': 2580, 'inlove': 2581, 'intolerance': 2582, 'island': 2583, 'jerusalem': 2584, 'knowing': 2585, 'level': 2586, 'lifes': 2587, 'losses': 2588, 'macdonald': 2589, 'mall': 2590, 'march': 2591, 'materials': 2592, 'mayor': 2593, 'menorah': 2594, 'mentalhealth': 2595, 'mentioned': 2596, 'mustfall': 2597, 'neighborhoods': 2598, 'nights': 2599, 'nonracist': 2600, 'path': 2601, 'performing': 2602, 'pls': 2603, 'posing': 2604, 'positivevibes': 2605, 'precisely': 2606, 'process': 2607, 'progressives': 2608, 'publicly': 2609, 'racebaiting': 2610, 'racemix': 2611, 'rambling': 2612, 'reprehensible': 2613, 'responded': 2614, 'retro': 2615, 'riddance': 2616, 'sales': 2617, 'scam': 2618, 'scream': 2619, 'settlement': 2620, 'shi': 2621, 'singer': 2622, 'sisters': 2623, 'situation': 2624, 'spot': 2625, 'springst': 2626, 'starbucks': 2627, 'statements': 2628, 'stoppe': 2629, 'taken': 2630, 'tattoo': 2631, 'tax': 2632, 'trade': 2633, 'ufc': 2634, 'uniteblue': 2635, 'universities': 2636, 'upf': 2637, 'uses': 2638, 'warning': 2639, 'yea': 2640, 'zuma': 2641, '08': 2642, '1200': 2643, '95': 2644, 'aboion': 2645, 'activist': 2646, 'affected': 2647, 'andor': 2648, 'announce': 2649, 'apologize': 2650, 'architecture': 2651, 'astrologer': 2652, 'b4': 2653, 'backlash': 2654, 'bae': 2655, 'begins': 2656, 'bhakt': 2657, 'birdsofafeather': 2658, 'blogpost': 2659, 'buliding': 2660, 'butt': 2661, 'caturday': 2662, 'centre': 2663, 'champ': 2664, 'clintons': 2665, 'condescension': 2666, 'couples': 2667, 'damned': 2668, 'dating': 2669, 'despise': 2670, 'doctor': 2671, 'dumbest': 2672, 'dumptrump': 2673, 'endorses': 2674, 'entrepreneur': 2675, 'escaped': 2676, 'fascists': 2677, 'felt': 2678, 'footage': 2679, 'fyi': 2680, 'goodluck': 2681, 'grace': 2682, 'hawaii': 2683, 'helpcovedolphins': 2684, 'hoping': 2685, 'huh': 2686, 'hus': 2687, 'inferior': 2688, 'james': 2689, 'lds': 2690, 'libcrib': 2691, 'lunatic': 2692, 'malema': 2693, 'mansplaining': 2694, 'melancholy': 2695, 'mere': 2696, 'mins': 2697, 'mummersparade': 2698, 'nick': 2699, 'niggas': 2700, 'ocean': 2701, 'ootd': 2702, 'oscar': 2703, 'pastor': 2704, 'perform': 2705, 'piano': 2706, 'plettenburgbay': 2707, 'poopin': 2708, 'potter': 2709, 'props': 2710, 'prosecute': 2711, 'rage': 2712, 'resolution': 2713, 'respond': 2714, 'rnc': 2715, 'shackles': 2716, 'sheets': 2717, 'sitting': 2718, 'slap': 2719, 'soccer': 2720, 'spent': 2721, 'spirit': 2722, 'stones': 2723, 'strength': 2724, 'stress': 2725, 'thegreenpalmcottage': 2726, 'therapy': 2727, 'thiza': 2728, 'thyini': 2729, 'transition': 2730, 'trc': 2731, 'tuned': 2732, 'tyranny': 2733, 'unappetizing': 2734, 'venue': 2735, 'views': 2736, 'westerncape': 2737, '1117': 2738, '14th': 2739, '16': 2740, '700000': 2741, 'according': 2742, 'bacon': 2743, 'ban': 2744, 'banned': 2745, 'battle': 2746, 'bff': 2747, 'bih': 2748, 'blast': 2749, 'bodies': 2750, 'breaks': 2751, 'brothers': 2752, 'cards': 2753, 'carlpaladinoelection': 2754, 'childrens': 2755, 'claimed': 2756, 'cochairman': 2757, 'colinpowell': 2758, 'colour': 2759, 'con': 2760, 'condoleezzarice': 2761, 'condone': 2762, 'controversy': 2763, 'cooking': 2764, 'crap': 2765, 'crashed': 2766, 'crew': 2767, 'defense': 2768, 'defunding': 2769, 'deliberately': 2770, 'discrepancy': 2771, 'dive': 2772, 'donation': 2773, 'dontbuythesun': 2774, 'doublestandards': 2775, 'employees': 2776, 'engage': 2777, 'essentialoils': 2778, 'euro': 2779, 'excouncillor': 2780, 'exists': 2781, 'explains': 2782, 'fed': 2783, 'filled': 2784, 'fixed': 2785, 'fuher': 2786, 'goodtimes': 2787, 'gravity': 2788, 'greece': 2789, 'grimmie': 2790, 'hacketts': 2791, 'heels': 2792, 'horrific': 2793, 'humour': 2794, 'ibiza': 2795, 'idwp': 2796, 'immature': 2797, 'indeed': 2798, 'inspired': 2799, 'inuit': 2800, 'iran': 2801, 'ireland': 2802, 'ivanka': 2803, 'jimmy': 2804, 'laughter': 2805, 'league': 2806, 'lgbtq': 2807, 'lights': 2808, 'liz': 2809, 'logins': 2810, 'loveislove': 2811, 'loveit': 2812, 'mexicans': 2813, 'mouthpiece': 2814, 'mrampmrs': 2815, 'murder': 2816, 'narrow': 2817, 'negative': 2818, 'neighbors': 2819, 'neonazis': 2820, 'nightclub': 2821, 'nsfw': 2822, 'offended': 2823, 'organiser': 2824, 'ouch': 2825, 'p21': 2826, 'performer': 2827, 'planet': 2828, 'politician': 2829, 'queue': 2830, 'radical': 2831, 'rags': 2832, 'repost': 2833, 'represent': 2834, 'resource': 2835, 'results': 2836, 'ripchristinagrimmie': 2837, 'rother': 2838, 'ruin': 2839, 'savor': 2840, 'scapelliti': 2841, 'scrutinize': 2842, 'seconds': 2843, 'secretaryofstate': 2844, 'simply': 2845, 'sociopath': 2846, 'spiced': 2847, 'spoke': 2848, 'status': 2849, 'studio': 2850, 'sturgeon': 2851, 'suspected': 2852, 'taught': 2853, 'tories': 2854, 'toy': 2855, 'treacy': 2856, 'tube': 2857, 'types': 2858, 'unfair': 2859, 'unleashes': 2860, 'usgtgtgt': 2861, 'wasted': 2862, 'wings': 2863, 'wise': 2864, 'xoxo': 2865, '1930s': 2866, 'al': 2867, 'antiislamist': 2868, 'arresting': 2869, 'athletes': 2870, 'attempt': 2871, 'awards': 2872, 'aware': 2873, 'bds': 2874, 'bliss': 2875, 'blunt': 2876, 'bouncingbaby': 2877, 'bridge': 2878, 'cameronstaff': 2879, 'camstaff': 2880, 'cannon': 2881, 'christ': 2882, 'clarity': 2883, 'click': 2884, 'climatechange': 2885, 'clock': 2886, 'closeted': 2887, 'confession': 2888, 'copy': 2889, 'coronial': 2890, 'cowardcops': 2891, 'cracker': 2892, 'creating': 2893, 'cross': 2894, 'cultures': 2895, 'damon': 2896, 'delicious': 2897, 'differently': 2898, 'duck': 2899, 'endorse': 2900, 'everyones': 2901, 'exercise': 2902, 'feet': 2903, 'feliz': 2904, 'fine': 2905, 'freedoms': 2906, 'fully': 2907, 'gate': 2908, 'genius': 2909, 'gig': 2910, 'greenspan': 2911, 'greenspans': 2912, 'growth': 2913, 'grp': 2914, 'hamids': 2915, 'handsome': 2916, 'happiest': 2917, 'harder': 2918, 'holds': 2919, 'horribly': 2920, 'igers': 2921, 'indianapolis': 2922, 'injured': 2923, 'inquest': 2924, 'interesting': 2925, 'issued': 2926, 'italian': 2927, 'jim': 2928, 'journo': 2929, 'kindle': 2930, 'kurds': 2931, 'lede': 2932, 'liked': 2933, 'lmfao': 2934, 'lock': 2935, 'losangeles': 2936, 'mama': 2937, 'marks': 2938, 'melancholymusic': 2939, 'michael': 2940, 'mike': 2941, 'minions': 2942, 'misogny': 2943, 'mysoginist': 2944, 'negro': 2945, 'neilerikson': 2946, 'nuff': 2947, 'numbers': 2948, 'odd': 2949, 'ofc': 2950, 'officerstaff': 2951, 'overwhelmingly': 2952, 'parenting': 2953, 'per': 2954, 'pissedoffitalian': 2955, 'plane': 2956, 'plus': 2957, 'product': 2958, 'profit': 2959, 'proven': 2960, 'pt': 2961, 'quality': 2962, 'quoteoftheday': 2963, 'races': 2964, 'rat': 2965, 'reflect': 2966, 'refuted': 2967, 'regressive': 2968, 'removed': 2969, 'rewrite': 2970, 'salt': 2971, 'sam': 2972, 'seeks': 2973, 'sends': 2974, 'senseless': 2975, 'sequel': 2976, 'session': 2977, 'shill': 2978, 'should': 2979, 'shuckinandjivin': 2980, 'silly': 2981, 'snowball': 2982, 'societies': 2983, 'sooo': 2984, 'spew': 2985, 'spin': 2986, 'stalking': 2987, 'stars': 2988, 'subtle': 2989, 'survey': 2990, 'swimming': 2991, 'taxpayers': 2992, 'threats': 2993, 'timing': 2994, 'topic': 2995, 'toures': 2996, 'traitorump': 2997, 'ultimately': 2998, 'ushistory': 2999, 'wade': 3000, 'whiteguys': 3001, 'whoever': 3002, 'yep': 3003, 'yum': 3004, '1000x': 3005, '1625': 3006, '21st': 3007, '31': 3008, '4th': 3009, 'accepted': 3010, 'affirmativeaction': 3011, 'airingofthegrievances': 3012, 'alligator': 3013, 'amerikkka': 3014, 'april': 3015, 'arrogant': 3016, 'asianmen': 3017, 'attorney': 3018, 'ball': 3019, 'band': 3020, 'barrackobama': 3021, 'beauregard': 3022, 'becomes': 3023, 'birmingham': 3024, 'blackonblack': 3025, 'blackwomen': 3026, 'bonuses': 3027, 'bunch': 3028, 'cam': 3029, 'candide': 3030, 'center': 3031, 'challenged': 3032, 'chaos': 3033, 'charity': 3034, 'cheap': 3035, 'chocolate': 3036, 'christmasadve': 3037, 'cities': 3038, 'climatecrimes': 3039, 'clip': 3040, 'closed': 3041, 'colombia': 3042, 'columbiamd': 3043, 'comedians': 3044, 'complain': 3045, 'condition': 3046, 'cons': 3047, 'consequences': 3048, 'courage': 3049, 'coz': 3050, 'created': 3051, 'crusader': 3052, 'datingapps': 3053, 'dealing': 3054, 'define': 3055, 'dig': 3056, 'discussed': 3057, 'diy': 3058, 'documentary': 3059, 'dogwhistle': 3060, 'drug': 3061, 'dwyane': 3062, 'eac': 3063, 'ecocide': 3064, 'edm': 3065, 'elitist': 3066, 'empowerment': 3067, 'epidemic': 3068, 'evolution': 3069, 'exhausted': 3070, 'extremely': 3071, 'extremist': 3072, 'f4f': 3073, 'fashio': 3074, 'fb': 3075, 'films': 3076, 'finepass': 3077, 'flagday2016': 3078, 'folk': 3079, 'foolin': 3080, 'gamer': 3081, 'gaming': 3082, 'generation': 3083, 'gf': 3084, 'goodday': 3085, 'google': 3086, 'graduates': 3087, 'greentown': 3088, 'help700': 3089, 'henrico': 3090, 'highfashion': 3091, 'historic': 3092, 'hocomd': 3093, 'hocoschools': 3094, 'hoe': 3095, 'impact': 3096, 'impoance': 3097, 'innovative': 3098, 'iphone': 3099, 'irony': 3100, 'istanbul': 3101, 'jeffery': 3102, 'josephjett': 3103, 'kitty': 3104, 'kumbaya': 3105, 'l4l': 3106, 'launched': 3107, 'lee': 3108, 'lifeisgood': 3109, 'lo': 3110, 'lure': 3111, 'magic': 3112, 'makeamericagreatagain': 3113, 'malia': 3114, 'match': 3115, 'multimillion': 3116, 'nationality': 3117, 'nye': 3118, 'paner': 3119, 'philandocastile': 3120, 'plethora': 3121, 'poland': 3122, 'popularvote': 3123, 'porns': 3124, 'pple': 3125, 'princecharles': 3126, 'private': 3127, 'projecting': 3128, 'rapists': 3129, 'requirement': 3130, 'resigned': 3131, 'resolve': 3132, 'runs': 3133, 'safety': 3134, 'sat': 3135, 'saturda': 3136, 'scene': 3137, 'screening': 3138, 'settlements': 3139, 'sexuality': 3140, 'shelvey': 3141, 'showed': 3142, 'slave': 3143, 'smoke': 3144, 'snowflake': 3145, 'societys': 3146, 'spo': 3147, 'stick': 3148, 'takuyakimura': 3149, 'techjunkiejhthe': 3150, 'threatening': 3151, 'treacherous': 3152, 'trumptrain': 3153, 'tupperwear': 3154, 'unforgettable': 3155, 'unpacks': 3156, 'uyanze': 3157, 'values': 3158, 'vps': 3159, 'waiter': 3160, 'waitingf': 3161, 'walls': 3162, 'warm': 3163, 'warns': 3164, 'wednesdaywisdom': 3165, 'wellness': 3166, 'whitefish': 3167, 'wildest': 3168, 'wilsons': 3169, 'womenmalayalam': 3170, 'woodrow': 3171, '247': 3172, '3rd': 3173, 'absurd': 3174, 'access': 3175, 'across': 3176, 'agitator': 3177, 'amazed': 3178, 'annoyed': 3179, 'apps': 3180, 'asks': 3181, 'assumptions': 3182, 'bailey': 3183, 'belongs': 3184, 'benghazi': 3185, 'biased': 3186, 'bills': 3187, 'binds': 3188, 'bjp': 3189, 'blackandwhite': 3190, 'blackpeople': 3191, 'boo': 3192, 'caer': 3193, 'canadian': 3194, 'candidates': 3195, 'cbb': 3196, 'ccot': 3197, 'celtic': 3198, 'cheerleader': 3199, 'christian': 3200, 'circle': 3201, 'classroom': 3202, 'co': 3203, 'coach': 3204, 'colts': 3205, 'commonsense': 3206, 'concur': 3207, 'corrupt': 3208, 'couesy': 3209, 'cox': 3210, 'cream': 3211, 'crude': 3212, 'disabilities': 3213, 'discriminating': 3214, 'ed': 3215, 'educating': 3216, 'emails': 3217, 'employee': 3218, 'enlightenment': 3219, 'ex': 3220, 'exploitation': 3221, 'fa': 3222, 'facepalm': 3223, 'failure': 3224, 'fave': 3225, 'finest': 3226, 'fk': 3227, 'foods': 3228, 'fragility': 3229, 'fries': 3230, 'friyay': 3231, 'funeral': 3232, 'gangs': 3233, 'gtgt': 3234, 'guncontrol': 3235, 'handmade': 3236, 'hilarious': 3237, 'hip': 3238, 'homicides': 3239, 'hospital': 3240, 'illegalguns': 3241, 'illegitimate': 3242, 'image': 3243, 'imwithher': 3244, 'inequity': 3245, 'islamofascist': 3246, 'italy': 3247, 'janeelliot': 3248, 'jessica': 3249, 'journal': 3250, 'justicereform': 3251, 'justsaying': 3252, 'kenya': 3253, 'keys': 3254, 'ki': 3255, 'legal': 3256, 'letstalkabout': 3257, 'lgbti': 3258, 'likeforlike': 3259, 'listened': 3260, 'madness': 3261, 'material': 3262, 'matters': 3263, 'mercy': 3264, 'mon': 3265, 'monetary': 3266, 'moon': 3267, 'morganfreeman': 3268, 'mum': 3269, 'nbavote': 3270, 'oligarch': 3271, 'opkkk': 3272, 'optrump': 3273, 'outrageous': 3274, 'pen': 3275, 'plutocrat': 3276, 'poc': 3277, 'pointed': 3278, 'population': 3279, 'posting': 3280, 'prayer': 3281, 'pres': 3282, 'push': 3283, 'realise': 3284, 'refuses': 3285, 'rep': 3286, 'resent': 3287, 'rule': 3288, 'screen': 3289, 'sent': 3290, 'sh': 3291, 'slip': 3292, 'slur': 3293, 'socialjus': 3294, 'steps': 3295, 'stockholm': 3296, 'sunderland': 3297, 'surrounded': 3298, 'talented': 3299, 'teeth': 3300, 'theatre': 3301, 'thepoliticalmillennial': 3302, 'thomas': 3303, 'threat': 3304, 'toddler': 3305, 'tonights': 3306, 'toronto': 3307, 'touches': 3308, 'ultra': 3309, 'victoria': 3310, 'videogames': 3311, 'willing': 3312, 'window': 3313, 'zionazi': 3314, '1000': 3315, '18': 3316, '63': 3317, 'actors': 3318, 'aidingenemies': 3319, 'air': 3320, 'alexjones': 3321, 'ampor': 3322, 'analysis': 3323, 'andrew': 3324, 'angel': 3325, 'antisemtism': 3326, 'approx': 3327, 'assistants': 3328, 'attempted': 3329, 'aus': 3330, 'basically': 3331, 'bastard': 3332, 'beast': 3333, 'beating': 3334, 'bihdaygirl': 3335, 'billionaire': 3336, 'bomb': 3337, 'broader': 3338, 'burn': 3339, 'burning': 3340, 'businesses': 3341, 'celebrities': 3342, 'childfree': 3343, 'commie': 3344, 'commonwealth': 3345, 'competition': 3346, 'complaint': 3347, 'confessions': 3348, 'convo': 3349, 'coverups': 3350, 'damonsajnani': 3351, 'decide': 3352, 'discuss': 3353, 'disgraced': 3354, 'disrespectful': 3355, 'divide': 3356, 'download': 3357, 'draintheswamp': 3358, 'dreads': 3359, 'economicapahied': 3360, 'emoji': 3361, 'employed': 3362, 'endless': 3363, 'enemy': 3364, 'engagement': 3365, 'extoion17': 3366, 'fasting': 3367, 'featuring': 3368, 'festive': 3369, 'fish': 3370, 'fitfam': 3371, 'foolish': 3372, 'forced': 3373, 'french': 3374, 'glass': 3375, 'gods': 3376, 'governor': 3377, 'hatredgreedamp': 3378, 'hereticfoundation': 3379, 'houston': 3380, 'indiedev': 3381, 'inflict': 3382, 'insecurity': 3383, 'islamaphobia': 3384, 'islamophobes': 3385, 'jackals': 3386, 'jewishsupremacist': 3387, 'joins': 3388, 'justify': 3389, 'kicking': 3390, 'laid': 3391, 'le': 3392, 'leading': 3393, 'legislative': 3394, 'livesbut': 3395, 'logo': 3396, 'lows': 3397, 'machine': 3398, 'maria': 3399, 'marvel': 3400, 'maryland': 3401, 'mdcollege': 3402, 'meaningless': 3403, 'minded': 3404, 'muslimpos': 3405, 'nationalism': 3406, 'neighborhood': 3407, 'neoconfederate': 3408, 'neoliberalism': 3409, 'nominee': 3410, 'nowhere': 3411, 'nursery': 3412, 'nut': 3413, 'obamajarretts': 3414, 'onelove': 3415, 'oppose': 3416, 'oprah': 3417, 'org': 3418, 'original': 3419, 'oust': 3420, 'overlap': 3421, 'papa': 3422, 'pas': 3423, 'pays': 3424, 'plants': 3425, 'plate': 3426, 'players': 3427, 'plays': 3428, 'powerhungrytraitors': 3429, 'price': 3430, 'prioritized': 3431, 'provide': 3432, 'putins': 3433, 'rare': 3434, 'ref': 3435, 'remedies': 3436, 'reminding': 3437, 'researchers': 3438, 'roof': 3439, 'rushlimbaugh': 3440, 'sajnani': 3441, 'salutbut': 3442, 'seller': 3443, 'senate': 3444, 'sexual': 3445, 'sharp': 3446, 'shift': 3447, 'shis': 3448, 'shout': 3449, 'sins': 3450, 'slaver': 3451, 'smth': 3452, 'snow': 3453, 'sob': 3454, 'sohappy': 3455, 'sons': 3456, 'stages': 3457, 'storytelling': 3458, 'summeime': 3459, 'suppression': 3460, 'sushi': 3461, 'sydney': 3462, 'teaches': 3463, 'tennessee': 3464, 'th': 3465, 'throwback': 3466, 'tolerated': 3467, 'topless': 3468, 'tough': 3469, 'track': 3470, 'transsexy': 3471, 'truthfully': 3472, 'ukip': 3473, 'unitednations': 3474, 'useless': 3475, 'victory': 3476, 'weekends': 3477, 'weight': 3478, 'weightloss': 3479, 'westerners': 3480, 'where': 3481, 'woohoo': 3482, 'woow': 3483, 'worried': 3484, 'written': 3485, 'yyc': 3486, '200k': 3487, '2016in4worlds': 3488, '40': 3489, '99c': 3490, 'absolute': 3491, 'accurately': 3492, 'adorable': 3493, 'alapoet': 3494, 'allows': 3495, 'aloha': 3496, 'alternative': 3497, 'altrightcards': 3498, 'amen': 3499, 'amendment': 3500, 'angels': 3501, 'anne': 3502, 'antisemites': 3503, 'anton': 3504, 'application': 3505, 'arm': 3506, 'atsi': 3507, 'attend': 3508, 'australians': 3509, 'balance': 3510, 'barackobama': 3511, 'bastards': 3512, 'bbq': 3513, 'beard': 3514, 'begun': 3515, 'behaving': 3516, 'bestfriends': 3517, 'biases': 3518, 'blacklivesmatters': 3519, 'blind': 3520, 'blog42': 3521, 'bonus': 3522, 'boos': 3523, 'bright': 3524, 'british': 3525, 'busily': 3526, 'buzzfeeds': 3527, 'cab': 3528, 'caoons': 3529, 'cars': 3530, 'cathedral': 3531, 'chelski': 3532, 'christinagrimmie': 3533, 'closer': 3534, 'cock': 3535, 'code': 3536, 'compassion': 3537, 'contempt': 3538, 'cowboy': 3539, 'cows': 3540, 'crow': 3541, 'cup': 3542, 'curse': 3543, 'dancers': 3544, 'darkness': 3545, 'deeper': 3546, 'defines': 3547, 'deleted': 3548, 'delivered': 3549, 'deploying': 3550, 'deseret': 3551, 'details': 3552, 'discovered': 3553, 'donny': 3554, 'drinking': 3555, 'ect': 3556, 'emirati': 3557, 'endthenation': 3558, 'ethnic': 3559, 'euros': 3560, 'everytime': 3561, 'excuses': 3562, 'eye': 3563, 'facebooks': 3564, 'facility': 3565, 'falls': 3566, 'ferguson': 3567, 'five': 3568, 'freedomofspeech': 3569, 'funded': 3570, 'gandhi': 3571, 'genderinequality': 3572, 'georgia': 3573, 'glasses': 3574, 'goal': 3575, 'goverment': 3576, 'grab': 3577, 'greetings': 3578, 'gwot': 3579, 'hammock': 3580, 'happykwanzaa': 3581, 'hiding': 3582, 'higher': 3583, 'humanrightswatch': 3584, 'humor': 3585, 'iamnotabe': 3586, 'ig': 3587, 'images': 3588, 'incite': 3589, 'inse': 3590, 'insensitive': 3591, 'instapic': 3592, 'instrumental': 3593, 'intelligence': 3594, 'interfering': 3595, 'iraq': 3596, 'jcpenney': 3597, 'jissus': 3598, 'karl': 3599, 'karma': 3600, 'killallwhitemen': 3601, 'komo': 3602, 'korea': 3603, 'lapdog': 3604, 'lately': 3605, 'launching': 3606, 'lbgt': 3607, 'leg': 3608, 'lessons': 3609, 'letting': 3610, 'loveyou': 3611, 'loyal': 3612, 'madrid': 3613, 'manhate': 3614, 'manspreading': 3615, 'materia': 3616, 'melanin': 3617, 'melo': 3618, 'merica': 3619, 'mindfulness': 3620, 'mmiw': 3621, 'mmiwg': 3622, 'mormonism': 3623, 'motivated': 3624, 'mud': 3625, 'narcissist': 3626, 'nerd': 3627, 'neutral': 3628, 'nofascist2017': 3629, 'nominate': 3630, 'nunes': 3631, 'obvious': 3632, 'oils': 3633, 'opkillingbaythecove': 3634, 'oppress': 3635, 'organic': 3636, 'oughta': 3637, 'outdated': 3638, 'overcome': 3639, 'packed': 3640, 'pamgelleheracist': 3641, 'pass': 3642, 'patriarchy': 3643, 'pe': 3644, 'peh': 3645, 'perspective': 3646, 'pet': 3647, 'phenomenon': 3648, 'pipe': 3649, 'poem': 3650, 'poke': 3651, 'poray': 3652, 'potential': 3653, 'pressure': 3654, 'prostitutes': 3655, 'proudly': 3656, 'publishing': 3657, 'pulls': 3658, 'quebecs': 3659, 'rapeugee': 3660, 'regas': 3661, 'registered': 3662, 'reputation': 3663, 'requires': 3664, 'rightfully': 3665, 'rima': 3666, 'roads': 3667, 'robe': 3668, 'romance': 3669, 'roses': 3670, 'san': 3671, 'seat': 3672, 'selfhating': 3673, 'sentiments': 3674, 'shades': 3675, 'shall': 3676, 'shooter': 3677, 'simpleton': 3678, 'skate': 3679, 'slaves': 3680, 'smell': 3681, 'solve': 3682, 'somehow': 3683, 'sophisticated': 3684, 'spanish': 3685, 'spending': 3686, 'standards': 3687, 'stoked': 3688, 'tackling': 3689, 'taharrush': 3690, 'taharrushgamea': 3691, 'ten': 3692, 'text': 3693, 'tht': 3694, 'thurs': 3695, 'tiny': 3696, 'treated': 3697, 'trendy': 3698, 'un': 3699, 'unloads': 3700, 'usual': 3701, 'verbalassault': 3702, 'voices': 3703, 'volunteer': 3704, 'voterid': 3705, 'vsco': 3706, 'whene': 3707, 'whether': 3708, 'whitepriv': 3709, 'whitesupremacists': 3710, 'wisconsinmadison': 3711, 'yell': 3712, 'zen': 3713, 'zionist': 3714, '2016in4': 3715, '2a': 3716, '74': 3717, 'aches': 3718, 'activists': 3719, 'aims': 3720, 'allblack': 3721, 'am': 3722, 'ampamp': 3723, 'appalling': 3724, 'aryanbrotherhood': 3725, 'asshat': 3726, 'associated': 3727, 'attorneygeneral': 3728, 'authorities': 3729, 'award': 3730, 'bandying': 3731, 'barcelona': 3732, 'bash': 3733, 'befriending': 3734, 'belief': 3735, 'bernie': 3736, 'binge': 3737, 'bitching': 3738, 'blackface': 3739, 'blessings': 3740, 'boat': 3741, 'bolt': 3742, 'boom': 3743, 'boot': 3744, 'bots': 3745, 'bracelet': 3746, 'bradshaw': 3747, 'bud': 3748, 'buddhism': 3749, 'buddhist': 3750, 'burmese': 3751, 'butthucreepers': 3752, 'buzzing': 3753, 'cafenoirproject': 3754, 'camron': 3755, 'caucasians': 3756, 'censored': 3757, 'chokes': 3758, 'chronic': 3759, 'cityjournal': 3760, 'clown': 3761, 'coldplay': 3762, 'commentsyou': 3763, 'complicity': 3764, 'confidence': 3765, 'conservatives': 3766, 'cooper': 3767, 'cousins': 3768, 'cowboys': 3769, 'crabs': 3770, 'crosscheckingvoter': 3771, 'cunt': 3772, 'defeat': 3773, 'degrading': 3774, 'delayed': 3775, 'delighted': 3776, 'delivery': 3777, 'depressing': 3778, 'derek': 3779, 'desire': 3780, 'detre': 3781, 'developer': 3782, 'diaz': 3783, 'discourse': 3784, 'discussing': 3785, 'disgusted': 3786, 'doplants': 3787, 'dw': 3788, 'edchat': 3789, 'email': 3790, 'emma': 3791, 'emojis': 3792, 'ended': 3793, 'eng': 3794, 'entitled': 3795, 'eurusd': 3796, 'expe': 3797, 'expectations': 3798, 'familytime': 3799, 'famous': 3800, 'fav': 3801, 'filter': 3802, 'filthy': 3803, 'forher': 3804, 'fowoh': 3805, 'friendly': 3806, 'fuel': 3807, 'genuinely': 3808, 'gifts': 3809, 'grave': 3810, 'guitar': 3811, 'hardly': 3812, 'hategroup': 3813, 'heroes': 3814, 'hired': 3815, 'historian': 3816, 'hits': 3817, 'hockey': 3818, 'holding': 3819, 'honest': 3820, 'hoodie': 3821, 'hypocrites': 3822, 'illegalisraelisettlements': 3823, 'immediately': 3824, 'impeachtrump': 3825, 'incident': 3826, 'increased': 3827, 'indignenous': 3828, 'interference': 3829, 'internal': 3830, 'israelisettlements': 3831, 'ivankatrump': 3832, 'jc': 3833, 'jewworldorder': 3834, 'jobsearch': 3835, 'kindledeals': 3836, 'kingconrad': 3837, 'knocks': 3838, 'latina': 3839, 'letter': 3840, 'lettuce': 3841, 'lied': 3842, 'lil': 3843, 'limiting': 3844, 'lines': 3845, 'loa': 3846, 'luv': 3847, 'mans': 3848, 'manus': 3849, 'maters': 3850, 'mediamisogyny': 3851, 'minds': 3852, 'mirror': 3853, 'mps': 3854, 'murderer': 3855, 'nbc': 3856, 'network': 3857, 'nevermypresident': 3858, 'nonwhite': 3859, 'notfunny': 3860, 'np': 3861, 'obesity': 3862, 'omfg': 3863, 'onto': 3864, 'opi': 3865, 'oppressi': 3866, 'originated': 3867, 'oven': 3868, 'parliament': 3869, 'passes': 3870, 'philosopher': 3871, 'plaster': 3872, 'position': 3873, 'pound': 3874, 'prankster': 3875, 'prejudiced': 3876, 'prevent': 3877, 'preying': 3878, 'prisoners': 3879, 'properly': 3880, 'prorapist': 3881, 'protection': 3882, 'proves': 3883, 'pumped': 3884, 'puts': 3885, 'putting': 3886, 'qualified': 3887, 'quickly': 3888, 'quite': 3889, 'racewar': 3890, 'raison': 3891, 'ranting': 3892, 'rants': 3893, 'recently': 3894, 'redacts': 3895, 'reemergence': 3896, 'regularass': 3897, 'retweeted': 3898, 'revealed': 3899, 'rhymes': 3900, 'riding': 3901, 'rohingyas': 3902, 'root': 3903, 'sainsburys': 3904, 'sanders': 3905, 'sands': 3906, 'sara': 3907, 'sarcastic': 3908, 'scalds': 3909, 'scores': 3910, 'scotus': 3911, 'sdgs': 3912, 'search': 3913, 'seeking': 3914, 'selah': 3915, 'shoppers': 3916, 'shyan': 3917, 'sided': 3918, 'sis': 3919, 'size': 3920, 'slavelabor': 3921, 'slices': 3922, 'sm': 3923, 'smileyong': 3924, 'smog': 3925, 'smooth': 3926, 'snap': 3927, 'stance': 3928, 'storm': 3929, 'stripe': 3930, 'sundayfunday': 3931, 'superhero': 3932, 'suppoed': 3933, 'supremecou': 3934, 'suspend': 3935, 'swim': 3936, 'tarnishing': 3937, 'techjunkiejhaltright': 3938, 'thoughj': 3939, 'threatened': 3940, 'thrive': 3941, 'ticking': 3942, 'tolerance': 3943, 'tomlin': 3944, 'traveling': 3945, 'tshi': 3946, 'tune': 3947, 'twisted': 3948, 'unhappy': 3949, 'usher': 3950, 'utterly': 3951, 'vanilla': 3952, 'vanity': 3953, 'veteran': 3954, 'wages': 3955, 'weaponry': 3956, 'weddings': 3957, 'wen': 3958, 'whiterace': 3959, 'wildlife': 3960, 'wohless': 3961, 'writer': 3962, 'wus': 3963, 'yeg': 3964, 'yellow': 3965, '2raise': 3966, '2stand': 3967, '35': 3968, '50s': 3969, '630': 3970, '700': 3971, '80snostalgia': 3972, '90': 3973, 'abetting': 3974, 'abou': 3975, 'actress': 3976, 'adores': 3977, 'advancement': 3978, 'adveisments': 3979, 'advocates': 3980, 'aftr': 3981, 'airlines': 3982, 'alexpascal': 3983, 'allowing': 3984, 'amazon': 3985, 'americad': 3986, 'amy': 3987, 'annoying': 3988, 'antonyelchin': 3989, 'approval': 3990, 'arc': 3991, 'arkansas': 3992, 'arrests': 3993, 'asean': 3994, 'asking': 3995, 'assailed': 3996, 'aungsansuukyi': 3997, 'awake': 3998, 'babe': 3999, 'badges': 4000, 'bank': 4001, 'beaten': 4002, 'bf': 4003, 'bigly': 4004, 'bird': 4005, 'bistro': 4006, 'blowjob': 4007, 'bn': 4008, 'bob': 4009, 'boston': 4010, 'boycottjcpenney': 4011, 'brands': 4012, 'breitba': 4013, 'brighton': 4014, 'brownshi': 4015, 'bubbles': 4016, 'burma': 4017, 'burninhell': 4018, 'bustymilf': 4019, 'calmly': 4020, 'candidate': 4021, 'cares': 4022, 'cease': 4023, 'ceased': 4024, 'celebrity': 4025, 'century': 4026, 'channel': 4027, 'chops': 4028, 'claycountydevelopmentcorp': 4029, 'cmon': 4030, 'collectively': 4031, 'comics': 4032, 'commenting': 4033, 'communists': 4034, 'computer': 4035, 'confederateflag': 4036, 'confident': 4037, 'construction': 4038, 'controlled': 4039, 'conversations': 4040, 'convinced': 4041, 'corporatebs': 4042, 'corrupttrump': 4043, 'cosmeticsbrand': 4044, 'coworker': 4045, 'crooked': 4046, 'culturalenrichment': 4047, 'curriculum': 4048, 'cwt': 4049, 'declining': 4050, 'deeply': 4051, 'demagoguery': 4052, 'democrat': 4053, 'desperate': 4054, 'destructive': 4055, 'didnot': 4056, 'digged': 4057, 'disadvantaged': 4058, 'disavow': 4059, 'disparo': 4060, 'distos': 4061, 'district': 4062, 'dnc': 4063, 'dollars': 4064, 'downtown': 4065, 'drops': 4066, 'drown': 4067, 'drugs': 4068, 'dumbingdownofamerica': 4069, 'dutch': 4070, 'easily': 4071, 'easteuropeans': 4072, 'ebook': 4073, 'eco': 4074, 'emo': 4075, 'empowerd': 4076, 'en': 4077, 'encounter': 4078, 'enjoyable': 4079, 'entitlement': 4080, 'entreteiment': 4081, 'equated': 4082, 'excitement': 4083, 'explanations': 4084, 'extinction': 4085, 'factophobia': 4086, 'failing': 4087, 'familybut': 4088, 'fe': 4089, 'feds': 4090, 'fighters': 4091, 'firstnations': 4092, 'focus': 4093, 'fr': 4094, 'freaking': 4095, 'funday': 4096, 'geek': 4097, 'generations': 4098, 'glee': 4099, 'gollybar': 4100, 'goodes': 4101, 'grass': 4102, 'griffiths': 4103, 'groom': 4104, 'guaranteed': 4105, 'guessing': 4106, 'gutter': 4107, 'hamzayusuf': 4108, 'hannity': 4109, 'hansen': 4110, 'harlem': 4111, 'hat': 4112, 'hater': 4113, 'headed': 4114, 'headlock': 4115, 'herb': 4116, 'highstool': 4117, 'hitlerjugend': 4118, 'honesty': 4119, 'hooked': 4120, 'iah': 4121, 'idiotic': 4122, 'igmilitia': 4123, 'impo': 4124, 'inadequate': 4125, 'inherent': 4126, 'inspire': 4127, 'ironies': 4128, 'jail': 4129, 'jake': 4130, 'joined': 4131, 'jokeoftheday': 4132, 'judged': 4133, 'kajal': 4134, 'khan': 4135, 'kingfisher': 4136, 'kitazato': 4137, 'kitazatoshibasaburo': 4138, 'kitten': 4139, 'kkkblm': 4140, 'ku': 4141, 'lit': 4142, 'loses': 4143, 'loudest': 4144, 'ltlt': 4145, 'magnettherapy': 4146, 'mandate': 4147, 'manifestdestiny': 4148, 'manning': 4149, 'marine': 4150, 'mate': 4151, 'meal': 4152, 'melania': 4153, 'melted': 4154, 'memo': 4155, 'memory': 4156, 'messages': 4157, 'mfs': 4158, 'migitorio': 4159, 'miles': 4160, 'mondaymotivation': 4161, 'mongers': 4162, 'morals': 4163, 'muslimbrotherhood': 4164, 'myoneresolution': 4165, 'named': 4166, 'net': 4167, 'newyears': 4168, 'nobelprize': 4169, 'noise': 4170, 'nomore': 4171, 'noting': 4172, 'notmyprez': 4173, 'nowe': 4174, 'objectify': 4175, 'opkillingbay': 4176, 'oral': 4177, 'ouuh': 4178, 'palladino': 4179, 'pamelaramseytaylor': 4180, 'paradise': 4181, 'patriotism': 4182, 'pauline': 4183, 'penney': 4184, 'pennsylvania': 4185, 'peoples': 4186, 'persecution': 4187, 'pewpewlife': 4188, 'photoshoot': 4189, 'pittsburgh': 4190, 'pivo': 4191, 'plain': 4192, 'plantation': 4193, 'prank': 4194, 'presses': 4195, 'prices': 4196, 'pridemake': 4197, 'priviledge': 4198, 'prolapsed': 4199, 'psycho': 4200, 'pussygrabber': 4201, 'qatar': 4202, 'quietly': 4203, 'react': 4204, 'rector': 4205, 'reflects': 4206, 'rehired': 4207, 'remake': 4208, 'resources': 4209, 'responsibilities': 4210, 'returning': 4211, 'rife': 4212, 'rockineve': 4213, 'rousey': 4214, 'route': 4215, 'rupemurdochs': 4216, 'saudi': 4217, 'scotland': 4218, 'season4': 4219, 'segregate': 4220, 'servant': 4221, 'setting': 4222, 'sexoffender': 4223, 'sexpredator': 4224, 'sexualassult': 4225, 'shock': 4226, 'shoutout': 4227, 'signed': 4228, 'simplistic': 4229, 'skank': 4230, 'skinned': 4231, 'skinny': 4232, 'sleeping': 4233, 'sleepy': 4234, 'sliding': 4235, 'sme': 4236, 'snobs': 4237, 'solidarity': 4238, 'stab': 4239, 'stolen': 4240, 'stops': 4241, 'supermistict': 4242, 'survivor': 4243, 'swear': 4244, 'sweep': 4245, 'tan': 4246, 'taxevader': 4247, 'teenager': 4248, 'teenagers': 4249, 'tetanusimmunization': 4250, 'thecove': 4251, 'thekingsspeech': 4252, 'theview': 4253, 'thirdworldcanada': 4254, 'threaten': 4255, 'tinyfingeredpuppet': 4256, 'tip': 4257, 'tolerate': 4258, 'touring': 4259, 'tpc': 4260, 'trampled': 4261, 'travelling': 4262, 'trompas': 4263, 'trumpas': 4264, 'trumpdarkzone': 4265, 'uglylady': 4266, 'ukraines': 4267, 'uks': 4268, 'unequal': 4269, 'unjust': 4270, 'verbally': 4271, 'volatile': 4272, 'volodymyrviatrovych': 4273, 'voter': 4274, 'whe': 4275, 'whiteaustralianpolicy': 4276, 'whoa': 4277, 'whose': 4278, 'witness': 4279, 'womenissues': 4280, 'womenonly': 4281, 'woodrowwilson': 4282, 'worldcampaign': 4283, 'worship': 4284, 'wwii': 4285, 'yesu': 4286, 'yields': 4287, 'yolo': 4288, 'zilles': 4289, '1pun': 4290, '2016in4word': 4291, '280': 4292, '342': 4293, '360': 4294, '40404': 4295, '83': 4296, 'accent': 4297, 'acne': 4298, 'administration': 4299, 'adves': 4300, 'alcohol': 4301, 'amid': 4302, 'annual': 4303, 'antics': 4304, 'apologies': 4305, 'asap': 4306, 'asylumseekers': 4307, 'attract': 4308, 'aufstehn': 4309, 'aussie': 4310, 'author': 4311, 'azealia': 4312, 'banksjimmy': 4313, 'basic': 4314, 'bdsm': 4315, 'bee': 4316, 'beingassuming': 4317, 'berlin': 4318, 'besties': 4319, 'bestseller': 4320, 'bihcontrol': 4321, 'blogging': 4322, 'bloody': 4323, 'bold': 4324, 'boring': 4325, 'breast': 4326, 'breed': 4327, 'bringbackpage3': 4328, 'buried': 4329, 'buying': 4330, 'bwahahahahaha': 4331, 'camber': 4332, 'cambma': 4333, 'camping': 4334, 'capabilities': 4335, 'catholicchurch': 4336, 'causes': 4337, 'causing': 4338, 'cave': 4339, 'cell': 4340, 'challenging': 4341, 'charges': 4342, 'chomsky': 4343, 'chrismukkah': 4344, 'christmassy': 4345, 'cite': 4346, 'col': 4347, 'colorblind': 4348, 'colors': 4349, 'combined': 4350, 'communities': 4351, 'complained': 4352, 'conflict': 4353, 'congressman': 4354, 'consent': 4355, 'consultant': 4356, 'convince': 4357, 'corruptgop': 4358, 'cost': 4359, 'coulda': 4360, 'cresc': 4361, 'cspan': 4362, 'day1': 4363, 'ddnt': 4364, 'decolonization': 4365, 'decolonizing': 4366, 'decree': 4367, 'dependent': 4368, 'dept': 4369, 'detention': 4370, 'dickhole': 4371, 'directly': 4372, 'disneyland': 4373, 'dividerofanation': 4374, 'dm': 4375, 'donaldduck': 4376, 'doubt': 4377, 'douchelord': 4378, 'drill': 4379, 'drinkdark': 4380, 'drs': 4381, 'dts': 4382, 'echoes': 4383, 'economy': 4384, 'eg': 4385, 'egomaniac': 4386, 'eltham': 4387, 'enabler': 4388, 'entered': 4389, 'escape': 4390, 'essay': 4391, 'everythingisracist': 4392, 'exaggeration': 4393, 'extending': 4394, 'faisal': 4395, 'farmer': 4396, 'feelgood': 4397, 'fix': 4398, 'fliers': 4399, 'flyer': 4400, 'fn': 4401, 'foreign': 4402, 'frnds': 4403, 'fullretard': 4404, 'furore': 4405, 'galery': 4406, 'gbpusd': 4407, 'gegenrechts': 4408, 'genderstereotypes': 4409, 'generationkkk': 4410, 'germans': 4411, 'goodonedean': 4412, 'govegan': 4413, 'grandmother': 4414, 'greek': 4415, 'grou': 4416, 'grows': 4417, 'gu': 4418, 'had': 4419, 'hangs': 4420, 'harvardsq': 4421, 'heat': 4422, 'helpless': 4423, 'hepburn': 4424, 'hid': 4425, 'highest': 4426, 'hillbots': 4427, 'hires': 4428, 'hispters': 4429, 'historicalamnesia': 4430, 'hm': 4431, 'holy': 4432, 'homegrown': 4433, 'horror': 4434, 'howell': 4435, 'humble': 4436, 'hump': 4437, 'icecream': 4438, 'ideals': 4439, 'ie': 4440, 'illustration': 4441, 'impoed': 4442, 'inhumane': 4443, 'innate': 4444, 'innercity': 4445, 'instahappy': 4446, 'integrity': 4447, 'intentions': 4448, 'international': 4449, 'internationallaw': 4450, 'intersectionality': 4451, 'intrumpsamerica': 4452, 'inveed': 4453, 'investigate': 4454, 'iqg': 4455, 'islamichomophobicrapistwho': 4456, 'jackson': 4457, 'jealousy': 4458, 'jewelry': 4459, 'joie': 4460, 'journalist': 4461, 'journalists': 4462, 'jump': 4463, 'kanyewest': 4464, 'kerala': 4465, 'kettled': 4466, 'lactation': 4467, 'lasvegas': 4468, 'latent': 4469, 'laughs': 4470, 'lauren': 4471, 'lazio': 4472, 'legacy': 4473, 'levin': 4474, 'li': 4475, 'liars': 4476, 'lib': 4477, 'libearian': 4478, 'liberalism': 4479, 'lulic': 4480, 'marxist': 4481, 'mascot': 4482, 'massage': 4483, 'mature': 4484, 'mccain': 4485, 'mcmillanjames': 4486, 'meaning': 4487, 'meanwhile': 4488, 'medicine': 4489, 'meditations': 4490, 'meds': 4491, 'merkel': 4492, 'metoo': 4493, 'metrics': 4494, 'mobile': 4495, 'mode': 4496, 'mosque': 4497, 'mp': 4498, 'msdhu': 4499, 'murders': 4500, 'mystery': 4501, 'nationalanthem': 4502, 'nationalbestfriendsday': 4503, 'need2know': 4504, 'neocolonialism': 4505, 'neonazi': 4506, 'neve': 4507, 'newzeeland': 4508, 'nia': 4509, 'noafd': 4510, 'nokkk': 4511, 'nonazis': 4512, 'nonprofitstatus': 4513, 'nonsens': 4514, 'nooltham': 4515, 'nopegida': 4516, 'nostrache': 4517, 'notallmen': 4518, 'notrump': 4519, 'nov': 4520, 'nowplaying': 4521, 'nutshell': 4522, 'nzpol': 4523, 'objective': 4524, 'obviously': 4525, 'occupation': 4526, 'oft': 4527, 'op': 4528, 'outfit': 4529, 'outraged': 4530, 'overwhelming': 4531, 'owes': 4532, 'passing': 4533, 'passionate': 4534, 'peek': 4535, 'pennyloafer': 4536, 'periscope': 4537, 'personalised': 4538, 'pets': 4539, 'photographer': 4540, 'physical': 4541, 'pissing': 4542, 'poems': 4543, 'politically': 4544, 'popefrancis': 4545, 'populism': 4546, 'pornisforlosers': 4547, 'praised': 4548, 'precious': 4549, 'precrime': 4550, 'preprogrammed': 4551, 'profession': 4552, 'propoganda': 4553, 'prospect': 4554, 'protects': 4555, 'protesters': 4556, 'provoking': 4557, 'prowar': 4558, 'ps': 4559, 'purposedly': 4560, 'pushing': 4561, 'putinism': 4562, 'quebec': 4563, 'raceplay': 4564, 'raise': 4565, 'ratings': 4566, 'reagan': 4567, 'recording': 4568, 'recovery': 4569, 'refer': 4570, 'relaxed': 4571, 'reliability': 4572, 'religionofpeace': 4573, 'represents': 4574, 'reproductiverights': 4575, 'repug': 4576, 'required': 4577, 'returnmyvinyl': 4578, 'reversed': 4579, 'revoltingly': 4580, 'rr': 4581, 'samehypocrite': 4582, 'sandyhook': 4583, 'sans': 4584, 'satisfied': 4585, 'saturdayblogshare': 4586, 'scale': 4587, 'scifi': 4588, 'seem': 4589, 'selfies': 4590, 'selfloathing': 4591, 'senad': 4592, 'sexualbullying': 4593, 'shower': 4594, 'sisterinlaw': 4595, 'six': 4596, 'smaimmigration': 4597, 'smells': 4598, 'socialclass': 4599, 'socialized': 4600, 'soundbite': 4601, 'sovereignty': 4602, 'speakers': 4603, 'spiritual': 4604, 'standwithisrael': 4605, 'storyville': 4606, 'strangely': 4607, 'straya': 4608, 'studying': 4609, 'sunbury': 4610, 'sundaymorning': 4611, 'sunglasses': 4612, 'survived': 4613, 'svpol': 4614, 'systems': 4615, 'tables': 4616, 'takin': 4617, 'tamirrice': 4618, 'taragon': 4619, 'taste': 4620, 'taxwriteoff': 4621, 'tb': 4622, 'tendencies': 4623, 'tha': 4624, 'thatmakes': 4625, 'theme': 4626, 'thissuppoing': 4627, 'thou': 4628, 'thx': 4629, 'timwise': 4630, 'titties': 4631, 'tokyo': 4632, 'tornado': 4633, 'tra': 4634, 'trendolizer': 4635, 'trht': 4636, 'tries': 4637, 'twitch': 4638, 'tws': 4639, 'uber': 4640, 'ufc207': 4641, 'um': 4642, 'unamerican': 4643, 'unbornlivesmatter': 4644, 'undermine': 4645, 'unfittobeinpublicoffice': 4646, 'uniting': 4647, 'unprecedented': 4648, 'uppity': 4649, 'upsetting': 4650, 'vaw': 4651, 'venezuela': 4652, 'vk': 4653, 'vocabulary': 4654, 'void': 4655, 'wakeupamericaca': 4656, 'walked': 4657, 'walks': 4658, 'wave': 4659, 'weddingplanning': 4660, 'whiteknight': 4661, 'willoughbys': 4662, 'wishlist': 4663, 'wmuslim': 4664, 'woobietuesday': 4665, 'worldview': 4666, 'yelchin': 4667, 'yousuck': 4668, 'zeroattacks': 4669, '12313': 4670, '200': 4671, '20days': 4672, '24hrs': 4673, '43': 4674, '60minutes': 4675, '952': 4676, 'aap': 4677, 'accused': 4678, 'acted': 4679, 'adventures': 4680, 'affair': 4681, 'africanamericans': 4682, 'ali': 4683, 'alternet': 4684, 'amateur': 4685, 'ambition': 4686, 'ana': 4687, 'anchor': 4688, 'android': 4689, 'announcement': 4690, 'answered': 4691, 'answers': 4692, 'anxious': 4693, 'applauding': 4694, 'applies': 4695, 'assured': 4696, 'aswell': 4697, 'ate': 4698, 'atheist': 4699, 'avoice': 4700, 'bags': 4701, 'baseball': 4702, 'batman': 4703, 'batshit': 4704, 'becomin': 4705, 'beensqueezing': 4706, 'bestfeature': 4707, 'bestoftheweek': 4708, 'bigger': 4709, 'biggotry': 4710, 'bihdays': 4711, 'bikini': 4712, 'bjork': 4713, 'blackvote': 4714, 'bruh': 4715, 'brunch': 4716, 'brunette': 4717, 'bts': 4718, 'cabinet': 4719, 'canadas': 4720, 'cancer': 4721, 'capitol': 4722, 'carolina': 4723, 'carolsbycandlelight': 4724, 'carriefisher': 4725, 'catalunya': 4726, 'cbc2017': 4727, 'challenges': 4728, 'chicken': 4729, 'childlabor': 4730, 'childrape': 4731, 'choking': 4732, 'clouds': 4733, 'codeword': 4734, 'comfo': 4735, 'communitythey': 4736, 'compet': 4737, 'condolences': 4738, 'confined': 4739, 'confirmed': 4740, 'conjob': 4741, 'conormcgregor': 4742, 'cpc': 4743, 'cpcldr': 4744, 'crackers': 4745, 'cramped': 4746, 'craving': 4747, 'crazybengiefbps': 4748, 'crcker': 4749, 'credibility': 4750, 'creepy': 4751, 'critical': 4752, 'crony': 4753, 'crybully': 4754, 'cucumbers': 4755, 'currently': 4756, 'dat': 4757, 'defeated': 4758, 'defend': 4759, 'defile': 4760, 'defineepitome': 4761, 'demand': 4762, 'depoation': 4763, 'deserves': 4764, 'deviant': 4765, 'dipshit': 4766, 'disingenuous': 4767, 'distant': 4768, 'dna': 4769, 'draining': 4770, 'drop': 4771, 'duke': 4772, 'dumbism': 4773, 'duty': 4774, 'dwd': 4775, 'ear': 4776, 'ebonics': 4777, 'edc': 4778, 'editor': 4779, 'eid': 4780, 'electronic': 4781, 'ellen': 4782, 'elxn42': 4783, 'emperor': 4784, 'enabled': 4785, 'encouraging': 4786, 'endlessly': 4787, 'endofthe': 4788, 'engaged': 4789, 'episodes': 4790, 'epitomeofsorrow': 4791, 'equivalent': 4792, 'estate': 4793, 'everyon': 4794, 'exams': 4795, 'excit': 4796, 'excludedmarginalised': 4797, 'exemplifies': 4798, 'experienced': 4799, 'exploring': 4800, 'eyetalian': 4801, 'f1': 4802, 'fabricate': 4803, 'faiths': 4804, 'fck': 4805, 'fearmongering': 4806, 'fibromyalgia': 4807, 'fiends': 4808, 'fill': 4809, 'finland': 4810, 'firstworldproblems': 4811, 'fled': 4812, 'flood': 4813, 'fooled': 4814, 'forest': 4815, 'freeways': 4816, 'fugly': 4817, 'gameofthrones': 4818, 'gazza': 4819, 'genes': 4820, 'gewitter': 4821, 'glitter': 4822, 'globe': 4823, 'goodmood': 4824, 'goodness': 4825, 'goon': 4826, 'goppropaganda': 4827, 'graffiti': 4828, 'grand': 4829, 'grind': 4830, 'gt9': 4831, 'gutted': 4832, 'havin': 4833, 'heel': 4834, 'heights': 4835, 'heroines': 4836, 'hi5': 4837, 'hispanicvote': 4838, 'hoes': 4839, 'hug': 4840, 'hugh': 4841, 'hvg': 4842, 'hypocritical': 4843, 'identity': 4844, 'idk': 4845, 'idol': 4846, 'ik': 4847, 'illustrated': 4848, 'imagemakeovers': 4849, 'indecent': 4850, 'independent': 4851, 'index': 4852, 'indoctrinating': 4853, 'infants': 4854, 'inn': 4855, 'instacool': 4856, 'instagrams': 4857, 'insulting': 4858, 'invented': 4859, 'ios': 4860, 'isisnt': 4861, 'islamicterrorist': 4862, 'israelikpop': 4863, 'itstheendoftheworldand': 4864, 'jabs': 4865, 'jeans': 4866, 'johnrkhoward': 4867, 'joint': 4868, 'jumpedthesh': 4869, 'justified': 4870, 'kneejerk': 4871, 'knitting': 4872, 'kpopdance': 4873, 'labeling': 4874, 'lan': 4875, 'laser': 4876, 'latepost': 4877, 'latinos': 4878, 'laurenduca': 4879, 'leather': 4880, 'leeds': 4881, 'leejasper': 4882, 'lesbian': 4883, 'letsgo': 4884, 'liberallogic': 4885, 'library': 4886, 'littleenglander': 4887, 'log': 4888, 'loteni': 4889, 'lovemylife': 4890, 'lovers': 4891, 'luicalibres': 4892, 'luxury': 4893, 'mac': 4894, 'macys': 4895, 'mafuckas': 4896, 'magical': 4897, 'makeo': 4898, 'malay': 4899, 'malcolm': 4900, 'management': 4901, 'manhattan': 4902, 'marry': 4903, 'massacre': 4904, 'materi': 4905, 'math': 4906, 'mcconnell': 4907, 'mediamy': 4908, 'mediocre': 4909, 'mention': 4910, 'merocrush': 4911, 'merry': 4912, 'micheleobama': 4913, 'microsoft': 4914, 'misogynisttrump': 4915, 'misunderstood': 4916, 'mob': 4917, 'morality': 4918, 'moves': 4919, 'mrminority': 4920, 'mrs': 4921, 'mtvwhiteguys': 4922, 'muir': 4923, 'murray': 4924, 'murrayhaters': 4925, 'namaste': 4926, 'narratives': 4927, 'native': 4928, 'nephew': 4929, 'neveoolate': 4930, 'nevr': 4931, 'niece': 4932, 'niga': 4933, 'nikita': 4934, 'nm': 4935, 'nope': 4936, 'notfit': 4937, 'notracist': 4938, 'nuclear': 4939, 'obamacare': 4940, 'obamanation': 4941, 'ongoing': 4942, 'opkillingbayseashepherd': 4943, 'oppressor': 4944, 'option': 4945, 'ottawa': 4946, 'oveurning': 4947, 'ow': 4948, 'packyourshitobama': 4949, 'pastors': 4950, 'payday': 4951, 'perfumer': 4952, 'persons': 4953, 'phrases': 4954, 'piggy': 4955, 'plannedparenthood': 4956, 'pleasure': 4957, 'pointer': 4958, 'possibility': 4959, 'pre': 4960, 'presidentobama': 4961, 'priceless': 4962, 'prohibition': 4963, 'prosecuted': 4964, 'pub': 4965, 'publicity': 4966, 'puppet': 4967, 'purchase': 4968, 'putinplant': 4969, 'putinsbitch': 4970, 'quoted': 4971, 'racecard': 4972, 'rachelmaddow': 4973, 'racistdeathwish': 4974, 'raging': 4975, 'raising': 4976, 'raw': 4977, 'referendum': 4978, 'referring': 4979, 'refresher': 4980, 'refugee': 4981, 'relation': 4982, 'remains': 4983, 'reparations': 4984, 'reply': 4985, 'repping': 4986, 'resigns': 4987, 'rhashtags': 4988, 'romantic': 4989, 'ruined': 4990, 'sa': 4991, 'sandniggers': 4992, 'satisfy': 4993, 'schwandorfchwandorf': 4994, 'seal': 4995, 'secrets': 4996, 'selected': 4997, 'selflove': 4998, 'sham': 4999, 'sic': 5000, 'signofthetimes': 5001, 'sjwlogic': 5002, 'sjws': 5003, 'slogan': 5004, 'snowpeople': 5005, 'sociopathic': 5006, 'somewhere': 5007, 'sooooo': 5008, 'soundcloud': 5009, 'spanking': 5010, 'specifically': 5011, 'spoty': 5012, 'staging': 5013, 'starkes': 5014, 'statementdoubling': 5015, 'stellar': 5016, 'sticker': 5017, 'stoned': 5018, 'stood': 5019, 'strategy': 5020, 'strawberry': 5021, 'streetfighter': 5022, 'subhuman': 5023, 'suggest': 5024, 'suicidal': 5025, 'summer2016': 5026, 'summit': 5027, 'surface': 5028, 'swamp': 5029, 'sweetpotato': 5030, 'swinging': 5031, 'tag': 5032, 'tamanna': 5033, 'taxpayer': 5034, 'teamcancer': 5035, 'teammichaelpalage': 5036, 'teens': 5037, 'temptation': 5038, 'thanksjackasses': 5039, 'thighhigh': 5040, 'thru': 5041, 'thursdaythoughts': 5042, 'tie': 5043, 'tis': 5044, 'token': 5045, 'tomorrows': 5046, 'topoli': 5047, 'trailer': 5048, 'trees': 5049, 'trough': 5050, 'trumpconaist': 5051, 'trumpleaks': 5052, 'trumpracist': 5053, 'turkey': 5054, 'twonitwittrumpsuppoers': 5055, 'unabated': 5056, 'unleashyourjoy': 5057, 'unlike': 5058, 'upcoming': 5059, 'updates': 5060, 'uskkk': 5061, 'utah': 5062, 'venusexchange': 5063, 'vicious': 5064, 'victimise': 5065, 'vscocam': 5066, 'waged': 5067, 'wakemeupwhen': 5068, 'wanother': 5069, 'warnung': 5070, 'washout': 5071, 'wetter': 5072, 'wetterwarnung': 5073, 'wifebeater': 5074, 'wilfully': 5075, 'wlk': 5076, 'wnt': 5077, 'wog': 5078, 'womenwednesday': 5079, 'wondered': 5080, 'worldwide': 5081, 'wwi': 5082, 'youtuber': 5083, 'zimbabwe': 5084, 'zit': 5085, '11th': 5086, '13thdocumentary': 5087, '14000': 5088, '19': 5089, '1996': 5090, '1gabba': 5091, '230pmet': 5092, '27': 5093, '946': 5094, 'abd': 5095, 'abpoli': 5096, 'accepting': 5097, 'accountable': 5098, 'activities': 5099, 'actual': 5100, 'af': 5101, 'afghanistan': 5102, 'alex': 5103, 'algona': 5104, 'altogether': 5105, 'alway': 5106, 'ameture': 5107, 'ampthey': 5108, 'angryignorantand': 5109, 'animalrights': 5110, 'appearance': 5111, 'aptly': 5112, 'arabian': 5113, 'arebut': 5114, 'argentine': 5115, 'argues': 5116, 'arise': 5117, 'aryans': 5118, 'attempting': 5119, 'avoid': 5120, 'awarded': 5121, 'backhanders': 5122, 'bailreform': 5123, 'bangladeshis': 5124, 'bbuk': 5125, 'bees': 5126, 'beforesex': 5127, 'behaviour': 5128, 'berberian': 5129, 'bethlehem': 5130, 'bio': 5131, 'biological': 5132, 'bitcoin': 5133, 'bla': 5134, 'blackbrown': 5135, 'blacklives': 5136, 'blazing': 5137, 'bloggers': 5138, 'boycottaustralia': 5139, 'boyden': 5140, 'braids': 5141, 'brainsjust': 5142, 'brazilians': 5143, 'breathes': 5144, 'bullies': 5145, 'bush': 5146, 'bustyescos': 5147, 'calendar': 5148, 'camps': 5149, 'canadians': 5150, 'cannabis': 5151, 'caoonish': 5152, 'capetown': 5153, 'careful': 5154, 'caroline': 5155, 'centred': 5156, 'ceo': 5157, 'cheese': 5158, 'childabuse': 5159, 'choosing': 5160, 'chumps': 5161, 'churchs': 5162, 'cinema': 5163, 'civilians': 5164, 'cjreform': 5165, 'claiming': 5166, 'cleansing': 5167, 'coherently': 5168, 'compared': 5169, 'compass': 5170, 'competency': 5171, 'constantly': 5172, 'contract': 5173, 'contradictions': 5174, 'copkiller': 5175, 'copped': 5176, 'countered': 5177, 'countys': 5178, 'cousin': 5179, 'crookedtrump': 5180, 'crosses': 5181, 'cuck': 5182, 'cuddles': 5183, 'culturesways': 5184, 'datesletstalkabout': 5185, 'deathpenalty': 5186, 'definitive': 5187, 'delusional': 5188, 'deming': 5189, 'democrate': 5190, 'demonize': 5191, 'depl': 5192, 'designed': 5193, 'desirehatred': 5194, 'dicks': 5195, 'differences': 5196, 'dignity': 5197, 'disallowd': 5198, 'disenfranchised': 5199, 'dncchair': 5200, 'dolled': 5201, 'domesticviolence': 5202, 'donations': 5203, 'doublestandard': 5204, 'drama': 5205, 'dummies': 5206, 'dylann': 5207, 'earrings': 5208, 'edgy': 5209, 'electrical': 5210, 'emirat': 5211, 'empress': 5212, 'eod': 5213, 'eren': 5214, 'ethnically': 5215, 'etsy': 5216, 'evasion': 5217, 'exam': 5218, 'existence': 5219, 'explanation': 5220, 'exposing': 5221, 'express': 5222, 'expressions': 5223, 'eyesvote': 5224, 'fakenewsmedia': 5225, 'farah': 5226, 'fare': 5227, 'fatty': 5228, 'fea': 5229, 'filth': 5230, 'finals': 5231, 'flattering': 5232, 'fluently': 5233, 'follow4follow': 5234, 'foot': 5235, 'forcing': 5236, 'freethenipple': 5237, 'freshmen': 5238, 'fruit': 5239, 'fuckdonaldtrump': 5240, 'furniture': 5241, 'gains': 5242, 'game7': 5243, 'geared': 5244, 'gel': 5245, 'gen': 5246, 'grabbing': 5247, 'graceless': 5248, 'grandma': 5249, 'grandpa': 5250, 'grenades': 5251, 'guard': 5252, 'gwent': 5253, 'hacking': 5254, 'hardcoresex': 5255, 'hatefilled': 5256, 'hathaway': 5257, 'hawk': 5258, 'healand': 5259, 'heater': 5260, 'hebrews': 5261, 'helm': 5262, 'herbal': 5263, 'hillbilly': 5264, 'hiphop': 5265, 'hitting': 5266, 'hiv': 5267, 'host': 5268, 'household': 5269, 'hubby': 5270, 'hull': 5271, 'hyperviolent': 5272, 'idolized': 5273, 'iefascistdemocracy': 5274, 'ifyourenotwhiteyourenotracist': 5275, 'ifyousawinyouwhatisee': 5276, 'ignorancehate': 5277, 'illegally': 5278, 'imagination': 5279, 'imitations': 5280, 'immigrant': 5281, 'impacts': 5282, 'inc': 5283, 'indie': 5284, 'indiegamedev': 5285, 'indisputably': 5286, 'infection': 5287, 'infidel': 5288, 'infographic': 5289, 'inhumans': 5290, 'intelligent': 5291, 'interacts': 5292, 'intrusively': 5293, 'irritations': 5294, 'islamophobic': 5295, 'israelis': 5296, 'it': 5297, 'italians': 5298, 'jehovas': 5299, 'jemele': 5300, 'jersey': 5301, 'jlaw': 5302, 'joseph': 5303, 'juan': 5304, 'juvenilejustice': 5305, 'karen': 5306, 'keithellisons': 5307, 'keithx': 5308, 'keshi': 5309, 'kicks': 5310, 'kindleunlimited': 5311, 'kingdom': 5312, 'knowledge': 5313, 'koetankauk': 5314, 'ksa': 5315, 'kwanza': 5316, 'leftistlunatics': 5317, 'legislature': 5318, 'lgbtfamilybut': 5319, 'lid': 5320, 'lifecoach': 5321, 'lisa': 5322, 'liverpool': 5323, 'loud': 5324, 'lunatics': 5325, 'lynching': 5326, 'mailbox': 5327, 'marathi': 5328, 'marginalized': 5329, 'marxism': 5330, 'marxs': 5331, 'masterkeyexperience': 5332, 'melbourne': 5333, 'meltingpot': 5334, 'messiah': 5335, 'metaphor': 5336, 'mic': 5337, 'mini': 5338, 'miriam': 5339, 'misandry': 5340, 'mispronouncing': 5341, 'mocked': 5342, 'mommy': 5343, 'moms': 5344, 'mormon': 5345, 'moronsmatter': 5346, 'moto': 5347, 'moveable': 5348, 'msian': 5349, 'munmid': 5350, 'mutual': 5351, 'mylove': 5352, 'nackt': 5353, 'nc': 5354, 'ndn': 5355, 'neighbours': 5356, 'newghostbusters': 5357, 'newyearsevemay': 5358, 'noone': 5359, 'norman': 5360, 'notamericaschoice': 5361, 'notmypresidnet': 5362, 'novel': 5363, 'nword': 5364, 'nyt': 5365, 'offensivememes69': 5366, 'onlywhitechristmas': 5367, 'oppression': 5368, 'outhmm': 5369, 'overexcited': 5370, 'panthers': 5371, 'panties': 5372, 'paper': 5373, 'peeps': 5374, 'pegida': 5375, 'perpetuating': 5376, 'petous': 5377, 'picked': 5378, 'playground': 5379, 'playlist': 5380, 'plenty': 5381, 'points': 5382, 'politicalmillennial': 5383, 'polls': 5384, 'porait': 5385, 'pornstars': 5386, 'pot': 5387, 'praise': 5388, 'preferences': 5389, 'prick': 5390, 'principal': 5391, 'productive': 5392, 'profitable': 5393, 'profiting': 5394, 'proimmigrant': 5395, 'promotions': 5396, 'prompts': 5397, 'pros': 5398, 'prosecutorialmisconduct': 5399, 'proudtobeanamerican': 5400, 'ps4': 5401, 'pussies': 5402, 'racewarchess': 5403, 'radio2': 5404, 'raids': 5405, 'random': 5406, 'rd': 5407, 'regnant': 5408, 'rendition': 5409, 'repulsion': 5410, 'repulsive': 5411, 'resilience': 5412, 'resolutions': 5413, 'responds': 5414, 'reunited': 5415, 'rhonda': 5416, 'richie': 5417, 'riders': 5418, 'rightie': 5419, 'rocks': 5420, 'rotten': 5421, 'rubio': 5422, 'russias': 5423, 'sabotage': 5424, 'saddles': 5425, 'sadness': 5426, 'santaproject': 5427, 'sblaar16': 5428, 'scapegoat': 5429, 'screenshot': 5430, 'sdoh': 5431, 'secretly': 5432, 'selling': 5433, 'semiretarded': 5434, 'sets': 5435, 'seventies': 5436, 'shadowed': 5437, 'shagged': 5438, 'shairi': 5439, 'sheep': 5440, 'shouts': 5441, 'showmeyoureawoman': 5442, 'skies': 5443, 'slapcam': 5444, 'sleeps': 5445, 'slug': 5446, 'slutty': 5447, 'songwords': 5448, 'soo': 5449, 'southsudan': 5450, 'spectrum': 5451, 'spokesperson': 5452, 'ss': 5453, 'stain': 5454, 'staying': 5455, 'staywoke': 5456, 'stockpiles': 5457, 'stopped': 5458, 'stopthewars': 5459, 'striker': 5460, 'struggling': 5461, 'studied': 5462, 'subversive': 5463, 'sulfur': 5464, 'supermariorun': 5465, 'suppos': 5466, 'swap': 5467, 'sympathisers': 5468, 'table': 5469, 'tape': 5470, 'tasar': 5471, 'tens': 5472, 'terraced': 5473, 'testing': 5474, 'tiffany': 5475, 'tl': 5476, 'toward': 5477, 'trapped': 5478, 'trouble': 5479, 'trumphate': 5480, 'trumpuniversity': 5481, 'unacceptable': 5482, 'uncivilised': 5483, 'uneducated': 5484, 'unicef': 5485, 'unsatisfied': 5486, 'upenn': 5487, 'usa2017': 5488, 'usdcad': 5489, 'ux': 5490, 'various': 5491, 'vengeance': 5492, 'verified': 5493, 'vetting': 5494, 'vi': 5495, 'vintage': 5496, 'vip': 5497, 'virgin': 5498, 'voteleave': 5499, 'wahhhh': 5500, 'waited': 5501, 'waking': 5502, 'wang': 5503, 'waronwomen': 5504, 'washington': 5505, 'wee': 5506, 'wellbeing': 5507, 'whaling': 5508, 'whatsoever': 5509, 'whenever': 5510, 'whiners': 5511, 'whiteland': 5512, 'whitepower': 5513, 'whitesboro': 5514, 'whoop': 5515, 'wingers': 5516, 'wit': 5517, 'yoyas': 5518, 'zionists': 5519, '15thcentury': 5520, '2017npr': 5521, '22': 5522, '23': 5523, '49': 5524, '72': 5525, '940pm': 5526, '99cents': 5527, '99p': 5528, 'absence': 5529, 'achilles': 5530, 'actively': 5531, 'activism': 5532, 'adamitv': 5533, 'adveising': 5534, 'afamhist': 5535, 'aka': 5536, 'alleges': 5537, 'amoral': 5538, 'amounts': 5539, 'andrewanglin': 5540, 'angrygaymafia': 5541, 'anna': 5542, 'announced': 5543, 'anothr': 5544, 'antiblack': 5545, 'apa': 5546, 'appendix': 5547, 'applicable': 5548, 'appropriating': 5549, 'assaults': 5550, 'asssssworst': 5551, 'attendants': 5552, 'atwater': 5553, 'audism': 5554, 'authoritarian': 5555, 'averse': 5556, 'background': 5557, 'bake': 5558, 'bald': 5559, 'bambi': 5560, 'bates': 5561, 'bath': 5562, 'bats': 5563, 'baycat': 5564, 'berniebros': 5565, 'betwn': 5566, 'bible': 5567, 'bikinis': 5568, 'bin': 5569, 'bitchesbelike': 5570, 'blasting': 5571, 'bleating': 5572, 'bloated': 5573, 'bloggersblast': 5574, 'blowing': 5575, 'bogard': 5576, 'bogota': 5577, 'bottle': 5578, 'boycotting': 5579, 'bragged': 5580, 'brave': 5581, 'bregret': 5582, 'bridges': 5583, 'britains': 5584, 'broad': 5585, 'buggeroffboris': 5586, 'bureau': 5587, 'busi': 5588, 'cable': 5589, 'candy': 5590, 'caoon': 5591, 'capone': 5592, 'carefully': 5593, 'carolyn': 5594, 'chaer': 5595, 'chapter': 5596, 'charlespaladinos': 5597, 'cheaters': 5598, 'chepstow': 5599, 'chihuahuas': 5600, 'chri': 5601, 'chump': 5602, 'cleaning': 5603, 'climate': 5604, 'clio': 5605, 'cocktails': 5606, 'codified': 5607, 'colluder': 5608, 'colored': 5609, 'colours': 5610, 'comic': 5611, 'commit': 5612, 'comrade': 5613, 'conces': 5614, 'condemn': 5615, 'contender': 5616, 'cook': 5617, 'cookies': 5618, 'costars': 5619, 'cottoned': 5620, 'countryis': 5621, 'crashes': 5622, 'creates': 5623, 'crook': 5624, 'cuba': 5625, 'cutie': 5626, 'cutsie': 5627, 'cycling': 5628, 'cymru': 5629, 'damage': 5630, 'dankmemes': 5631, 'dares': 5632, 'daughters': 5633, 'deafed': 5634, 'deafinprison': 5635, 'deceit': 5636, 'decriminaliz': 5637, 'defenders': 5638, 'delhi': 5639, 'demanding': 5640, 'demonic': 5641, 'demonstrated': 5642, 'denzelwashington': 5643, 'deploraball': 5644, 'destroying': 5645, 'detoxdiet': 5646, 'dictionaries': 5647, 'diff': 5648, 'digits': 5649, 'direction': 5650, 'director': 5651, 'dis': 5652, 'discredit': 5653, 'displaced': 5654, 'djt': 5655, 'doors': 5656, 'draftdodger': 5657, 'drivers': 5658, 'drsuess': 5659, 'eahs': 5660, 'echo': 5661, 'educated': 5662, 'electoralcollege': 5663, 'elects': 5664, 'embrac': 5665, 'empowering': 5666, 'error': 5667, 'espn': 5668, 'eternity': 5669, 'eugenie': 5670, 'eurosceptic': 5671, 'evan': 5672, 'everlord': 5673, 'example': 5674, 'excrement': 5675, 'expecttrumps': 5676, 'extra': 5677, 'fallacy': 5678, 'falling': 5679, 'fam': 5680, 'familiesfamilycircus': 5681, 'faster': 5682, 'fatherhood': 5683, 'fbi': 5684, 'fearlessly': 5685, 'federline': 5686, 'feedback': 5687, 'feigned': 5688, 'females': 5689, 'flagday': 5690, 'flushed': 5691, 'forbidden': 5692, 'foretold': 5693, 'fragile': 5694, 'freed': 5695, 'ft': 5696, 'fundraising': 5697, 'gang': 5698, 'gays': 5699, 'gee': 5700, 'genderequity': 5701, 'genuine': 5702, 'gin': 5703, 'giphy': 5704, 'glasgow': 5705, 'goodlife': 5706, 'goyim': 5707, 'gracemugabe': 5708, 'gracious': 5709, 'grade': 5710, 'greater': 5711, 'grey': 5712, 'gt6': 5713, 'guzzling': 5714, 'h20': 5715, 'hahaha': 5716, 'haim': 5717, 'hamilton': 5718, 'hanging': 5719, 'hanukkah': 5720, 'harbaugh': 5721, 'harmful': 5722, 'hasbeen': 5723, 'hbd': 5724, 'headline': 5725, 'heck': 5726, 'herschlags': 5727, 'hidden': 5728, 'hollaback': 5729, 'homemade': 5730, 'homophobiaenablin': 5731, 'homoresistant': 5732, 'hye': 5733, 'hyena': 5734, 'idiocy': 5735, 'ignorantly': 5736, 'illegals': 5737, 'impressions': 5738, 'inbred': 5739, 'incidents': 5740, 'increase': 5741, 'increasing': 5742, 'indoctrination': 5743, 'infantile': 5744, 'ink': 5745, 'insanity': 5746, 'institutionalized': 5747, 'intermittently': 5748, 'interterrestrial': 5749, 'invest': 5750, 'invisible': 5751, 'iqgt95': 5752, 'irishman': 5753, 'islamicradicals': 5754, 'islamopho': 5755, 'isolationist': 5756, 'jailed': 5757, 'jerodtwin': 5758, 'jgirl': 5759, 'joemixon': 5760, 'jonjo': 5761, 'journos': 5762, 'joyful': 5763, 'joytrain': 5764, 'juice': 5765, 'jumpedtheshark': 5766, 'keepyoureyesontheprizemlk': 5767, 'kicked': 5768, 'kickitout': 5769, 'kimburrell': 5770, 'kindly': 5771, 'kisses': 5772, 'kissmyarse': 5773, 'koolaid': 5774, 'kpop': 5775, 'krakow': 5776, 'laborviolations': 5777, 'labour': 5778, 'lackey': 5779, 'lanasprayberry': 5780, 'latesnews': 5781, 'lawmakers': 5782, 'leaveeu': 5783, 'letsdothis': 5784, 'lg': 5785, 'liberalisme': 5786, 'lipstick': 5787, 'littlemix': 5788, 'liveleap': 5789, 'lobotomy': 5790, 'loosahs': 5791, 'losers': 5792, 'loveyourself': 5793, 'lpc': 5794, 'mail': 5795, 'mailboxpride': 5796, 'malayalees': 5797, 'malign': 5798, 'malikrichmond': 5799, 'marrying': 5800, 'massincarceration': 5801, 'mating': 5802, 'measure': 5803, 'medical': 5804, 'merely': 5805, 'mfer': 5806, 'midtown': 5807, 'milwaukee': 5808, 'minnesotavikings': 5809, 'minoritypresident': 5810, 'miserable': 5811, 'misrepresentation': 5812, 'mogage': 5813, 'monkey': 5814, 'monsters': 5815, 'moonman': 5816, 'morgan': 5817, 'mugabe': 5818, 'multiracial': 5819, 'mylife': 5820, 'naacp': 5821, 'narcissimmisogyny': 5822, 'nashville': 5823, 'neogaf': 5824, 'neverdelta': 5825, 'nohate': 5826, 'nonjews': 5827, 'nowadays': 5828, 'nudist': 5829, 'nudity': 5830, 'nutrition': 5831, 'nye16': 5832, 'oab': 5833, 'observation': 5834, 'obsidian': 5835, 'offences': 5836, 'offering': 5837, 'opponent': 5838, 'opposing': 5839, 'oprollredroll': 5840, 'orangeisthenewblack': 5841, 'orlandonightclubshooting': 5842, 'outdoors': 5843, 'outsider': 5844, 'ovr': 5845, 'owned': 5846, 'pack': 5847, 'paisanhack': 5848, 'pandy': 5849, 'pants': 5850, 'paranormal': 5851, 'pasture': 5852, 'pattismith': 5853, 'peotus': 5854, 'permit': 5855, 'perpetuates': 5856, 'phones': 5857, 'picturebangladeshi': 5858, 'pissed': 5859, 'playa': 5860, 'pleased': 5861, 'poet': 5862, 'precision': 5863, 'premiere': 5864, 'prepare': 5865, 'preparing': 5866, 'presidentpussygrabber': 5867, 'prize': 5868, 'production': 5869, 'professional': 5870, 'proo': 5871, 'propalestine': 5872, 'propey': 5873, 'pubfriction': 5874, 'pulled': 5875, 'pupil': 5876, 'purge': 5877, 'putinpuppet': 5878, 'qualify': 5879, 'questioned': 5880, 'quiz': 5881, 'qusay': 5882, 'racetraitors': 5883, 'racismfacinghis': 5884, 'raining': 5885, 'range': 5886, 'rap': 5887, 'reactionary': 5888, 'readingbuying': 5889, 'reagans': 5890, 'realhousewives': 5891, 'reappraise': 5892, 'recipe': 5893, 'reconciliation': 5894, 'reg': 5895, 'regret': 5896, 'reich': 5897, 'republicantrash': 5898, 'reso': 5899, 'reson': 5900, 'responsible': 5901, 'rethinkdiscipline': 5902, 'returns': 5903, 'rhoa': 5904, 'rn': 5905, 'roadtrip': 5906, 'robinson': 5907, 'rottweilers': 5908, 'routine': 5909, 'royal': 5910, 'rugby': 5911, 'sand': 5912, 'sandy': 5913, 'schedule': 5914, 'schuster': 5915, 'sciencefiction': 5916, 'score': 5917, 'seattle': 5918, 'selection': 5919, 'separate': 5920, 'servants': 5921, 'sexualharassment': 5922, 'sheree': 5923, 'shitler': 5924, 'shocker': 5925, 'simon': 5926, 'sirf': 5927, 'sistersofcolour': 5928, 'slash': 5929, 'smiley': 5930, 'sms': 5931, 'snaps': 5932, 'soft': 5933, 'solutions': 5934, 'souls': 5935, 'southsudankot': 5936, 'spam': 5937, 'spirituality': 5938, 'staer': 5939, 'starspangledbanner': 5940, 'steubenville': 5941, 'stocking': 5942, 'stpp': 5943, 'strange': 5944, 'strawberries': 5945, 'struggles': 5946, 'stuffed': 5947, 'stupidto': 5948, 'sucked': 5949, 'sucklahoma': 5950, 'suggests': 5951, 'suing': 5952, 'surgical': 5953, 'suspended': 5954, 'swag': 5955, 'swampmonster': 5956, 'swastikas': 5957, 'sweden': 5958, 'talkradio': 5959, 'tennis': 5960, 'third': 5961, 'throwbackthursday': 5962, 'tiesto': 5963, 'tinyhands': 5964, 'tits': 5965, 'transphobia': 5966, 'trentmays': 5967, 'tribalism': 5968, 'truthand': 5969, 'tsa': 5970, 'tt': 5971, 'tucker': 5972, 'twitte': 5973, 'tyrant': 5974, 'uday': 5975, 'ultraohodox': 5976, 'understanding': 5977, 'upbeat': 5978, 'usdjpy': 5979, 'uspoli': 5980, 'usually': 5981, 'uttered': 5982, 'validating': 5983, 'vestal': 5984, 'victimhood': 5985, 'vlicobs': 5986, 'vot': 5987, 'wailed': 5988, 'wattpad': 5989, 'waves': 5990, 'wb': 5991, 'webcam': 5992, 'weirdo': 5993, 'wellduh': 5994, 'wells': 5995, 'what2inlieuof': 5996, 'whate': 5997, 'wherford': 5998, 'whiteamerica': 5999, 'whiteisis': 6000, 'whiter': 6001, 'whitesheet': 6002, 'willsmith': 6003, 'wmn': 6004, 'wn': 6005, 'womans': 6006, 'womenrelated': 6007, 'womensissues': 6008, 'womensrights': 6009, 'wordsalad': 6010, 'worksheet': 6011, 'worlds': 6012, 'wwdc2016': 6013, 'wwe': 6014, 'wwwsmallgirlsexcom': 6015, 'wyou': 6016, 'xd': 6017, 'xenophobe': 6018, 'yavin': 6019, 'youngturks': 6020, 'zurich': 6021, '2015': 6022, '2b': 6023, '38billion': 6024, '399': 6025, '48': 6026, '500': 6027, '5wordtrumplethinskin': 6028, '6th': 6029, '8': 6030, 'aberdeen': 6031, 'accident': 6032, 'accounts': 6033, 'acct': 6034, 'achieve': 6035, 'acknowledge': 6036, 'admin': 6037, 'afl': 6038, 'alarm': 6039, 'alcoholic': 6040, 'aleppo': 6041, 'alliesforblacklives': 6042, 'amarinder': 6043, 'americadoesntwantyou': 6044, 'amsterdam': 6045, 'annoy': 6046, 'anslinger': 6047, 'antimorality': 6048, 'appreciation': 6049, 'ark': 6050, 'armpit': 6051, 'ashiq': 6052, 'atherapy': 6053, 'attire': 6054, 'attorneys': 6055, 'autocorrects': 6056, 'avenue': 6057, 'aw': 6058, 'awork': 6059, 'bagdad': 6060, 'bankruptcy': 6061, 'bcot': 6062, 'berlinattack': 6063, 'bestoftheday': 6064, 'bestsellers': 6065, 'betrays': 6066, 'beutiful': 6067, 'beverlywhalingreinstated': 6068, 'biracial': 6069, 'blackman': 6070, 'blueeyesbrowneyes': 6071, 'blues': 6072, 'bodyimage': 6073, 'boycotts': 6074, 'breathe': 6075, 'burka': 6076, 'butterfly': 6077, 'cafe': 6078, 'cambersands': 6079, 'canpoli': 6080, 'cantankerous': 6081, 'carlpaladinos': 6082, 'cast': 6083, 'cbc': 6084, 'cctv': 6085, 'celebrations': 6086, 'cept': 6087, 'championed': 6088, 'chanapa': 6089, 'changing': 6090, 'chauvinist': 6091, 'choked': 6092, 'chris': 6093, 'christia': 6094, 'classifying': 6095, 'clickbait': 6096, 'closely': 6097, 'closing': 6098, 'cm': 6099, 'coast': 6100, 'coldplaywembley': 6101, 'combover': 6102, 'comingsoon': 6103, 'comprehend': 6104, 'confirm': 6105, 'consideration': 6106, 'contact': 6107, 'coopting': 6108, 'cosplay': 6109, 'crackalogic': 6110, 'craft': 6111, 'crazed': 6112, 'creativity': 6113, 'cried': 6114, 'crimes': 6115, 'cringe': 6116, 'culpable': 6117, 'curry': 6118, 'cussed': 6119, 'daddys': 6120, 'dangerwith': 6121, 'danske': 6122, 'darn': 6123, 'deals': 6124, 'debacle': 6125, 'deltaairlines': 6126, 'demoralizing': 6127, 'denied': 6128, 'destiny': 6129, 'disaster': 6130, 'disc': 6131, 'disciples': 6132, 'discoved': 6133, 'dismantling': 6134, 'disneylooney': 6135, 'dissent': 6136, 'divider': 6137, 'dl': 6138, 'domestic': 6139, 'domestics': 6140, 'domnica': 6141, 'donaldtrumps': 6142, 'dublin': 6143, 'duca': 6144, 'dumbfuck': 6145, 'dzange': 6146, 'earlier': 6147, 'educationfest': 6148, 'effects': 6149, 'eggs': 6150, 'egregious': 6151, 'elected': 6152, 'ellicottsquarebuilding': 6153, 'emb': 6154, 'emotion': 6155, 'enable': 6156, 'encounters': 6157, 'endhate': 6158, 'endofcivilization': 6159, 'entrenched': 6160, 'erica': 6161, 'errand': 6162, 'euros2016': 6163, 'everydayheroes': 6164, 'expensive': 6165, 'expes': 6166, 'failed': 6167, 'fails': 6168, 'fakenewsale': 6169, 'fakequity': 6170, 'fargo': 6171, 'farm': 6172, 'feelinggood': 6173, 'fights': 6174, 'finale': 6175, 'finishing': 6176, 'flat': 6177, 'floor': 6178, 'fml': 6179, 'followed': 6180, 'follower': 6181, 'follows': 6182, 'fooh': 6183, 'forecasted': 6184, 'forecasts': 6185, 'freeman': 6186, 'frm': 6187, 'frog': 6188, 'funde': 6189, 'fuuuuuuuuuuuuck': 6190, 'gardening': 6191, 'gaza': 6192, 'gemini': 6193, 'genocidedefender': 6194, 'getreal': 6195, 'gfy': 6196, 'girles': 6197, 'glastonbury': 6198, 'glorious': 6199, 'golden': 6200, 'goodread': 6201, 'grief': 6202, 'gt10': 6203, 'gt4': 6204, 'gtfoh': 6205, 'hags': 6206, 'hairly': 6207, 'handle': 6208, 'hangout': 6209, 'hatefear': 6210, 'hc': 6211, 'heabreak': 6212, 'healthmen': 6213, 'herbalremedies': 6214, 'highlight': 6215, 'hillarylost': 6216, 'hipster': 6217, 'hmmmsounds': 6218, 'homosexuality': 6219, 'hoodiesup': 6220, 'horse': 6221, 'hostility': 6222, 'hotline': 6223, 'hr': 6224, 'hube': 6225, 'huffington': 6226, 'humphrey': 6227, 'ignoring': 6228, 'imo': 6229, 'implication': 6230, 'improve': 6231, 'in2017iwantto': 6232, 'incompitent': 6233, 'inevitable': 6234, 'infinite': 6235, 'influence': 6236, 'insident': 6237, 'inspiring': 6238, 'instilling': 6239, 'intercourse': 6240, 'intervene': 6241, 'itmaybe': 6242, 'itz': 6243, 'jacket': 6244, 'jewellery': 6245, 'jewishjoke': 6246, 'jind': 6247, 'jocox': 6248, 'jolly': 6249, 'juniors': 6250, 'karachi': 6251, 'keyboard': 6252, 'kit': 6253, 'klux': 6254, 'korean': 6255, 'las': 6256, 'latinas': 6257, 'laughable': 6258, 'lifeguards': 6259, 'lineup': 6260, 'lion': 6261, 'lip': 6262, 'loathsome': 6263, 'lockerroomtalk': 6264, 'lolol': 6265, 'longhair': 6266, 'lookin': 6267, 'lostpopularvote': 6268, 'loudly': 6269, 'lush': 6270, 'maga3x': 6271, 'malvern': 6272, 'manipur': 6273, 'manmade': 6274, 'marathon': 6275, 'marchampit': 6276, 'marco': 6277, 'mateen': 6278, 'matt': 6279, 'med': 6280, 'meets': 6281, 'mei': 6282, 'mentalillness': 6283, 'metal': 6284, 'midweek': 6285, 'misogony': 6286, 'missyou': 6287, 'mix': 6288, 'monsoon': 6289, 'mormonis': 6290, 'mothers': 6291, 'moviemad': 6292, 'musictherapy': 6293, 'muthafucka': 6294, 'myth': 6295, 'naming': 6296, 'narcissistic': 6297, 'natalie': 6298, 'naturism': 6299, 'nds': 6300, 'negativity': 6301, 'newmusic': 6302, 'nightout': 6303, 'nofur': 6304, 'norrisgreen': 6305, 'noyulin': 6306, 'nu': 6307, 'obsessed': 6308, 'oldwoman': 6309, 'opendemocracy': 6310, 'opened': 6311, 'ordinary': 6312, 'orientation': 6313, 'overrode': 6314, 'pageant': 6315, 'paint': 6316, 'pamelaramseytaylorpraised': 6317, 'parentage': 6318, 'passengers': 6319, 'payintheusa': 6320, 'peppa': 6321, 'perks': 6322, 'personal': 6323, 'peruvian': 6324, 'piggies': 6325, 'pitch': 6326, 'pizzagate': 6327, 'placed': 6328, 'plymouth': 6329, 'policeman': 6330, 'polishgirl': 6331, 'poweothewomen': 6332, 'pr': 6333, 'predictions': 6334, 'prep': 6335, 'presentation': 6336, 'presidenttrump': 6337, 'presiding': 6338, 'prevalent': 6339, 'preview': 6340, 'primed': 6341, 'print': 6342, 'prints': 6343, 'pro': 6344, 'progresverebel': 6345, 'psychopath': 6346, 'pulsenightclub': 6347, 'puntohost': 6348, 'purchased': 6349, 'racialequity': 6350, 'rad': 6351, 'rainy': 6352, 'randomly': 6353, 'recast': 6354, 'receive': 6355, 'reefers': 6356, 'reel': 6357, 'refers': 6358, 'reminds': 6359, 'remix': 6360, 'remote': 6361, 'repetitive': 6362, 'replacyclay': 6363, 'republicwhitepower': 6364, 'reunion': 6365, 'ridiculous': 6366, 'rightist': 6367, 'robbiewilliams': 6368, 'rolling': 6369, 'saga': 6370, 'saysomething': 6371, 'scars': 6372, 'scenes': 6373, 'scraps': 6374, 'screwed': 6375, 'scurvy': 6376, 'sewage': 6377, 'shitfest': 6378, 'shots': 6379, 'shrouded': 6380, 'sizescuts': 6381, 'sketch': 6382, 'skills': 6383, 'skincare': 6384, 'smaller': 6385, 'socks': 6386, 'soed': 6387, 'soooo': 6388, 'soreloser': 6389, 'sources': 6390, 'spa': 6391, 'speed': 6392, 'square': 6393, 'stadium': 6394, 'starek': 6395, 'starwars': 6396, 'starwarsrogueone': 6397, 'stateampfederal': 6398, 'steak': 6399, 'stopsexism': 6400, 'stoptalking': 6401, 'stupids': 6402, 'substantive': 6403, 'sue': 6404, 'suffer': 6405, 'superb': 6406, 'surprises': 6407, 'sweetie': 6408, 'swing': 6409, 'swingtanzenverboten': 6410, 'symbols': 6411, 'sympathized': 6412, 'talks': 6413, 'tattler': 6414, 'tedinstitute': 6415, 'tefugeeswelcome': 6416, 'tenacity': 6417, 'termed': 6418, 'terrorize': 6419, 'tests': 6420, 'theconjuring2': 6421, 'throwing': 6422, 'thugs': 6423, 'tiger': 6424, 'toonsthemed': 6425, 'toys': 6426, 'trails': 6427, 'tree': 6428, 'trek': 6429, 'tremendous': 6430, 'trolling': 6431, 'tweeting': 6432, 'twins': 6433, 'twitterbot': 6434, 'unchained': 6435, 'uncov': 6436, 'unfit': 6437, 'upload': 6438, 'urdu': 6439, 'urging': 6440, 'users': 6441, 'vedic': 6442, 'vendor': 6443, 'verizon': 6444, 'vid': 6445, 'villy': 6446, 'visiting': 6447, 'vital': 6448, 'vlog': 6449, 'vogue': 6450, 'vulgar': 6451, 'wakeuppeopl3': 6452, 'warcraft': 6453, 'warsforoil': 6454, 'watc': 6455, 'wembley': 6456, 'whining': 6457, 'whitefragility': 6458, 'whiteman': 6459, 'whiteslavers': 6460, 'whiteslavery': 6461, 'whitey': 6462, 'williams': 6463, 'wiunion': 6464, 'womandegrading': 6465, 'woo': 6466, 'woot': 6467, 'wotching': 6468, 'ww': 6469, 'xboxe3': 6470, 'yiannopoulos': 6471, 'yoy': 6472, 'zelda': 6473, 'zone': 6474, '10k': 6475, '123': 6476, '12mill': 6477, '13479': 6478, '201': 6479, '20th': 6480, '26th': 6481, '28': 6482, '2nites': 6483, '4maps': 6484, '53': 6485, '60': 6486, '6yearolds': 6487, '9th': 6488, 'abandoned': 6489, 'abeed': 6490, 'aberrant': 6491, 'abundance': 6492, 'accessories': 6493, 'active': 6494, 'adding': 6495, 'advance': 6496, 'advantage': 6497, 'affiliated': 6498, 'ageoftrump': 6499, 'ahhhh': 6500, 'aists': 6501, 'aleksandr': 6502, 'allbc': 6503, 'allgrownup': 6504, 'amanda': 6505, 'ampfunny': 6506, 'anacoes': 6507, 'anf': 6508, 'antiegalitarianism': 6509, 'antisemetic': 6510, 'antiwhites': 6511, 'archangels': 6512, 'areclueless': 6513, 'armenian': 6514, 'army': 6515, 'arnt': 6516, 'arrests60mill': 6517, 'assets': 6518, 'asslicker': 6519, 'astrology': 6520, 'atm': 6521, 'attractive': 6522, 'audusd': 6523, 'augusta': 6524, 'authority': 6525, 'avitar': 6526, 'babygirl': 6527, 'badal': 6528, 'barely': 6529, 'bedifferent': 6530, 'beginner': 6531, 'bellyup': 6532, 'berating': 6533, 'beside': 6534, 'bestof2016': 6535, 'betrayed': 6536, 'bite': 6537, 'blacksagainsttrump': 6538, 'blackvotehispanicvote': 6539, 'blessampkeep': 6540, 'blessedt': 6541, 'blessing': 6542, 'bones': 6543, 'booking': 6544, 'bookpublishing': 6545, 'bottles': 6546, 'bottom': 6547, 'bound': 6548, 'bozzoli': 6549, 'brag': 6550, 'breath': 6551, 'bringiton': 6552, 'brithday': 6553, 'brussels': 6554, 'bt': 6555, 'burbank': 6556, 'cali': 6557, 'camera': 6558, 'cardiff': 6559, 'cared': 6560, 'carlpalidino': 6561, 'carlpalladino': 6562, 'catcalled': 6563, 'cd': 6564, 'cedm': 6565, 'celebrated': 6566, 'cent': 6567, 'central': 6568, 'champagne': 6569, 'cheeky': 6570, 'chess': 6571, 'chile': 6572, 'chinguas': 6573, 'citizenaction': 6574, 'citizenry': 6575, 'citys': 6576, 'classifieds': 6577, 'coaching': 6578, 'colorful': 6579, 'communismislam': 6580, 'competitiveness': 6581, 'completed': 6582, 'condoning': 6583, 'connected': 6584, 'connection': 6585, 'consider': 6586, 'constrained': 6587, 'consumers': 6588, 'continuous': 6589, 'cora': 6590, 'cornered': 6591, 'corps': 6592, 'correlation': 6593, 'cos': 6594, 'croatia': 6595, 'cruise': 6596, 'cuddle': 6597, 'dadsavailable': 6598, 'daisy': 6599, 'dates': 6600, 'dayoff': 6601, 'deadly': 6602, 'decor': 6603, 'defended': 6604, 'defined': 6605, 'degrees': 6606, 'dese': 6607, 'desse': 6608, 'devil': 6609, 'dey': 6610, 'diagram': 6611, 'dictate': 6612, 'diddo': 6613, 'diisgusted': 6614, 'disappointment': 6615, 'disdain': 6616, 'distance': 6617, 'distinct': 6618, 'doctrine': 6619, 'doggy': 6620, 'dogsarejoy': 6621, 'doublestan': 6622, 'dreamcatcher': 6623, 'dropping': 6624, 'drove': 6625, 'ducks': 6626, 'dugin': 6627, 'dust': 6628, 'economicsanctions': 6629, 'edit': 6630, 'embodied': 6631, 'embraceyourself': 6632, 'emergency': 6633, 'emmastone': 6634, 'engrus': 6635, 'enjoyed': 6636, 'epcot': 6637, 'era': 6638, 'eurgbp': 6639, 'euroconservatism': 6640, 'excite': 6641, 'exhibition': 6642, 'existed': 6643, 'existing': 6644, 'explained': 6645, 'explore': 6646, 'expressed': 6647, 'fairytail': 6648, 'fantasy': 6649, 'faraz': 6650, 'farmers': 6651, 'fate': 6652, 'fault': 6653, 'featured': 6654, 'feb': 6655, 'fightback': 6656, 'firstfamily': 6657, 'flt': 6658, 'footballbut': 6659, 'founder': 6660, 'frances': 6661, 'fridays': 6662, 'gain': 6663, 'galib': 6664, 'garage': 6665, 'garbagepailkids': 6666, 'gates': 6667, 'gator': 6668, 'gators': 6669, 'genealogy': 6670, 'germanys': 6671, 'ghost': 6672, 'goofy': 6673, 'gopro': 6674, 'gr8': 6675, 'grabyourwallet': 6676, 'graduating': 6677, 'granted': 6678, 'grassrootsaction': 6679, 'greatday': 6680, 'gross': 6681, 'guidance': 6682, 'gunviolence': 6683, 'habits': 6684, 'happynewyear2017': 6685, 'harassers': 6686, 'harry': 6687, 'harsh': 6688, 'heed': 6689, 'heeded': 6690, 'heritage': 6691, 'highereducation': 6692, 'hilton': 6693, 'himselffor': 6694, 'homeopathic': 6695, 'honor': 6696, 'horizonalways': 6697, 'hottweets': 6698, 'humiliating': 6699, 'humorous': 6700, 'hustle': 6701, 'hyped': 6702, 'icons': 6703, 'ideal': 6704, 'in': 6705, 'incl': 6706, 'independence': 6707, 'inexistence': 6708, 'informing': 6709, 'inspirechange': 6710, 'instagay': 6711, 'instasize': 6712, 'internalize': 6713, 'iqbal': 6714, 'irl': 6715, 'irrefutable': 6716, 'isr': 6717, 'jacobmarley': 6718, 'jarodtaylor': 6719, 'jerk': 6720, 'ji': 6721, 'jibes': 6722, 'judd': 6723, 'justin': 6724, 'justjustice': 6725, 'ka': 6726, 'katiesikphoto': 6727, 'kelly': 6728, 'kept': 6729, 'kin': 6730, 'kits': 6731, 'label': 6732, 'large': 6733, 'largelyunrepentant': 6734, 'larger': 6735, 'launches': 6736, 'lawyer': 6737, 'leak': 6738, 'lean': 6739, 'leavers': 6740, 'leftard': 6741, 'legitimizing': 6742, 'lehman': 6743, 'lemans24': 6744, 'libtards': 6745, 'limited': 6746, 'lips': 6747, 'lithuania': 6748, 'livelypics': 6749, 'locks': 6750, 'lovebeingalegend': 6751, 'loveher': 6752, 'loveisland': 6753, 'loveknow': 6754, 'lovinglife': 6755, 'lowest': 6756, 'lt33': 6757, 'lunchtime': 6758, 'mam': 6759, 'managed': 6760, 'manga': 6761, 'mannequinchallenge': 6762, 'masjids': 6763, 'matthew': 6764, 'meat': 6765, 'megaupload': 6766, 'mentioning': 6767, 'merge': 6768, 'mess': 6769, 'mgt': 6770, 'miloyiannopoulos': 6771, 'mindful': 6772, 'misconceptions': 6773, 'mm': 6774, 'mock': 6775, 'models': 6776, 'mohsin': 6777, 'moi': 6778, 'mondaymorning': 6779, 'moneyovermorals': 6780, 'mongering': 6781, 'monthly': 6782, 'motivational': 6783, 'mountain': 6784, 'movements': 6785, 'mug': 6786, 'musicvideo': 6787, 'muzzies': 6788, 'nap': 6789, 'narcissism': 6790, 'natt': 6791, 'neo': 6792, 'neverforget': 6793, 'newlook': 6794, 'newyorker': 6795, 'noborders': 6796, 'nofear': 6797, 'nohern': 6798, 'nosurprise': 6799, 'notmypresidenttheresista': 6800, 'november': 6801, 'nowi': 6802, 'oakland': 6803, 'odds': 6804, 'officebearers': 6805, 'opposite': 6806, 'ordination': 6807, 'outta': 6808, 'owner': 6809, 'padded': 6810, 'paies': 6811, 'pair': 6812, 'panda': 6813, 'peacein2017': 6814, 'penis': 6815, 'personally': 6816, 'pharrell': 6817, 'phase': 6818, 'philippines': 6819, 'photographs': 6820, 'pieces': 6821, 'placement': 6822, 'planned': 6823, 'pokemon': 6824, 'policecivilian': 6825, 'politicalcorrectness': 6826, 'polygamist': 6827, 'pops': 6828, 'possibly': 6829, 'prayfoheworld': 6830, 'predator': 6831, 'pregnancy': 6832, 'premier': 6833, 'preparation': 6834, 'presidential': 6835, 'pretending': 6836, 'primary': 6837, 'procreate': 6838, 'producer': 6839, 'profiled': 6840, 'prom': 6841, 'promotion': 6842, 'propose': 6843, 'psychology': 6844, 'publicpolicy': 6845, 'publicserviceannouncement': 6846, 'pulseshooting': 6847, 'punks': 6848, 'puppies': 6849, 'qualifications': 6850, 'queens': 6851, 'raceonly': 6852, 'raghuramrajan': 6853, 'ramadhan': 6854, 'rarethous': 6855, 'rate': 6856, 'rats': 6857, 'rbc': 6858, 'reali': 6859, 'rebeccas': 6860, 'receiving': 6861, 'recognition': 6862, 'reflected': 6863, 'reggae': 6864, 'regime': 6865, 'reinventimpossible': 6866, 'rejects': 6867, 'relations': 6868, 'relaxation': 6869, 'respected': 6870, 'resultant': 6871, 'rg': 6872, 'ridiculously': 6873, 'rifle': 6874, 'rings': 6875, 'river': 6876, 'rob': 6877, 'roughly': 6878, 'roughwaters': 6879, 'runner': 6880, 'russiagate': 6881, 'saddest': 6882, 'sakesstop': 6883, 'salute': 6884, 'sanfrancisco': 6885, 'satire': 6886, 'scatter': 6887, 'scientists': 6888, 'sculpted': 6889, 'seaside': 6890, 'security': 6891, 'selfish': 6892, 'serving': 6893, 'seven': 6894, 'sewer': 6895, 'sf': 6896, 'shameless': 6897, 'shark': 6898, 'shuts': 6899, 'sigh': 6900, 'signing': 6901, 'sketches': 6902, 'skinheads': 6903, 'smith': 6904, 'snatch': 6905, 'socialist': 6906, 'sofa': 6907, 'solved': 6908, 'spaces': 6909, 'standard': 6910, 'stays': 6911, 'stepmom': 6912, 'stereotyping': 6913, 'stocks': 6914, 'stranger': 6915, 'strident': 6916, 'styles': 6917, 'substantially': 6918, 'suffrage': 6919, 'sundays': 6920, 'sunnyday': 6921, 'sunrise': 6922, 'supremacists': 6923, 'surly': 6924, 'surrogate': 6925, 'sway': 6926, 'sweety': 6927, 'tacky': 6928, 'tagsforlikes': 6929, 'tainting': 6930, 'talked': 6931, 'tall': 6932, 'taylor': 6933, 'teaser': 6934, 'techno': 6935, 'tellall': 6936, 'terming': 6937, 'texts': 6938, 'thesexismproject': 6939, 'thinkbigsundaywithmarsha': 6940, 'thrilled': 6941, 'throat': 6942, 'thwaed': 6943, 'tick': 6944, 'tom': 6945, 'tower': 6946, 'triggerwarning': 6947, 'troubles': 6948, 'truck': 6949, 'truckload': 6950, 'truism': 6951, 'trumpinauguration': 6952, 'tunes': 6953, 'tupac': 6954, 'tyruswong': 6955, 'ultranationalism': 6956, 'unfounate': 6957, 'uni': 6958, 'unique': 6959, 'unite': 6960, 'unpatriotic': 6961, 'unreal': 6962, 'unwanted': 6963, 'ups': 6964, 'usshootingsaccidentsabuse': 6965, 'vividsydney': 6966, 'voicing': 6967, 'vomit': 6968, 'votersuppression': 6969, 'vp': 6970, 'waime': 6971, 'wakes': 6972, 'warcraftmovie': 6973, 'warmonger': 6974, 'wasi': 6975, 'wellthese': 6976, 'weneed': 6977, 'wew': 6978, 'wig': 6979, 'witho': 6980, 'womensordination': 6981, 'wooden': 6982, 'worlddanger': 6983, 'year8': 6984, 'yelling': 6985, 'yogi': 6986, 'yong': 6987, '01': 6988, '100k': 6989, '1306': 6990, '1600': 6991, '18th': 6992, '1960': 6993, '2012': 6994, '2016election': 6995, '2016highlights': 6996, '2016i': 6997, '2016release': 6998, '2017fail': 6999, '299': 7000, '2day': 7001, '3': 7002, '4': 7003, '400000': 7004, '8990': 7005, '8th': 7006, '93': 7007, 'ability': 7008, 'ace': 7009, 'admit': 7010, 'adrenaline': 7011, 'affect': 7012, 'ages': 7013, 'alumni': 7014, 'amreading': 7015, 'andysessions': 7016, 'annihilated': 7017, 'appletstag': 7018, 'aquifer': 7019, 'archives': 7020, 'ascot': 7021, 'askingoh': 7022, 'associates': 7023, 'ating': 7024, 'attending': 7025, 'audience': 7026, 'awaiting': 7027, 'ba': 7028, 'baiting': 7029, 'bali': 7030, 'bangkok': 7031, 'barry': 7032, 'bashful': 7033, 'bathroom': 7034, 'beats': 7035, 'became': 7036, 'beers': 7037, 'beings': 7038, 'belgium': 7039, 'bentley': 7040, 'beyou': 7041, 'bk': 7042, 'blacklistin': 7043, 'bleed': 7044, 'blueeyes': 7045, 'bml': 7046, 'boating': 7047, 'boil': 7048, 'bondage': 7049, 'bonding': 7050, 'boost': 7051, 'boycotttigerwoods': 7052, 'bp': 7053, 'bracelets': 7054, 'brazil': 7055, 'bridetobe': 7056, 'bristol': 7057, 'brooklyn': 7058, 'browns': 7059, 'brutal': 7060, 'bubble': 7061, 'bugg': 7062, 'buildings': 7063, 'built': 7064, 'bunny': 7065, 'burger': 7066, 'burnt': 7067, 'canadiangp': 7068, 'cancelled': 7069, 'candid': 7070, 'candles': 7071, 'capitalist': 7072, 'caption': 7073, 'casting': 7074, 'catching': 7075, 'catsoftwitter': 7076, 'cereal': 7077, 'ceremony': 7078, 'champions': 7079, 'cheering': 7080, 'chihuahua': 7081, 'citation': 7082, 'classifies': 7083, 'closest': 7084, 'cloud': 7085, 'clue': 7086, 'cnv': 7087, 'coffin': 7088, 'cognizant': 7089, 'colonisation': 7090, 'colourism': 7091, 'comfy': 7092, 'commutations': 7093, 'complementarian': 7094, 'complementarianism': 7095, 'conaist': 7096, 'condescending': 7097, 'connect': 7098, 'considered': 7099, 'conspiracy': 7100, 'consumer': 7101, 'contain': 7102, 'contest': 7103, 'coolestlifehack': 7104, 'cope': 7105, 'cpi': 7106, 'crafts': 7107, 'creature': 7108, 'cress': 7109, 'cricket': 7110, 'critiquing': 7111, 'cuckold': 7112, 'cuties': 7113, 'dallas': 7114, 'damning': 7115, 'dan': 7116, 'dancer': 7117, 'demeaning': 7118, 'designer': 7119, 'destigmatize': 7120, 'destination': 7121, 'digital': 7122, 'dinosaurs': 7123, 'dissapointed': 7124, 'distinguished': 7125, 'dividing': 7126, 'dkpol': 7127, 'dogsofinstagram': 7128, 'doin': 7129, 'donuldtrump': 7130, 'dope': 7131, 'dordarshan': 7132, 'downwiththetriarchy': 7133, 'dragged': 7134, 'dragon': 7135, 'dressed': 7136, 'drfranciscresswelsing': 7137, 'dropped': 7138, 'duh': 7139, 'dull': 7140, 'editing': 7141, 'editorial': 7142, 'eek': 7143, 'elitists': 7144, 'elses': 7145, 'emtec': 7146, 'enjoylife': 7147, 'evolutions': 7148, 'factsguide': 7149, 'fascinating': 7150, 'fashionblogger': 7151, 'favorites': 7152, 'feliz2017': 7153, 'fest': 7154, 'fetish': 7155, 'filming': 7156, 'fishing': 7157, 'fitbit': 7158, 'fostercare': 7159, 'frank': 7160, 'freedownload': 7161, 'freemasons': 7162, 'frustrating': 7163, 'fucku2016': 7164, 'gallery': 7165, 'gamergirl': 7166, 'gaypride': 7167, 'ghana': 7168, 'ghostbusters': 7169, 'girlpower': 7170, 'giveaway': 7171, 'gladshesgone': 7172, 'glastofest': 7173, 'glow': 7174, 'gm': 7175, 'goin': 7176, 'grad': 7177, 'grandsons': 7178, 'greens': 7179, 'gs': 7180, 'gt2': 7181, 'guarani': 7182, 'guessed': 7183, 'guests': 7184, 'hall': 7185, 'happ': 7186, 'harmony': 7187, 'hav': 7188, 'headache': 7189, 'heavy': 7190, 'highlights': 7191, 'highly': 7192, 'highs': 7193, 'hiking': 7194, 'himhe': 7195, 'historically': 7196, 'holier': 7197, 'homesweethome': 7198, 'homework': 7199, 'honey': 7200, 'honored': 7201, 'hook': 7202, 'however': 7203, 'humping': 7204, 'hurry': 7205, 'idiocracy': 7206, 'iftar': 7207, 'illegalsettlements': 7208, 'iloveyou': 7209, 'immanuel': 7210, 'indonesia': 7211, 'inferioritycomplex': 7212, 'inked': 7213, 'interviews': 7214, 'ios10': 7215, 'islamaphobic': 7216, 'jack': 7217, 'jackass': 7218, 'jazz': 7219, 'jeremyjoseph': 7220, 'jon': 7221, 'judea': 7222, 'jumping': 7223, 'juneteenth': 7224, 'jus': 7225, 'kanye': 7226, 'kathy': 7227, 'kejriwal': 7228, 'kellyann': 7229, 'keynote': 7230, 'kim': 7231, 'km': 7232, 'kscrashcorrectors': 7233, 'lab': 7234, 'ladyboy': 7235, 'landing': 7236, 'landscape': 7237, 'lane': 7238, 'lang': 7239, 'largely': 7240, 'lay': 7241, 'leaked': 7242, 'leb': 7243, 'led': 7244, 'libey': 7245, 'lightroom': 7246, 'lily': 7247, 'linguistics': 7248, 'livelife': 7249, 'loneliness': 7250, 'lovemyjob': 7251, 'macbook': 7252, 'maimane': 7253, 'makeuptransformation': 7254, 'manypols': 7255, 'mario': 7256, 'mat': 7257, 'mcdaniel': 7258, 'mcfadden': 7259, 'measured': 7260, 'mega': 7261, 'menu': 7262, 'messengerjust': 7263, 'messy': 7264, 'mf': 7265, 'mickey': 7266, 'mindsconsole': 7267, 'mistakes': 7268, 'modi': 7269, 'momtips': 7270, 'mornings': 7271, 'motorcycle': 7272, 'mount': 7273, 'mouse': 7274, 'municipalities': 7275, 'musical': 7276, 'mydubai': 7277, 'naboomspruit': 7278, 'nafris': 7279, 'nationalbestfriendday': 7280, 'nationdivider': 7281, 'navy': 7282, 'nd': 7283, 'ndtv': 7284, 'netherlands': 7285, 'newcastle': 7286, 'newstar': 7287, 'newyorkcity': 7288, 'nicole': 7289, 'nigelfarage': 7290, 'noballs': 7291, 'nobdy': 7292, 'nohope': 7293, 'nomandate': 7294, 'nomnom': 7295, 'nottingham': 7296, 'numb': 7297, 'obamalegacy': 7298, 'obstacles': 7299, 'oppounities': 7300, 'oppressed': 7301, 'oppressive': 7302, 'opt': 7303, 'optimistic': 7304, 'orgasmic': 7305, 'orlandohorror': 7306, 'outdoor': 7307, 'overwatch': 7308, 'overweight': 7309, 'owners': 7310, 'pages': 7311, 'pakis': 7312, 'pap': 7313, 'papalbull': 7314, 'parent': 7315, 'parking': 7316, 'peacekeepers': 7317, 'pedophile': 7318, 'pelosi': 7319, 'peru': 7320, 'picking': 7321, 'plant': 7322, 'polygeny': 7323, 'potato': 7324, 'pougal': 7325, 'pounds': 7326, 'pout': 7327, 'prayersfororlando': 7328, 'pretend': 7329, 'prime': 7330, 'problemofwhiteness': 7331, 'projects': 7332, 'proverb': 7333, 'pseudoscience': 7334, 'pussygrabberinchief': 7335, 'quack': 7336, 'quantities': 7337, 'queenat90': 7338, 'quotas': 7339, 'rabid': 7340, 'raciolinguistics': 7341, 'rains': 7342, 'raised': 7343, 'rampant': 7344, 'recognize': 7345, 'recommend': 7346, 'reddit': 7347, 'redhead': 7348, 'reginald': 7349, 'regrann': 7350, 'relate': 7351, 'relates': 7352, 'relied': 7353, 'religious': 7354, 'remainers': 7355, 'remind': 7356, 'restinpeace': 7357, 'restricted': 7358, 'returningcitizens': 7359, 'rome': 7360, 'royals': 7361, 'russianhack': 7362, 'sacrifice': 7363, 'sake': 7364, 'samachar': 7365, 'samaria': 7366, 'samples': 7367, 'samwu': 7368, 'saturdaynight': 7369, 'saturdays': 7370, 'sauce': 7371, 'scotiabank': 7372, 'script': 7373, 'sdlive': 7374, 'sdob': 7375, 'se': 7376, 'searching': 7377, 'seek': 7378, 'seeklearning': 7379, 'selena': 7380, 'selfietime': 7381, 'selfporait': 7382, 'shared': 7383, 'sharks': 7384, 'shattered': 7385, 'shopalyssas': 7386, 'shou': 7387, 'showcase': 7388, 'singapore': 7389, 'skywilliams': 7390, 'slide': 7391, 'slit': 7392, 'smackdown': 7393, 'smacks': 7394, 'smug': 7395, 'sneezy': 7396, 'socialmooc': 7397, 'speechless': 7398, 'spinning': 7399, 'split': 7400, 'spoiled': 7401, 'spoilers': 7402, 'spoilt': 7403, 'spotify': 7404, 'spotlight': 7405, 'squirrel': 7406, 'stable': 7407, 'stafresh': 7408, 'stanley': 7409, 'steph': 7410, 'stephen': 7411, 'straght': 7412, 'streets': 7413, 'suddenly': 7414, 'sukhbir': 7415, 'sup': 7416, 'superior': 7417, 'sweat': 7418, 'swift': 7419, 'syfy': 7420, 'tattoosleeves': 7421, 'teamd': 7422, 'teams': 7423, 'teamwork': 7424, 'technology': 7425, 'temecula': 7426, 'tent': 7427, 'terminology': 7428, 'terrorattack': 7429, 'thatblacklivesmatter': 7430, 'thatzille': 7431, 'theater': 7432, 'thebest': 7433, 'thefuture': 7434, 'thefutureisfemale': 7435, 'theresamay': 7436, 'thiss': 7437, 'thrones': 7438, 'tight': 7439, 'timenow': 7440, 'toast': 7441, 'tommorow': 7442, 'tonyawards': 7443, 'tory': 7444, 'tossing': 7445, 'toured': 7446, 'trading': 7447, 'trafficking': 7448, 'tragedies': 7449, 'trail': 7450, 'trample': 7451, 'travels': 7452, 'treasure': 7453, 'treatment': 7454, 'trend': 7455, 'trends': 7456, 'tribute': 7457, 'truestory': 7458, 'trump2016': 7459, 'trumpocalypse': 7460, 'truths': 7461, 'tuesdaymotivation': 7462, 'underlying': 7463, 'underway': 7464, 'unitedstateschampion': 7465, 'universityofwisconsinmadison': 7466, 'unknown': 7467, 'unwell': 7468, 'updated': 7469, 'upside': 7470, 'uschampion': 7471, 'vancouver': 7472, 'venice': 7473, 'veto': 7474, 'viewed': 7475, 'viewing': 7476, 'vinyl': 7477, 'vinyls': 7478, 'violently': 7479, 'votes': 7480, 'vr': 7481, 'watermelon': 7482, 'weed': 7483, 'weekly': 7484, 'wehate': 7485, 'welleducated': 7486, 'welsing': 7487, 'wenger': 7488, 'whic': 7489, 'whiskey': 7490, 'whiteperson': 7491, 'whiteskinned': 7492, 'william': 7493, 'wilmington': 7494, 'winners': 7495, 'womensmarch': 7496, 'wood': 7497, 'worldoceansday': 7498, 'wrap': 7499, 'wsaying': 7500, 'wwdc': 7501, 'xboxone': 7502, 'xenopbobia': 7503, 'yehtut': 7504, 'yiannopoulosis': 7505, 'younger': 7506, 'zeenews': 7507, 'zoo': 7508, '02': 7509, '10th': 7510, '15th': 7511, '17th': 7512, '2013': 7513, '29': 7514, '2pac': 7515, '42': 7516, '564943': 7517, '6417153640': 7518, '80': 7519, 'a': 7520, 'acoustic': 7521, 'actorslife': 7522, 'addiction': 7523, 'afford': 7524, 'afrofuturism': 7525, 'ah': 7526, 'ahhh': 7527, 'ahole': 7528, 'allah': 7529, 'alligators': 7530, 'alongside': 7531, 'anarchism': 7532, 'anthems': 7533, 'antiaging': 7534, 'antijewish': 7535, 'apologists': 7536, 'apostolis': 7537, 'appeared': 7538, 'apply': 7539, 'appointment': 7540, 'appreciated': 7541, 'appreciating': 7542, 'appropriate': 7543, 'approved': 7544, 'arriving': 7545, 'arsenal': 7546, 'asleep': 7547, 'astonished': 7548, 'atmosphere': 7549, 'attraction': 7550, 'aunt': 7551, 'autumn': 7552, 'avengers': 7553, 'awareness': 7554, 'awww': 7555, 'babys': 7556, 'backed': 7557, 'backtoschool': 7558, 'backyard': 7559, 'badger': 7560, 'balanced': 7561, 'balloons': 7562, 'bankrupt': 7563, 'battlefield': 7564, 'bay': 7565, 'baylor': 7566, 'beachlife': 7567, 'beg': 7568, 'believing': 7569, 'bell': 7570, 'beloved': 7571, 'benidorm': 7572, 'berniesanders': 7573, 'bffs': 7574, 'billy': 7575, 'biopolitics': 7576, 'blanket': 7577, 'blocks': 7578, 'blow': 7579, 'bombs': 7580, 'bond': 7581, 'booty': 7582, 'bother': 7583, 'bowling': 7584, 'brainwashed': 7585, 'breakingnews': 7586, 'brochure': 7587, 'brokenquotes': 7588, 'bron': 7589, 'bros': 7590, 'bubbly': 7591, 'bump': 7592, 'burgers': 7593, 'burst': 7594, 'cage': 7595, 'callin': 7596, 'camden': 7597, 'canadianvalues': 7598, 'capitalstb': 7599, 'captain': 7600, 'caring': 7601, 'carnival': 7602, 'carry': 7603, 'carrying': 7604, 'cases': 7605, 'casual': 7606, 'catdjt': 7607, 'cheating': 7608, 'chelsea': 7609, 'cherry': 7610, 'childrenofcolor': 7611, 'chillin': 7612, 'chip': 7613, 'chips': 7614, 'chronicpain': 7615, 'cliches': 7616, 'coalitionist': 7617, 'cockwomble': 7618, 'coconut': 7619, 'coffe': 7620, 'collage': 7621, 'colleagues': 7622, 'com': 7623, 'combo': 7624, 'communitystandards': 7625, 'commute': 7626, 'companies': 7627, 'compare': 7628, 'compensatory': 7629, 'compilation': 7630, 'conditioning': 7631, 'constitutional': 7632, 'corner': 7633, 'coybig': 7634, 'crash': 7635, 'creator': 7636, 'crossed': 7637, 'custom': 7638, 'cuteness': 7639, 'cyprus': 7640, 'daddysgirl': 7641, 'dairy': 7642, 'daniel': 7643, 'darling': 7644, 'dave': 7645, 'deceive': 7646, 'decent': 7647, 'decisions': 7648, 'dedicated': 7649, 'del': 7650, 'deleteyouraccount': 7651, 'deliver': 7652, 'denim': 7653, 'depth': 7654, 'determination': 7655, 'determined': 7656, 'dev': 7657, 'disabled': 7658, 'discover': 7659, 'display': 7660, 'distress': 7661, 'disturbing': 7662, 'divorce': 7663, 'donthecon': 7664, 'dorm': 7665, 'dorset': 7666, 'doubletree': 7667, 'douchewaffle': 7668, 'drumpf': 7669, 'earn': 7670, 'earned': 7671, 'ecstatic': 7672, 'edition': 7673, 'eevalancaster': 7674, 'eight': 7675, 'elegant': 7676, 'elephants': 7677, 'employment': 7678, 'endorsement': 7679, 'engineer': 7680, 'environmentalist': 7681, 'ergo': 7682, 'esp': 7683, 'eva': 7684, 'exceptional': 7685, 'excess': 7686, 'exci': 7687, 'experiencing': 7688, 'expressing': 7689, 'extraordinaire': 7690, 'eyed': 7691, 'factor': 7692, 'fallen': 7693, 'falseequavalency': 7694, 'falsenarrative': 7695, 'familyfun': 7696, 'fancy': 7697, 'farewell': 7698, 'fascismo': 7699, 'fashionillustration': 7700, 'fatherday': 7701, 'fauxnews': 7702, 'feat': 7703, 'feelin': 7704, 'feelthebern': 7705, 'ffs': 7706, 'field': 7707, 'fifa17': 7708, 'fighter': 7709, 'fightlikeagirl': 7710, 'fingers': 7711, 'fingerscrossed': 7712, 'fitnessaddict': 7713, 'flash': 7714, 'flashbackfriday': 7715, 'flcrashcorrectors': 7716, 'flies': 7717, 'flippant': 7718, 'florence': 7719, 'flow': 7720, 'focused': 7721, 'followback': 7722, 'forgiveness': 7723, 'forgotten': 7724, 'fotiadis': 7725, 'founate': 7726, 'frat': 7727, 'freespirit': 7728, 'fridayfun': 7729, 'fridaynight': 7730, 'frozen': 7731, 'fuckin': 7732, 'fundamentalism': 7733, 'funtimes': 7734, 'gained': 7735, 'gamers': 7736, 'gardens': 7737, 'gary': 7738, 'gear': 7739, 'gentle': 7740, 'getaway': 7741, 'girltime': 7742, 'girly': 7743, 'glamping': 7744, 'glowing': 7745, 'glutenfree': 7746, 'goa': 7747, 'goat': 7748, 'goodbook': 7749, 'goodies': 7750, 'goodtime': 7751, 'gordie': 7752, 'gosh': 7753, 'grandparents': 7754, 'gray': 7755, 'growup': 7756, 'grumpycat': 7757, 'grunge': 7758, 'gt15': 7759, 'gud': 7760, 'guncontrolnow': 7761, 'gypsy': 7762, 'haircut': 7763, 'harvest': 7764, 'hats': 7765, 'hbo': 7766, 'heads': 7767, 'healthandfitness': 7768, 'healthylife': 7769, 'herbalife': 7770, 'heros': 7771, 'hijab': 7772, 'hippie': 7773, 'holly': 7774, 'homosex': 7775, 'hormone': 7776, 'huing': 7777, 'hulk': 7778, 'hurryup': 7779, 'ibiza2016': 7780, 'ignores': 7781, 'illinois': 7782, 'incineration': 7783, 'includes': 7784, 'inconsequen': 7785, 'innovation': 7786, 'insights': 7787, 'insta': 7788, 'instaaoftheday': 7789, 'instafollow': 7790, 'instamoment': 7791, 'institution': 7792, 'insure': 7793, 'intend': 7794, 'interiordesign': 7795, 'internetmarketing': 7796, 'intersectional': 7797, 'introduction': 7798, 'investment': 7799, 'invite': 7800, 'ion': 7801, 'ipad': 7802, 'iswar': 7803, 'items': 7804, 'jam': 7805, 'jamaica': 7806, 'jetlagged': 7807, 'johnny': 7808, 'joining': 7809, 'jp': 7810, 'jr': 7811, 'junior': 7812, 'kh': 7813, 'kharkiv': 7814, 'kharkivgram': 7815, 'kidding': 7816, 'killings': 7817, 'kings': 7818, 'knicks': 7819, 'knocked': 7820, 'kylie': 7821, 'lasses': 7822, 'latergram': 7823, 'laura': 7824, 'lb': 7825, 'legendary': 7826, 'lessismore': 7827, 'lifted': 7828, 'lincoln': 7829, 'lingerie': 7830, 'lived': 7831, 'livemusic': 7832, 'liye': 7833, 'loads': 7834, 'looki': 7835, 'loosing': 7836, 'lounge': 7837, 'lovequotes': 7838, 'lovewins': 7839, 'lucy': 7840, 'lyon': 7841, 'magicrealism': 7842, 'maintain': 7843, 'mallorca': 7844, 'managing': 7845, 'marbs': 7846, 'marseille': 7847, 'mask': 7848, 'masters': 7849, 'matches': 7850, 'max': 7851, 'meetings': 7852, 'megyn': 7853, 'mid': 7854, 'miller': 7855, 'minneapolis': 7856, 'miscegenation': 7857, 'mission': 7858, 'mizzou': 7859, 'modeling': 7860, 'modelling': 7861, 'momma': 7862, 'monthsary': 7863, 'montreal': 7864, 'mounts': 7865, 'movingon': 7866, 'muhammadali': 7867, 'mygirl': 7868, 'myhappyplace': 7869, 'nah': 7870, 'nailed': 7871, 'nano': 7872, 'nationala': 7873, 'natsu': 7874, 'netanyahou': 7875, 'netanyahuspeech': 7876, 'networking': 7877, 'newproject': 7878, 'news24': 7879, 'nightmare': 7880, 'ninja': 7881, 'nite': 7882, 'nj': 7883, 'no1': 7884, 'notes': 7885, 'nothappy': 7886, 'nurses': 7887, 'obsession': 7888, 'occupiers': 7889, 'ohhhh': 7890, 'oj': 7891, 'older': 7892, 'oldest': 7893, 'olds': 7894, 'ole': 7895, 'omar': 7896, 'ooh': 7897, 'opens': 7898, 'oppressorsprivileged': 7899, 'optimism': 7900, 'oregon': 7901, 'overwhelmed': 7902, 'owls': 7903, 'package': 7904, 'pakistan': 7905, 'palestinians': 7906, 'paners': 7907, 'passpo': 7908, 'patiently': 7909, 'patio': 7910, 'patriarchal': 7911, 'paytime': 7912, 'peanut': 7913, 'pearl': 7914, 'perfectly': 7915, 'personality': 7916, 'philip': 7917, 'pie': 7918, 'pissedoff': 7919, 'pit': 7920, 'pixar': 7921, 'plight': 7922, 'pll': 7923, 'po': 7924, 'poll': 7925, 'polo': 7926, 'poman': 7927, 'pose': 7928, 'prayingfororlando': 7929, 'pregnant': 7930, 'premium': 7931, 'presents': 7932, 'prettiest': 7933, 'prevented': 7934, 'prob': 7935, 'productivity': 7936, 'programme': 7937, 'promises': 7938, 'proper': 7939, 'provides': 7940, 'puff': 7941, 'pug': 7942, 'pull': 7943, 'punjabis': 7944, 'qotd': 7945, 'quotestags': 7946, 'quotestagsapp': 7947, 'racebaiter': 7948, 'realestate': 7949, 'realtalk': 7950, 'recognizing': 7951, 'recovered': 7952, 'redcove': 7953, 'reducing': 7954, 'rehearsal': 7955, 'relatable': 7956, 'relationshipgoals': 7957, 'repoingsystem': 7958, 'repostapp': 7959, 'reviews': 7960, 'rice': 7961, 'ris': 7962, 'risks': 7963, 'roles': 7964, 'ronda': 7965, 'royalascot': 7966, 'rss': 7967, 'saddened': 7968, 'salon': 7969, 'sameshitnewyear': 7970, 'sample': 7971, 'sassy': 7972, 'saturdaymorning': 7973, 'saving': 7974, 'scott': 7975, 'seahawks': 7976, 'seed': 7977, 'semantics': 7978, 'semi': 7979, 'senior': 7980, 'sentence': 7981, 'services': 7982, 'setup': 7983, 'shabbatshalom': 7984, 'shady': 7985, 'shakes': 7986, 'shane': 7987, 'shockfactor': 7988, 'shoe': 7989, 'shopthemint': 7990, 'shore': 7991, 'singlelife': 7992, 'skiing': 7993, 'slaveowner': 7994, 'slowly': 7995, 'smaphone': 7996, 'snack': 7997, 'sneak': 7998, 'sniff': 7999, 'sober': 8000, 'solid': 8001, 'solution': 8002, 'songwriter': 8003, 'sorrynotsorry': 8004, 'southampton': 8005, 'speaker': 8006, 'species': 8007, 'spreadlovethischristmas': 8008, 'stirs': 8009, 'stock': 8010, 'stomach': 8011, 'str': 8012, 'stray': 8013, 'stressfree': 8014, 'strips': 8015, 'stuakenyon81': 8016, 'sugar': 8017, 'suppoive': 8018, 'suppose': 8019, 'surely': 8020, 'sustainable': 8021, 'switzerland': 8022, 'taiwan': 8023, 'tds': 8024, 'tenerife': 8025, 'theistic': 8026, 'thevoice': 8027, 'thi': 8028, 'thunder': 8029, 'thursdays': 8030, 'tomhiddleston': 8031, 'torn': 8032, 'tournament': 8033, 'trained': 8034, 'tranny': 8035, 'traveler': 8036, 'trolls': 8037, 'tropical': 8038, 'trumplies': 8039, 'trumpproofamerica': 8040, 'tule': 8041, 'udta': 8042, 'ukraineblog': 8043, 'unintentionally': 8044, 'universe': 8045, 'upgrade': 8046, 'ushers': 8047, 'valley': 8048, 'vietnam': 8049, 'vijay': 8050, 'vscogood': 8051, 'waronchristmas': 8052, 'warren': 8053, 'weakening': 8054, 'wealthy': 8055, 'weapons': 8056, 'weareorlando': 8057, 'weirdos': 8058, 'western': 8059, 'whitefeminism': 8060, 'wholesome': 8061, 'whopped': 8062, 'wid': 8063, 'wide': 8064, 'wil': 8065, 'wimbledon': 8066, 'wisconsin': 8067, 'wohy': 8068, 'wp': 8069, 'wth': 8070, 'xbox': 8071, 'yen': 8072, 'yesssss': 8073, 'youall': 8074, '06': 8075, '0806': 8076, '1': 8077, '101': 8078, '10alltypespos': 8079, '11400': 8080, '1206': 8081, '1500': 8082, '1900': 8083, '1d': 8084, '1k': 8085, '1q': 8086, '2': 8087, '2014': 8088, '2018': 8089, '22nd': 8090, '25th': 8091, '26': 8092, '2k': 8093, '35th': 8094, '37': 8095, '45': 8096, '4o4o4': 8097, '5': 8098, '50islamicinfo': 8099, '6': 8100, '79': 8101, '80s': 8102, '9': 8103, '900': 8104, '90th': 8105, '999': 8106, 'abba': 8107, 'abc': 8108, 'abrahamhicks': 8109, 'academy': 8110, 'acceptance': 8111, 'aching': 8112, 'acquired': 8113, 'adds': 8114, 'adele': 8115, 'adhan': 8116, 'adopt': 8117, 'adultedu': 8118, 'adults': 8119, 'affectionately': 8120, 'aft': 8121, 'aid': 8122, 'aim': 8123, 'aladdin': 8124, 'alaska': 8125, 'albea': 8126, 'albums': 8127, 'aldub11thmonthsary': 8128, 'alhamdulillah': 8129, 'aliens': 8130, 'alright': 8131, 'alternatively': 8132, 'amigos': 8133, 'amodu': 8134, 'amount': 8135, 'anderson': 8136, 'andreas': 8137, 'andy': 8138, 'anon': 8139, 'antiracist': 8140, 'ap': 8141, 'apament': 8142, 'apaments': 8143, 'appear': 8144, 'appearing': 8145, 'appears': 8146, 'ar15': 8147, 'argentina': 8148, 'arrives': 8149, 'aruba': 8150, 'asianladyboy': 8151, 'aside': 8152, 'atk': 8153, 'audio': 8154, 'audition': 8155, 'autism': 8156, 'avgeek': 8157, 'avocado': 8158, 'awasome': 8159, 'baba': 8160, 'backpack': 8161, 'badday': 8162, 'banana': 8163, 'bang': 8164, 'baptized': 8165, 'bargain': 8166, 'barn': 8167, 'bass': 8168, 'battlefield1': 8169, 'bck': 8170, 'beachbody': 8171, 'beauties': 8172, 'beautifulday': 8173, 'beep': 8174, 'beeroclock': 8175, 'beinlife': 8176, 'belated': 8177, 'belfast': 8178, 'belong': 8179, 'besides': 8180, 'bi': 8181, 'bicycle': 8182, 'bigbang': 8183, 'bihdaycake': 8184, 'bihdaykazuki': 8185, 'bilal': 8186, 'bisexual': 8187, 'blackmen': 8188, 'blackperson': 8189, 'blacksupremacist': 8190, 'blaming': 8191, 'blissful': 8192, 'blooms': 8193, 'boarding': 8194, 'bobby': 8195, 'bodrum': 8196, 'bon': 8197, 'booze': 8198, 'bora': 8199, 'bordercollie': 8200, 'bothered': 8201, 'bounce': 8202, 'bowl': 8203, 'boyfriends': 8204, 'brains': 8205, 'breakup': 8206, 'breeds': 8207, 'breeze': 8208, 'bremain': 8209, 'brew': 8210, 'brian': 8211, 'bridesmaid': 8212, 'bridesmaids': 8213, 'brighten': 8214, 'brisbane': 8215, 'broadcast': 8216, 'broadway': 8217, 'budget': 8218, 'bulgaria': 8219, 'bullet': 8220, 'bullied': 8221, 'bum': 8222, 'bury': 8223, 'butterflies': 8224, 'buttholemouth': 8225, 'buttons': 8226, 'cactus': 8227, 'cakes': 8228, 'calories': 8229, 'camper': 8230, 'canceled': 8231, 'cape': 8232, 'cardio': 8233, 'carlos': 8234, 'carol': 8235, 'carpet': 8236, 'carving': 8237, 'cash': 8238, 'cashing': 8239, 'casino': 8240, 'catchup': 8241, 'ceainly': 8242, 'celebritydeath': 8243, 'censor': 8244, 'centralpark': 8245, 'champion': 8246, 'charlotte': 8247, 'cheaper': 8248, 'checkcashing': 8249, 'cheesy': 8250, 'chef': 8251, 'cherish': 8252, 'cheryl': 8253, 'childcare': 8254, 'childs': 8255, 'chilled': 8256, 'chilling': 8257, 'chills': 8258, 'chuffed': 8259, 'clannad': 8260, 'clash': 8261, 'closet': 8262, 'cloudchaser': 8263, 'cloudy': 8264, 'cmtawards': 8265, 'coat': 8266, 'cocktail': 8267, 'coloring': 8268, 'comfoable': 8269, 'comparisons': 8270, 'complaining': 8271, 'comprehensive': 8272, 'concerned': 8273, 'conclusion': 8274, 'conduct': 8275, 'confuse': 8276, 'conjuring': 8277, 'constant': 8278, 'contagious': 8279, 'contented': 8280, 'convention': 8281, 'costs': 8282, 'couch': 8283, 'county': 8284, 'coupon': 8285, 'covers': 8286, 'craftbeer': 8287, 'crapsac': 8288, 'creations': 8289, 'crochet': 8290, 'crown': 8291, 'crush': 8292, 'cube': 8293, 'cure': 8294, 'curly': 8295, 'cursedchild': 8296, 'customerservice': 8297, 'cutting': 8298, 'cycle': 8299, 'czech': 8300, 'daddygavemeamillion': 8301, 'danny': 8302, 'darkest': 8303, 'datenight': 8304, 'davis': 8305, 'dawn': 8306, 'deafening': 8307, 'debut': 8308, 'deciding': 8309, 'deck': 8310, 'decorative': 8311, 'decors': 8312, 'dee': 8313, 'defame': 8314, 'degree': 8315, 'dementeddonny': 8316, 'dental': 8317, 'dentist': 8318, 'depament': 8319, 'deserved': 8320, 'designs': 8321, 'detox': 8322, 'developing': 8323, 'devops': 8324, 'diego': 8325, 'digging': 8326, 'dining': 8327, 'disagrees': 8328, 'disappoint': 8329, 'disappointing': 8330, 'discount': 8331, 'dislike': 8332, 'disneygatorattack': 8333, 'distractions': 8334, 'dogslife': 8335, 'dogsoftwitter': 8336, 'doj': 8337, 'doll': 8338, 'downloaded': 8339, 'drags': 8340, 'drake': 8341, 'drakeandjosh': 8342, 'draymond': 8343, 'dreaming': 8344, 'dresses': 8345, 'dubai': 8346, 'dukhi': 8347, 'easiest': 8348, 'eastcoast': 8349, 'eaten': 8350, 'economicapaheid': 8351, 'ecourse': 8352, 'eddie': 8353, 'egg': 8354, 'eia': 8355, 'elementary': 8356, 'eli': 8357, 'elite': 8358, 'em2016': 8359, 'embarrassing': 8360, 'emptiness': 8361, 'enchanting': 8362, 'endorsed': 8363, 'enjoyment': 8364, 'enroute': 8365, 'enter': 8366, 'enthusiasm': 8367, 'eternal': 8368, 'eureferendum': 8369, 'expand': 8370, 'expecting': 8371, 'exposeracism': 8372, 'eyebrow': 8373, 'facemask': 8374, 'faded': 8375, 'falsely': 8376, 'fathersday2016': 8377, 'fathersdaymessage': 8378, 'faves': 8379, 'favs': 8380, 'feared': 8381, 'fearless': 8382, 'fears': 8383, 'features': 8384, 'february': 8385, 'fees': 8386, 'felling': 8387, 'fetishes': 8388, 'fever': 8389, 'fewer': 8390, 'fg': 8391, 'fianc': 8392, 'figured': 8393, 'filling': 8394, 'financial': 8395, 'finds': 8396, 'fing': 8397, 'finn': 8398, 'firefly': 8399, 'firenze': 8400, 'fireworks': 8401, 'fits': 8402, 'flags': 8403, 'flip': 8404, 'floating': 8405, 'floral': 8406, 'flourishing': 8407, 'fomc': 8408, 'fotokuapp': 8409, 'fouh': 8410, 'fra': 8411, 'freshsta': 8412, 'fridge': 8413, 'fuming': 8414, 'gals': 8415, 'gameshow': 8416, 'gawa': 8417, 'gayboy': 8418, 'gbpjpy': 8419, 'gd': 8420, 'gem': 8421, 'generous': 8422, 'giftideas': 8423, 'giggles': 8424, 'godbless': 8425, 'gossip': 8426, 'grads': 8427, 'greatful': 8428, 'greet': 8429, 'grill': 8430, 'grown': 8431, 'growthhacking': 8432, 'gunman': 8433, 'habit': 8434, 'hairdresser': 8435, 'hairstyle': 8436, 'halfway': 8437, 'hamza': 8438, 'handjob': 8439, 'handwritten': 8440, 'happyfathersday': 8441, 'hardest': 8442, 'hardworking': 8443, 'harmless': 8444, 'harmonious': 8445, 'hashtags': 8446, 'haul': 8447, 'havefun': 8448, 'hawaiian': 8449, 'hd': 8450, 'heaache': 8451, 'healthyliving': 8452, 'heavenly': 8453, 'hehe': 8454, 'helpful': 8455, 'helpme': 8456, 'henderson': 8457, 'highfive': 8458, 'hint': 8459, 'hippy': 8460, 'ho': 8461, 'homedecor': 8462, 'homeless': 8463, 'homies': 8464, 'honeymoon': 8465, 'hongkong': 8466, 'honour': 8467, 'hoo': 8468, 'hoodies': 8469, 'horses': 8470, 'hoshi': 8471, 'hottie': 8472, 'hs': 8473, 'hudson': 8474, 'hyper': 8475, 'ian': 8476, 'ii': 8477, 'il': 8478, 'imessage': 8479, 'imperfections': 8480, 'included': 8481, 'indieauthor': 8482, 'indiegame': 8483, 'indirect': 8484, 'indulge': 8485, 'industrial': 8486, 'infamous': 8487, 'inflation': 8488, 'ing': 8489, 'injuries': 8490, 'inner': 8491, 'innocence': 8492, 'instafashion': 8493, 'instafood': 8494, 'installation': 8495, 'instalove': 8496, 'intense': 8497, 'intent': 8498, 'intermarket': 8499, 'internship': 8500, 'introduce': 8501, 'invited2jive': 8502, 'iron': 8503, 'irrelevant': 8504, 'ita': 8505, 'itunes': 8506, 'jackie': 8507, 'jacksons': 8508, 'jacob': 8509, 'jar': 8510, 'jivemap': 8511, 'jocoxmp': 8512, 'jungle': 8513, 'justinb': 8514, 'kaaba': 8515, 'karaoke': 8516, 'katie': 8517, 'kaye': 8518, 'keith': 8519, 'kent': 8520, 'kg': 8521, 'kickoff': 8522, 'kiev': 8523, 'knees': 8524, 'knock': 8525, 'ko': 8526, 'kro': 8527, 'kylielipkit': 8528, 'landed': 8529, 'lands': 8530, 'laptop': 8531, 'lastday': 8532, 'lasting': 8533, 'latenights': 8534, 'latin': 8535, 'laying': 8536, 'lbj': 8537, 'leaf': 8538, 'legit': 8539, 'legs': 8540, 'lemonade': 8541, 'leo': 8542, 'lifetime': 8543, 'lift': 8544, 'limit': 8545, 'lions': 8546, 'literacy': 8547, 'livestream': 8548, 'lobster': 8549, 'locked': 8550, 'lokiday': 8551, 'lopez': 8552, 'lopezchiro': 8553, 'loserpizza': 8554, 'louis': 8555, 'louisville': 8556, 'lousy': 8557, 'lovehim': 8558, 'loveme': 8559, 'lovin': 8560, 'luckygirl': 8561, 'luis': 8562, 'luke': 8563, 'lyft': 8564, 'maam': 8565, 'maintenance': 8566, 'majesty': 8567, 'maker': 8568, 'malaysia': 8569, 'maneuver': 8570, 'manor': 8571, 'mar': 8572, 'marbella': 8573, 'marble': 8574, 'massachusetts': 8575, 'mater': 8576, 'maternity': 8577, 'mcflurry': 8578, 'mebeforeyou': 8579, 'membership': 8580, 'memorial': 8581, 'menner': 8582, 'mensfashion': 8583, 'menswear': 8584, 'mentally': 8585, 'mentions': 8586, 'mentor': 8587, 'mentors': 8588, 'mercedes': 8589, 'midnight': 8590, 'mighty': 8591, 'migraine': 8592, 'mile': 8593, 'mileycyrus': 8594, 'milf': 8595, 'min': 8596, 'misses': 8597, 'mitb': 8598, 'mk': 8599, 'mojo': 8600, 'mommylife': 8601, 'mondays': 8602, 'moore': 8603, 'moredun': 8604, 'motherhoodunscripted': 8605, 'motto': 8606, 'mourning': 8607, 'ms': 8608, 'mtb': 8609, 'mua': 8610, 'mumbai': 8611, 'munich': 8612, 'muscles': 8613, 'museum': 8614, 'musician': 8615, 'myhea': 8616, 'mytraining': 8617, 'nab': 8618, 'nail': 8619, 'nascar': 8620, 'naturally': 8621, 'necklace': 8622, 'neither': 8623, 'nelsonmandela': 8624, 'newcar': 8625, 'newday': 8626, 'newest': 8627, 'newjersey': 8628, 'newjob': 8629, 'newly': 8630, 'nhs': 8631, 'ni': 8632, 'nightmares': 8633, 'nightshift': 8634, 'nike': 8635, 'nine': 8636, 'nojustice': 8637, 'nonsense': 8638, 'noooo': 8639, 'norfolk': 8640, 'norm': 8641, 'nuts': 8642, 'nxt': 8643, 'nz': 8644, 'nzdusd': 8645, 'objectification': 8646, 'october': 8647, 'officials': 8648, 'offline': 8649, 'ohwell': 8650, 'ojmadeinamerica': 8651, 'oldschool': 8652, 'omw': 8653, 'oooh': 8654, 'opinions': 8655, 'orders': 8656, 'organization': 8657, 'orlandostrong': 8658, 'orlandounited': 8659, 'osaka': 8660, 'otaku': 8661, 'otw': 8662, 'ounce': 8663, 'owe': 8664, 'owl': 8665, 'oxford': 8666, 'pace': 8667, 'pakistani': 8668, 'palace': 8669, 'palette': 8670, 'pamper': 8671, 'pampered': 8672, 'panel': 8673, 'parade': 8674, 'pasty': 8675, 'patch': 8676, 'paths': 8677, 'pb': 8678, 'peaked': 8679, 'pedophiles': 8680, 'pencil': 8681, 'penguin': 8682, 'pepper': 8683, 'perfume': 8684, 'peter': 8685, 'pharrellwilliams': 8686, 'phil': 8687, 'philadelphia': 8688, 'philly': 8689, 'photobooth': 8690, 'phuket': 8691, 'picnic': 8692, 'pinoy': 8693, 'plaza': 8694, 'pllseason7': 8695, 'plot': 8696, 'pokemonsunmoon': 8697, 'pole': 8698, 'poop': 8699, 'positivethinking': 8700, 'postive': 8701, 'potd': 8702, 'prefer': 8703, 'prepared': 8704, 'presence': 8705, 'presenter': 8706, 'principle': 8707, 'probation': 8708, 'probe': 8709, 'probs': 8710, 'professionals': 8711, 'prosecco': 8712, 'prosperity': 8713, 'protein': 8714, 'providing': 8715, 'punjabi': 8716, 'punk': 8717, 'pup': 8718, 'puppylove': 8719, 'purple': 8720, 'qampa': 8721, 'que': 8722, 'rahulgandhi': 8723, 'raid': 8724, 'rainforest': 8725, 'ramadankareem': 8726, 'ramsey': 8727, 'reactions': 8728, 'recorded': 8729, 'recover': 8730, 'recovers': 8731, 'reflection': 8732, 'refs': 8733, 'regardless': 8734, 'related': 8735, 'releases': 8736, 'releasing': 8737, 'relief': 8738, 'remembered': 8739, 'remembering': 8740, 'renee': 8741, 'replace': 8742, 'replaced': 8743, 'representations': 8744, 'requested': 8745, 'residents': 8746, 'restore': 8747, 'retire': 8748, 'retweets': 8749, 'rick': 8750, 'rider': 8751, 'rifles': 8752, 'ripantonyelchin': 8753, 'robin': 8754, 'rockette': 8755, 'rooms': 8756, 'rope': 8757, 'rotterdam': 8758, 'royaltyfreemusic': 8759, 'ryan': 8760, 'sabbath': 8761, 'sabon': 8762, 'samsunggalaxys2': 8763, 'sana': 8764, 'sandiego': 8765, 'sandwich': 8766, 'satanists': 8767, 'saterday': 8768, 'scenario': 8769, 'scenery': 8770, 'scheme': 8771, 'scratch': 8772, 'section': 8773, 'secure': 8774, 'secured': 8775, 'segment': 8776, 'selfharm': 8777, 'selfharming': 8778, 'semester': 8779, 'seniors': 8780, 'sensitive': 8781, 'seoul': 8782, 'serenity': 8783, 'server': 8784, 'sexting': 8785, 'shape': 8786, 'shares': 8787, 'shaved': 8788, 'sheboutit': 8789, 'shining': 8790, 'shld': 8791, 'shooters': 8792, 'shops': 8793, 'shos': 8794, 'shoulder': 8795, 'showtime': 8796, 'shrm16': 8797, 'shuaibu': 8798, 'simples': 8799, 'sitges': 8800, 'sixty': 8801, 'ski': 8802, 'skynews': 8803, 'slice': 8804, 'slim': 8805, 'slimmingworld': 8806, 'smallthings': 8807, 'smash': 8808, 'smdh': 8809, 'smilemore': 8810, 'smilepowerday': 8811, 'smoothie': 8812, 'snd': 8813, 'snug': 8814, 'sole': 8815, 'solo': 8816, 'somerset': 8817, 'sophie': 8818, 'soundtrack': 8819, 'sp': 8820, 'spare': 8821, 'spectacle': 8822, 'speeches': 8823, 'spirited': 8824, 'spoil': 8825, 'spoonie': 8826, 'spotted': 8827, 'ss17': 8828, 'stairs': 8829, 'starring': 8830, 'staypositive': 8831, 'staystrong': 8832, 'staytuned': 8833, 'steals': 8834, 'stella': 8835, 'stepping': 8836, 'stopping': 8837, 'stopthehate': 8838, 'stores': 8839, 'straw': 8840, 'strikes': 8841, 'stroller': 8842, 'studies': 8843, 'stumble': 8844, 'stylish': 8845, 'subject': 8846, 'submitted': 8847, 'subtitles': 8848, 'subway': 8849, 'succeed': 8850, 'suffering': 8851, 'suicides': 8852, 'summers': 8853, 'sundaymood': 8854, 'superficial': 8855, 'superman': 8856, 'supernatural': 8857, 'supremely': 8858, 'sweetdreams': 8859, 'sweethea': 8860, 'sympathy': 8861, 'tacloban': 8862, 'taichi': 8863, 'tarot': 8864, 'tasty': 8865, 'tattoos': 8866, 'taylorswift1989': 8867, 'tbh': 8868, 'teachers': 8869, 'teamfollowback': 8870, 'teddy': 8871, 'tee': 8872, 'tequila': 8873, 'terribly': 8874, 'thalaivaa': 8875, 'thousand': 8876, 'thriving': 8877, 'thumb': 8878, 'tin': 8879, 'tinyplanet': 8880, 'titanic': 8881, 'todo': 8882, 'toe': 8883, 'toes': 8884, 'tools': 8885, 'tops': 8886, 'toptweeters': 8887, 'toxic': 8888, 'tracks': 8889, 'tracy': 8890, 'traditional': 8891, 'transfer': 8892, 'treasures': 8893, 'tribune': 8894, 'trick': 8895, 'trims': 8896, 'triste': 8897, 'truelove': 8898, 'trumpis': 8899, 'tryna': 8900, 'tu': 8901, 'tubbytoons': 8902, 'turnup': 8903, 'twin': 8904, 'udtapunjableaked': 8905, 'ukrunchat': 8906, 'ultimate': 8907, 'unable': 8908, 'unbo': 8909, 'underfire': 8910, 'understands': 8911, 'understatement': 8912, 'uniform': 8913, 'unitedstates': 8914, 'unlimited': 8915, 'unwavering': 8916, 'upgraded': 8917, 'upsideofflorida': 8918, 'usb': 8919, 'user': 8920, 'va': 8921, 'vacay': 8922, 'valentines': 8923, 'valued': 8924, 'van': 8925, 'vegetables': 8926, 'vehicles': 8927, 'venture': 8928, 'vibe': 8929, 'viernes': 8930, 'vino': 8931, 'vocal': 8932, 'vocals': 8933, 'wage': 8934, 'wakow': 8935, 'washing': 8936, 'wcw': 8937, 'weaker': 8938, 'wealth': 8939, 'weddinganniversary': 8940, 'weddingday': 8941, 'weddingdress': 8942, 'wendy': 8943, 'whack': 8944, 'wheels': 8945, 'whisky': 8946, 'whiteprivelage': 8947, 'wi': 8948, 'wilson': 8949, 'winnipeg': 8950, 'winterfashion': 8951, 'witch': 8952, 'wks': 8953, 'womensrighttochoose': 8954, 'workfromhome': 8955, 'worrying': 8956, 'wounded': 8957, 'writerslife': 8958, 'wthe': 8959, 'xo': 8960, 'yah': 8961, 'ye': 8962, 'ynwa': 8963, 'yu': 8964, 'yusuf': 8965, 'zoro': 8966, '03': 8967, '04': 8968, '05': 8969, '061116': 8970, '0612': 8971, '0624': 8972, '07950': 8973, '100000': 8974, '100happydays': 8975, '1299': 8976, '1400': 8977, '1499': 8978, '150': 8979, '1995': 8980, '19th': 8981, '1day': 8982, '1stammendment': 8983, '201617': 8984, '2100': 8985, '25cricket': 8986, '28th': 8987, '29th': 8988, '2days': 8989, '2k16': 8990, '2nite': 8991, '2the': 8992, '2weeks': 8993, '3000': 8994, '30th': 8995, '34': 8996, '36': 8997, '38': 8998, '3worldnews': 8999, '3yr': 9000, '400': 9001, '40th': 9002, '44': 9003, '46': 9004, '47pm': 9005, '4k': 9006, '55': 9007, '57': 9008, '59': 9009, '5k': 9010, '5sos': 9011, '64': 9012, '67': 9013, '6yearswithinfinite': 9014, '73': 9015, '7th': 9016, '84': 9017, '90s': 9018, 'aaa': 9019, 'aaron': 9020, 'ab': 9021, 'aboutlastnight': 9022, 'ac': 9023, 'academia': 9024, 'acc': 9025, 'accidents': 9026, 'accuse': 9027, 'ache': 9028, 'achieved': 9029, 'achievement': 9030, 'addition': 9031, 'addressing': 9032, 'adidas': 9033, 'adopted': 9034, 'adve': 9035, 'affects': 9036, 'afro': 9037, 'agencies': 9038, 'agr': 9039, 'ahh': 9040, 'alarms': 9041, 'alice': 9042, 'alicia': 9043, 'allen': 9044, 'allsmiles': 9045, 'almighty': 9046, 'almostthere': 9047, 'alohafriday': 9048, 'alot': 9049, 'alps': 9050, 'alum': 9051, 'amo': 9052, 'ampblessed': 9053, 'ampfamilies': 9054, 'amused': 9055, 'ancient': 9056, 'angeles': 9057, 'angelic': 9058, 'angus': 9059, 'animalabuse': 9060, 'animaladvocate': 9061, 'animated': 9062, 'annaswelshzoo': 9063, 'annefrank': 9064, 'anniversaries': 9065, 'anorexia': 9066, 'antonio': 9067, 'anytime': 9068, 'anyways': 9069, 'anz': 9070, 'apparel': 9071, 'applications': 9072, 'apt': 9073, 'arab': 9074, 'archery': 9075, 'areas': 9076, 'arena': 9077, 'arepa': 9078, 'argue': 9079, 'arguing': 9080, 'aria': 9081, 'arianagrandedrawing': 9082, 'arizona': 9083, 'asf': 9084, 'aspect': 9085, 'assessment': 9086, 'asylum': 9087, 'athlete': 9088, 'attacking': 9089, 'attends': 9090, 'attic': 9091, 'aug': 9092, 'automatic': 9093, 'awe': 9094, 'awesomeness': 9095, 'awhile': 9096, 'awkward': 9097, 'aworks': 9098, 'aya': 9099, 'aye': 9100, 'az': 9101, 'backward': 9102, 'badass': 9103, 'badminton': 9104, 'baker': 9105, 'baking': 9106, 'balloon': 9107, 'bands': 9108, 'barbiets93': 9109, 'bare': 9110, 'bars': 9111, 'bbcnews': 9112, 'bcoz': 9113, 'bd': 9114, 'bdayspl': 9115, 'beachday': 9116, 'beatz': 9117, 'beaut': 9118, 'bedroom': 9119, 'bedtime': 9120, 'bei': 9121, 'bel': 9122, 'beliefs': 9123, 'believed': 9124, 'belly': 9125, 'ben': 9126, 'bengaluru': 9127, 'bent': 9128, 'bestdad': 9129, 'bestfriendsday': 9130, 'beta': 9131, 'bethechange': 9132, 'betrayal': 9133, 'bias': 9134, 'bieber': 9135, 'bigender': 9136, 'bihdaypresent': 9137, 'bingewatching': 9138, 'bites': 9139, 'bits': 9140, 'bittersweet': 9141, 'blackmaverick12': 9142, 'blend': 9143, 'blocking': 9144, 'blogginggals': 9145, 'blondie': 9146, 'bloom': 9147, 'blowoutsbringhappiness': 9148, 'blows': 9149, 'bluehand': 9150, 'bluesky': 9151, 'bnz': 9152, 'bo3': 9153, 'bobross': 9154, 'boe': 9155, 'bogotadc': 9156, 'bogumday': 9157, 'boise': 9158, 'boj': 9159, 'bone': 9160, 'boogie': 9161, 'bookclub': 9162, 'bookreview': 9163, 'booster': 9164, 'bopanna': 9165, 'bourbon': 9166, 'boxes': 9167, 'braces': 9168, 'branch': 9169, 'branding': 9170, 'brandnew': 9171, 'breathing': 9172, 'breezy': 9173, 'brentwood': 9174, 'bridal': 9175, 'brokenhea': 9176, 'brooks': 9177, 'browning': 9178, 'browser': 9179, 'bucket': 9180, 'builders': 9181, 'buisness': 9182, 'bummer': 9183, 'burns': 9184, 'businessoppounity': 9185, 'bust': 9186, 'butter': 9187, 'button': 9188, 'buttoning': 9189, 'buzz': 9190, 'calico': 9191, 'camgirl': 9192, 'campaigning': 9193, 'campers': 9194, 'campus': 9195, 'canal': 9196, 'cannotwait': 9197, 'cantsleep': 9198, 'cantstopsmiling': 9199, 'canvas': 9200, 'cap': 9201, 'capable': 9202, 'captured': 9203, 'careers': 9204, 'carefree': 9205, 'careless': 9206, 'carmineryderrr': 9207, 'catlover': 9208, 'caucasian': 9209, 'cdn': 9210, 'cecily': 9211, 'cedarrapids': 9212, 'celebs': 9213, 'cellphone': 9214, 'champs': 9215, 'chandigarh': 9216, 'characters': 9217, 'charleston': 9218, 'charlie': 9219, 'charming': 9220, 'charms': 9221, 'cheat': 9222, 'cheated': 9223, 'checking': 9224, 'checkout': 9225, 'cheerful': 9226, 'cheesecake': 9227, 'chennai': 9228, 'chester': 9229, 'chi': 9230, 'chickens': 9231, 'chipotle': 9232, 'choke': 9233, 'cielo': 9234, 'cincinnati': 9235, 'cinemas': 9236, 'cipd': 9237, 'classical': 9238, 'cleaneating': 9239, 'cleanse': 9240, 'cleared': 9241, 'clears': 9242, 'clever': 9243, 'climbing': 9244, 'clinches': 9245, 'clueless': 9246, 'coaches': 9247, 'coc': 9248, 'cocoa': 9249, 'cod': 9250, 'coke': 9251, 'collab': 9252, 'collapses': 9253, 'collect': 9254, 'colorado': 9255, 'colou': 9256, 'colourful': 9257, 'comeonengland': 9258, 'commentary': 9259, 'commerzbank': 9260, 'communal': 9261, 'communication': 9262, 'compete': 9263, 'complaints': 9264, 'complicated': 9265, 'composed': 9266, 'concern': 9267, 'congressional': 9268, 'connecticut': 9269, 'conscious': 9270, 'constitution': 9271, 'cont': 9272, 'contestants': 9273, 'contracts': 9274, 'contribute': 9275, 'conversion': 9276, 'cooke': 9277, 'cooler': 9278, 'copaamerica': 9279, 'core': 9280, 'cork': 9281, 'cornwall': 9282, 'corporate': 9283, 'cosmetics': 9284, 'costume': 9285, 'cotd': 9286, 'cptplanespotter': 9287, 'cracked': 9288, 'cracking': 9289, 'cracks': 9290, 'craftfest': 9291, 'crappy': 9292, 'cries': 9293, 'crowdfunding': 9294, 'crowds': 9295, 'crueltyfree': 9296, 'crystal': 9297, 'crystals': 9298, 'css': 9299, 'cubs': 9300, 'cue': 9301, 'cuisine': 9302, 'cum': 9303, 'cupcakes': 9304, 'cuppa': 9305, 'cups': 9306, 'curb': 9307, 'curiosity': 9308, 'curious': 9309, 'curvy': 9310, 'cushions': 9311, 'customized': 9312, 'cuts': 9313, 'cybersecurity': 9314, 'dachshund': 9315, 'daddies': 9316, 'dadsday': 9317, 'damaged': 9318, 'danbury': 9319, 'dancelife': 9320, 'dances': 9321, 'darker': 9322, 'darkknight': 9323, 'deadpool': 9324, 'dearfellowwhitepeople': 9325, 'dears': 9326, 'decides': 9327, 'decorating': 9328, 'deepest': 9329, 'delegates': 9330, 'demise': 9331, 'den': 9332, 'dennis': 9333, 'derogatory': 9334, 'descending': 9335, 'desses': 9336, 'devastating': 9337, 'diadelpadre': 9338, 'diner': 9339, 'dinosaur': 9340, 'diploma': 9341, 'dipped': 9342, 'disbanded': 9343, 'discharge': 9344, 'discontinued': 9345, 'dish': 9346, 'dishonest': 9347, 'disneys': 9348, 'disorder': 9349, 'distressed': 9350, 'ditch': 9351, 'divulgaoeparceria': 9352, 'dixie': 9353, 'diysos': 9354, 'djlife': 9355, 'dnaquotes': 9356, 'dnt': 9357, 'doc': 9358, 'doggie': 9359, 'doggies': 9360, 'dome': 9361, 'donalds': 9362, 'doodle': 9363, 'dosti': 9364, 'dovish': 9365, 'dowhatyoulove': 9366, 'dozens': 9367, 'dp': 9368, 'draft': 9369, 'draghi': 9370, 'dragonfly': 9371, 'drained': 9372, 'dread': 9373, 'dreamscometrue': 9374, 'dreamteam': 9375, 'dressage': 9376, 'dressing': 9377, 'dri': 9378, 'dribbble': 9379, 'drives': 9380, 'drought': 9381, 'drugaddicts': 9382, 'drum': 9383, 'drums': 9384, 'dumbass': 9385, 'dvbmultimediagroup': 9386, 'dvd': 9387, 'dvr': 9388, 'dye': 9389, 'dynamic': 9390, 'dysfunction': 9391, 'earp': 9392, 'ears': 9393, 'ece': 9394, 'eden': 9395, 'edfringe2016': 9396, 'edits': 9397, 'edmonton': 9398, 'eeek': 9399, 'effos': 9400, 'ek': 9401, 'el': 9402, 'electric': 9403, 'elegy': 9404, 'elements': 9405, 'eliminate': 9406, 'elizabeth': 9407, 'ella': 9408, 'emea': 9409, 'encouragement': 9410, 'energetic': 9411, 'engaging': 9412, 'engerland': 9413, 'engineering': 9414, 'engwal': 9415, 'entirely': 9416, 'entrance': 9417, 'envy': 9418, 'episode13': 9419, 'equally': 9420, 'esco': 9421, 'essex': 9422, 'esteem': 9423, 'euro16': 9424, 'eurocup2016': 9425, 'evans': 9426, 'evenings': 9427, 'everest': 9428, 'everythings': 9429, 'evn': 9430, 'exact': 9431, 'exc': 9432, 'excellence': 9433, 'excitem': 9434, 'execs': 9435, 'expansion': 9436, 'expat': 9437, 'expos': 9438, 'expression': 9439, 'extended': 9440, 'extraordinaryladyspeaks': 9441, 'eyeing': 9442, 'facial': 9443, 'facing': 9444, 'fairy': 9445, 'fallschurch': 9446, 'familia': 9447, 'familytrip': 9448, 'fandom': 9449, 'fangirling': 9450, 'farage': 9451, 'farming': 9452, 'fatherandson': 9453, 'fatherly': 9454, 'fathersdayquotes': 9455, 'favor': 9456, 'fblogger': 9457, 'fd': 9458, 'fearing': 9459, 'feast': 9460, 'fedup': 9461, 'felicidad': 9462, 'fellas': 9463, 'feta': 9464, 'fiestar': 9465, 'filibuster': 9466, 'finance': 9467, 'firefly2016': 9468, 'fireflymusicfestival': 9469, 'firsttime': 9470, 'fishburn': 9471, 'fitting': 9472, 'fixit': 9473, 'flame': 9474, 'flamingo': 9475, 'flightofalifetime': 9476, 'flooding': 9477, 'floridian': 9478, 'flourish': 9479, 'flowerlove': 9480, 'flyers': 9481, 'focusing': 9482, 'followforfollow': 9483, 'followusoninstagram': 9484, 'foodstagram': 9485, 'forces': 9486, 'ford': 9487, 'foreverliving': 9488, 'forgive': 9489, 'forgivers': 9490, 'forums': 9491, 'fpace': 9492, 'frame': 9493, 'francisco': 9494, 'frankly': 9495, 'freddie': 9496, 'freetime': 9497, 'fri': 9498, 'fruits': 9499, 'fuckoff': 9500, 'fucks': 9501, 'fuelled': 9502, 'fulfilled': 9503, 'furbaby': 9504, 'furry': 9505, 'g': 9506, 'gabby': 9507, 'galway': 9508, 'gap': 9509, 'gas': 9510, 'gathering': 9511, 'gazal': 9512, 'gearing': 9513, 'gentleman': 9514, 'gentlemen': 9515, 'geranium': 9516, 'germanyhetalia': 9517, 'gettin': 9518, 'gg': 9519, 'ghosts': 9520, 'giddy': 9521, 'gifted': 9522, 'glam': 9523, 'glamorousiam': 9524, 'glory': 9525, 'glover': 9526, 'gloves': 9527, 'golinuntern': 9528, 'gonetoosoon': 9529, 'goodnews': 9530, 'goodvibesonly': 9531, 'gossips': 9532, 'gotmelike': 9533, 'gplay': 9534, 'gps': 9535, 'graders': 9536, 'grades': 9537, 'grandad': 9538, 'grandfather': 9539, 'grandson': 9540, 'grapes': 9541, 'greatquotes': 9542, 'greg': 9543, 'grieve': 9544, 'grilled': 9545, 'grimm': 9546, 'grimmies': 9547, 'grin': 9548, 'grooming': 9549, 'grounded': 9550, 'grove': 9551, 'growingup': 9552, 'grrr': 9553, 'gtgtgt': 9554, 'gummy': 9555, 'gunna': 9556, 'gunners': 9557, 'guru': 9558, 'hacked': 9559, 'hackney': 9560, 'hackneywick': 9561, 'hai': 9562, 'handbag': 9563, 'hangin': 9564, 'hannah': 9565, 'hap': 9566, 'happend': 9567, 'happiest5k': 9568, 'happines': 9569, 'happycampers': 9570, 'harambe': 9571, 'healthyeating': 9572, 'heavenoneah': 9573, 'hedgehog': 9574, 'hen': 9575, 'henna': 9576, 'henry': 9577, 'herd': 9578, 'hhmatters': 9579, 'hijacked': 9580, 'hike': 9581, 'hindi': 9582, 'historyancient': 9583, 'hmm': 9584, 'holland': 9585, 'homeland': 9586, 'homeon': 9587, 'hometown': 9588, 'hooligans': 9589, 'hospitals': 9590, 'hosted': 9591, 'housewife': 9592, 'housing': 9593, 'html': 9594, 'huaweiceifiedspecialistpresalesaccessnetwork': 9595, 'hung': 9596, 'hunt': 9597, 'hunting': 9598, 'husbands': 9599, 'hutchings': 9600, 'iampossible': 9601, 'iftaar': 9602, 'igersbnw': 9603, 'ikea': 9604, 'illusion': 9605, 'iloveit': 9606, 'ily': 9607, 'imissyou': 9608, 'impressed': 9609, 'impressive': 9610, 'inbox': 9611, 'incense': 9612, 'income': 9613, 'independenceday': 9614, 'inpain': 9615, 'insideout': 9616, 'instadog': 9617, 'instagirl': 9618, 'instantly': 9619, 'instatravel': 9620, 'instatraveling': 9621, 'int': 9622, 'integral': 9623, 'intro': 9624, 'introducing': 9625, 'invade': 9626, 'inventive': 9627, 'inventory': 9628, 'investigation': 9629, 'invitation': 9630, 'inwoo': 9631, 'iphoneonly': 9632, 'ipsy': 9633, 'irate': 9634, 'irresponsible': 9635, 'isaac': 9636, 'islamist': 9637, 'isle': 9638, 'isolated': 9639, 'itsfriday': 9640, 'itworks': 9641, 'iv': 9642, 'ivy': 9643, 'jacksonville': 9644, 'jaguar': 9645, 'jaguarfpace': 9646, 'jaguarstb': 9647, 'jai': 9648, 'jakaa': 9649, 'jeep': 9650, 'jenner': 9651, 'jerry': 9652, 'jet': 9653, 'jk': 9654, 'johnson': 9655, 'jones': 9656, 'jordan': 9657, 'joshua': 9658, 'judgement': 9659, 'judy': 9660, 'juiceplus': 9661, 'jumpsuit': 9662, 'junaricrm': 9663, 'june16': 9664, 'jungkook': 9665, 'jungkookday': 9666, 'justinbieber': 9667, 'justme': 9668, 'kardashian': 9669, 'katiequeue': 9670, 'kayla': 9671, 'ke': 9672, 'keeper': 9673, 'keisha': 9674, 'kennel': 9675, 'kern': 9676, 'kickstaer': 9677, 'killers': 9678, 'kilo': 9679, 'kinds': 9680, 'knee': 9681, 'koran': 9682, 'ks': 9683, 'kudos': 9684, 'ky': 9685, 'kyrie': 9686, 'lads': 9687, 'languages': 9688, 'lap': 9689, 'lasted': 9690, 'lastnight': 9691, 'latino': 9692, 'latvia': 9693, 'laughed': 9694, 'laundry': 9695, 'lawrence': 9696, 'lax': 9697, 'laziness': 9698, 'lbloggers': 9699, 'leagueoflegends': 9700, 'leanin15': 9701, 'legally': 9702, 'leukemia': 9703, 'lfc': 9704, 'liam': 9705, 'lichfield': 9706, 'lifehacks': 9707, 'lifeisbeautiful': 9708, 'lifestyleblogger': 9709, 'lightning': 9710, 'lightweight': 9711, 'likescam': 9712, 'limits': 9713, 'liners': 9714, 'linstagram': 9715, 'linzy': 9716, 'liquor': 9717, 'listento': 9718, 'lists': 9719, 'lite': 9720, 'lnicjustanevilbday': 9721, 'lnp': 9722, 'lobby': 9723, 'lois': 9724, 'lola': 9725, 'lollipop': 9726, 'longweekend': 9727, 'loop': 9728, 'los': 9729, 'loveconquershate': 9730, 'lovecraft': 9731, 'lovefollow': 9732, 'lovelovelove': 9733, 'lovemyfamily': 9734, 'loveofmylife': 9735, 'lower': 9736, 'ludicrous': 9737, 'luggage': 9738, 'maccosmetics': 9739, 'maddow': 9740, 'maialas': 9741, 'maine': 9742, 'makemoney': 9743, 'makemoneyonline': 9744, 'makeover': 9745, 'makeupaddict': 9746, 'makingmemories': 9747, 'malta': 9748, 'mampms': 9749, 'mango': 9750, 'manipulation': 9751, 'mantra': 9752, 'marcus': 9753, 'markets': 9754, 'mary': 9755, 'mattruff': 9756, 'maui': 9757, 'maza': 9758, 'mba': 9759, 'meatloaf': 9760, 'medium': 9761, 'mehn': 9762, 'melee': 9763, 'melissa': 9764, 'melodic': 9765, 'memorable': 9766, 'meow': 9767, 'merch': 9768, 'messing': 9769, 'michaelacoel': 9770, 'midsummer': 9771, 'milestone': 9772, 'mills': 9773, 'mindblown': 9774, 'minecraft': 9775, 'miniature': 9776, 'minnesota': 9777, 'mint': 9778, 'miracle': 9779, 'mirrors': 9780, 'mis': 9781, 'mistress': 9782, 'mlm': 9783, 'mohenjodaro': 9784, 'momentum': 9785, 'momlife': 9786, 'monica': 9787, 'monitor': 9788, 'moose': 9789, 'mor': 9790, 'morocco': 9791, 'motor': 9792, 'mourn': 9793, 'moveon': 9794, 'mrrat395': 9795, 'msgs': 9796, 'mubarak': 9797, 'muffins': 9798, 'multivitamins': 9799, 'mums': 9800, 'muna': 9801, 'muse': 9802, 'musicians': 9803, 'musicislife': 9804, 'myanmars': 9805, 'mybihday': 9806, 'myhappycapture': 9807, 'mykonos': 9808, 'mymood': 9809, 'mytime': 9810, 'naitik': 9811, 'naruto': 9812, 'nationallobsterday': 9813, 'nationalroseday': 9814, 'nationals': 9815, 'natureperfection': 9816, 'nebraska': 9817, 'necessity': 9818, 'needa': 9819, 'neighbour': 9820, 'neil': 9821, 'netflixandchill': 9822, 'nevada': 9823, 'newsong': 9824, 'newspaper': 9825, 'newsta': 9826, 'nfl': 9827, 'nhl': 9828, 'niceday': 9829, 'nicely': 9830, 'nigel': 9831, 'nigth': 9832, 'nikkei': 9833, 'nikon': 9834, 'nintendo': 9835, 'nintendoe3': 9836, 'noble': 9837, 'nohampton': 9838, 'nohcarolina': 9839, 'nomakeup': 9840, 'nomnomnom': 9841, 'nonton': 9842, 'norway': 9843, 'notifications': 9844, 'notokay': 9845, 'nowlinkup': 9846, 'nra': 9847, 'nuanced': 9848, 'nuascannan': 9849, 'nursing': 9850, 'obs': 9851, 'ocd': 9852, 'oclock': 9853, 'ohh': 9854, 'ohhhhh': 9855, 'oitnb4': 9856, 'oitnbchat': 9857, 'oitnbseason4': 9858, 'olathe': 9859, 'om': 9860, 'omaha': 9861, 'omarosas': 9862, 'onion': 9863, 'onpoint': 9864, 'ontheredcarpet': 9865, 'oomf': 9866, 'opener': 9867, 'opera': 9868, 'operating': 9869, 'operation': 9870, 'oppa': 9871, 'options': 9872, 'ordering': 9873, 'organised': 9874, 'organising': 9875, 'organized': 9876, 'oscarpretorious': 9877, 'oslo': 9878, 'outfitoftheday': 9879, 'outlook': 9880, 'overall': 9881, 'overloaded': 9882, 'overnight': 9883, 'oxygen': 9884, 'oysters': 9885, 'pablo': 9886, 'pac': 9887, 'packaging': 9888, 'paddington': 9889, 'paicipate': 9890, 'paicular': 9891, 'paicularly': 9892, 'painful': 9893, 'pairs': 9894, 'paleo': 9895, 'pals': 9896, 'pancakes': 9897, 'panic': 9898, 'paperback': 9899, 'papers': 9900, 'papi': 9901, 'par': 9902, 'para': 9903, 'paranoid': 9904, 'parente': 9905, 'parks': 9906, 'passenger': 9907, 'password': 9908, 'pasta': 9909, 'pastel': 9910, 'patrick': 9911, 'pavillions': 9912, 'paycheck': 9913, 'pearly': 9914, 'peepz': 9915, 'peonies': 9916, 'percent': 9917, 'perfection': 9918, 'permanent': 9919, 'persian': 9920, 'personaldevelopment': 9921, 'persuade': 9922, 'phd': 9923, 'phelps': 9924, 'phew': 9925, 'pickles': 9926, 'picstitch': 9927, 'pilots': 9928, 'pin': 9929, 'pineapple': 9930, 'pint': 9931, 'pinterest': 9932, 'pity': 9933, 'pjs': 9934, 'playoffs': 9935, 'playstation': 9936, 'playtime': 9937, 'png': 9938, 'pnw': 9939, 'pocket': 9940, 'pol': 9941, 'polaroid': 9942, 'poolside': 9943, 'popcorn': 9944, 'popped': 9945, 'poppy': 9946, 'pork': 9947, 'positively': 9948, 'postion': 9949, 'poussey': 9950, 'pov': 9951, 'ppls': 9952, 'prayforchristina': 9953, 'prayforoakland': 9954, 'prepped': 9955, 'presentations': 9956, 'presidency': 9957, 'pressed': 9958, 'preventable': 9959, 'previously': 9960, 'primaries': 9961, 'printed': 9962, 'printing': 9963, 'prior': 9964, 'priority': 9965, 'prizes': 9966, 'probab': 9967, 'procrastinate': 9968, 'prof': 9969, 'prolly': 9970, 'promised': 9971, 'protected': 9972, 'protecting': 9973, 'protestors': 9974, 'proving': 9975, 'psalm': 9976, 'psychic': 9977, 'published': 9978, 'puglife': 9979, 'pugs': 9980, 'pulsenightclubshooting': 9981, 'pune': 9982, 'punished': 9983, 'puppets': 9984, 'pursue': 9985, 'python': 9986, 'qb': 9987, 'quad': 9988, 'queenie': 9989, 'quillen': 9990, 'rainnyday': 9991, 'rainyday': 9992, 'rajan': 9993, 'ramdan': 9994, 'ramon': 9995, 'ramzan': 9996, 'raped': 9997, 'rapflashback': 9998, 'rappers': 9999, 'rationalize': 10000, 'rave': 10001, 'raves': 10002, 'reaches': 10003, 'reaching': 10004, 'reachout': 10005, 'realbeauty': 10006, 'realizing': 10007, 'reassuring': 10008, 'rebel': 10009, 'recipes': 10010, 'reconsider': 10011, 'redneck': 10012, 'refreshed': 10013, 'refuse': 10014, 'regards': 10015, 'region': 10016, 'register': 10017, 'regram': 10018, 'regular': 10019, 'reignofkong': 10020, 'relative': 10021, 'relieve': 10022, 'relieved': 10023, 'remarkable': 10024, 'reminded': 10025, 'remotely': 10026, 'removes': 10027, 'renaissance': 10028, 'renovation': 10029, 'rent': 10030, 'repeating': 10031, 'repoing': 10032, 'repos': 10033, 'reservation': 10034, 'restaurants': 10035, 'retailtherapy': 10036, 'returned': 10037, 'retweeting': 10038, 'revenue': 10039, 'revival': 10040, 'rewarding': 10041, 'rewards': 10042, 'rhythm': 10043, 'rihanna': 10044, 'rising': 10045, 'rite': 10046, 'robbie': 10047, 'robbins': 10048, 'rocking': 10049, 'rocknroll': 10050, 'roger': 10051, 'rom': 10052, 'romania': 10053, 'roosevelt': 10054, 'ross': 10055, 'roy': 10056, 'rssxactaccounts': 10057, 'rub': 10058, 'rul': 10059, 'runhappy': 10060, 'rus': 10061, 'rush': 10062, 'russians': 10063, 'sacramento': 10064, 'safely': 10065, 'sailing': 10066, 'saint': 10067, 'saints': 10068, 'salary': 10069, 'salut': 10070, 'samsung': 10071, 'sandler': 10072, 'satnight': 10073, 'saves': 10074, 'sc': 10075, 'scan': 10076, 'scarf': 10077, 'scenic': 10078, 'scheduled': 10079, 'schnauzer': 10080, 'schoolofrock': 10081, 'scuba': 10082, 'sd': 10083, 'seductive': 10084, 'seemed': 10085, 'selca': 10086, 'select': 10087, 'selenagomez': 10088, 'selfcare': 10089, 'selfhelp': 10090, 'selfrespect': 10091, 'seminar': 10092, 'senator': 10093, 'sequence': 10094, 'served': 10095, 'seth': 10096, 'settle': 10097, 'several': 10098, 'sexest': 10099, 'shakespeare': 10100, 'shakur': 10101, 'shell': 10102, 'ship': 10103, 'shipping': 10104, 'shitshow': 10105, 'shofilm': 10106, 'shoplocal': 10107, 'shopper': 10108, 'siblings': 10109, 'signal': 10110, 'silveragatka': 10111, 'singers': 10112, 'singles': 10113, 'sinking': 10114, 'sisterhood': 10115, 'skill': 10116, 'skilled': 10117, 'skype': 10118, 'sl': 10119, 'sleepless': 10120, 'sleepover': 10121, 'slime': 10122, 'smaccdub': 10123, 'smallbiz': 10124, 'smores': 10125, 'snacks': 10126, 'snails': 10127, 'snake': 10128, 'snapchater': 10129, 'snatched': 10130, 'sno': 10131, 'soap': 10132, 'socialanxiety': 10133, 'socialists': 10134, 'socute': 10135, 'soexcited': 10136, 'someday': 10137, 'somethin': 10138, 'songwriting': 10139, 'sooner': 10140, 'sore': 10141, 'sorrow': 10142, 'sosad': 10143, 'soulmate': 10144, 'soulmates': 10145, 'sounding': 10146, 'soup': 10147, 'southbeach': 10148, 'specialday': 10149, 'spice': 10150, 'spicy': 10151, 'sponsoring': 10152, 'sponsors': 10153, 'spots': 10154, 'spray': 10155, 'sprayed': 10156, 'spreading': 10157, 'spree': 10158, 'springfield': 10159, 'squadgoals': 10160, 'squeaking': 10161, 'squid': 10162, 'stamford': 10163, 'stan': 10164, 'starawards': 10165, 'staups': 10166, 'stayed': 10167, 'stealing': 10168, 'steam': 10169, 'stepdad': 10170, 'sterling': 10171, 'stil': 10172, 'sto': 10173, 'stockmusic': 10174, 'stopgunviolence': 10175, 'strangers': 10176, 'streaming': 10177, 'stressed': 10178, 'strip': 10179, 'stripes': 10180, 'striving': 10181, 'strolling': 10182, 'struggle': 10183, 'stubborn': 10184, 'studios': 10185, 'stunt': 10186, 'stylehausboutique': 10187, 'stylistic': 10188, 'su': 10189, 'subscribe': 10190, 'succubus': 10191, 'suckin': 10192, 'sucking': 10193, 'suffered': 10194, 'suits': 10195, 'sum': 10196, 'sunbathing': 10197, 'sunnies': 10198, 'sunsets': 10199, 'superstar': 10200, 'surf': 10201, 'surgery': 10202, 'surprising': 10203, 'susan': 10204, 'suspect': 10205, 'sweater': 10206, 'sweetest': 10207, 'swimsuit': 10208, 'switch': 10209, 'symptoms': 10210, 'tablet': 10211, 'tacos': 10212, 'tactics': 10213, 'tanhai': 10214, 'tanks': 10215, 'tanned': 10216, 'tapas': 10217, 'tasting': 10218, 'tdl': 10219, 'tease': 10220, 'teletubbiesusa': 10221, 'temporary': 10222, 'tend': 10223, 'teresa': 10224, 'terrific': 10225, 'terrified': 10226, 'testimony': 10227, 'tgifriday': 10228, 'tgifridays': 10229, 'thankfulthursday': 10230, 'thanking': 10231, 'theatres': 10232, 'thefosters': 10233, 'thegoodlife': 10234, 'themusketeers': 10235, 'thesis': 10236, 'thick': 10237, 'thirst': 10238, 'thoughtleaders': 10239, 'thoughtsandprayers': 10240, 'threw': 10241, 'thriller': 10242, 'thrilling': 10243, 'throne': 10244, 'ties': 10245, 'tim': 10246, 'timeline': 10247, 'tinyplanetbuf': 10248, 'titles': 10249, 'tix': 10250, 'tmrw': 10251, 'tn': 10252, 'tnx': 10253, 'toa': 10254, 'todd': 10255, 'togetherness': 10256, 'ton': 10257, 'toned': 10258, 'topics': 10259, 'totellthetruth': 10260, 'touched': 10261, 'tourism': 10262, 'toyota': 10263, 'tradition': 10264, 'traditions': 10265, 'trainers': 10266, 'trainhard': 10267, 'trans': 10268, 'travelgram': 10269, 'travelingram': 10270, 'traveltuesday': 10271, 'treating': 10272, 'triathlon': 10273, 'tricks': 10274, 'triumph': 10275, 'troopingthecolour': 10276, 'trusted': 10277, 'truthful': 10278, 'tules': 10279, 'tumblr': 10280, 'tuning': 10281, 'turner': 10282, 'tutorial': 10283, 'twist': 10284, 'tx': 10285, 'tym': 10286, 'ufeellucky': 10287, 'uh': 10288, 'ukulele': 10289, 'unbreakable': 10290, 'unchanged': 10291, 'unconditional': 10292, 'unexpected': 10293, 'unlikely': 10294, 'unnecessary': 10295, 'unprofessional': 10296, 'unstable': 10297, 'untill': 10298, 'uploaded': 10299, 'uploading': 10300, 'urself': 10301, 'utter': 10302, 'uv': 10303, 'validation': 10304, 'valuable': 10305, 'veggie': 10306, 'veggies': 10307, 'ventura': 10308, 'verona': 10309, 'versatile': 10310, 'verse': 10311, 'vet': 10312, 'veterans': 10313, 'vets': 10314, 'victorious': 10315, 'videoclip': 10316, 'vinci': 10317, 'visa': 10318, 'vitiating': 10319, 'vitorr': 10320, 'viual': 10321, 'vlogger': 10322, 'vm': 10323, 'vodka': 10324, 'voltaire': 10325, 'volunteering': 10326, 'vox': 10327, 'vscodaily': 10328, 'vulnerable': 10329, 'wacky': 10330, 'wakeupamerica': 10331, 'wal': 10332, 'walkies': 10333, 'wallpaper': 10334, 'wallpapers': 10335, 'walt': 10336, 'wanting': 10337, 'wardrobe': 10338, 'warned': 10339, 'warrior': 10340, 'wash': 10341, 'wat': 10342, 'watchthisspace': 10343, 'watering': 10344, 'waving': 10345, 'wayne': 10346, 'wd': 10347, 'webcammodel': 10348, 'webdesign': 10349, 'webster': 10350, 'weddingcountdown': 10351, 'weddingpay': 10352, 'weekender': 10353, 'weighed': 10354, 'welfare': 10355, 'wesley': 10356, 'westpac': 10357, 'whatajoke': 10358, 'wheel': 10359, 'whispers': 10360, 'whistling': 10361, 'wifi': 10362, 'willow': 10363, 'wind': 10364, 'windows10': 10365, 'windy': 10366, 'wives': 10367, 'wknd': 10368, 'wme': 10369, 'wmy': 10370, 'wohooo': 10371, 'wonderfully': 10372, 'wondering': 10373, 'woods': 10374, 'wordpress': 10375, 'worker': 10376, 'workshops': 10377, 'worries': 10378, 'wrapping': 10379, 'wrecked': 10380, 'wrestling': 10381, 'wti': 10382, 'xxxx': 10383, 'yard': 10384, 'yayyyy': 10385, 'yelp': 10386, 'yg': 10387, 'yogalove': 10388, 'youngforever': 10389, 'youuu': 10390, 'yul': 10391, 'zombie': 10392, 'zomg': 10393, 'zoological': 10394, 'zootopia': 10395})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "#set batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load an iterator\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.clean_tweet),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "qG4xwKapjnza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#hidden = [batch size, num layers * num directions,hid dim]\n",
        "#cell = [batch size, num layers * num directions,hid dim]\n",
        "\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, text, n_layers, \n",
        "                 bidirectional, dropout):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        self.embedding_dim = embedding_dim\n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data = text.vectors\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=True, \n",
        "                          #  dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        # #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "\n",
        "        #packed sequence\n",
        "        packed_embedded = pack_padded_sequence(embedded, text_lengths,batch_first=True, \n",
        "                                                            enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        # hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        # dense_outputs=self.fc(hidden)\n",
        "        # outputs=self.act(dense_outputs) \n",
        "\n",
        "        # hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        out_forward = output[range(len(output)), text_lengths - 1, :self.embedding_dim]\n",
        "        out_reverse = output[:, 0, self.embedding_dim:]\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "        dense_outputs = self.dropout(out_reduced)\n",
        "        dense_outputs=self.fc(dense_outputs)\n",
        "\n",
        "        dense_outputs = torch.squeeze(dense_outputs, 1)\n",
        "        outputs = torch.sigmoid(dense_outputs)\n",
        "        #Final activation function\n",
        "        # outputs=F.log_softmax(dense_outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "VKSGkL1skcgT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_vocab = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 50\n",
        "num_output_nodes = 1\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "#instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, TEXT.vocab, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)"
      ],
      "metadata": {
        "id": "3p0YFjHvjzd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#architecture\n",
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Initialize the pretrained embedding\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4BUZ_a6kkfr",
        "outputId": "ed9cb896-4826-455d-ba71-0ab2b8e33126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(10396, 100)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (lstm): LSTM(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 1,161,301 trainable parameters\n",
            "torch.Size([10396, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.00001)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "sw8IJDbX7BOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    print(\"Sum of predicted\",preds.sum())\n",
        "    # print(y)\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "\n",
        "    accuracy = 0.0\n",
        "    for i, y_ in enumerate(preds):\n",
        "      if y_ == y[i]:\n",
        "          accuracy += 1.0   \n",
        "      accuracy = accuracy / len(preds)\n",
        "      #print('Test Accuracy: ', accuracy)\n",
        "              \n",
        "      # print('Classification Report:')\n",
        "      # print(classification_report(y, preds, labels=[1,0], digits=4))\n",
        "      \n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "QxF8RWfYkst1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upsampled Strategy 1"
      ],
      "metadata": {
        "id": "xd0h95ms6btE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # #set the model in training phase\n",
        "    # model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        #resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        #retrieve text and no. of words\n",
        "        text, text_lengths = batch.clean_tweet   \n",
        "        #convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze() \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        #compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "sIrwEHCMmi2J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    iter = 0\n",
        "\n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            iter += 1\n",
        "            #retrieve text and no. of words\n",
        "            text, text_lengths = batch.clean_tweet\n",
        "            \n",
        "            #convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze()\n",
        "            print(predictions)\n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            if iter % 100:\n",
        "              print(\"Loss\", loss)\n",
        "              print(\"Acc\", acc)\n",
        "            \n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "THxrq5RomkQz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "#set the model in training phase\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPbif2BRmol4",
        "outputId": "e5b6879a-5628-4c6b-f736-b536a9efa805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tensor([0.4806, 0.4758, 0.4799, 0.4684, 0.4697, 0.4760, 0.4742, 0.4743, 0.4732,\n",
            "        0.4747, 0.4720, 0.4727, 0.4728, 0.4774, 0.4740, 0.4738, 0.4780, 0.4757,\n",
            "        0.4720, 0.4724, 0.4782, 0.4730, 0.4767, 0.4744, 0.4743, 0.4770, 0.4751,\n",
            "        0.4765, 0.4724, 0.4766, 0.4757, 0.4763, 0.4722, 0.4763, 0.4749, 0.4765,\n",
            "        0.4759, 0.4779, 0.4772, 0.4756, 0.4765, 0.4741, 0.4799, 0.4768, 0.4739,\n",
            "        0.4760, 0.4775, 0.4740, 0.4721, 0.4806, 0.4755, 0.4746, 0.4741, 0.4764,\n",
            "        0.4786, 0.4741, 0.4739, 0.4757, 0.4778, 0.4762, 0.4739, 0.4803, 0.4759,\n",
            "        0.4757])\n",
            "Sum of predicted tensor(30.4244)\n",
            "Loss tensor(0.6992)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4787, 0.4737, 0.4731, 0.4761, 0.4799, 0.4770, 0.4681, 0.4799, 0.4756,\n",
            "        0.4775, 0.4776, 0.4839, 0.4762, 0.4785, 0.4781, 0.4761, 0.4706, 0.4758,\n",
            "        0.4706, 0.4790, 0.4739, 0.4743, 0.4806, 0.4749, 0.4717, 0.4776, 0.4774,\n",
            "        0.4767, 0.4747, 0.4739, 0.4769, 0.4746, 0.4764, 0.4718, 0.4768, 0.4812,\n",
            "        0.4684, 0.4757, 0.4711, 0.4722, 0.4774, 0.4776, 0.4769, 0.4754, 0.4740,\n",
            "        0.4774, 0.4751, 0.4748, 0.4746, 0.4739, 0.4766, 0.4725, 0.4773, 0.4736,\n",
            "        0.4749, 0.4755, 0.4750, 0.4754, 0.4762, 0.4757, 0.4776, 0.4739, 0.4725,\n",
            "        0.4774])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4754, 0.4766, 0.4808, 0.4764, 0.4794, 0.4795, 0.4747, 0.4744, 0.4763,\n",
            "        0.4760, 0.4721, 0.4773, 0.4741, 0.4827, 0.4766, 0.4812, 0.4764, 0.4703,\n",
            "        0.4759, 0.4751, 0.4716, 0.4752, 0.4726, 0.4759, 0.4759, 0.4766, 0.4783,\n",
            "        0.4749, 0.4784, 0.4745, 0.4723, 0.4730, 0.4775, 0.4761, 0.4799, 0.4721,\n",
            "        0.4745, 0.4780, 0.4748, 0.4812, 0.4780, 0.4753, 0.4737, 0.4790, 0.4721,\n",
            "        0.4746, 0.4777, 0.4839, 0.4706, 0.4788, 0.4748, 0.4720, 0.4715, 0.4817,\n",
            "        0.4725, 0.4736, 0.4752, 0.4694, 0.4702, 0.4741, 0.4756, 0.4745, 0.4774,\n",
            "        0.4739])\n",
            "Sum of predicted tensor(30.4446)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4740, 0.4762, 0.4684, 0.4722, 0.4781, 0.4742, 0.4799, 0.4750, 0.4786,\n",
            "        0.4722, 0.4761, 0.4752, 0.4729, 0.4774, 0.4795, 0.4720, 0.4732, 0.4757,\n",
            "        0.4794, 0.4747, 0.4726, 0.4762, 0.4754, 0.4720, 0.4746, 0.4762, 0.4734,\n",
            "        0.4790, 0.4752, 0.4757, 0.4733, 0.4787, 0.4766, 0.4694, 0.4716, 0.4767,\n",
            "        0.4757, 0.4726, 0.4743, 0.4744, 0.4772, 0.4738, 0.4750, 0.4770, 0.4749,\n",
            "        0.4712, 0.4752, 0.4786, 0.4743, 0.4735, 0.4753, 0.4736, 0.4746, 0.4731,\n",
            "        0.4757, 0.4778, 0.4759, 0.4758, 0.4764, 0.4779, 0.4765, 0.4769, 0.4771,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4129)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4754, 0.4783, 0.4764, 0.4711, 0.4722, 0.4765, 0.4812, 0.4753, 0.4729,\n",
            "        0.4771, 0.4772, 0.4735, 0.4757, 0.4780, 0.4757, 0.4839, 0.4769, 0.4745,\n",
            "        0.4712, 0.4768, 0.4738, 0.4739, 0.4725, 0.4728, 0.4743, 0.4777, 0.4746,\n",
            "        0.4725, 0.4770, 0.4715, 0.4768, 0.4779, 0.4762, 0.4812, 0.4778, 0.4718,\n",
            "        0.4765, 0.4754, 0.4737, 0.4786, 0.4720, 0.4776, 0.4735, 0.4725, 0.4781,\n",
            "        0.4742, 0.4741, 0.4741, 0.4741, 0.4770, 0.4749, 0.4768, 0.4812, 0.4812,\n",
            "        0.4703, 0.4739, 0.4720, 0.4709, 0.4785, 0.4753, 0.4752, 0.4756, 0.4763,\n",
            "        0.4684])\n",
            "Sum of predicted tensor(30.4240)\n",
            "Loss tensor(0.6907)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4745, 0.4742, 0.4796, 0.4759, 0.4751, 0.4744, 0.4761, 0.4743, 0.4723,\n",
            "        0.4776, 0.4735, 0.4757, 0.4811, 0.4784, 0.4801, 0.4735, 0.4780, 0.4765,\n",
            "        0.4726, 0.4799, 0.4761, 0.4743, 0.4769, 0.4722, 0.4757, 0.4735, 0.4757,\n",
            "        0.4790, 0.4773, 0.4778, 0.4762, 0.4721, 0.4761, 0.4762, 0.4717, 0.4777,\n",
            "        0.4739, 0.4746, 0.4754, 0.4764, 0.4790, 0.4780, 0.4760, 0.4733, 0.4767,\n",
            "        0.4710, 0.4748, 0.4770, 0.4771, 0.4747, 0.4720, 0.4744, 0.4715, 0.4715,\n",
            "        0.4737, 0.4755, 0.4760, 0.4703, 0.4735, 0.4758, 0.4748, 0.4736, 0.4812,\n",
            "        0.4780])\n",
            "Sum of predicted tensor(30.4313)\n",
            "Loss tensor(0.7024)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4763, 0.4724, 0.4743, 0.4769, 0.4697, 0.4770, 0.4751, 0.4768, 0.4735,\n",
            "        0.4697, 0.4769, 0.4761, 0.4727, 0.4725, 0.4770, 0.4742, 0.4745, 0.4749,\n",
            "        0.4725, 0.4748, 0.4712, 0.4753, 0.4776, 0.4715, 0.4722, 0.4754, 0.4759,\n",
            "        0.4761, 0.4733, 0.4785, 0.4747, 0.4775, 0.4757, 0.4813, 0.4767, 0.4760,\n",
            "        0.4767, 0.4694, 0.4740, 0.4749, 0.4741, 0.4741, 0.4733, 0.4766, 0.4738,\n",
            "        0.4772, 0.4751, 0.4694, 0.4709, 0.4694, 0.4735, 0.4803, 0.4780, 0.4746,\n",
            "        0.4728, 0.4734, 0.4793, 0.4790, 0.4775, 0.4724, 0.4711, 0.4722, 0.4720,\n",
            "        0.4763])\n",
            "Sum of predicted tensor(30.3779)\n",
            "Loss tensor(0.6974)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4790, 0.4749, 0.4787, 0.4720, 0.4757, 0.4693, 0.4719, 0.4735, 0.4729,\n",
            "        0.4766, 0.4748, 0.4765, 0.4804, 0.4761, 0.4755, 0.4773, 0.4770, 0.4725,\n",
            "        0.4734, 0.4791, 0.4759, 0.4725, 0.4761, 0.4725, 0.4768, 0.4755, 0.4741,\n",
            "        0.4769, 0.4730, 0.4737, 0.4799, 0.4761, 0.4778, 0.4736, 0.4775, 0.4757,\n",
            "        0.4739, 0.4763, 0.4785, 0.4736, 0.4766, 0.4782, 0.4731, 0.4762, 0.4737,\n",
            "        0.4781, 0.4769, 0.4749, 0.4706, 0.4762, 0.4763, 0.4783, 0.4675, 0.4718,\n",
            "        0.4810, 0.4744, 0.4790, 0.4758, 0.4747, 0.4799, 0.4717, 0.4768, 0.4758,\n",
            "        0.4776])\n",
            "Sum of predicted tensor(30.4324)\n",
            "Loss tensor(0.6998)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4775, 0.4799, 0.4720, 0.4752, 0.4759, 0.4728, 0.4745, 0.4747, 0.4737,\n",
            "        0.4703, 0.4793, 0.4768, 0.4774, 0.4781, 0.4780, 0.4744, 0.4715, 0.4742,\n",
            "        0.4775, 0.4744, 0.4735, 0.4677, 0.4745, 0.4740, 0.4738, 0.4768, 0.4736,\n",
            "        0.4726, 0.4797, 0.4746, 0.4768, 0.4766, 0.4754, 0.4745, 0.4754, 0.4734,\n",
            "        0.4799, 0.4748, 0.4757, 0.4722, 0.4743, 0.4735, 0.4762, 0.4728, 0.4757,\n",
            "        0.4736, 0.4742, 0.4715, 0.4747, 0.4739, 0.4780, 0.4745, 0.4743, 0.4737,\n",
            "        0.4794, 0.4761, 0.4779, 0.4723, 0.4747, 0.4796, 0.4768, 0.4757, 0.4733,\n",
            "        0.4764])\n",
            "Sum of predicted tensor(30.4068)\n",
            "Loss tensor(0.7021)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4721, 0.4787, 0.4749, 0.4754, 0.4764, 0.4732, 0.4731, 0.4715, 0.4763,\n",
            "        0.4708, 0.4754, 0.4769, 0.4735, 0.4803, 0.4743, 0.4797, 0.4760, 0.4786,\n",
            "        0.4776, 0.4747, 0.4711, 0.4799, 0.4766, 0.4764, 0.4719, 0.4761, 0.4768,\n",
            "        0.4699, 0.4739, 0.4755, 0.4799, 0.4752, 0.4780, 0.4725, 0.4736, 0.4750,\n",
            "        0.4714, 0.4798, 0.4754, 0.4781, 0.4797, 0.4770, 0.4733, 0.4799, 0.4720,\n",
            "        0.4743, 0.4726, 0.4779, 0.4747, 0.4748, 0.4737, 0.4723, 0.4759, 0.4744,\n",
            "        0.4754, 0.4725, 0.4810, 0.4720, 0.4749, 0.4795, 0.4736, 0.4725, 0.4745,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4174)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4776, 0.4770, 0.4760, 0.4725, 0.4755, 0.4717, 0.4737, 0.4761, 0.4749,\n",
            "        0.4735, 0.4769, 0.4777, 0.4745, 0.4740, 0.4790, 0.4753, 0.4739, 0.4751,\n",
            "        0.4786, 0.4763, 0.4744, 0.4706, 0.4761, 0.4773, 0.4745, 0.4709, 0.4763,\n",
            "        0.4806, 0.4788, 0.4742, 0.4725, 0.4784, 0.4758, 0.4776, 0.4760, 0.4776,\n",
            "        0.4742, 0.4758, 0.4764, 0.4736, 0.4749, 0.4816, 0.4766, 0.4839, 0.4770,\n",
            "        0.4717, 0.4788, 0.4710, 0.4753, 0.4701, 0.4799, 0.4734, 0.4720, 0.4788,\n",
            "        0.4786, 0.4770, 0.4740, 0.4778, 0.4735, 0.4777, 0.4758, 0.4725, 0.4803,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4469)\n",
            "Loss tensor(0.6950)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4723, 0.4730, 0.4763, 0.4788, 0.4678, 0.4737, 0.4778, 0.4745, 0.4768,\n",
            "        0.4729, 0.4750, 0.4768, 0.4781, 0.4810, 0.4754, 0.4802, 0.4746, 0.4758,\n",
            "        0.4752, 0.4761, 0.4834, 0.4728, 0.4717, 0.4736, 0.4777, 0.4746, 0.4696,\n",
            "        0.4788, 0.4771, 0.4724, 0.4799, 0.4757, 0.4745, 0.4775, 0.4775, 0.4772,\n",
            "        0.4764, 0.4732, 0.4760, 0.4778, 0.4754, 0.4733, 0.4761, 0.4786, 0.4766,\n",
            "        0.4722, 0.4722, 0.4721, 0.4756, 0.4734, 0.4691, 0.4751, 0.4795, 0.4762,\n",
            "        0.4747, 0.4739, 0.4767, 0.4766, 0.4749, 0.4765, 0.4750, 0.4735, 0.4757,\n",
            "        0.4726])\n",
            "Sum of predicted tensor(30.4250)\n",
            "Loss tensor(0.6867)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4752, 0.4771, 0.4783, 0.4773, 0.4760, 0.4733, 0.4758, 0.4766, 0.4805,\n",
            "        0.4750, 0.4727, 0.4806, 0.4764, 0.4705, 0.4799, 0.4758, 0.4737, 0.4760,\n",
            "        0.4727, 0.4711, 0.4722, 0.4760, 0.4774, 0.4757, 0.4748, 0.4738, 0.4776,\n",
            "        0.4745, 0.4738, 0.4793, 0.4757, 0.4698, 0.4759, 0.4807, 0.4778, 0.4736,\n",
            "        0.4813, 0.4788, 0.4708, 0.4757, 0.4754, 0.4770, 0.4752, 0.4751, 0.4706,\n",
            "        0.4760, 0.4759, 0.4794, 0.4705, 0.4756, 0.4767, 0.4768, 0.4751, 0.4780,\n",
            "        0.4798, 0.4763, 0.4784, 0.4753, 0.4777, 0.4771, 0.4687, 0.4709, 0.4794,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4436)\n",
            "Loss tensor(0.7016)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4725, 0.4734, 0.4733, 0.4743, 0.4769, 0.4735, 0.4708, 0.4774, 0.4674,\n",
            "        0.4750, 0.4762, 0.4753, 0.4765, 0.4743, 0.4748, 0.4712, 0.4787, 0.4807,\n",
            "        0.4805, 0.4792, 0.4750, 0.4752, 0.4736, 0.4746, 0.4715, 0.4705, 0.4781,\n",
            "        0.4734, 0.4701, 0.4750, 0.4786, 0.4762, 0.4741, 0.4782, 0.4799, 0.4735,\n",
            "        0.4781, 0.4761, 0.4743, 0.4747, 0.4789, 0.4734, 0.4754, 0.4714, 0.4756,\n",
            "        0.4786, 0.4784, 0.4724, 0.4787, 0.4695, 0.4747, 0.4758, 0.4769, 0.4750,\n",
            "        0.4734, 0.4766, 0.4805, 0.4805, 0.4771, 0.4778, 0.4817, 0.4696, 0.4774,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.4235)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4734, 0.4715, 0.4765, 0.4760, 0.4734, 0.4722, 0.4717, 0.4788, 0.4754,\n",
            "        0.4750, 0.4766, 0.4799, 0.4810, 0.4806, 0.4787, 0.4734, 0.4798, 0.4751,\n",
            "        0.4718, 0.4709, 0.4749, 0.4758, 0.4804, 0.4784, 0.4765, 0.4783, 0.4765,\n",
            "        0.4749, 0.4766, 0.4742, 0.4741, 0.4784, 0.4791, 0.4749, 0.4730, 0.4749,\n",
            "        0.4772, 0.4760, 0.4678, 0.4769, 0.4764, 0.4727, 0.4759, 0.4769, 0.4762,\n",
            "        0.4738, 0.4763, 0.4701, 0.4727, 0.4734, 0.4732, 0.4789, 0.4745, 0.4733,\n",
            "        0.4762, 0.4766, 0.4752, 0.4727, 0.4775, 0.4760, 0.4787, 0.4798, 0.4726,\n",
            "        0.4738])\n",
            "Sum of predicted tensor(30.4338)\n",
            "Loss tensor(0.6990)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4765, 0.4799, 0.4746, 0.4786, 0.4761, 0.4760, 0.4752, 0.4799, 0.4768,\n",
            "        0.4774, 0.4740, 0.4804, 0.4720, 0.4683, 0.4739, 0.4719, 0.4695, 0.4784,\n",
            "        0.4739, 0.4774, 0.4755, 0.4724, 0.4768, 0.4787, 0.4697, 0.4765, 0.4766,\n",
            "        0.4809, 0.4788, 0.4763, 0.4739, 0.4776, 0.4767, 0.4694, 0.4747, 0.4722,\n",
            "        0.4688, 0.4682, 0.4755, 0.4749, 0.4753, 0.4758, 0.4758, 0.4764, 0.4744,\n",
            "        0.4777, 0.4783, 0.4771, 0.4770, 0.4733, 0.4724, 0.4761, 0.4760, 0.4747,\n",
            "        0.4767, 0.4780, 0.4758, 0.4734, 0.4712, 0.4737, 0.4757, 0.4721, 0.4750,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4101)\n",
            "Loss tensor(0.6978)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4742, 0.4747, 0.4721, 0.4798, 0.4760, 0.4770, 0.4744, 0.4757, 0.4708,\n",
            "        0.4774, 0.4783, 0.4759, 0.4769, 0.4744, 0.4745, 0.4778, 0.4774, 0.4711,\n",
            "        0.4698, 0.4770, 0.4755, 0.4776, 0.4817, 0.4753, 0.4794, 0.4771, 0.4766,\n",
            "        0.4733, 0.4745, 0.4739, 0.4759, 0.4760, 0.4766, 0.4728, 0.4774, 0.4712,\n",
            "        0.4749, 0.4732, 0.4732, 0.4805, 0.4722, 0.4767, 0.4760, 0.4769, 0.4751,\n",
            "        0.4773, 0.4808, 0.4725, 0.4754, 0.4760, 0.4749, 0.4775, 0.4712, 0.4805,\n",
            "        0.4799, 0.4755, 0.4683, 0.4760, 0.4710, 0.4727, 0.4734, 0.4808, 0.4810,\n",
            "        0.4805])\n",
            "Sum of predicted tensor(30.4443)\n",
            "Loss tensor(0.6879)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4778, 0.4782, 0.4762, 0.4743, 0.4713, 0.4763, 0.4773, 0.4718, 0.4817,\n",
            "        0.4728, 0.4731, 0.4743, 0.4755, 0.4765, 0.4761, 0.4734, 0.4714, 0.4765,\n",
            "        0.4778, 0.4734, 0.4802, 0.4697, 0.4753, 0.4684, 0.4760, 0.4686, 0.4717,\n",
            "        0.4757, 0.4700, 0.4771, 0.4766, 0.4773, 0.4712, 0.4764, 0.4718, 0.4769,\n",
            "        0.4753, 0.4784, 0.4791, 0.4763, 0.4757, 0.4759, 0.4747, 0.4740, 0.4780,\n",
            "        0.4723, 0.4751, 0.4755, 0.4722, 0.4740, 0.4761, 0.4745, 0.4764, 0.4741,\n",
            "        0.4769, 0.4728, 0.4718, 0.4759, 0.4787, 0.4749, 0.4731, 0.4817, 0.4760,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4013)\n",
            "tensor([0.4786, 0.4740, 0.4753, 0.4714, 0.4764, 0.4745, 0.4731, 0.4762, 0.4766,\n",
            "        0.4786, 0.4799, 0.4754, 0.4754, 0.4724, 0.4754, 0.4754, 0.4750, 0.4788,\n",
            "        0.4764, 0.4749, 0.4773, 0.4739, 0.4744, 0.4798, 0.4731, 0.4761, 0.4734,\n",
            "        0.4701, 0.4739, 0.4723, 0.4700, 0.4757, 0.4780, 0.4698, 0.4745, 0.4794,\n",
            "        0.4754, 0.4768, 0.4763, 0.4784, 0.4751, 0.4754, 0.4750, 0.4717, 0.4769,\n",
            "        0.4789, 0.4733, 0.4769, 0.4766, 0.4758, 0.4779, 0.4781, 0.4807, 0.4721,\n",
            "        0.4760, 0.4800, 0.4763, 0.4723, 0.4788, 0.4759, 0.4782, 0.4688, 0.4765,\n",
            "        0.4800])\n",
            "Sum of predicted tensor(30.4396)\n",
            "Loss tensor(0.6859)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4760, 0.4759, 0.4731, 0.4728, 0.4715, 0.4737, 0.4759, 0.4743, 0.4732,\n",
            "        0.4718, 0.4743, 0.4720, 0.4731, 0.4743, 0.4805, 0.4734, 0.4749, 0.4815,\n",
            "        0.4763, 0.4764, 0.4720, 0.4802, 0.4718, 0.4755, 0.4771, 0.4722, 0.4749,\n",
            "        0.4749, 0.4792, 0.4764, 0.4796, 0.4835, 0.4678, 0.4756, 0.4787, 0.4805,\n",
            "        0.4754, 0.4791, 0.4724, 0.4762, 0.4749, 0.4746, 0.4729, 0.4762, 0.4729,\n",
            "        0.4793, 0.4736, 0.4780, 0.4764, 0.4711, 0.4760, 0.4673, 0.4777, 0.4758,\n",
            "        0.4799, 0.4793, 0.4777, 0.4720, 0.4771, 0.4753, 0.4743, 0.4774, 0.4766,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4277)\n",
            "Loss tensor(0.6887)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4773, 0.4794, 0.4794, 0.4781, 0.4727, 0.4749, 0.4787, 0.4766, 0.4763,\n",
            "        0.4784, 0.4705, 0.4730, 0.4759, 0.4736, 0.4754, 0.4725, 0.4735, 0.4738,\n",
            "        0.4714, 0.4771, 0.4791, 0.4768, 0.4776, 0.4754, 0.4752, 0.4773, 0.4781,\n",
            "        0.4781, 0.4764, 0.4756, 0.4743, 0.4788, 0.4750, 0.4708, 0.4740, 0.4750,\n",
            "        0.4750, 0.4718, 0.4789, 0.4766, 0.4779, 0.4772, 0.4739, 0.4757, 0.4775,\n",
            "        0.4760, 0.4745, 0.4767, 0.4758, 0.4756, 0.4750, 0.4748, 0.4739, 0.4784,\n",
            "        0.4817, 0.4764, 0.4763, 0.4775, 0.4765, 0.4771, 0.4702, 0.4765, 0.4788,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4569)\n",
            "Loss tensor(0.7065)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4747, 0.4759, 0.4758, 0.4729, 0.4739, 0.4743, 0.4780, 0.4739, 0.4761,\n",
            "        0.4771, 0.4712, 0.4751, 0.4760, 0.4763, 0.4755, 0.4805, 0.4697, 0.4740,\n",
            "        0.4755, 0.4740, 0.4765, 0.4737, 0.4756, 0.4768, 0.4817, 0.4766, 0.4728,\n",
            "        0.4787, 0.4739, 0.4765, 0.4747, 0.4746, 0.4748, 0.4683, 0.4743, 0.4763,\n",
            "        0.4811, 0.4732, 0.4746, 0.4771, 0.4783, 0.4735, 0.4765, 0.4747, 0.4766,\n",
            "        0.4796, 0.4756, 0.4773, 0.4744, 0.4752, 0.4793, 0.4799, 0.4694, 0.4728,\n",
            "        0.4713, 0.4778, 0.4722, 0.4750, 0.4819, 0.4746, 0.4760, 0.4817, 0.4749,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4377)\n",
            "Loss tensor(0.6909)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4760, 0.4743, 0.4799, 0.4751, 0.4802, 0.4757, 0.4784, 0.4794, 0.4758,\n",
            "        0.4774, 0.4757, 0.4755, 0.4765, 0.4777, 0.4722, 0.4733, 0.4743, 0.4777,\n",
            "        0.4749, 0.4757, 0.4774, 0.4739, 0.4771, 0.4762, 0.4764, 0.4727, 0.4799,\n",
            "        0.4755, 0.4808, 0.4768, 0.4749, 0.4713, 0.4808, 0.4710, 0.4780, 0.4761,\n",
            "        0.4805, 0.4754, 0.4757, 0.4711, 0.4752, 0.4778, 0.4774, 0.4766, 0.4807,\n",
            "        0.4798, 0.4750, 0.4731, 0.4751, 0.4750, 0.4791, 0.4799, 0.4779, 0.4765,\n",
            "        0.4775, 0.4791, 0.4834, 0.4759, 0.4782, 0.4777, 0.4736, 0.4804, 0.4749,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.5026)\n",
            "Loss tensor(0.6982)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4806, 0.4769, 0.4727, 0.4731, 0.4784, 0.4759, 0.4743, 0.4750, 0.4771,\n",
            "        0.4757, 0.4766, 0.4787, 0.4761, 0.4723, 0.4715, 0.4793, 0.4780, 0.4722,\n",
            "        0.4804, 0.4766, 0.4737, 0.4760, 0.4762, 0.4731, 0.4700, 0.4743, 0.4793,\n",
            "        0.4784, 0.4711, 0.4799, 0.4722, 0.4681, 0.4678, 0.4700, 0.4802, 0.4714,\n",
            "        0.4769, 0.4774, 0.4778, 0.4770, 0.4782, 0.4797, 0.4685, 0.4771, 0.4721,\n",
            "        0.4755, 0.4779, 0.4781, 0.4763, 0.4742, 0.4751, 0.4793, 0.4809, 0.4799,\n",
            "        0.4778, 0.4735, 0.4679, 0.4738, 0.4750, 0.4788, 0.4760, 0.4765, 0.4774,\n",
            "        0.4754])\n",
            "Sum of predicted tensor(30.4373)\n",
            "Loss tensor(0.7043)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4755, 0.4749, 0.4731, 0.4768, 0.4733, 0.4723, 0.4747, 0.4788, 0.4757,\n",
            "        0.4799, 0.4786, 0.4733, 0.4753, 0.4782, 0.4774, 0.4757, 0.4778, 0.4741,\n",
            "        0.4751, 0.4752, 0.4747, 0.4744, 0.4691, 0.4767, 0.4798, 0.4707, 0.4737,\n",
            "        0.4737, 0.4765, 0.4739, 0.4766, 0.4799, 0.4786, 0.4701, 0.4769, 0.4768,\n",
            "        0.4709, 0.4741, 0.4805, 0.4788, 0.4760, 0.4730, 0.4760, 0.4755, 0.4737,\n",
            "        0.4740, 0.4788, 0.4769, 0.4729, 0.4760, 0.4806, 0.4764, 0.4707, 0.4740,\n",
            "        0.4746, 0.4765, 0.4683, 0.4768, 0.4754, 0.4743, 0.4751, 0.4754, 0.4750,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4227)\n",
            "Loss tensor(0.6890)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4779, 0.4771, 0.4730, 0.4720, 0.4763, 0.4759, 0.4805, 0.4765, 0.4746,\n",
            "        0.4763, 0.4764, 0.4745, 0.4745, 0.4715, 0.4710, 0.4743, 0.4799, 0.4793,\n",
            "        0.4770, 0.4740, 0.4771, 0.4723, 0.4765, 0.4762, 0.4751, 0.4768, 0.4707,\n",
            "        0.4782, 0.4702, 0.4790, 0.4744, 0.4712, 0.4765, 0.4758, 0.4736, 0.4755,\n",
            "        0.4766, 0.4730, 0.4821, 0.4771, 0.4750, 0.4753, 0.4782, 0.4735, 0.4754,\n",
            "        0.4766, 0.4763, 0.4736, 0.4780, 0.4773, 0.4713, 0.4745, 0.4768, 0.4730,\n",
            "        0.4748, 0.4752, 0.4738, 0.4748, 0.4796, 0.4808, 0.4740, 0.4758, 0.4762,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.6891)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4709, 0.4791, 0.4761, 0.4768, 0.4780, 0.4770, 0.4762, 0.4721, 0.4750,\n",
            "        0.4783, 0.4753, 0.4770, 0.4772, 0.4758, 0.4738, 0.4762, 0.4748, 0.4794,\n",
            "        0.4715, 0.4817, 0.4804, 0.4761, 0.4754, 0.4714, 0.4756, 0.4729, 0.4762,\n",
            "        0.4758, 0.4753, 0.4760, 0.4765, 0.4746, 0.4744, 0.4771, 0.4756, 0.4736,\n",
            "        0.4821, 0.4734, 0.4767, 0.4742, 0.4768, 0.4708, 0.4783, 0.4744, 0.4754,\n",
            "        0.4751, 0.4775, 0.4731, 0.4710, 0.4715, 0.4751, 0.4752, 0.4753, 0.4731,\n",
            "        0.4762, 0.4697, 0.4749, 0.4735, 0.4771, 0.4777, 0.4765, 0.4769, 0.4736,\n",
            "        0.4778])\n",
            "Sum of predicted tensor(30.4322)\n",
            "Loss tensor(0.6857)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4798, 0.4701, 0.4761, 0.4675, 0.4786, 0.4759, 0.4758, 0.4731, 0.4779,\n",
            "        0.4762, 0.4756, 0.4740, 0.4743, 0.4798, 0.4738, 0.4739, 0.4743, 0.4768,\n",
            "        0.4754, 0.4747, 0.4744, 0.4736, 0.4753, 0.4744, 0.4772, 0.4804, 0.4756,\n",
            "        0.4762, 0.4773, 0.4757, 0.4748, 0.4782, 0.4762, 0.4744, 0.4776, 0.4761,\n",
            "        0.4766, 0.4771, 0.4765, 0.4748, 0.4675, 0.4714, 0.4770, 0.4761, 0.4741,\n",
            "        0.4696, 0.4781, 0.4738, 0.4762, 0.4757, 0.4745, 0.4744, 0.4799, 0.4775,\n",
            "        0.4763, 0.4726, 0.4756, 0.4766, 0.4730, 0.4777, 0.4745, 0.4753, 0.4759,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.4260)\n",
            "Loss tensor(0.6928)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4775, 0.4709, 0.4791, 0.4744, 0.4677, 0.4772, 0.4729, 0.4777, 0.4716,\n",
            "        0.4715, 0.4772, 0.4767, 0.4729, 0.4736, 0.4771, 0.4753, 0.4733, 0.4780,\n",
            "        0.4814, 0.4779, 0.4807, 0.4745, 0.4774, 0.4750, 0.4774, 0.4731, 0.4775,\n",
            "        0.4747, 0.4712, 0.4782, 0.4776, 0.4715, 0.4779, 0.4711, 0.4741, 0.4760,\n",
            "        0.4729, 0.4805, 0.4762, 0.4762, 0.4740, 0.4731, 0.4777, 0.4818, 0.4763,\n",
            "        0.4717, 0.4775, 0.4716, 0.4756, 0.4760, 0.4758, 0.4731, 0.4710, 0.4768,\n",
            "        0.4798, 0.4778, 0.4783, 0.4723, 0.4779, 0.4791, 0.4792, 0.4770, 0.4765,\n",
            "        0.4744])\n",
            "Sum of predicted tensor(30.4421)\n",
            "Loss tensor(0.7027)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4789, 0.4786, 0.4704, 0.4777, 0.4818, 0.4723, 0.4701, 0.4787, 0.4765,\n",
            "        0.4789, 0.4729, 0.4775, 0.4690, 0.4765, 0.4753, 0.4750, 0.4771, 0.4746,\n",
            "        0.4753, 0.4751, 0.4771, 0.4776, 0.4748, 0.4745, 0.4749, 0.4776, 0.4797,\n",
            "        0.4765, 0.4713, 0.4731, 0.4776, 0.4744, 0.4741, 0.4726, 0.4738, 0.4777,\n",
            "        0.4738, 0.4713, 0.4793, 0.4726, 0.4799, 0.4744, 0.4788, 0.4747, 0.4701,\n",
            "        0.4754, 0.4733, 0.4801, 0.4767, 0.4756, 0.4768, 0.4738, 0.4744, 0.4762,\n",
            "        0.4760, 0.4775, 0.4738, 0.4750, 0.4780, 0.4756, 0.4760, 0.4720, 0.4724,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4292)\n",
            "Loss tensor(0.6826)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4786, 0.4730, 0.4755, 0.4766, 0.4775, 0.4751, 0.4735, 0.4765, 0.4742,\n",
            "        0.4791, 0.4729, 0.4757, 0.4745, 0.4698, 0.4762, 0.4763, 0.4748, 0.4772,\n",
            "        0.4751, 0.4777, 0.4738, 0.4817, 0.4708, 0.4795, 0.4756, 0.4785, 0.4786,\n",
            "        0.4762, 0.4698, 0.4802, 0.4777, 0.4782, 0.4718, 0.4813, 0.4741, 0.4738,\n",
            "        0.4731, 0.4766, 0.4736, 0.4768, 0.4705, 0.4723, 0.4753, 0.4757, 0.4784,\n",
            "        0.4744, 0.4768, 0.4761, 0.4726, 0.4780, 0.4729, 0.4762, 0.4775, 0.4745,\n",
            "        0.4765, 0.4791, 0.4769, 0.4764, 0.4772, 0.4756, 0.4762, 0.4771, 0.4806,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4521)\n",
            "Loss tensor(0.6874)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4765, 0.4711, 0.4701, 0.4729, 0.4751, 0.4762, 0.4784, 0.4757, 0.4801,\n",
            "        0.4763, 0.4770, 0.4676, 0.4757, 0.4710, 0.4763, 0.4743, 0.4802, 0.4783,\n",
            "        0.4740, 0.4738, 0.4735, 0.4817, 0.4793, 0.4780, 0.4739, 0.4739, 0.4728,\n",
            "        0.4712, 0.4774, 0.4767, 0.4736, 0.4754, 0.4750, 0.4735, 0.4769, 0.4740,\n",
            "        0.4759, 0.4730, 0.4763, 0.4763, 0.4675, 0.4793, 0.4774, 0.4766, 0.4737,\n",
            "        0.4752, 0.4773, 0.4771, 0.4742, 0.4751, 0.4760, 0.4753, 0.4777, 0.4738,\n",
            "        0.4725, 0.4728, 0.4723, 0.4726, 0.4785, 0.4769, 0.4717, 0.4753, 0.4754,\n",
            "        0.4794])\n",
            "Sum of predicted tensor(30.4121)\n",
            "Loss tensor(0.6963)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4777, 0.4757, 0.4788, 0.4744, 0.4754, 0.4799, 0.4748, 0.4762, 0.4742,\n",
            "        0.4751, 0.4777, 0.4795, 0.4753, 0.4752, 0.4721, 0.4786, 0.4712, 0.4788,\n",
            "        0.4731, 0.4702, 0.4748, 0.4702, 0.4759, 0.4778, 0.4722, 0.4786, 0.4723,\n",
            "        0.4771, 0.4770, 0.4753, 0.4751, 0.4733, 0.4741, 0.4786, 0.4762, 0.4763,\n",
            "        0.4780, 0.4824, 0.4741, 0.4710, 0.4759, 0.4736, 0.4745, 0.4795, 0.4726,\n",
            "        0.4760, 0.4745, 0.4712, 0.4777, 0.4713, 0.4818, 0.4738, 0.4808, 0.4730,\n",
            "        0.4775, 0.4752, 0.4756, 0.4791, 0.4767, 0.4760, 0.4775, 0.4759, 0.4790,\n",
            "        0.4703])\n",
            "Sum of predicted tensor(30.4432)\n",
            "Loss tensor(0.6979)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4740, 0.4773, 0.4744, 0.4751, 0.4745, 0.4753, 0.4709, 0.4779, 0.4775,\n",
            "        0.4713, 0.4746, 0.4751, 0.4740, 0.4769, 0.4746, 0.4782, 0.4747, 0.4773,\n",
            "        0.4786, 0.4740, 0.4775, 0.4747, 0.4776, 0.4774, 0.4803, 0.4758, 0.4712,\n",
            "        0.4783, 0.4779, 0.4715, 0.4760, 0.4738, 0.4788, 0.4762, 0.4778, 0.4762,\n",
            "        0.4732, 0.4765, 0.4725, 0.4767, 0.4794, 0.4805, 0.4773, 0.4786, 0.4749,\n",
            "        0.4758, 0.4753, 0.4798, 0.4748, 0.4737, 0.4766, 0.4818, 0.4756, 0.4804,\n",
            "        0.4739, 0.4713, 0.4762, 0.4714, 0.4768, 0.4738, 0.4744, 0.4775, 0.4775,\n",
            "        0.4765])\n",
            "Sum of predicted tensor(30.4600)\n",
            "Loss tensor(0.7004)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4762, 0.4744, 0.4773, 0.4714, 0.4775, 0.4788, 0.4772, 0.4729, 0.4744,\n",
            "        0.4768, 0.4775, 0.4712, 0.4710, 0.4776, 0.4750, 0.4753, 0.4833, 0.4746,\n",
            "        0.4744, 0.4725, 0.4786, 0.4739, 0.4750, 0.4759, 0.4768, 0.4751, 0.4668,\n",
            "        0.4760, 0.4773, 0.4761, 0.4788, 0.4746, 0.4732, 0.4710, 0.4821, 0.4806,\n",
            "        0.4758, 0.4763, 0.4717, 0.4737, 0.4766, 0.4798, 0.4784, 0.4752, 0.4746,\n",
            "        0.4766, 0.4744, 0.4736, 0.4716, 0.4778, 0.4780, 0.4715, 0.4738, 0.4784,\n",
            "        0.4749, 0.4789, 0.4709, 0.4775, 0.4775, 0.4753, 0.4763, 0.4772, 0.4763,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.4364)\n",
            "Loss tensor(0.7017)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4755, 0.4730, 0.4746, 0.4739, 0.4738, 0.4738, 0.4789, 0.4754, 0.4756,\n",
            "        0.4762, 0.4775, 0.4763, 0.4733, 0.4686, 0.4762, 0.4750, 0.4802, 0.4733,\n",
            "        0.4741, 0.4722, 0.4723, 0.4678, 0.4765, 0.4744, 0.4709, 0.4770, 0.4783,\n",
            "        0.4709, 0.4742, 0.4740, 0.4777, 0.4762, 0.4744, 0.4766, 0.4737, 0.4696,\n",
            "        0.4763, 0.4757, 0.4795, 0.4761, 0.4762, 0.4719, 0.4702, 0.4727, 0.4708,\n",
            "        0.4803, 0.4699, 0.4748, 0.4758, 0.4744, 0.4756, 0.4765, 0.4771, 0.4757,\n",
            "        0.4796, 0.4710, 0.4736, 0.4716, 0.4798, 0.4749, 0.4765, 0.4766, 0.4744,\n",
            "        0.4752])\n",
            "Sum of predicted tensor(30.3846)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4792, 0.4722, 0.4768, 0.4783, 0.4736, 0.4740, 0.4729, 0.4723, 0.4677,\n",
            "        0.4788, 0.4762, 0.4775, 0.4774, 0.4767, 0.4779, 0.4779, 0.4733, 0.4770,\n",
            "        0.4763, 0.4795, 0.4745, 0.4773, 0.4725, 0.4689, 0.4757, 0.4775, 0.4745,\n",
            "        0.4765, 0.4748, 0.4764, 0.4747, 0.4753, 0.4675, 0.4707, 0.4744, 0.4736,\n",
            "        0.4751, 0.4741, 0.4789, 0.4738, 0.4731, 0.4765, 0.4774, 0.4763, 0.4735,\n",
            "        0.4676, 0.4770, 0.4798, 0.4779, 0.4783, 0.4751, 0.4793, 0.4755, 0.4788,\n",
            "        0.4737, 0.4705, 0.4765, 0.4765, 0.4784, 0.4770, 0.4791, 0.4768, 0.4744,\n",
            "        0.4786])\n",
            "Sum of predicted tensor(30.4298)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4773, 0.4727, 0.4789, 0.4744, 0.4772, 0.4763, 0.4756, 0.4718, 0.4699,\n",
            "        0.4725, 0.4711, 0.4732, 0.4774, 0.4755, 0.4823, 0.4747, 0.4765, 0.4747,\n",
            "        0.4747, 0.4747, 0.4739, 0.4758, 0.4758, 0.4748, 0.4736, 0.4784, 0.4753,\n",
            "        0.4773, 0.4723, 0.4730, 0.4726, 0.4741, 0.4785, 0.4729, 0.4760, 0.4761,\n",
            "        0.4784, 0.4728, 0.4743, 0.4751, 0.4739, 0.4745, 0.4717, 0.4748, 0.4767,\n",
            "        0.4769, 0.4806, 0.4773, 0.4714, 0.4776, 0.4789, 0.4787, 0.4708, 0.4745,\n",
            "        0.4749, 0.4690, 0.4743, 0.4823, 0.4735, 0.4802, 0.4714, 0.4797, 0.4737,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4182)\n",
            "Loss tensor(0.6948)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4768, 0.4761, 0.4750, 0.4725, 0.4767, 0.4804, 0.4747, 0.4779, 0.4773,\n",
            "        0.4785, 0.4798, 0.4785, 0.4723, 0.4755, 0.4729, 0.4711, 0.4743, 0.4754,\n",
            "        0.4708, 0.4750, 0.4829, 0.4768, 0.4773, 0.4770, 0.4720, 0.4741, 0.4764,\n",
            "        0.4744, 0.4773, 0.4729, 0.4742, 0.4791, 0.4747, 0.4786, 0.4787, 0.4749,\n",
            "        0.4746, 0.4711, 0.4744, 0.4725, 0.4738, 0.4762, 0.4751, 0.4786, 0.4748,\n",
            "        0.4699, 0.4761, 0.4741, 0.4733, 0.4774, 0.4794, 0.4775, 0.4752, 0.4752,\n",
            "        0.4743, 0.4760, 0.4760, 0.4741, 0.4720, 0.4750, 0.4747, 0.4787, 0.4727,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4345)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4725, 0.4750, 0.4748, 0.4740, 0.4741, 0.4793, 0.4817, 0.4782, 0.4727,\n",
            "        0.4743, 0.4740, 0.4747, 0.4747, 0.4761, 0.4741, 0.4737, 0.4726, 0.4684,\n",
            "        0.4706, 0.4738, 0.4789, 0.4741, 0.4764, 0.4762, 0.4798, 0.4743, 0.4749,\n",
            "        0.4750, 0.4764, 0.4767, 0.4727, 0.4740, 0.4738, 0.4772, 0.4753, 0.4735,\n",
            "        0.4764, 0.4706, 0.4760, 0.4748, 0.4780, 0.4753, 0.4744, 0.4718, 0.4723,\n",
            "        0.4726, 0.4737, 0.4754, 0.4730, 0.4787, 0.4762, 0.4773, 0.4776, 0.4738,\n",
            "        0.4753, 0.4699, 0.4722, 0.4767, 0.4769, 0.4768, 0.4738, 0.4739, 0.4767,\n",
            "        0.4719])\n",
            "Sum of predicted tensor(30.3904)\n",
            "Loss tensor(0.6988)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4720, 0.4756, 0.4751, 0.4780, 0.4775, 0.4772, 0.4741, 0.4745, 0.4745,\n",
            "        0.4695, 0.4766, 0.4728, 0.4798, 0.4744, 0.4752, 0.4764, 0.4829, 0.4773,\n",
            "        0.4786, 0.4753, 0.4744, 0.4760, 0.4727, 0.4696, 0.4747, 0.4762, 0.4794,\n",
            "        0.4811, 0.4751, 0.4789, 0.4773, 0.4727, 0.4722, 0.4719, 0.4765, 0.4806,\n",
            "        0.4741, 0.4730, 0.4744, 0.4726, 0.4731, 0.4728, 0.4802, 0.4762, 0.4720,\n",
            "        0.4747, 0.4744, 0.4765, 0.4741, 0.4785, 0.4721, 0.4754, 0.4727, 0.4735,\n",
            "        0.4720, 0.4750, 0.4707, 0.4754, 0.4760, 0.4787, 0.4787, 0.4731, 0.4722,\n",
            "        0.4746])\n",
            "Sum of predicted tensor(30.4134)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4737, 0.4758, 0.4777, 0.4759, 0.4761, 0.4770, 0.4761, 0.4744, 0.4751,\n",
            "        0.4755, 0.4773, 0.4770, 0.4822, 0.4750, 0.4753, 0.4712, 0.4738, 0.4757,\n",
            "        0.4811, 0.4712, 0.4753, 0.4744, 0.4764, 0.4756, 0.4794, 0.4760, 0.4764,\n",
            "        0.4751, 0.4746, 0.4775, 0.4753, 0.4723, 0.4734, 0.4796, 0.4775, 0.4747,\n",
            "        0.4749, 0.4723, 0.4729, 0.4693, 0.4787, 0.4741, 0.4723, 0.4768, 0.4776,\n",
            "        0.4714, 0.4745, 0.4776, 0.4732, 0.4727, 0.4747, 0.4829, 0.4765, 0.4773,\n",
            "        0.4763, 0.4768, 0.4811, 0.4772, 0.4767, 0.4773, 0.4772, 0.4694, 0.4773,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4472)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4771, 0.4750, 0.4754, 0.4772, 0.4764, 0.4720, 0.4749, 0.4773,\n",
            "        0.4713, 0.4761, 0.4758, 0.4703, 0.4765, 0.4773, 0.4745, 0.4784, 0.4787,\n",
            "        0.4739, 0.4708, 0.4754, 0.4731, 0.4780, 0.4785, 0.4708, 0.4693, 0.4746,\n",
            "        0.4745, 0.4763, 0.4761, 0.4693, 0.4762, 0.4750, 0.4742, 0.4794, 0.4720,\n",
            "        0.4750, 0.4772, 0.4794, 0.4768, 0.4731, 0.4789, 0.4755, 0.4759, 0.4708,\n",
            "        0.4695, 0.4774, 0.4747, 0.4746, 0.4738, 0.4787, 0.4773, 0.4752, 0.4762,\n",
            "        0.4773, 0.4748, 0.4787, 0.4773, 0.4724, 0.4750, 0.4771, 0.4783, 0.4691,\n",
            "        0.4777])\n",
            "Sum of predicted tensor(30.4190)\n",
            "Loss tensor(0.7088)\n",
            "Acc tensor(0.3438)\n",
            "tensor([0.4773, 0.4768, 0.4726, 0.4713, 0.4762, 0.4734, 0.4722, 0.4750, 0.4722,\n",
            "        0.4746, 0.4744, 0.4726, 0.4783, 0.4745, 0.4763, 0.4748, 0.4747, 0.4773,\n",
            "        0.4802, 0.4748, 0.4775, 0.4721, 0.4760, 0.4747, 0.4792, 0.4731, 0.4760,\n",
            "        0.4769, 0.4710, 0.4773, 0.4787, 0.4740, 0.4831, 0.4728, 0.4746, 0.4747,\n",
            "        0.4772, 0.4741, 0.4733, 0.4748, 0.4787, 0.4752, 0.4774, 0.4727, 0.4749,\n",
            "        0.4730, 0.4785, 0.4741, 0.4702, 0.4736, 0.4750, 0.4729, 0.4755, 0.4722,\n",
            "        0.4771, 0.4737, 0.4733, 0.4780, 0.4724, 0.4735, 0.4731, 0.4724, 0.4741,\n",
            "        0.4759])\n",
            "Sum of predicted tensor(30.3977)\n",
            "Loss tensor(0.6962)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4751, 0.4762, 0.4739, 0.4755, 0.4749, 0.4760, 0.4762, 0.4733, 0.4749,\n",
            "        0.4723, 0.4754, 0.4733, 0.4769, 0.4717, 0.4742, 0.4720, 0.4794, 0.4787,\n",
            "        0.4789, 0.4768, 0.4790, 0.4775, 0.4775, 0.4726, 0.4749, 0.4713, 0.4736,\n",
            "        0.4755, 0.4789, 0.4768, 0.4718, 0.4747, 0.4811, 0.4729, 0.4782, 0.4711,\n",
            "        0.4783, 0.4744, 0.4739, 0.4720, 0.4699, 0.4729, 0.4757, 0.4755, 0.4731,\n",
            "        0.4786, 0.4727, 0.4727, 0.4754, 0.4712, 0.4739, 0.4746, 0.4796, 0.4768,\n",
            "        0.4730, 0.4770, 0.4765, 0.4716, 0.4773, 0.4723, 0.4708, 0.4774, 0.4789,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4032)\n",
            "Loss tensor(0.7034)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4746, 0.4743, 0.4718, 0.4751, 0.4714, 0.4805, 0.4763, 0.4811, 0.4728,\n",
            "        0.4730, 0.4752, 0.4797, 0.4687, 0.4764, 0.4742, 0.4812, 0.4752, 0.4755,\n",
            "        0.4747, 0.4741, 0.4771, 0.4699, 0.4802, 0.4763, 0.4789, 0.4747, 0.4744,\n",
            "        0.4720, 0.4811, 0.4740, 0.4740, 0.4760, 0.4777, 0.4749, 0.4757, 0.4752,\n",
            "        0.4789, 0.4755, 0.4760, 0.4700, 0.4738, 0.4695, 0.4761, 0.4750, 0.4791,\n",
            "        0.4750, 0.4744, 0.4802, 0.4806, 0.4751, 0.4705, 0.4757, 0.4741, 0.4757,\n",
            "        0.4741, 0.4775, 0.4794, 0.4699, 0.4711, 0.4768, 0.4743, 0.4728, 0.4737,\n",
            "        0.4735])\n",
            "Sum of predicted tensor(30.4164)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4766, 0.4687, 0.4719, 0.4776, 0.4713, 0.4759, 0.4756, 0.4747,\n",
            "        0.4751, 0.4765, 0.4765, 0.4727, 0.4735, 0.4756, 0.4764, 0.4744, 0.4741,\n",
            "        0.4775, 0.4744, 0.4739, 0.4734, 0.4746, 0.4765, 0.4696, 0.4755, 0.4738,\n",
            "        0.4729, 0.4729, 0.4794, 0.4762, 0.4755, 0.4763, 0.4713, 0.4788, 0.4794,\n",
            "        0.4754, 0.4747, 0.4726, 0.4736, 0.4713, 0.4728, 0.4766, 0.4764, 0.4713,\n",
            "        0.4746, 0.4738, 0.4742, 0.4719, 0.4728, 0.4761, 0.4714, 0.4811, 0.4763,\n",
            "        0.4777, 0.4788, 0.4764, 0.4781, 0.4754, 0.4709, 0.4827, 0.4779, 0.4811,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4090)\n",
            "Loss tensor(0.7067)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4731, 0.4742, 0.4809, 0.4748, 0.4832, 0.4769, 0.4790, 0.4783, 0.4768,\n",
            "        0.4681, 0.4775, 0.4784, 0.4762, 0.4701, 0.4720, 0.4730, 0.4727, 0.4707,\n",
            "        0.4772, 0.4753, 0.4773, 0.4795, 0.4761, 0.4770, 0.4748, 0.4754, 0.4771,\n",
            "        0.4773, 0.4765, 0.4786, 0.4811, 0.4768, 0.4752, 0.4781, 0.4766, 0.4783,\n",
            "        0.4760, 0.4729, 0.4827, 0.4733, 0.4701, 0.4801, 0.4751, 0.4741, 0.4739,\n",
            "        0.4754, 0.4769, 0.4754, 0.4827, 0.4799, 0.4799, 0.4707, 0.4752, 0.4710,\n",
            "        0.4770, 0.4764, 0.4758, 0.4760, 0.4712, 0.4768, 0.4761, 0.4727, 0.4750,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4627)\n",
            "Loss tensor(0.6931)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4770, 0.4743, 0.4776, 0.4740, 0.4705, 0.4765, 0.4768, 0.4760, 0.4747,\n",
            "        0.4764, 0.4692, 0.4792, 0.4764, 0.4773, 0.4713, 0.4744, 0.4727, 0.4761,\n",
            "        0.4804, 0.4786, 0.4782, 0.4754, 0.4769, 0.4785, 0.4726, 0.4762, 0.4747,\n",
            "        0.4748, 0.4719, 0.4719, 0.4788, 0.4811, 0.4784, 0.4804, 0.4713, 0.4755,\n",
            "        0.4723, 0.4804, 0.4830, 0.4742, 0.4736, 0.4750, 0.4736, 0.4713, 0.4773,\n",
            "        0.4744, 0.4737, 0.4757, 0.4721, 0.4746, 0.4773, 0.4804, 0.4762, 0.4708,\n",
            "        0.4716, 0.4748, 0.4804, 0.4733, 0.4748, 0.4783, 0.4747, 0.4729, 0.4783,\n",
            "        0.4712])\n",
            "Sum of predicted tensor(30.4326)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4760, 0.4726, 0.4761, 0.4753, 0.4766, 0.4791, 0.4739, 0.4768, 0.4712,\n",
            "        0.4687, 0.4729, 0.4754, 0.4773, 0.4761, 0.4804, 0.4787, 0.4830, 0.4750,\n",
            "        0.4746, 0.4711, 0.4788, 0.4711, 0.4722, 0.4727, 0.4773, 0.4726, 0.4738,\n",
            "        0.4741, 0.4726, 0.4756, 0.4735, 0.4712, 0.4747, 0.4811, 0.4781, 0.4799,\n",
            "        0.4731, 0.4761, 0.4697, 0.4729, 0.4719, 0.4762, 0.4763, 0.4761, 0.4738,\n",
            "        0.4721, 0.4709, 0.4744, 0.4754, 0.4758, 0.4712, 0.4730, 0.4773, 0.4763,\n",
            "        0.4783, 0.4790, 0.4764, 0.4687, 0.4737, 0.4746, 0.4712, 0.4747, 0.4764,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.3926)\n",
            "Loss tensor(0.6875)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4731, 0.4777, 0.4750, 0.4745, 0.4746, 0.4724, 0.4764, 0.4745, 0.4701,\n",
            "        0.4786, 0.4758, 0.4778, 0.4786, 0.4736, 0.4718, 0.4734, 0.4783, 0.4755,\n",
            "        0.4805, 0.4732, 0.4765, 0.4804, 0.4827, 0.4726, 0.4751, 0.4729, 0.4757,\n",
            "        0.4713, 0.4714, 0.4767, 0.4727, 0.4734, 0.4752, 0.4768, 0.4785, 0.4807,\n",
            "        0.4738, 0.4811, 0.4752, 0.4727, 0.4715, 0.4740, 0.4694, 0.4729, 0.4772,\n",
            "        0.4714, 0.4745, 0.4781, 0.4762, 0.4770, 0.4705, 0.4764, 0.4779, 0.4770,\n",
            "        0.4811, 0.4712, 0.4714, 0.4705, 0.4756, 0.4715, 0.4763, 0.4708, 0.4752,\n",
            "        0.4755])\n",
            "Sum of predicted tensor(30.4041)\n",
            "Loss tensor(0.6964)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4792, 0.4724, 0.4748, 0.4724, 0.4759, 0.4767, 0.4725, 0.4757, 0.4760,\n",
            "        0.4787, 0.4744, 0.4783, 0.4710, 0.4735, 0.4778, 0.4757, 0.4801, 0.4693,\n",
            "        0.4713, 0.4771, 0.4775, 0.4705, 0.4769, 0.4738, 0.4681, 0.4731, 0.4751,\n",
            "        0.4783, 0.4747, 0.4693, 0.4766, 0.4735, 0.4734, 0.4731, 0.4721, 0.4745,\n",
            "        0.4745, 0.4752, 0.4743, 0.4733, 0.4744, 0.4750, 0.4778, 0.4731, 0.4746,\n",
            "        0.4740, 0.4742, 0.4801, 0.4712, 0.4751, 0.4777, 0.4742, 0.4724, 0.4724,\n",
            "        0.4736, 0.4744, 0.4801, 0.4744, 0.4760, 0.4754, 0.4784, 0.4730, 0.4730,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.3800)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4717, 0.4777, 0.4730, 0.4776, 0.4746, 0.4734, 0.4766, 0.4721, 0.4705,\n",
            "        0.4721, 0.4785, 0.4733, 0.4781, 0.4805, 0.4752, 0.4715, 0.4782, 0.4775,\n",
            "        0.4786, 0.4777, 0.4771, 0.4792, 0.4761, 0.4715, 0.4757, 0.4734, 0.4775,\n",
            "        0.4698, 0.4771, 0.4786, 0.4717, 0.4779, 0.4746, 0.4765, 0.4746, 0.4756,\n",
            "        0.4681, 0.4796, 0.4701, 0.4693, 0.4777, 0.4711, 0.4788, 0.4766, 0.4752,\n",
            "        0.4778, 0.4746, 0.4725, 0.4777, 0.4771, 0.4758, 0.4755, 0.4754, 0.4745,\n",
            "        0.4793, 0.4766, 0.4757, 0.4762, 0.4750, 0.4710, 0.4736, 0.4782, 0.4767,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4194)\n",
            "Loss tensor(0.6820)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4747, 0.4765, 0.4709, 0.4811, 0.4758, 0.4717, 0.4760, 0.4699, 0.4811,\n",
            "        0.4743, 0.4765, 0.4748, 0.4720, 0.4731, 0.4766, 0.4737, 0.4758, 0.4785,\n",
            "        0.4801, 0.4765, 0.4775, 0.4767, 0.4724, 0.4725, 0.4698, 0.4705, 0.4761,\n",
            "        0.4759, 0.4701, 0.4733, 0.4745, 0.4744, 0.4805, 0.4744, 0.4726, 0.4746,\n",
            "        0.4751, 0.4756, 0.4724, 0.4752, 0.4750, 0.4750, 0.4731, 0.4721, 0.4731,\n",
            "        0.4730, 0.4775, 0.4726, 0.4714, 0.4775, 0.4775, 0.4736, 0.4726, 0.4744,\n",
            "        0.4775, 0.4777, 0.4752, 0.4732, 0.4778, 0.4686, 0.4726, 0.4744, 0.4771,\n",
            "        0.4750])\n",
            "Sum of predicted tensor(30.3809)\n",
            "Loss tensor(0.6969)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4759, 0.4722, 0.4719, 0.4748, 0.4801, 0.4745, 0.4729, 0.4807, 0.4771,\n",
            "        0.4767, 0.4765, 0.4747, 0.4722, 0.4739, 0.4736, 0.4752, 0.4757, 0.4730,\n",
            "        0.4738, 0.4788, 0.4760, 0.4759, 0.4719, 0.4768, 0.4717, 0.4660, 0.4738,\n",
            "        0.4743, 0.4766, 0.4744, 0.4789, 0.4771, 0.4745, 0.4767, 0.4777, 0.4753,\n",
            "        0.4744, 0.4743, 0.4708, 0.4744, 0.4758, 0.4720, 0.4709, 0.4757, 0.4742,\n",
            "        0.4767, 0.4753, 0.4765, 0.4728, 0.4776, 0.4731, 0.4718, 0.4752, 0.4811,\n",
            "        0.4744, 0.4717, 0.4758, 0.4775, 0.4751, 0.4751, 0.4738, 0.4708, 0.4767,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.3919)\n",
            "Loss tensor(0.6834)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0.4811, 0.4748, 0.4722, 0.4789, 0.4729, 0.4709, 0.4752, 0.4732, 0.4751,\n",
            "        0.4707, 0.4750, 0.4812, 0.4773, 0.4717, 0.4761, 0.4812, 0.4790, 0.4777,\n",
            "        0.4751, 0.4759, 0.4823, 0.4709, 0.4763, 0.4792, 0.4811, 0.4761, 0.4726,\n",
            "        0.4733, 0.4801, 0.4769, 0.4756, 0.4761, 0.4755, 0.4733, 0.4762, 0.4758,\n",
            "        0.4734, 0.4660, 0.4764, 0.4828, 0.4777, 0.4734, 0.4750, 0.4703, 0.4768,\n",
            "        0.4762, 0.4734, 0.4744, 0.4660, 0.4728, 0.4717, 0.4660, 0.4757, 0.4762,\n",
            "        0.4748, 0.4709, 0.4728, 0.4768, 0.4741, 0.4828, 0.4762, 0.4733, 0.4764,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.4146)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4736, 0.4730, 0.4677, 0.4731, 0.4753, 0.4722, 0.4760, 0.4772, 0.4765,\n",
            "        0.4766, 0.4726, 0.4721, 0.4744, 0.4731, 0.4761, 0.4771, 0.4718, 0.4768,\n",
            "        0.4734, 0.4731, 0.4726, 0.4731, 0.4752, 0.4726, 0.4766, 0.4722, 0.4736,\n",
            "        0.4758, 0.4730, 0.4731, 0.4722, 0.4815, 0.4734, 0.4768, 0.4724, 0.4737,\n",
            "        0.4780, 0.4765, 0.4734, 0.4779, 0.4765, 0.4761, 0.4733, 0.4815, 0.4709,\n",
            "        0.4756, 0.4788, 0.4752, 0.4717, 0.4710, 0.4767, 0.4790, 0.4734, 0.4823,\n",
            "        0.4746, 0.4799, 0.4769, 0.4717, 0.4817, 0.4811, 0.4733, 0.4739, 0.4812,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4056)\n",
            "Loss tensor(0.6790)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0.4789, 0.4763, 0.4732, 0.4747, 0.4762, 0.4755, 0.4750, 0.4745, 0.4760,\n",
            "        0.4758, 0.4803])\n",
            "Sum of predicted tensor(5.2364)\n",
            "Loss tensor(0.6734)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.88%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.83%\n",
            "Sum of predicted tensor(30.4383, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4441, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4050, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4548, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3822, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3813, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4176, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3714, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3845, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4147, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4239, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4217, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4168, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4283, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4023, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4537, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4318, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4505, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4324, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3664, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3822, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4126, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4432, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4219, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3822, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4428, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4615, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3814, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4144, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4100, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3871, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4431, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3825, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4471, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4040, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4399, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3706, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4157, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3888, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4100, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4029, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4013, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4307, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3304, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4603, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4018, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3913, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4289, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4225, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4151, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3978, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3921, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4125, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4161, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4303, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4207, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3890, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3075, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4626, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4458, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3843, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4071, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4123, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4419, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4407, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4223, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4814, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4295, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4520, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4534, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4082, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4399, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4315, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3953, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3993, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3859, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4584, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4757, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3341, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3654, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4097, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4191, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3223, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3972, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4314, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4356, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4032, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4421, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4384, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3648, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3717, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4351, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4352, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4106, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4458, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4014, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4014, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4211, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4140, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4351, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3875, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4458, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3966, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4291, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4113, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4120, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3947, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3860, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4521, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3934, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4250, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4377, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4556, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3501, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4460, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4025, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4429, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4263, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4393, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3871, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4178, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4470, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4089, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3609, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4344, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4448, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4103, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4384, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4289, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4435, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3830, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3939, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4311, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3199, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4276, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4097, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4102, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4148, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4021, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3016, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4147, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4493, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4634, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4580, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4435, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4365, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4273, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3979, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3942, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3699, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4061, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4457, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4168, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3974, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4505, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4498, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3939, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3746, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4297, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4256, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4305, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4568, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3179, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4164, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4410, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4237, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3852, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4316, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4087, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4125, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4254, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3464, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3688, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3427, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3763, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4370, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3913, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4246, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4505, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3713, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4352, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4268, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3729, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3890, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4178, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3702, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3884, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4131, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4448, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4025, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3894, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3928, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4213, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4070, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3871, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4287, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3810, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3648, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3866, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3534, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4353, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3437, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4170, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4578, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4186, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4088, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4557, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3999, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3958, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4465, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3471, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3859, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4188, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4237, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3550, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4391, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4458, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4453, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4443, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4078, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4476, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4592, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4288, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4055, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4241, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4409, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4556, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4209, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4210, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3645, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4075, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4067, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4585, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3815, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4222, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3753, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4042, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4897, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4090, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3870, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4394, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3842, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4260, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3819, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4151, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4105, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4043, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3896, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3999, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3771, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4357, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4207, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4274, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3347, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4526, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4375, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4635, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4278, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4514, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4518, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4315, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4428, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4072, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4562, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4003, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4321, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3840, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4124, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4037, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4055, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3846, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4177, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4256, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4304, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4079, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4423, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4370, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4106, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4381, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4315, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4003, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3853, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4225, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4411, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3867, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3837, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4218, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3818, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4212, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4514, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3997, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4302, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3217, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4518, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3639, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3823, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4438, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4030, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4012, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4281, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3476, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3632, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4156, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4040, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4347, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3974, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4427, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4100, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4298, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4105, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4313, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4176, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4088, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4170, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4403, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3380, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3675, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4182, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3894, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4362, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4030, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3945, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3844, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4205, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4468, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3694, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4207, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4227, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4263, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4063, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4129, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3562, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4280, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3732, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4715, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3715, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4078, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3844, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3411, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4442, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4136, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3873, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3803, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3988, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4381, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3635, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4391, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4429, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4000, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4368, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4313, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3851, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3904, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4388, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4461, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3896, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4009, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4213, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4340, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4094, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4138, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4359, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4180, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4245, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3427, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3661, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4211, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3564, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4380, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4023, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3751, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3513, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3995, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4064, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4213, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3701, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4131, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3771, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3933, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4457, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4193, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3909, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4452, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3719, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4537, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3990, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4062, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4120, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3995, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4112, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4056, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4324, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4264, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3788, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4126, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4061, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4225, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4647, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4640, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4758, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4500, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4409, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4466, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3165, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4452, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3949, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4484, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3381, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4189, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4314, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3907, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4278, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4149, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4330, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4185, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4299, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4152, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4507, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4096, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4199, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4106, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4153, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4451, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3841, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4366, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4668, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4401, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4312, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4495, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3933, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4492, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4034, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3964, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4329, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4390, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4358, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4102, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4596, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3898, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4362, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4374, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4273, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4006, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4324, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4461, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4166, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3617, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4441, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4279, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4203, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4524, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3954, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4251, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4171, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3777, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4197, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4267, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4310, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3442, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4560, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4322, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4741, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3528, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4448, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3516, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4239, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4609, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4200, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4426, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4076, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3507, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3495, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4161, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4377, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4027, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3931, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4865, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4193, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4690, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4116, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4296, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4365, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3843, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3764, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3193, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3789, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4263, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4115, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3620, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4270, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3697, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4151, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4238, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4318, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3307, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3976, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4177, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4034, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4303, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3972, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4009, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4521, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4023, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4389, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3526, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3369, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4048, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(11.4037, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3951, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4588, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3604, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3371, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4376, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4777, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3986, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4091, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3818, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4160, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4439, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4160, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4223, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4557, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3908, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3890, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3853, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4271, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4284, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3771, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4525, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4360, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4292, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4212, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3172, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3800, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4639, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4151, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3718, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3912, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4121, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3499, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4036, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4104, grad_fn=<SumBackward0>)\n",
            "tensor([0.4724, 0.4748, 0.4738, 0.4735, 0.4737, 0.4741, 0.4717, 0.4744, 0.4728,\n",
            "        0.4768, 0.4749, 0.4741, 0.4721, 0.4746, 0.4741, 0.4739, 0.4741, 0.4744,\n",
            "        0.4743, 0.4741, 0.4714, 0.4746, 0.4739, 0.4714, 0.4762, 0.4746, 0.4741,\n",
            "        0.4707, 0.4707, 0.4721, 0.4742, 0.4733, 0.4744, 0.4721, 0.4741, 0.4727,\n",
            "        0.4741, 0.4741, 0.4714, 0.4764, 0.4744, 0.4725, 0.4699, 0.4751, 0.4728,\n",
            "        0.4718, 0.4756, 0.4755, 0.4754, 0.4735, 0.4713, 0.4721, 0.4750, 0.4741,\n",
            "        0.4722, 0.4737, 0.4762, 0.4728, 0.4741, 0.4741, 0.4718, 0.4744, 0.4741,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.3146)\n",
            "Loss tensor(0.6672)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0.4764, 0.4737, 0.4744, 0.4743, 0.4758, 0.4723, 0.4777, 0.4754, 0.4763,\n",
            "        0.4743, 0.4775, 0.4762, 0.4719, 0.4705, 0.4729, 0.4705, 0.4720, 0.4771,\n",
            "        0.4697, 0.4750, 0.4727, 0.4708, 0.4725, 0.4742, 0.4755, 0.4729, 0.4790,\n",
            "        0.4740, 0.4703, 0.4715, 0.4718, 0.4716, 0.4742, 0.4779, 0.4721, 0.4759,\n",
            "        0.4709, 0.4722, 0.4752, 0.4725, 0.4744, 0.4704, 0.4773, 0.4744, 0.4757,\n",
            "        0.4740, 0.4744, 0.4715, 0.4715, 0.4755, 0.4722, 0.4736, 0.4771, 0.4731,\n",
            "        0.4755, 0.4722, 0.4745, 0.4738, 0.4729, 0.4744, 0.4719, 0.4729, 0.4787,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.3289)\n",
            "Loss tensor(0.6699)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0.4771, 0.4773, 0.4717, 0.4724, 0.4775, 0.4745, 0.4734, 0.4702, 0.4729,\n",
            "        0.4741, 0.4755, 0.4775, 0.4718, 0.4723, 0.4760, 0.4740, 0.4730, 0.4725,\n",
            "        0.4740, 0.4723, 0.4716, 0.4707, 0.4739, 0.4671, 0.4735, 0.4803, 0.4760,\n",
            "        0.4744, 0.4740, 0.4716, 0.4752, 0.4739, 0.4735, 0.4734, 0.4745, 0.4744,\n",
            "        0.4796, 0.4727, 0.4724, 0.4730, 0.4758, 0.4732, 0.4776, 0.4744, 0.4718,\n",
            "        0.4741, 0.4748, 0.4708, 0.4745, 0.4744, 0.4720, 0.4745, 0.4735, 0.4713,\n",
            "        0.4722, 0.4714, 0.4740, 0.4752, 0.4754, 0.4740, 0.4743, 0.4722, 0.4742,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.3231)\n",
            "Loss tensor(0.6818)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4755, 0.4693, 0.4764, 0.4718, 0.4768, 0.4763, 0.4789, 0.4773, 0.4745,\n",
            "        0.4740, 0.4717, 0.4754, 0.4716, 0.4741, 0.4744, 0.4734, 0.4740, 0.4788,\n",
            "        0.4724, 0.4723, 0.4710, 0.4747, 0.4739, 0.4744, 0.4733, 0.4718, 0.4739,\n",
            "        0.4748, 0.4755, 0.4719, 0.4714, 0.4744, 0.4693, 0.4730, 0.4740, 0.4730,\n",
            "        0.4686, 0.4758, 0.4702, 0.4708, 0.4715, 0.4748, 0.4726, 0.4733, 0.4800,\n",
            "        0.4777, 0.4742, 0.4708, 0.4739, 0.4719, 0.4758, 0.4718, 0.4742, 0.4722,\n",
            "        0.4724, 0.4780, 0.4737, 0.4741, 0.4750, 0.4765, 0.4771, 0.4735, 0.4708,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.3246)\n",
            "Loss tensor(0.6674)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0.4760, 0.4709, 0.4728, 0.4743, 0.4736, 0.4712, 0.4726, 0.4748, 0.4717,\n",
            "        0.4753, 0.4751, 0.4728, 0.4745, 0.4752, 0.4733, 0.4765, 0.4721, 0.4704,\n",
            "        0.4738, 0.4768, 0.4716, 0.4738, 0.4730, 0.4772, 0.4705, 0.4746, 0.4721,\n",
            "        0.4740, 0.4746, 0.4819, 0.4751, 0.4763, 0.4782, 0.4716, 0.4720, 0.4772,\n",
            "        0.4742, 0.4738, 0.4724, 0.4739, 0.4693, 0.4757, 0.4747, 0.4739, 0.4746,\n",
            "        0.4750, 0.4705, 0.4762, 0.4742, 0.4746, 0.4766, 0.4729, 0.4733, 0.4722,\n",
            "        0.4742, 0.4757, 0.4740, 0.4734, 0.4671, 0.4725, 0.4737, 0.4744, 0.4738,\n",
            "        0.4708])\n",
            "Sum of predicted tensor(30.3249)\n",
            "Loss tensor(0.6743)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4761, 0.4754, 0.4732, 0.4715, 0.4756, 0.4772, 0.4742, 0.4722, 0.4727,\n",
            "        0.4800, 0.4760, 0.4700, 0.4730, 0.4743, 0.4749, 0.4735, 0.4738, 0.4746,\n",
            "        0.4753, 0.4771, 0.4745, 0.4743, 0.4800, 0.4735, 0.4819, 0.4705, 0.4748,\n",
            "        0.4735, 0.4761, 0.4748, 0.4777, 0.4743, 0.4742, 0.4735, 0.4778, 0.4746,\n",
            "        0.4762, 0.4735, 0.4769, 0.4747, 0.4715, 0.4754, 0.4735, 0.4764, 0.4739,\n",
            "        0.4752, 0.4768, 0.4707, 0.4759, 0.4717, 0.4745, 0.4750, 0.4745, 0.4735,\n",
            "        0.4772, 0.4731, 0.4735, 0.4737, 0.4750, 0.4724, 0.4745, 0.4697, 0.4758,\n",
            "        0.4730])\n",
            "Sum of predicted tensor(30.3745)\n",
            "Loss tensor(0.6824)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0.4711, 0.4731, 0.4760, 0.4771, 0.4713, 0.4717, 0.4732, 0.4742, 0.4705,\n",
            "        0.4716, 0.4762, 0.4778, 0.4743, 0.4738, 0.4746, 0.4746, 0.4763, 0.4807,\n",
            "        0.4777, 0.4778, 0.4753, 0.4703, 0.4700, 0.4745, 0.4763, 0.4772, 0.4705,\n",
            "        0.4738, 0.4695, 0.4817, 0.4761, 0.4750, 0.4726, 0.4772, 0.4718, 0.4767,\n",
            "        0.4698, 0.4738, 0.4763, 0.4760, 0.4730, 0.4753, 0.4746, 0.4711, 0.4732,\n",
            "        0.4768, 0.4740, 0.4738, 0.4738, 0.4735, 0.4705, 0.4694, 0.4718, 0.4757,\n",
            "        0.4717, 0.4755, 0.4715, 0.4758, 0.4752, 0.4727, 0.4760, 0.4748, 0.4753,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3485)\n",
            "Loss tensor(0.6912)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4713, 0.4738, 0.4742, 0.4748, 0.4772, 0.4761, 0.4745, 0.4800, 0.4750,\n",
            "        0.4775, 0.4807, 0.4705, 0.4724, 0.4745, 0.4754, 0.4740, 0.4724, 0.4754,\n",
            "        0.4730, 0.4766, 0.4735, 0.4733, 0.4736, 0.4711, 0.4761, 0.4742, 0.4763,\n",
            "        0.4763, 0.4745, 0.4751, 0.4744, 0.4739, 0.4740, 0.4721, 0.4738, 0.4768,\n",
            "        0.4697, 0.4760, 0.4760, 0.4775, 0.4758, 0.4753, 0.4771, 0.4748, 0.4713,\n",
            "        0.4715, 0.4745, 0.4748, 0.4747, 0.4760, 0.4761, 0.4703, 0.4718, 0.4731,\n",
            "        0.4696, 0.4728, 0.4724, 0.4818, 0.4713, 0.4748, 0.4772, 0.4774, 0.4712,\n",
            "        0.4744])\n",
            "Sum of predicted tensor(30.3678)\n",
            "Loss tensor(0.6870)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4724, 0.4780, 0.4726, 0.4729, 0.4807, 0.4752, 0.4713, 0.4777, 0.4749,\n",
            "        0.4766, 0.4718, 0.4710, 0.4755, 0.4753, 0.4750, 0.4706, 0.4735, 0.4728,\n",
            "        0.4711, 0.4700, 0.4743, 0.4767, 0.4715, 0.4752, 0.4738, 0.4761, 0.4769,\n",
            "        0.4746, 0.4735, 0.4766, 0.4735, 0.4711, 0.4748, 0.4754, 0.4742, 0.4746,\n",
            "        0.4743, 0.4743, 0.4743, 0.4713, 0.4758, 0.4747, 0.4777, 0.4712, 0.4727,\n",
            "        0.4781, 0.4748, 0.4734, 0.4711, 0.4748, 0.4735, 0.4772, 0.4777, 0.4788,\n",
            "        0.4800, 0.4772, 0.4722, 0.4748, 0.4718, 0.4715, 0.4674, 0.4748, 0.4717,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.3562)\n",
            "Loss tensor(0.6907)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4750, 0.4724, 0.4733, 0.4722, 0.4735, 0.4735, 0.4738, 0.4750, 0.4762,\n",
            "        0.4748, 0.4737, 0.4743, 0.4772, 0.4757, 0.4735, 0.4778, 0.4778, 0.4760,\n",
            "        0.4751, 0.4736, 0.4777, 0.4745, 0.4739, 0.4776, 0.4746, 0.4729, 0.4736,\n",
            "        0.4756, 0.4711, 0.4748, 0.4812, 0.4724, 0.4727, 0.4763, 0.4740, 0.4752,\n",
            "        0.4752, 0.4734, 0.4745, 0.4749, 0.4729, 0.4750, 0.4706, 0.4761, 0.4724,\n",
            "        0.4738, 0.4760, 0.4739, 0.4683, 0.4783, 0.4743, 0.4735, 0.4800, 0.4766,\n",
            "        0.4777, 0.4754, 0.4743, 0.4745, 0.4715, 0.4756, 0.4711, 0.4721, 0.4724,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.3682)\n",
            "Loss tensor(0.6792)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0.4735, 0.4713, 0.4740, 0.4731, 0.4752, 0.4702, 0.4740, 0.4832, 0.4749,\n",
            "        0.4832, 0.4753, 0.4772, 0.4724, 0.4749, 0.4740, 0.4757, 0.4748, 0.4718,\n",
            "        0.4745, 0.4755, 0.4774, 0.4700, 0.4735, 0.4748, 0.4746, 0.4759, 0.4731,\n",
            "        0.4752, 0.4728, 0.4738, 0.4740, 0.4700, 0.4736, 0.4736, 0.4712, 0.4708,\n",
            "        0.4686, 0.4769, 0.4686, 0.4743, 0.4738, 0.4753, 0.4723, 0.4747, 0.4722,\n",
            "        0.4713, 0.4775, 0.4728, 0.4686, 0.4749, 0.4739, 0.4709, 0.4718, 0.4731,\n",
            "        0.4748, 0.4717, 0.4736, 0.4766, 0.4760, 0.4733, 0.4761, 0.4737, 0.4750,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.3312)\n",
            "Loss tensor(0.6719)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0.4745, 0.4756, 0.4688, 0.4746, 0.4716, 0.4723, 0.4769, 0.4702, 0.4752,\n",
            "        0.4693, 0.4693, 0.4702, 0.4722, 0.4711, 0.4740, 0.4747, 0.4793, 0.4732,\n",
            "        0.4760, 0.4728, 0.4740, 0.4702, 0.4705, 0.4755, 0.4771, 0.4727, 0.4800,\n",
            "        0.4738, 0.4797, 0.4741, 0.4736, 0.4727, 0.4700, 0.4755, 0.4743, 0.4738,\n",
            "        0.4750, 0.4751, 0.4790, 0.4718, 0.4755, 0.4787, 0.4758, 0.4771, 0.4787,\n",
            "        0.4742, 0.4745, 0.4728, 0.4781, 0.4750, 0.4802, 0.4729, 0.4749, 0.4743,\n",
            "        0.4766, 0.4730, 0.4747, 0.4720, 0.4781, 0.4768, 0.4747, 0.4701, 0.4752,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.3589)\n",
            "Loss tensor(0.6866)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4718, 0.4726, 0.4755, 0.4747, 0.4704, 0.4744, 0.4722, 0.4702, 0.4747,\n",
            "        0.4718, 0.4730, 0.4735, 0.4734, 0.4713, 0.4742, 0.4767, 0.4739, 0.4756,\n",
            "        0.4751, 0.4760, 0.4755, 0.4728, 0.4713, 0.4749, 0.4749, 0.4751, 0.4768,\n",
            "        0.4712, 0.4713, 0.4746, 0.4788, 0.4760, 0.4734, 0.4738, 0.4755, 0.4730,\n",
            "        0.4713, 0.4760, 0.4781, 0.4734, 0.4729, 0.4752, 0.4718, 0.4736, 0.4772,\n",
            "        0.4740, 0.4720, 0.4755, 0.4732, 0.4740, 0.4832, 0.4758, 0.4741, 0.4707,\n",
            "        0.4768, 0.4736, 0.4775, 0.4708, 0.4716, 0.4749, 0.4713, 0.4730, 0.4775,\n",
            "        0.4698])\n",
            "Sum of predicted tensor(30.3389)\n",
            "Loss tensor(0.6748)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4716, 0.4773, 0.4733, 0.4738, 0.4727, 0.4748, 0.4733, 0.4720, 0.4737,\n",
            "        0.4725, 0.4694, 0.4712, 0.4776, 0.4768, 0.4713, 0.4722, 0.4752, 0.4769,\n",
            "        0.4731, 0.4730, 0.4746, 0.4765, 0.4765, 0.4744, 0.4766, 0.4773, 0.4700,\n",
            "        0.4751, 0.4762, 0.4738, 0.4708, 0.4734, 0.4702, 0.4713, 0.4707, 0.4776,\n",
            "        0.4777, 0.4713, 0.4728, 0.4760, 0.4773, 0.4730, 0.4770, 0.4751, 0.4803,\n",
            "        0.4700, 0.4702, 0.4752, 0.4732, 0.4790, 0.4753, 0.4740, 0.4745, 0.4760,\n",
            "        0.4760, 0.4740, 0.4746, 0.4788, 0.4721, 0.4750, 0.4709, 0.4739, 0.4802,\n",
            "        0.4776])\n",
            "Sum of predicted tensor(30.3579)\n",
            "Loss tensor(0.6820)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4729, 0.4757, 0.4728, 0.4736, 0.4744, 0.4713, 0.4793, 0.4781, 0.4788,\n",
            "        0.4751, 0.4745, 0.4724, 0.4760, 0.4755, 0.4775, 0.4721, 0.4729, 0.4738,\n",
            "        0.4765, 0.4769, 0.4751, 0.4713, 0.4775, 0.4713, 0.4750, 0.4731, 0.4745,\n",
            "        0.4744, 0.4744, 0.4737, 0.4739, 0.4741, 0.4749, 0.4702, 0.4736, 0.4753,\n",
            "        0.4775, 0.4720, 0.4760, 0.4763, 0.4725, 0.4705, 0.4768, 0.4793, 0.4805,\n",
            "        0.4788, 0.4756, 0.4744, 0.4692, 0.4729, 0.4753, 0.4767, 0.4693, 0.4729,\n",
            "        0.4762, 0.4756, 0.4735, 0.4757, 0.4746, 0.4713, 0.4718, 0.4765, 0.4751,\n",
            "        0.4794])\n",
            "Sum of predicted tensor(30.3786)\n",
            "Loss tensor(0.6776)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0.4746, 0.4755, 0.4686, 0.4802, 0.4732, 0.4749, 0.4724, 0.4746, 0.4734,\n",
            "        0.4738, 0.4730, 0.4708, 0.4736, 0.4749, 0.4729, 0.4760, 0.4751, 0.4751,\n",
            "        0.4756, 0.4763, 0.4751, 0.4717, 0.4774, 0.4761, 0.4768, 0.4700, 0.4724,\n",
            "        0.4727, 0.4805, 0.4751, 0.4758, 0.4746, 0.4757, 0.4727, 0.4713, 0.4764,\n",
            "        0.4744, 0.4716, 0.4758, 0.4746, 0.4753, 0.4756, 0.4702, 0.4750, 0.4776,\n",
            "        0.4727, 0.4715, 0.4752, 0.4734, 0.4775, 0.4752, 0.4728, 0.4747, 0.4710,\n",
            "        0.4706, 0.4751, 0.4745, 0.4741, 0.4757, 0.4787, 0.4775, 0.4755, 0.4742,\n",
            "        0.4720])\n",
            "Sum of predicted tensor(30.3607)\n",
            "Loss tensor(0.6696)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0.4760, 0.4769, 0.4701, 0.4732, 0.4779, 0.4712, 0.4729, 0.4713, 0.4790,\n",
            "        0.4746, 0.4721, 0.4714, 0.4756, 0.4745, 0.4765, 0.4678, 0.4776, 0.4803,\n",
            "        0.4713, 0.4753, 0.4717, 0.4712, 0.4712, 0.4753, 0.4768, 0.4745, 0.4713,\n",
            "        0.4720, 0.4739, 0.4747, 0.4749, 0.4702, 0.4701, 0.4749, 0.4727, 0.4733,\n",
            "        0.4757, 0.4732, 0.4732, 0.4745, 0.4764, 0.4752, 0.4686, 0.4745, 0.4694,\n",
            "        0.4756, 0.4774, 0.4751, 0.4772, 0.4730, 0.4749, 0.4790, 0.4734, 0.4805,\n",
            "        0.4726, 0.4788, 0.4720, 0.4762, 0.4740, 0.4705, 0.4749, 0.4768, 0.4748,\n",
            "        0.4789])\n",
            "Sum of predicted tensor(30.3509)\n",
            "Loss tensor(0.6758)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4748, 0.4742, 0.4787, 0.4733, 0.4744, 0.4693, 0.4738, 0.4712, 0.4760,\n",
            "        0.4758, 0.4700, 0.4719, 0.4752, 0.4737, 0.4738, 0.4727, 0.4723, 0.4709,\n",
            "        0.4760, 0.4756, 0.4755, 0.4686, 0.4749, 0.4743, 0.4757, 0.4723, 0.4790,\n",
            "        0.4758, 0.4776, 0.4716, 0.4790, 0.4752, 0.4713, 0.4739, 0.4751, 0.4742,\n",
            "        0.4769, 0.4713, 0.4713, 0.4701, 0.4740, 0.4686, 0.4713, 0.4755, 0.4790,\n",
            "        0.4734, 0.4768, 0.4742, 0.4715, 0.4778, 0.4708, 0.4805, 0.4686, 0.4760,\n",
            "        0.4752, 0.4753, 0.4718, 0.4741, 0.4726, 0.4761, 0.4765, 0.4747, 0.4787,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.3475)\n",
            "Loss tensor(0.6905)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4727, 0.4752, 0.4725, 0.4775, 0.4713, 0.4800, 0.4694, 0.4721, 0.4733,\n",
            "        0.4814, 0.4717, 0.4797, 0.4778, 0.4787, 0.4765, 0.4728, 0.4757, 0.4735,\n",
            "        0.4752, 0.4724, 0.4721, 0.4658, 0.4764, 0.4778, 0.4761, 0.4765, 0.4757,\n",
            "        0.4790, 0.4742, 0.4746, 0.4769, 0.4709, 0.4741, 0.4781, 0.4721, 0.4765,\n",
            "        0.4731, 0.4754, 0.4736, 0.4726, 0.4753, 0.4699, 0.4768, 0.4775, 0.4768,\n",
            "        0.4765, 0.4726, 0.4713, 0.4744, 0.4694, 0.4736, 0.4718, 0.4729, 0.4723,\n",
            "        0.4719, 0.4700, 0.4709, 0.4723, 0.4749, 0.4749, 0.4733, 0.4759, 0.4862,\n",
            "        0.4712])\n",
            "Sum of predicted tensor(30.3635)\n",
            "Loss tensor(0.6788)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0.4727, 0.4714, 0.4811, 0.4764, 0.4734, 0.4752, 0.4733, 0.4753, 0.4727,\n",
            "        0.4764, 0.4745, 0.4779, 0.4752, 0.4745, 0.4754, 0.4754, 0.4743, 0.4737,\n",
            "        0.4743, 0.4768, 0.4737, 0.4761, 0.4669, 0.4744, 0.4742, 0.4789, 0.4770,\n",
            "        0.4745, 0.4775, 0.4747, 0.4811, 0.4788, 0.4817, 0.4717, 0.4764, 0.4731,\n",
            "        0.4732, 0.4725, 0.4745, 0.4809, 0.4754, 0.4767, 0.4742, 0.4754, 0.4735,\n",
            "        0.4748, 0.4772, 0.4752, 0.4746, 0.4742, 0.4786, 0.4705, 0.4701, 0.4804,\n",
            "        0.4733, 0.4757, 0.4776, 0.4735, 0.4773, 0.4790, 0.4770, 0.4723, 0.4763,\n",
            "        0.4750])\n",
            "Sum of predicted tensor(30.4197)\n",
            "Loss tensor(0.7008)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4725, 0.4753, 0.4727, 0.4770, 0.4727, 0.4722, 0.4764, 0.4727, 0.4764,\n",
            "        0.4749, 0.4751, 0.4724, 0.4802, 0.4761, 0.4791, 0.4817, 0.4783, 0.4781,\n",
            "        0.4723, 0.4725, 0.4748, 0.4761, 0.4774, 0.4807, 0.4757, 0.4764, 0.4767,\n",
            "        0.4743, 0.4725, 0.4755, 0.4743, 0.4738, 0.4727, 0.4743, 0.4748, 0.4754,\n",
            "        0.4752, 0.4751, 0.4755, 0.4783, 0.4763, 0.4792, 0.4785, 0.4811, 0.4766,\n",
            "        0.4749, 0.4752, 0.4753, 0.4743, 0.4795, 0.4725, 0.4764, 0.4786, 0.4795,\n",
            "        0.4774, 0.4754, 0.4740, 0.4773, 0.4764, 0.4766, 0.4710, 0.4764, 0.4722,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4474)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4747, 0.4744, 0.4760, 0.4717, 0.4775, 0.4727, 0.4768, 0.4764, 0.4740,\n",
            "        0.4727, 0.4773, 0.4768, 0.4727, 0.4767, 0.4727, 0.4768, 0.4730, 0.4737,\n",
            "        0.4811, 0.4767, 0.4809, 0.4782, 0.4757, 0.4717, 0.4740, 0.4745, 0.4684,\n",
            "        0.4793, 0.4779, 0.4777, 0.4742, 0.4727, 0.4759, 0.4762, 0.4782, 0.4736,\n",
            "        0.4796, 0.4749, 0.4768, 0.4750, 0.4744, 0.4748, 0.4727, 0.4756, 0.4756,\n",
            "        0.4785, 0.4764, 0.4747, 0.4730, 0.4751, 0.4784, 0.4737, 0.4734, 0.4761,\n",
            "        0.4786, 0.4764, 0.4773, 0.4757, 0.4774, 0.4758, 0.4768, 0.4773, 0.4722,\n",
            "        0.4723])\n",
            "Sum of predicted tensor(30.4315)\n",
            "Loss tensor(0.7020)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4717, 0.4746, 0.4750, 0.4764, 0.4743, 0.4783, 0.4707, 0.4814, 0.4783,\n",
            "        0.4764, 0.4756, 0.4764, 0.4727, 0.4727, 0.4774, 0.4753, 0.4751, 0.4727,\n",
            "        0.4728, 0.4764, 0.4719, 0.4764, 0.4722, 0.4767, 0.4764, 0.4694, 0.4720,\n",
            "        0.4743, 0.4753, 0.4764, 0.4743, 0.4764, 0.4727, 0.4756, 0.4790, 0.4744,\n",
            "        0.4725, 0.4817, 0.4745, 0.4757, 0.4767, 0.4778, 0.4793, 0.4730, 0.4741,\n",
            "        0.4747, 0.4757, 0.4826, 0.4749, 0.4721, 0.4732, 0.4777, 0.4746, 0.4719,\n",
            "        0.4757, 0.4684, 0.4742, 0.4718, 0.4736, 0.4740, 0.4754, 0.4766, 0.4750,\n",
            "        0.4779])\n",
            "Sum of predicted tensor(30.4028)\n",
            "Loss tensor(0.6938)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4773, 0.4739, 0.4755, 0.4746, 0.4756, 0.4764, 0.4734, 0.4733, 0.4755,\n",
            "        0.4717, 0.4769, 0.4764, 0.4717, 0.4771, 0.4762, 0.4728, 0.4754, 0.4761,\n",
            "        0.4742, 0.4727, 0.4750, 0.4748, 0.4777, 0.4782, 0.4705, 0.4720, 0.4761,\n",
            "        0.4734, 0.4757, 0.4757, 0.4757, 0.4753, 0.4714, 0.4775, 0.4774, 0.4756,\n",
            "        0.4768, 0.4751, 0.4743, 0.4739, 0.4722, 0.4755, 0.4753, 0.4740, 0.4720,\n",
            "        0.4758, 0.4750, 0.4720, 0.4684, 0.4753, 0.4779, 0.4768, 0.4787, 0.4747,\n",
            "        0.4795, 0.4759, 0.4727, 0.4793, 0.4747, 0.4741, 0.4792, 0.4778, 0.4738,\n",
            "        0.4731])\n",
            "Sum of predicted tensor(30.4025)\n",
            "Loss tensor(0.7058)\n",
            "Acc tensor(0.3906)\n",
            "tensor([0.4775, 0.4793, 0.4758, 0.4756, 0.4723, 0.4757, 0.4749, 0.4750, 0.4755,\n",
            "        0.4749, 0.4745, 0.4710, 0.4764, 0.4727, 0.4690, 0.4783, 0.4790, 0.4737,\n",
            "        0.4732, 0.4760, 0.4717, 0.4737, 0.4744, 0.4787, 0.4753, 0.4722, 0.4719,\n",
            "        0.4802, 0.4749, 0.4734, 0.4757, 0.4775, 0.4765, 0.4786, 0.4696, 0.4741,\n",
            "        0.4732, 0.4764, 0.4773, 0.4727, 0.4745, 0.4774, 0.4750, 0.4786, 0.4795,\n",
            "        0.4792, 0.4768, 0.4707, 0.4756, 0.4727, 0.4707, 0.4727, 0.4764, 0.4735,\n",
            "        0.4748, 0.4738, 0.4714, 0.4764, 0.4684, 0.4764, 0.4740, 0.4710, 0.4811,\n",
            "        0.4739])\n",
            "Sum of predicted tensor(30.3929)\n",
            "Loss tensor(0.6986)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4739, 0.4756, 0.4749, 0.4755, 0.4707, 0.4754, 0.4735, 0.4707, 0.4724,\n",
            "        0.4746, 0.4764, 0.4764, 0.4751, 0.4764, 0.4784, 0.4761, 0.4692, 0.4757,\n",
            "        0.4758, 0.4766, 0.4771, 0.4727, 0.4733, 0.4701, 0.4732, 0.4777, 0.4746,\n",
            "        0.4802, 0.4753, 0.4752, 0.4793, 0.4752, 0.4743, 0.4747, 0.4764, 0.4739,\n",
            "        0.4754, 0.4760, 0.4764, 0.4743, 0.4811, 0.4774, 0.4740, 0.4714, 0.4749,\n",
            "        0.4753, 0.4764, 0.4786, 0.4750, 0.4706, 0.4747, 0.4755, 0.4781, 0.4764,\n",
            "        0.4792, 0.4792, 0.4755, 0.4758, 0.4717, 0.4764, 0.4747, 0.4741, 0.4789,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.4159)\n",
            "Loss tensor(0.6919)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4779, 0.4792, 0.4684, 0.4722, 0.4786, 0.4765, 0.4727, 0.4727, 0.4764,\n",
            "        0.4725, 0.4727, 0.4720, 0.4760, 0.4793, 0.4767, 0.4757, 0.4734, 0.4719,\n",
            "        0.4782, 0.4727, 0.4703, 0.4772, 0.4758, 0.4752, 0.4710, 0.4719, 0.4727,\n",
            "        0.4745, 0.4731, 0.4748, 0.4765, 0.4750, 0.4732, 0.4763, 0.4727, 0.4757,\n",
            "        0.4727, 0.4734, 0.4745, 0.4745, 0.4705, 0.4728, 0.4727, 0.4727, 0.4761,\n",
            "        0.4767, 0.4740, 0.4758, 0.4757, 0.4764, 0.4707, 0.4743, 0.4770, 0.4737,\n",
            "        0.4764, 0.4725, 0.4753, 0.4728, 0.4740, 0.4727, 0.4782, 0.4752, 0.4764,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3646)\n",
            "Loss tensor(0.6987)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4754, 0.4697, 0.4764, 0.4761, 0.4762, 0.4793, 0.4699, 0.4781, 0.4764,\n",
            "        0.4727, 0.4727, 0.4764, 0.4713, 0.4768, 0.4756, 0.4811, 0.4727, 0.4712,\n",
            "        0.4764, 0.4757, 0.4773, 0.4731, 0.4753, 0.4778, 0.4747, 0.4773, 0.4756,\n",
            "        0.4767, 0.4742, 0.4744, 0.4807, 0.4778, 0.4769, 0.4764, 0.4717, 0.4767,\n",
            "        0.4792, 0.4705, 0.4754, 0.4707, 0.4735, 0.4705, 0.4771, 0.4734, 0.4727,\n",
            "        0.4752, 0.4738, 0.4832, 0.4710, 0.4721, 0.4764, 0.4739, 0.4783, 0.4764,\n",
            "        0.4728, 0.4756, 0.4710, 0.4767, 0.4756, 0.4793, 0.4699, 0.4756, 0.4727,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.4056)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4719, 0.4727, 0.4725, 0.4747, 0.4713, 0.4734, 0.4748, 0.4789, 0.4726,\n",
            "        0.4767, 0.4764, 0.4773, 0.4710, 0.4752, 0.4751, 0.4719, 0.4765, 0.4779,\n",
            "        0.4787, 0.4750, 0.4760, 0.4753, 0.4743, 0.4763, 0.4735, 0.4769, 0.4809,\n",
            "        0.4747, 0.4735, 0.4760, 0.4749, 0.4767, 0.4732, 0.4807, 0.4731, 0.4781,\n",
            "        0.4754, 0.4754, 0.4748, 0.4727, 0.4727, 0.4767, 0.4792, 0.4725, 0.4720,\n",
            "        0.4739, 0.4773, 0.4747, 0.4727, 0.4735, 0.4779, 0.4732, 0.4753, 0.4744,\n",
            "        0.4815, 0.4764, 0.4750, 0.4731, 0.4705, 0.4738, 0.4727, 0.4737, 0.4773,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.3998)\n",
            "Loss tensor(0.6964)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4771, 0.4762, 0.4732, 0.4768, 0.4756, 0.4719, 0.4727, 0.4739, 0.4708,\n",
            "        0.4736, 0.4727, 0.4757, 0.4754, 0.4718, 0.4801, 0.4720, 0.4752, 0.4759,\n",
            "        0.4712, 0.4761, 0.4761, 0.4787, 0.4776, 0.4703, 0.4764, 0.4749, 0.4752,\n",
            "        0.4770, 0.4775, 0.4786, 0.4740, 0.4730, 0.4773, 0.4719, 0.4736, 0.4775,\n",
            "        0.4764, 0.4725, 0.4722, 0.4730, 0.4775, 0.4727, 0.4764, 0.4787, 0.4727,\n",
            "        0.4707, 0.4744, 0.4779, 0.4732, 0.4747, 0.4732, 0.4758, 0.4740, 0.4746,\n",
            "        0.4764, 0.4761, 0.4738, 0.4744, 0.4722, 0.4723, 0.4740, 0.4747, 0.4722,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3794)\n",
            "Loss tensor(0.6901)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4781, 0.4743, 0.4758, 0.4739, 0.4747, 0.4755, 0.4764, 0.4727, 0.4729,\n",
            "        0.4749, 0.4757, 0.4731, 0.4754, 0.4750, 0.4753, 0.4775, 0.4731, 0.4786,\n",
            "        0.4742, 0.4769, 0.4752, 0.4743, 0.4781, 0.4767, 0.4786, 0.4790, 0.4733,\n",
            "        0.4789, 0.4758, 0.4805, 0.4747, 0.4765, 0.4729, 0.4790, 0.4735, 0.4754,\n",
            "        0.4720, 0.4738, 0.4767, 0.4747, 0.4713, 0.4785, 0.4805, 0.4726, 0.4723,\n",
            "        0.4755, 0.4773, 0.4748, 0.4773, 0.4730, 0.4733, 0.4717, 0.4779, 0.4758,\n",
            "        0.4750, 0.4742, 0.4775, 0.4802, 0.4772, 0.4728, 0.4739, 0.4732, 0.4745,\n",
            "        0.4722])\n",
            "Sum of predicted tensor(30.4257)\n",
            "Loss tensor(0.6973)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4771, 0.4736, 0.4732, 0.4829, 0.4787, 0.4726, 0.4766, 0.4762, 0.4771,\n",
            "        0.4753, 0.4735, 0.4806, 0.4750, 0.4770, 0.4751, 0.4753, 0.4748, 0.4805,\n",
            "        0.4742, 0.4758, 0.4771, 0.4746, 0.4722, 0.4748, 0.4797, 0.4774, 0.4735,\n",
            "        0.4706, 0.4731, 0.4696, 0.4722, 0.4790, 0.4771, 0.4756, 0.4766, 0.4795,\n",
            "        0.4746, 0.4779, 0.4776, 0.4759, 0.4717, 0.4792, 0.4732, 0.4724, 0.4745,\n",
            "        0.4795, 0.4734, 0.4731, 0.4727, 0.4745, 0.4807, 0.4740, 0.4764, 0.4753,\n",
            "        0.4729, 0.4748, 0.4734, 0.4710, 0.4777, 0.4771, 0.4753, 0.4753, 0.4779,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4333)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4762, 0.4771, 0.4771, 0.4736, 0.4791, 0.4783, 0.4718, 0.4771, 0.4722,\n",
            "        0.4771, 0.4733, 0.4738, 0.4746, 0.4757, 0.4786, 0.4717, 0.4759, 0.4753,\n",
            "        0.4756, 0.4771, 0.4752, 0.4791, 0.4706, 0.4712, 0.4822, 0.4755, 0.4756,\n",
            "        0.4762, 0.4784, 0.4758, 0.4757, 0.4756, 0.4738, 0.4748, 0.4772, 0.4746,\n",
            "        0.4770, 0.4742, 0.4747, 0.4771, 0.4757, 0.4744, 0.4742, 0.4783, 0.4759,\n",
            "        0.4760, 0.4732, 0.4797, 0.4779, 0.4749, 0.4755, 0.4771, 0.4750, 0.4736,\n",
            "        0.4722, 0.4755, 0.4750, 0.4752, 0.4717, 0.4765, 0.4772, 0.4734, 0.4734,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4349)\n",
            "Loss tensor(0.6952)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4763, 0.4771, 0.4718, 0.4727, 0.4722, 0.4732, 0.4772, 0.4762, 0.4755,\n",
            "        0.4729, 0.4805, 0.4733, 0.4777, 0.4804, 0.4747, 0.4730, 0.4763, 0.4744,\n",
            "        0.4748, 0.4769, 0.4734, 0.4771, 0.4741, 0.4771, 0.4771, 0.4771, 0.4771,\n",
            "        0.4733, 0.4771, 0.4753, 0.4696, 0.4673, 0.4744, 0.4728, 0.4747, 0.4734,\n",
            "        0.4730, 0.4719, 0.4706, 0.4793, 0.4784, 0.4755, 0.4753, 0.4734, 0.4748,\n",
            "        0.4732, 0.4771, 0.4758, 0.4787, 0.4751, 0.4781, 0.4765, 0.4726, 0.4684,\n",
            "        0.4771, 0.4771, 0.4776, 0.4722, 0.4817, 0.4755, 0.4752, 0.4753, 0.4737,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.4025)\n",
            "Loss tensor(0.6996)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4769, 0.4754, 0.4767, 0.4748, 0.4774, 0.4746, 0.4768, 0.4741, 0.4779,\n",
            "        0.4771, 0.4771, 0.4732, 0.4773, 0.4745, 0.4756, 0.4703, 0.4771, 0.4728,\n",
            "        0.4722, 0.4797, 0.4781, 0.4763, 0.4769, 0.4732, 0.4749, 0.4733, 0.4718,\n",
            "        0.4729, 0.4806, 0.4725, 0.4734, 0.4750, 0.4768, 0.4692, 0.4722, 0.4759,\n",
            "        0.4757, 0.4771, 0.4742, 0.4771, 0.4726, 0.4776, 0.4740, 0.4770, 0.4712,\n",
            "        0.4692, 0.4771, 0.4743, 0.4771, 0.4778, 0.4753, 0.4762, 0.4696, 0.4733,\n",
            "        0.4753, 0.4733, 0.4722, 0.4771, 0.4748, 0.4728, 0.4817, 0.4767, 0.4839,\n",
            "        0.4730])\n",
            "Sum of predicted tensor(30.4114)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4722, 0.4720, 0.4754, 0.4785, 0.4771, 0.4731, 0.4771, 0.4764, 0.4793,\n",
            "        0.4771, 0.4771, 0.4767, 0.4771, 0.4771, 0.4715, 0.4807, 0.4771, 0.4732,\n",
            "        0.4755, 0.4717, 0.4728, 0.4813, 0.4753, 0.4740, 0.4771, 0.4771, 0.4766,\n",
            "        0.4770, 0.4736, 0.4752, 0.4728, 0.4760, 0.4711, 0.4799, 0.4733, 0.4748,\n",
            "        0.4721, 0.4742, 0.4722, 0.4783, 0.4796, 0.4771, 0.4836, 0.4754, 0.4747,\n",
            "        0.4715, 0.4783, 0.4774, 0.4773, 0.4755, 0.4771, 0.4752, 0.4775, 0.4821,\n",
            "        0.4748, 0.4734, 0.4771, 0.4748, 0.4706, 0.4761, 0.4759, 0.4776, 0.4734,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4517)\n",
            "Loss tensor(0.7021)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4789, 0.4756, 0.4745, 0.4757, 0.4753, 0.4751, 0.4763, 0.4726, 0.4744,\n",
            "        0.4741, 0.4771, 0.4756, 0.4772, 0.4723, 0.4746, 0.4735, 0.4712, 0.4748,\n",
            "        0.4772, 0.4707, 0.4739, 0.4769, 0.4767, 0.4758, 0.4774, 0.4760, 0.4843,\n",
            "        0.4706, 0.4775, 0.4741, 0.4771, 0.4722, 0.4749, 0.4724, 0.4753, 0.4771,\n",
            "        0.4779, 0.4771, 0.4778, 0.4777, 0.4763, 0.4755, 0.4750, 0.4771, 0.4722,\n",
            "        0.4765, 0.4733, 0.4710, 0.4771, 0.4771, 0.4750, 0.4760, 0.4733, 0.4768,\n",
            "        0.4732, 0.4771, 0.4791, 0.4707, 0.4752, 0.4748, 0.4706, 0.4722, 0.4752,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4157)\n",
            "Loss tensor(0.6971)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4749, 0.4779, 0.4772, 0.4733, 0.4743, 0.4724, 0.4734, 0.4771, 0.4762,\n",
            "        0.4767, 0.4769, 0.4771, 0.4760, 0.4771, 0.4735, 0.4715, 0.4751, 0.4728,\n",
            "        0.4718, 0.4763, 0.4771, 0.4747, 0.4734, 0.4722, 0.4767, 0.4793, 0.4751,\n",
            "        0.4825, 0.4755, 0.4749, 0.4778, 0.4762, 0.4771, 0.4738, 0.4720, 0.4695,\n",
            "        0.4699, 0.4771, 0.4771, 0.4732, 0.4717, 0.4765, 0.4728, 0.4751, 0.4771,\n",
            "        0.4747, 0.4748, 0.4781, 0.4771, 0.4753, 0.4722, 0.4771, 0.4735, 0.4717,\n",
            "        0.4706, 0.4801, 0.4754, 0.4779, 0.4706, 0.4780, 0.4771, 0.4704, 0.4743,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4032)\n",
            "Loss tensor(0.6951)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4741, 0.4770, 0.4771, 0.4751, 0.4713, 0.4771, 0.4769, 0.4801, 0.4733,\n",
            "        0.4746, 0.4771, 0.4723, 0.4726, 0.4806, 0.4791, 0.4817, 0.4751, 0.4772,\n",
            "        0.4742, 0.4730, 0.4722, 0.4729, 0.4707, 0.4760, 0.4728, 0.4789, 0.4748,\n",
            "        0.4767, 0.4762, 0.4690, 0.4722, 0.4772, 0.4771, 0.4753, 0.4771, 0.4733,\n",
            "        0.4733, 0.4743, 0.4771, 0.4752, 0.4766, 0.4772, 0.4757, 0.4750, 0.4738,\n",
            "        0.4710, 0.4759, 0.4756, 0.4777, 0.4771, 0.4754, 0.4724, 0.4779, 0.4758,\n",
            "        0.4734, 0.4797, 0.4745, 0.4777, 0.4778, 0.4765, 0.4678, 0.4702, 0.4771,\n",
            "        0.4764])\n",
            "Sum of predicted tensor(30.4199)\n",
            "Loss tensor(0.6928)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4750, 0.4794, 0.4760, 0.4754, 0.4779, 0.4746, 0.4771, 0.4781, 0.4706,\n",
            "        0.4798, 0.4754, 0.4712, 0.4789, 0.4744, 0.4722, 0.4771, 0.4743, 0.4707,\n",
            "        0.4730, 0.4789, 0.4748, 0.4771, 0.4722, 0.4706, 0.4741, 0.4791, 0.4858,\n",
            "        0.4777, 0.4735, 0.4705, 0.4787, 0.4754, 0.4771, 0.4744, 0.4740, 0.4725,\n",
            "        0.4793, 0.4723, 0.4756, 0.4751, 0.4768, 0.4771, 0.4688, 0.4748, 0.4732,\n",
            "        0.4821, 0.4714, 0.4771, 0.4734, 0.4760, 0.4753, 0.4771, 0.4812, 0.4755,\n",
            "        0.4775, 0.4771, 0.4730, 0.4771, 0.4746, 0.4792, 0.4754, 0.4753, 0.4787,\n",
            "        0.4722])\n",
            "Sum of predicted tensor(30.4396)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4760, 0.4776, 0.4776, 0.4746, 0.4746, 0.4732, 0.4771, 0.4733, 0.4745,\n",
            "        0.4745, 0.4754, 0.4723, 0.4758, 0.4792, 0.4743, 0.4706, 0.4722, 0.4715,\n",
            "        0.4746, 0.4770, 0.4717, 0.4775, 0.4777, 0.4742, 0.4760, 0.4732, 0.4759,\n",
            "        0.4768, 0.4756, 0.4771, 0.4771, 0.4752, 0.4771, 0.4751, 0.4730, 0.4747,\n",
            "        0.4765, 0.4809, 0.4771, 0.4771, 0.4729, 0.4726, 0.4754, 0.4742, 0.4744,\n",
            "        0.4791, 0.4765, 0.4771, 0.4713, 0.4771, 0.4793, 0.4750, 0.4772, 0.4722,\n",
            "        0.4751, 0.4753, 0.4749, 0.4740, 0.4744, 0.4770, 0.4771, 0.4771, 0.4722,\n",
            "        0.4786])\n",
            "Sum of predicted tensor(30.4256)\n",
            "Loss tensor(0.6970)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4732, 0.4734, 0.4722, 0.4772, 0.4728, 0.4771, 0.4791, 0.4739, 0.4730,\n",
            "        0.4760, 0.4706, 0.4780, 0.4750, 0.4762, 0.4696, 0.4797, 0.4793, 0.4776,\n",
            "        0.4771, 0.4752, 0.4768, 0.4739, 0.4760, 0.4748, 0.4783, 0.4763, 0.4792,\n",
            "        0.4748, 0.4766, 0.4728, 0.4753, 0.4732, 0.4752, 0.4757, 0.4776, 0.4728,\n",
            "        0.4755, 0.4773, 0.4762, 0.4682, 0.4728, 0.4725, 0.4791, 0.4762, 0.4810,\n",
            "        0.4800, 0.4784, 0.4767, 0.4771, 0.4768, 0.4722, 0.4732, 0.4753, 0.4720,\n",
            "        0.4728, 0.4697, 0.4729, 0.4817, 0.4771, 0.4817, 0.4742, 0.4756, 0.4771,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4360)\n",
            "Loss tensor(0.7105)\n",
            "Acc tensor(0.3281)\n",
            "tensor([0.4751, 0.4764, 0.4757, 0.4728, 0.4734, 0.4771, 0.4759, 0.4772, 0.4776,\n",
            "        0.4722, 0.4771, 0.4763, 0.4771, 0.4734, 0.4786, 0.4763, 0.4755, 0.4722,\n",
            "        0.4771, 0.4800, 0.4771, 0.4758, 0.4772, 0.4738, 0.4771, 0.4750, 0.4775,\n",
            "        0.4741, 0.4765, 0.4728, 0.4771, 0.4706, 0.4771, 0.4749, 0.4722, 0.4730,\n",
            "        0.4803, 0.4767, 0.4771, 0.4793, 0.4746, 0.4757, 0.4739, 0.4766, 0.4771,\n",
            "        0.4740, 0.4781, 0.4742, 0.4751, 0.4821, 0.4760, 0.4753, 0.4732, 0.4750,\n",
            "        0.4766, 0.4775, 0.4752, 0.4747, 0.4753, 0.4757, 0.4739, 0.4706, 0.4767,\n",
            "        0.4754])\n",
            "Sum of predicted tensor(30.4447)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4742, 0.4726, 0.4737, 0.4755, 0.4751, 0.4771, 0.4793, 0.4791, 0.4745,\n",
            "        0.4712, 0.4736, 0.4723, 0.4735, 0.4742, 0.4768, 0.4739, 0.4706, 0.4771,\n",
            "        0.4776, 0.4733, 0.4716, 0.4768, 0.4733, 0.4767, 0.4799, 0.4755, 0.4753,\n",
            "        0.4775, 0.4736, 0.4754, 0.4754, 0.4771, 0.4762, 0.4757, 0.4766, 0.4734,\n",
            "        0.4731, 0.4722, 0.4790, 0.4772, 0.4704, 0.4746, 0.4722, 0.4771, 0.4739,\n",
            "        0.4755, 0.4780, 0.4746, 0.4717, 0.4758, 0.4772, 0.4780, 0.4771, 0.4732,\n",
            "        0.4753, 0.4751, 0.4771, 0.4746, 0.4771, 0.4722, 0.4722, 0.4714, 0.4771,\n",
            "        0.4707])\n",
            "Sum of predicted tensor(30.3992)\n",
            "Loss tensor(0.7020)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4771, 0.4771, 0.4775, 0.4767, 0.4757, 0.4771, 0.4762, 0.4752, 0.4785,\n",
            "        0.4732, 0.4771, 0.4772, 0.4762, 0.4771, 0.4753, 0.4715, 0.4731, 0.4721,\n",
            "        0.4748, 0.4771, 0.4756, 0.4754, 0.4771, 0.4806, 0.4771, 0.4771, 0.4771,\n",
            "        0.4725, 0.4731, 0.4732, 0.4746, 0.4732, 0.4771, 0.4745, 0.4772, 0.4722,\n",
            "        0.4695, 0.4785, 0.4769, 0.4768, 0.4756, 0.4742, 0.4763, 0.4749, 0.4753,\n",
            "        0.4771, 0.4771, 0.4731, 0.4722, 0.4771, 0.4803, 0.4704, 0.4701, 0.4752,\n",
            "        0.4757, 0.4712, 0.4771, 0.4732, 0.4747, 0.4732, 0.4749, 0.4771, 0.4715,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4196)\n",
            "Loss tensor(0.7060)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4763, 0.4771, 0.4771, 0.4748, 0.4760, 0.4746, 0.4771, 0.4771, 0.4731,\n",
            "        0.4790, 0.4762, 0.4767, 0.4742, 0.4759, 0.4771, 0.4772, 0.4707, 0.4729,\n",
            "        0.4733, 0.4793, 0.4771, 0.4811, 0.4795, 0.4770, 0.4772, 0.4740, 0.4748,\n",
            "        0.4771, 0.4722, 0.4732, 0.4771, 0.4811, 0.4752, 0.4766, 0.4765, 0.4722,\n",
            "        0.4766, 0.4722, 0.4750, 0.4768, 0.4773, 0.4761, 0.4795, 0.4696, 0.4816,\n",
            "        0.4736, 0.4771, 0.4771, 0.4801, 0.4748, 0.4722, 0.4771, 0.4746, 0.4771,\n",
            "        0.4777, 0.4754, 0.4776, 0.4725, 0.4725, 0.4764, 0.4771, 0.4730, 0.4762,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4583)\n",
            "Loss tensor(0.6973)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4766, 0.4809, 0.4755, 0.4735, 0.4785, 0.4771, 0.4751, 0.4822, 0.4760,\n",
            "        0.4737, 0.4755, 0.4741, 0.4797, 0.4723, 0.4764, 0.4759, 0.4752, 0.4775,\n",
            "        0.4749, 0.4685, 0.4754, 0.4712, 0.4764, 0.4792, 0.4754, 0.4785, 0.4766,\n",
            "        0.4739, 0.4764, 0.4776, 0.4768, 0.4763, 0.4779, 0.4740, 0.4723, 0.4755,\n",
            "        0.4738, 0.4742, 0.4739, 0.4787, 0.4743, 0.4685, 0.4753, 0.4766, 0.4722,\n",
            "        0.4706, 0.4715, 0.4790, 0.4706, 0.4753, 0.4765, 0.4813, 0.4722, 0.4696,\n",
            "        0.4763, 0.4759, 0.4706, 0.4748, 0.4783, 0.4747, 0.4796, 0.4771, 0.4734,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.4244)\n",
            "Loss tensor(0.6894)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4791, 0.4718, 0.4802, 0.4766, 0.4782, 0.4766, 0.4778, 0.4757, 0.4685,\n",
            "        0.4740, 0.4726, 0.4762, 0.4715, 0.4728, 0.4779, 0.4724, 0.4758, 0.4765,\n",
            "        0.4754, 0.4742, 0.4811, 0.4746, 0.4765, 0.4752, 0.4757, 0.4766, 0.4770,\n",
            "        0.4740, 0.4772, 0.4758, 0.4706, 0.4762, 0.4716, 0.4749, 0.4708, 0.4766,\n",
            "        0.4727, 0.4759, 0.4762, 0.4757, 0.4733, 0.4726, 0.4770, 0.4760, 0.4742,\n",
            "        0.4770, 0.4757, 0.4752, 0.4751, 0.4720, 0.4811, 0.4759, 0.4716, 0.4749,\n",
            "        0.4733, 0.4775, 0.4737, 0.4715, 0.4739, 0.4767, 0.4754, 0.4766, 0.4746,\n",
            "        0.4766])\n",
            "Sum of predicted tensor(30.4100)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4776, 0.4731, 0.4715, 0.4764, 0.4743, 0.4770, 0.4739, 0.4734, 0.4766,\n",
            "        0.4752, 0.4770, 0.4718, 0.4757, 0.4748, 0.4766, 0.4749, 0.4794, 0.4761,\n",
            "        0.4730, 0.4757, 0.4753, 0.4762, 0.4754, 0.4754, 0.4770, 0.4741, 0.4775,\n",
            "        0.4734, 0.4743, 0.4766, 0.4746, 0.4748, 0.4766, 0.4719, 0.4766, 0.4748,\n",
            "        0.4722, 0.4715, 0.4702, 0.4766, 0.4760, 0.4685, 0.4766, 0.4771, 0.4764,\n",
            "        0.4775, 0.4760, 0.4741, 0.4773, 0.4754, 0.4760, 0.4754, 0.4787, 0.4735,\n",
            "        0.4753, 0.4752, 0.4756, 0.4773, 0.4744, 0.4779, 0.4701, 0.4761, 0.4765,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.4111)\n",
            "Loss tensor(0.6920)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4775, 0.4766, 0.4725, 0.4802, 0.4747, 0.4685, 0.4759, 0.4766, 0.4753,\n",
            "        0.4676, 0.4757, 0.4761, 0.4775, 0.4717, 0.4718, 0.4723, 0.4766, 0.4767,\n",
            "        0.4812, 0.4765, 0.4765, 0.4761, 0.4766, 0.4769, 0.4750, 0.4685, 0.4771,\n",
            "        0.4766, 0.4754, 0.4766, 0.4741, 0.4735, 0.4743, 0.4752, 0.4746, 0.4766,\n",
            "        0.4751, 0.4753, 0.4740, 0.4742, 0.4761, 0.4727, 0.4685, 0.4685, 0.4763,\n",
            "        0.4704, 0.4766, 0.4726, 0.4760, 0.4764, 0.4757, 0.4766, 0.4703, 0.4779,\n",
            "        0.4718, 0.4721, 0.4766, 0.4784, 0.4840, 0.4757, 0.4766, 0.4778, 0.4776,\n",
            "        0.4710])\n",
            "Sum of predicted tensor(30.3996)\n",
            "Loss tensor(0.7047)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4754, 0.4726, 0.4776, 0.4783, 0.4743, 0.4685, 0.4769, 0.4717, 0.4705,\n",
            "        0.4725, 0.4716, 0.4754, 0.4710, 0.4731, 0.4752, 0.4753, 0.4774, 0.4717,\n",
            "        0.4758, 0.4743, 0.4752, 0.4769, 0.4754, 0.4808, 0.4775, 0.4698, 0.4770,\n",
            "        0.4794, 0.4715, 0.4777, 0.4775, 0.4797, 0.4748, 0.4743, 0.4775, 0.4747,\n",
            "        0.4720, 0.4710, 0.4795, 0.4754, 0.4753, 0.4761, 0.4766, 0.4782, 0.4745,\n",
            "        0.4710, 0.4840, 0.4773, 0.4728, 0.4759, 0.4754, 0.4761, 0.4761, 0.4711,\n",
            "        0.4737, 0.4766, 0.4754, 0.4727, 0.4765, 0.4725, 0.4777, 0.4758, 0.4742,\n",
            "        0.4708])\n",
            "Sum of predicted tensor(30.4026)\n",
            "Loss tensor(0.6977)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4773, 0.4740, 0.4707, 0.4798, 0.4707, 0.4705, 0.4780, 0.4766, 0.4727,\n",
            "        0.4707, 0.4722, 0.4759, 0.4743, 0.4735, 0.4728, 0.4756, 0.4781, 0.4710,\n",
            "        0.4750, 0.4743, 0.4715, 0.4762, 0.4756, 0.4708, 0.4743, 0.4777, 0.4750,\n",
            "        0.4765, 0.4746, 0.4740, 0.4757, 0.4756, 0.4770, 0.4758, 0.4715, 0.4742,\n",
            "        0.4713, 0.4775, 0.4796, 0.4780, 0.4740, 0.4685, 0.4723, 0.4711, 0.4766,\n",
            "        0.4766, 0.4787, 0.4703, 0.4766, 0.4711, 0.4742, 0.4705, 0.4743, 0.4751,\n",
            "        0.4753, 0.4749, 0.4743, 0.4762, 0.4740, 0.4726, 0.4759, 0.4685, 0.4731,\n",
            "        0.4758])\n",
            "Sum of predicted tensor(30.3564)\n",
            "Loss tensor(0.6929)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4779, 0.4762, 0.4794, 0.4749, 0.4771, 0.4775, 0.4761, 0.4776, 0.4766,\n",
            "        0.4756, 0.4760, 0.4749, 0.4726, 0.4776, 0.4756, 0.4735, 0.4763, 0.4757,\n",
            "        0.4754, 0.4751, 0.4776, 0.4752, 0.4786, 0.4721, 0.4784, 0.4766, 0.4792,\n",
            "        0.4759, 0.4780, 0.4753, 0.4723, 0.4771, 0.4712, 0.4690, 0.4766, 0.4766,\n",
            "        0.4766, 0.4731, 0.4739, 0.4814, 0.4744, 0.4742, 0.4758, 0.4766, 0.4749,\n",
            "        0.4794, 0.4759, 0.4702, 0.4793, 0.4766, 0.4763, 0.4685, 0.4738, 0.4727,\n",
            "        0.4776, 0.4766, 0.4757, 0.4734, 0.4753, 0.4766, 0.4716, 0.4752, 0.4727,\n",
            "        0.4795])\n",
            "Sum of predicted tensor(30.4390)\n",
            "Loss tensor(0.7058)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4685, 0.4771, 0.4774, 0.4713, 0.4718, 0.4710, 0.4729, 0.4726, 0.4764,\n",
            "        0.4762, 0.4751, 0.4738, 0.4754, 0.4753, 0.4768, 0.4760, 0.4744, 0.4761,\n",
            "        0.4716, 0.4786, 0.4740, 0.4764, 0.4764, 0.4742, 0.4739, 0.4771, 0.4754,\n",
            "        0.4775, 0.4742, 0.4717, 0.4749, 0.4713, 0.4742, 0.4729, 0.4701, 0.4754,\n",
            "        0.4744, 0.4737, 0.4744, 0.4766, 0.4747, 0.4756, 0.4753, 0.4707, 0.4756,\n",
            "        0.4770, 0.4760, 0.4762, 0.4788, 0.4798, 0.4807, 0.4728, 0.4785, 0.4760,\n",
            "        0.4721, 0.4766, 0.4760, 0.4757, 0.4794, 0.4761, 0.4762, 0.4779, 0.4766,\n",
            "        0.4766])\n",
            "Sum of predicted tensor(30.4082)\n",
            "Loss tensor(0.6897)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4742, 0.4756, 0.4804, 0.4739, 0.4757, 0.4750, 0.4765, 0.4740, 0.4766,\n",
            "        0.4705, 0.4739, 0.4735, 0.4765, 0.4693, 0.4791, 0.4727, 0.4768, 0.4740,\n",
            "        0.4763, 0.4748, 0.4745, 0.4730, 0.4685, 0.4764, 0.4781, 0.4726, 0.4766,\n",
            "        0.4766, 0.4765, 0.4717, 0.4708, 0.4717, 0.4700, 0.4762, 0.4794, 0.4710,\n",
            "        0.4732, 0.4747, 0.4773, 0.4747, 0.4725, 0.4707, 0.4738, 0.4762, 0.4726,\n",
            "        0.4777, 0.4797, 0.4727, 0.4757, 0.4765, 0.4759, 0.4733, 0.4754, 0.4748,\n",
            "        0.4755, 0.4747, 0.4754, 0.4755, 0.4741, 0.4766, 0.4729, 0.4757, 0.4685,\n",
            "        0.4757])\n",
            "Sum of predicted tensor(30.3750)\n",
            "Loss tensor(0.6981)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4770, 0.4754, 0.4730, 0.4776, 0.4780, 0.4709, 0.4766, 0.4757, 0.4777,\n",
            "        0.4808, 0.4705, 0.4763, 0.4739, 0.4742, 0.4808, 0.4840, 0.4762, 0.4760,\n",
            "        0.4752, 0.4765, 0.4762, 0.4763, 0.4756, 0.4708, 0.4743, 0.4717, 0.4809,\n",
            "        0.4752, 0.4753, 0.4749, 0.4746, 0.4795, 0.4751, 0.4755, 0.4765, 0.4714,\n",
            "        0.4762, 0.4755, 0.4802, 0.4808, 0.4750, 0.4775, 0.4786, 0.4716, 0.4794,\n",
            "        0.4765, 0.4765, 0.4756, 0.4766, 0.4766, 0.4835, 0.4761, 0.4743, 0.4726,\n",
            "        0.4738, 0.4741, 0.4756, 0.4740, 0.4696, 0.4746, 0.4740, 0.4748, 0.4761,\n",
            "        0.4719])\n",
            "Sum of predicted tensor(30.4518)\n",
            "Loss tensor(0.6883)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4814, 0.4713, 0.4731, 0.4747, 0.4766, 0.4685, 0.4796, 0.4749, 0.4769,\n",
            "        0.4750, 0.4764, 0.4742, 0.4718, 0.4799, 0.4739, 0.4756, 0.4796, 0.4745,\n",
            "        0.4739, 0.4761, 0.4744, 0.4775, 0.4766, 0.4740, 0.4795, 0.4771, 0.4776,\n",
            "        0.4747, 0.4713, 0.4749, 0.4802, 0.4701, 0.4762, 0.4743, 0.4746, 0.4728,\n",
            "        0.4685, 0.4782, 0.4770, 0.4786, 0.4716, 0.4711, 0.4766, 0.4759, 0.4754,\n",
            "        0.4753, 0.4761, 0.4743, 0.4711, 0.4766, 0.4766, 0.4717, 0.4775, 0.4808,\n",
            "        0.4774, 0.4758, 0.4685, 0.4747, 0.4784, 0.4743, 0.4821, 0.4766, 0.4788,\n",
            "        0.4782])\n",
            "Sum of predicted tensor(30.4316)\n",
            "Loss tensor(0.7082)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0.4714, 0.4738, 0.4749, 0.4754, 0.4742, 0.4735, 0.4727, 0.4755, 0.4797,\n",
            "        0.4786, 0.4709, 0.4742, 0.4732, 0.4717, 0.4766, 0.4740, 0.4740, 0.4802,\n",
            "        0.4746, 0.4756, 0.4758, 0.4735, 0.4798, 0.4760, 0.4783, 0.4770, 0.4748,\n",
            "        0.4754, 0.4814, 0.4792, 0.4729, 0.4747, 0.4744, 0.4685, 0.4779, 0.4769,\n",
            "        0.4739, 0.4767, 0.4754, 0.4766, 0.4777, 0.4713, 0.4787, 0.4763, 0.4743,\n",
            "        0.4755, 0.4766, 0.4777, 0.4774, 0.4733, 0.4775, 0.4753, 0.4762, 0.4735,\n",
            "        0.4750, 0.4758, 0.4735, 0.4755, 0.4760, 0.4752, 0.4745, 0.4768, 0.4706,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4220)\n",
            "Loss tensor(0.6968)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4724, 0.4744, 0.4685, 0.4743, 0.4726, 0.4746, 0.4710, 0.4754, 0.4745,\n",
            "        0.4685, 0.4768, 0.4835, 0.4737, 0.4749, 0.4765, 0.4752, 0.4739, 0.4760,\n",
            "        0.4697, 0.4776, 0.4776, 0.4734, 0.4765, 0.4772, 0.4685, 0.4751, 0.4742,\n",
            "        0.4786, 0.4775, 0.4745, 0.4784, 0.4775, 0.4742, 0.4766, 0.4685, 0.4753,\n",
            "        0.4685, 0.4766, 0.4765, 0.4665, 0.4742, 0.4752, 0.4776, 0.4752, 0.4763,\n",
            "        0.4716, 0.4747, 0.4765, 0.4786, 0.4770, 0.4747, 0.4750, 0.4744, 0.4762,\n",
            "        0.4691, 0.4750, 0.4763, 0.4760, 0.4808, 0.4760, 0.4747, 0.4749, 0.4728,\n",
            "        0.4773])\n",
            "Sum of predicted tensor(30.3860)\n",
            "Loss tensor(0.6916)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4756, 0.4752, 0.4760, 0.4753, 0.4786, 0.4776, 0.4760, 0.4727, 0.4728,\n",
            "        0.4766, 0.4729, 0.4789, 0.4766, 0.4739, 0.4710, 0.4732, 0.4749, 0.4741,\n",
            "        0.4775, 0.4705, 0.4703, 0.4776, 0.4760, 0.4710, 0.4758, 0.4743, 0.4802,\n",
            "        0.4765, 0.4758, 0.4766, 0.4714, 0.4782, 0.4766, 0.4754, 0.4750, 0.4717,\n",
            "        0.4761, 0.4766, 0.4756, 0.4757, 0.4752, 0.4743, 0.4744, 0.4783, 0.4756,\n",
            "        0.4750, 0.4802, 0.4747, 0.4746, 0.4743, 0.4763, 0.4717, 0.4762, 0.4741,\n",
            "        0.4739, 0.4701, 0.4746, 0.4764, 0.4730, 0.4781, 0.4751, 0.4721, 0.4795,\n",
            "        0.4775])\n",
            "Sum of predicted tensor(30.4118)\n",
            "Loss tensor(0.7060)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4723, 0.4739, 0.4759, 0.4737, 0.4725, 0.4753, 0.4772, 0.4751, 0.4756,\n",
            "        0.4741, 0.4800, 0.4760, 0.4758, 0.4752, 0.4739, 0.4722, 0.4763, 0.4752,\n",
            "        0.4757, 0.4751, 0.4751, 0.4702, 0.4784, 0.4753, 0.4710, 0.4775, 0.4723,\n",
            "        0.4709, 0.4835, 0.4738, 0.4742, 0.4764, 0.4740, 0.4750, 0.4723, 0.4775,\n",
            "        0.4760, 0.4717, 0.4764, 0.4746, 0.4748, 0.4708, 0.4695, 0.4745, 0.4766,\n",
            "        0.4766, 0.4756, 0.4717, 0.4761, 0.4685, 0.4741, 0.4727, 0.4737, 0.4765,\n",
            "        0.4725, 0.4766, 0.4755, 0.4808, 0.4752, 0.4757, 0.4735, 0.4741, 0.4742,\n",
            "        0.4714])\n",
            "Sum of predicted tensor(30.3783)\n",
            "Loss tensor(0.6876)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4760, 0.4775, 0.4715, 0.4763, 0.4808, 0.4795, 0.4692, 0.4771, 0.4704,\n",
            "        0.4785, 0.4745, 0.4759, 0.4767, 0.4778, 0.4783, 0.4766, 0.4736, 0.4729,\n",
            "        0.4816, 0.4738, 0.4723, 0.4781, 0.4804, 0.4791, 0.4776, 0.4759, 0.4731,\n",
            "        0.4736, 0.4727, 0.4748, 0.4723, 0.4731, 0.4736, 0.4692, 0.4759, 0.4754,\n",
            "        0.4751, 0.4689, 0.4736, 0.4732, 0.4707, 0.4782, 0.4771, 0.4763, 0.4728,\n",
            "        0.4700, 0.4753, 0.4756, 0.4723, 0.4773, 0.4721, 0.4759, 0.4762, 0.4729,\n",
            "        0.4697, 0.4722, 0.4698, 0.4734, 0.4727, 0.4762, 0.4759, 0.4722, 0.4762,\n",
            "        0.4778])\n",
            "Sum of predicted tensor(30.3851)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4766, 0.4793, 0.4731, 0.4748, 0.4727, 0.4762, 0.4746, 0.4702, 0.4733,\n",
            "        0.4751, 0.4793, 0.4767, 0.4736, 0.4784, 0.4761, 0.4757, 0.4736, 0.4729,\n",
            "        0.4760, 0.4741, 0.4736, 0.4699, 0.4808, 0.4778, 0.4791, 0.4774, 0.4775,\n",
            "        0.4779, 0.4750, 0.4773, 0.4775, 0.4772, 0.4762, 0.4768, 0.4760, 0.4727,\n",
            "        0.4715, 0.4750, 0.4774, 0.4750, 0.4745, 0.4745, 0.4761, 0.4751, 0.4762,\n",
            "        0.4751, 0.4728, 0.4782, 0.4771, 0.4754, 0.4763, 0.4700, 0.4726, 0.4736,\n",
            "        0.4766, 0.4731, 0.4736, 0.4750, 0.4740, 0.4765, 0.4790, 0.4763, 0.4808,\n",
            "        0.4763])\n",
            "Sum of predicted tensor(30.4329)\n",
            "Loss tensor(0.6900)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4781, 0.4757, 0.4757, 0.4748, 0.4739, 0.4782, 0.4739, 0.4728, 0.4757,\n",
            "        0.4821, 0.4786, 0.4742, 0.4758, 0.4759, 0.4785, 0.4736, 0.4728, 0.4775,\n",
            "        0.4734, 0.4745, 0.4761, 0.4772, 0.4744, 0.4786, 0.4761, 0.4734, 0.4785,\n",
            "        0.4756, 0.4750, 0.4818, 0.4752, 0.4757, 0.4779, 0.4775, 0.4775, 0.4776,\n",
            "        0.4796, 0.4744, 0.4722, 0.4753, 0.4782, 0.4743, 0.4741, 0.4793, 0.4779,\n",
            "        0.4742, 0.4776, 0.4786, 0.4745, 0.4730, 0.4758, 0.4772, 0.4774, 0.4739,\n",
            "        0.4712, 0.4758, 0.4707, 0.4670, 0.4760, 0.4808, 0.4774, 0.4766, 0.4783,\n",
            "        0.4780])\n",
            "Sum of predicted tensor(30.4632)\n",
            "Loss tensor(0.6865)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4699, 0.4715, 0.4733, 0.4756, 0.4793, 0.4719, 0.4745, 0.4760, 0.4732,\n",
            "        0.4751, 0.4760, 0.4770, 0.4804, 0.4736, 0.4808, 0.4731, 0.4780, 0.4758,\n",
            "        0.4808, 0.4761, 0.4736, 0.4741, 0.4746, 0.4729, 0.4752, 0.4795, 0.4770,\n",
            "        0.4736, 0.4736, 0.4790, 0.4740, 0.4760, 0.4736, 0.4736, 0.4720, 0.4748,\n",
            "        0.4739, 0.4756, 0.4731, 0.4751, 0.4805, 0.4758, 0.4747, 0.4804, 0.4754,\n",
            "        0.4757, 0.4728, 0.4765, 0.4776, 0.4757, 0.4764, 0.4728, 0.4745, 0.4718,\n",
            "        0.4721, 0.4742, 0.4748, 0.4739, 0.4748, 0.4724, 0.4760, 0.4740, 0.4775,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4117)\n",
            "Loss tensor(0.6996)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4787, 0.4754, 0.4757, 0.4758, 0.4778, 0.4743, 0.4766, 0.4729, 0.4747,\n",
            "        0.4740, 0.4747, 0.4786, 0.4729, 0.4751, 0.4755, 0.4798, 0.4723, 0.4751,\n",
            "        0.4721, 0.4768, 0.4778, 0.4703, 0.4779, 0.4709, 0.4746, 0.4731, 0.4736,\n",
            "        0.4768, 0.4761, 0.4742, 0.4810, 0.4772, 0.4770, 0.4768, 0.4761, 0.4736,\n",
            "        0.4723, 0.4736, 0.4767, 0.4730, 0.4767, 0.4738, 0.4725, 0.4764, 0.4741,\n",
            "        0.4747, 0.4777, 0.4740, 0.4791, 0.4763, 0.4765, 0.4777, 0.4743, 0.4689,\n",
            "        0.4734, 0.4734, 0.4761, 0.4736, 0.4801, 0.4756, 0.4743, 0.4764, 0.4742,\n",
            "        0.4804])\n",
            "Sum of predicted tensor(30.4217)\n",
            "Loss tensor(0.6945)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4721, 0.4744, 0.4710, 0.4741, 0.4785, 0.4753, 0.4789, 0.4762, 0.4749,\n",
            "        0.4744, 0.4734, 0.4781, 0.4744, 0.4738, 0.4736, 0.4794, 0.4793, 0.4733,\n",
            "        0.4742, 0.4761, 0.4751, 0.4733, 0.4729, 0.4761, 0.4751, 0.4727, 0.4728,\n",
            "        0.4749, 0.4751, 0.4732, 0.4738, 0.4748, 0.4725, 0.4746, 0.4714, 0.4784,\n",
            "        0.4761, 0.4780, 0.4755, 0.4783, 0.4758, 0.4778, 0.4747, 0.4749, 0.4780,\n",
            "        0.4751, 0.4793, 0.4760, 0.4722, 0.4705, 0.4743, 0.4789, 0.4740, 0.4761,\n",
            "        0.4759, 0.4769, 0.4755, 0.4777, 0.4801, 0.4778, 0.4738, 0.4754, 0.4746,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4212)\n",
            "Loss tensor(0.6912)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4767, 0.4803, 0.4745, 0.4729, 0.4751, 0.4796, 0.4770, 0.4829, 0.4739,\n",
            "        0.4736, 0.4752, 0.4722, 0.4766, 0.4783, 0.4725, 0.4786, 0.4762, 0.4753,\n",
            "        0.4751, 0.4798, 0.4821, 0.4744, 0.4745, 0.4783, 0.4719, 0.4705, 0.4761,\n",
            "        0.4774, 0.4810, 0.4770, 0.4745, 0.4786, 0.4769, 0.4779, 0.4764, 0.4779,\n",
            "        0.4768, 0.4758, 0.4711, 0.4751, 0.4785, 0.4718, 0.4756, 0.4764, 0.4748,\n",
            "        0.4785, 0.4766, 0.4761, 0.4760, 0.4731, 0.4736, 0.4801, 0.4703, 0.4765,\n",
            "        0.4761, 0.4745, 0.4782, 0.4761, 0.4761, 0.4809, 0.4697, 0.4769, 0.4703,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4637)\n",
            "Loss tensor(0.6876)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4761, 0.4736, 0.4762, 0.4759, 0.4807, 0.4746, 0.4757, 0.4764, 0.4740,\n",
            "        0.4736, 0.4745, 0.4750, 0.4728, 0.4752, 0.4700, 0.4761, 0.4745, 0.4752,\n",
            "        0.4751, 0.4769, 0.4748, 0.4758, 0.4763, 0.4764, 0.4707, 0.4703, 0.4756,\n",
            "        0.4762, 0.4761, 0.4736, 0.4819, 0.4789, 0.4767, 0.4805, 0.4757, 0.4755,\n",
            "        0.4731, 0.4759, 0.4772, 0.4734, 0.4782, 0.4757, 0.4764, 0.4733, 0.4752,\n",
            "        0.4757, 0.4708, 0.4744, 0.4783, 0.4766, 0.4759, 0.4740, 0.4751, 0.4752,\n",
            "        0.4742, 0.4778, 0.4726, 0.4731, 0.4814, 0.4755, 0.4745, 0.4745, 0.4736,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4240)\n",
            "Loss tensor(0.6929)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4785, 0.4753, 0.4728, 0.4736, 0.4752, 0.4763, 0.4765, 0.4747, 0.4786,\n",
            "        0.4774, 0.4757, 0.4749, 0.4833, 0.4710, 0.4759, 0.4724, 0.4736, 0.4716,\n",
            "        0.4739, 0.4786, 0.4722, 0.4728, 0.4749, 0.4829, 0.4792, 0.4777, 0.4790,\n",
            "        0.4782, 0.4743, 0.4755, 0.4736, 0.4733, 0.4817, 0.4759, 0.4777, 0.4722,\n",
            "        0.4779, 0.4765, 0.4764, 0.4786, 0.4775, 0.4758, 0.4736, 0.4771, 0.4758,\n",
            "        0.4736, 0.4776, 0.4697, 0.4736, 0.4747, 0.4745, 0.4758, 0.4739, 0.4772,\n",
            "        0.4743, 0.4745, 0.4763, 0.4753, 0.4783, 0.4753, 0.4747, 0.4728, 0.4772,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4439)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4729, 0.4719, 0.4729, 0.4766, 0.4736, 0.4786, 0.4728, 0.4747, 0.4718,\n",
            "        0.4710, 0.4745, 0.4780, 0.4801, 0.4750, 0.4808, 0.4684, 0.4786, 0.4729,\n",
            "        0.4766, 0.4751, 0.4757, 0.4738, 0.4736, 0.4744, 0.4728, 0.4746, 0.4808,\n",
            "        0.4767, 0.4755, 0.4806, 0.4774, 0.4720, 0.4747, 0.4761, 0.4751, 0.4748,\n",
            "        0.4762, 0.4766, 0.4726, 0.4733, 0.4790, 0.4748, 0.4713, 0.4820, 0.4711,\n",
            "        0.4727, 0.4744, 0.4751, 0.4716, 0.4708, 0.4747, 0.4779, 0.4793, 0.4770,\n",
            "        0.4761, 0.4798, 0.4763, 0.4763, 0.4736, 0.4761, 0.4779, 0.4753, 0.4760,\n",
            "        0.4729])\n",
            "Sum of predicted tensor(30.4160)\n",
            "Loss tensor(0.6895)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4770, 0.4764, 0.4764, 0.4695, 0.4779, 0.4759, 0.4714, 0.4785, 0.4756,\n",
            "        0.4725, 0.4722, 0.4757, 0.4736, 0.4742, 0.4753, 0.4732, 0.4750, 0.4780,\n",
            "        0.4720, 0.4745, 0.4797, 0.4748, 0.4797, 0.4750, 0.4736, 0.4771, 0.4785,\n",
            "        0.4736, 0.4753, 0.4770, 0.4739, 0.4739, 0.4748, 0.4705, 0.4753, 0.4713,\n",
            "        0.4778, 0.4697, 0.4736, 0.4727, 0.4729, 0.4759, 0.4722, 0.4829, 0.4752,\n",
            "        0.4727, 0.4734, 0.4759, 0.4751, 0.4761, 0.4755, 0.4782, 0.4791, 0.4771,\n",
            "        0.4762, 0.4780, 0.4755, 0.4749, 0.4742, 0.4760, 0.4748, 0.4744, 0.4744,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4091)\n",
            "Loss tensor(0.6938)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4778, 0.4700, 0.4766, 0.4752, 0.4734, 0.4772, 0.4768, 0.4736, 0.4766,\n",
            "        0.4811, 0.4771, 0.4761, 0.4718, 0.4753, 0.4767, 0.4772, 0.4779, 0.4734,\n",
            "        0.4760, 0.4703, 0.4785, 0.4736, 0.4734, 0.4751, 0.4736, 0.4762, 0.4777,\n",
            "        0.4788, 0.4736, 0.4719, 0.4697, 0.4716, 0.4726, 0.4750, 0.4761, 0.4745,\n",
            "        0.4764, 0.4729, 0.4736, 0.4763, 0.4744, 0.4732, 0.4741, 0.4806, 0.4740,\n",
            "        0.4774, 0.4763, 0.4782, 0.4759, 0.4764, 0.4731, 0.4774, 0.4721, 0.4731,\n",
            "        0.4783, 0.4746, 0.4755, 0.4751, 0.4783, 0.4759, 0.4795, 0.4796, 0.4725,\n",
            "        0.4784])\n",
            "Sum of predicted tensor(30.4251)\n",
            "Loss tensor(0.6963)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4766, 0.4702, 0.4761, 0.4766, 0.4771, 0.4718, 0.4736, 0.4761, 0.4783,\n",
            "        0.4761, 0.4720, 0.4765, 0.4755, 0.4784, 0.4785, 0.4762, 0.4761, 0.4752,\n",
            "        0.4821, 0.4748, 0.4723, 0.4766, 0.4783, 0.4732, 0.4745, 0.4704, 0.4756,\n",
            "        0.4762, 0.4736, 0.4718, 0.4727, 0.4711, 0.4751, 0.4769, 0.4761, 0.4736,\n",
            "        0.4782, 0.4714, 0.4742, 0.4770, 0.4735, 0.4718, 0.4779, 0.4736, 0.4766,\n",
            "        0.4757, 0.4727, 0.4779, 0.4749, 0.4797, 0.4746, 0.4765, 0.4764, 0.4770,\n",
            "        0.4760, 0.4742, 0.4725, 0.4791, 0.4739, 0.4689, 0.4752, 0.4680, 0.4734,\n",
            "        0.4774])\n",
            "Sum of predicted tensor(30.4042)\n",
            "Loss tensor(0.6911)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4736, 0.4729, 0.4804, 0.4727, 0.4752, 0.4748, 0.4736, 0.4767, 0.4771,\n",
            "        0.4719, 0.4761, 0.4785, 0.4769, 0.4784, 0.4771, 0.4756, 0.4771, 0.4780,\n",
            "        0.4779, 0.4767, 0.4761, 0.4721, 0.4779, 0.4759, 0.4740, 0.4748, 0.4722,\n",
            "        0.4768, 0.4797, 0.4768, 0.4719, 0.4736, 0.4745, 0.4757, 0.4761, 0.4744,\n",
            "        0.4776, 0.4736, 0.4742, 0.4755, 0.4692, 0.4746, 0.4737, 0.4751, 0.4783,\n",
            "        0.4755, 0.4769, 0.4705, 0.4750, 0.4781, 0.4760, 0.4740, 0.4822, 0.4714,\n",
            "        0.4758, 0.4794, 0.4717, 0.4764, 0.4764, 0.4730, 0.4761, 0.4748, 0.4700,\n",
            "        0.4752])\n",
            "Sum of predicted tensor(30.4236)\n",
            "Loss tensor(0.7013)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4780, 0.4741, 0.4767, 0.4736, 0.4737, 0.4785, 0.4771, 0.4745, 0.4746,\n",
            "        0.4709, 0.4769, 0.4753, 0.4729, 0.4763, 0.4736, 0.4781, 0.4752, 0.4766,\n",
            "        0.4758, 0.4754, 0.4766, 0.4710, 0.4781, 0.4773, 0.4759, 0.4795, 0.4760,\n",
            "        0.4742, 0.4780, 0.4763, 0.4761, 0.4724, 0.4750, 0.4761, 0.4714, 0.4761,\n",
            "        0.4765, 0.4745, 0.4744, 0.4786, 0.4762, 0.4779, 0.4797, 0.4782, 0.4747,\n",
            "        0.4776, 0.4746, 0.4760, 0.4737, 0.4791, 0.4817, 0.4760, 0.4745, 0.4779,\n",
            "        0.4742, 0.4714, 0.4783, 0.4761, 0.4756, 0.4727, 0.4773, 0.4750, 0.4751,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4487)\n",
            "Loss tensor(0.6967)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4738, 0.4761, 0.4718, 0.4758, 0.4804, 0.4741, 0.4730, 0.4729, 0.4730,\n",
            "        0.4756, 0.4736, 0.4771, 0.4740, 0.4752, 0.4741, 0.4719, 0.4736, 0.4806,\n",
            "        0.4755, 0.4723, 0.4751, 0.4731, 0.4736, 0.4736, 0.4754, 0.4761, 0.4692,\n",
            "        0.4760, 0.4737, 0.4708, 0.4786, 0.4723, 0.4774, 0.4699, 0.4750, 0.4729,\n",
            "        0.4782, 0.4776, 0.4756, 0.4770, 0.4739, 0.4804, 0.4789, 0.4795, 0.4740,\n",
            "        0.4732, 0.4768, 0.4741, 0.4767, 0.4761, 0.4736, 0.4739, 0.4806, 0.4736,\n",
            "        0.4757, 0.4777, 0.4743, 0.4736, 0.4689, 0.4723, 0.4732, 0.4767, 0.4779,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.3957)\n",
            "Loss tensor(0.7093)\n",
            "Acc tensor(0.3438)\n",
            "tensor([0.4761, 0.4754, 0.4799, 0.4742, 0.4839, 0.4786, 0.4771, 0.4812, 0.4802,\n",
            "        0.4700, 0.4731, 0.4752, 0.4768, 0.4762, 0.4739, 0.4772, 0.4714, 0.4757,\n",
            "        0.4751, 0.4734, 0.4743, 0.4774, 0.4756, 0.4749, 0.4784, 0.4710, 0.4749,\n",
            "        0.4721, 0.4757, 0.4753, 0.4793, 0.4742, 0.4735, 0.4720, 0.4762, 0.4755,\n",
            "        0.4726, 0.4747, 0.4791, 0.4751, 0.4697, 0.4740, 0.4725, 0.4802, 0.4839,\n",
            "        0.4715, 0.4750, 0.4753, 0.4827, 0.4735, 0.4766, 0.4742, 0.4755, 0.4795,\n",
            "        0.4694, 0.4772, 0.4713, 0.4775, 0.4786, 0.4741, 0.4774, 0.4715, 0.4724,\n",
            "        0.4703])\n",
            "Sum of predicted tensor(30.4305)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4770, 0.4758, 0.4724, 0.4781, 0.4761, 0.4768, 0.4774, 0.4748, 0.4775,\n",
            "        0.4713, 0.4675, 0.4753, 0.4726, 0.4675, 0.4761, 0.4793, 0.4744, 0.4747,\n",
            "        0.4735, 0.4817, 0.4766, 0.4760, 0.4694, 0.4774, 0.4741, 0.4795, 0.4722,\n",
            "        0.4784, 0.4748, 0.4774, 0.4803, 0.4776, 0.4801, 0.4739, 0.4730, 0.4747,\n",
            "        0.4717, 0.4762, 0.4720, 0.4711, 0.4772, 0.4764, 0.4718, 0.4761, 0.4768,\n",
            "        0.4733, 0.4775, 0.4748, 0.4717, 0.4738, 0.4720, 0.4732, 0.4720, 0.4747,\n",
            "        0.4737, 0.4745, 0.4748, 0.4706, 0.4708, 0.4766, 0.4733, 0.4755, 0.4714,\n",
            "        0.4788])\n",
            "Sum of predicted tensor(30.3876)\n",
            "Loss tensor(0.6879)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4763, 0.4804, 0.4796, 0.4749, 0.4799, 0.4755, 0.4723, 0.4768, 0.4750,\n",
            "        0.4746, 0.4790, 0.4757, 0.4730, 0.4754, 0.4767, 0.4742, 0.4772, 0.4737,\n",
            "        0.4714, 0.4754, 0.4725, 0.4806, 0.4766, 0.4739, 0.4760, 0.4768, 0.4789,\n",
            "        0.4772, 0.4767, 0.4743, 0.4720, 0.4750, 0.4712, 0.4794, 0.4769, 0.4812,\n",
            "        0.4770, 0.4709, 0.4721, 0.4736, 0.4738, 0.4725, 0.4733, 0.4681, 0.4725,\n",
            "        0.4724, 0.4752, 0.4806, 0.4744, 0.4807, 0.4748, 0.4722, 0.4743, 0.4738,\n",
            "        0.4762, 0.4774, 0.4694, 0.4780, 0.4752, 0.4750, 0.4770, 0.4732, 0.4754,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4196)\n",
            "Loss tensor(0.6893)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4770, 0.4706, 0.4788, 0.4749, 0.4775, 0.4774, 0.4755, 0.4785, 0.4746,\n",
            "        0.4734, 0.4766, 0.4720, 0.4747, 0.4764, 0.4808, 0.4792, 0.4729, 0.4767,\n",
            "        0.4768, 0.4711, 0.4735, 0.4761, 0.4739, 0.4797, 0.4726, 0.4757, 0.4764,\n",
            "        0.4718, 0.4756, 0.4763, 0.4725, 0.4804, 0.4715, 0.4759, 0.4760, 0.4715,\n",
            "        0.4779, 0.4793, 0.4763, 0.4761, 0.4773, 0.4720, 0.4778, 0.4737, 0.4740,\n",
            "        0.4726, 0.4789, 0.4782, 0.4806, 0.4746, 0.4725, 0.4740, 0.4717, 0.4764,\n",
            "        0.4762, 0.4812, 0.4725, 0.4757, 0.4758, 0.4759, 0.4798, 0.4751, 0.4748,\n",
            "        0.4775])\n",
            "Sum of predicted tensor(30.4431)\n",
            "Loss tensor(0.6921)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4752, 0.4735, 0.4774, 0.4711, 0.4752, 0.4735, 0.4770, 0.4747, 0.4746,\n",
            "        0.4735, 0.4786, 0.4728, 0.4768, 0.4799, 0.4765, 0.4720, 0.4736, 0.4770,\n",
            "        0.4796, 0.4732, 0.4787, 0.4763, 0.4760, 0.4758, 0.4768, 0.4749, 0.4761,\n",
            "        0.4753, 0.4763, 0.4768, 0.4754, 0.4751, 0.4727, 0.4780, 0.4781, 0.4754,\n",
            "        0.4829, 0.4740, 0.4754, 0.4768, 0.4739, 0.4709, 0.4736, 0.4768, 0.4786,\n",
            "        0.4732, 0.4785, 0.4774, 0.4779, 0.4744, 0.4795, 0.4745, 0.4781, 0.4751,\n",
            "        0.4755, 0.4764, 0.4723, 0.4709, 0.4734, 0.4736, 0.4777, 0.4762, 0.4752,\n",
            "        0.4759])\n",
            "Sum of predicted tensor(30.4421)\n",
            "Loss tensor(0.6944)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4806, 0.4758, 0.4799, 0.4684, 0.4697, 0.4760, 0.4742, 0.4743, 0.4732,\n",
            "        0.4747, 0.4720, 0.4727, 0.4728, 0.4774, 0.4740, 0.4738, 0.4780, 0.4757,\n",
            "        0.4720, 0.4724, 0.4782, 0.4730, 0.4767, 0.4744, 0.4743, 0.4770, 0.4751,\n",
            "        0.4765, 0.4724, 0.4766, 0.4757, 0.4763, 0.4722, 0.4763, 0.4749, 0.4765,\n",
            "        0.4759, 0.4779, 0.4772, 0.4756, 0.4765, 0.4741, 0.4799, 0.4768, 0.4739,\n",
            "        0.4760, 0.4775, 0.4740, 0.4721, 0.4806, 0.4755, 0.4746, 0.4741, 0.4764,\n",
            "        0.4786, 0.4741, 0.4739, 0.4757, 0.4778, 0.4762, 0.4739, 0.4803, 0.4759,\n",
            "        0.4757])\n",
            "Sum of predicted tensor(30.4244)\n",
            "Loss tensor(0.6992)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4787, 0.4737, 0.4731, 0.4761, 0.4799, 0.4770, 0.4681, 0.4799, 0.4756,\n",
            "        0.4775, 0.4776, 0.4839, 0.4762, 0.4785, 0.4781, 0.4761, 0.4706, 0.4758,\n",
            "        0.4706, 0.4790, 0.4739, 0.4743, 0.4806, 0.4749, 0.4717, 0.4776, 0.4774,\n",
            "        0.4767, 0.4747, 0.4739, 0.4769, 0.4746, 0.4764, 0.4718, 0.4768, 0.4812,\n",
            "        0.4684, 0.4757, 0.4711, 0.4722, 0.4774, 0.4776, 0.4769, 0.4754, 0.4740,\n",
            "        0.4774, 0.4751, 0.4748, 0.4746, 0.4739, 0.4766, 0.4725, 0.4773, 0.4736,\n",
            "        0.4749, 0.4755, 0.4750, 0.4754, 0.4762, 0.4757, 0.4776, 0.4739, 0.4725,\n",
            "        0.4774])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4754, 0.4766, 0.4808, 0.4764, 0.4794, 0.4795, 0.4747, 0.4744, 0.4763,\n",
            "        0.4760, 0.4721, 0.4773, 0.4741, 0.4827, 0.4766, 0.4812, 0.4764, 0.4703,\n",
            "        0.4759, 0.4751, 0.4716, 0.4752, 0.4726, 0.4759, 0.4759, 0.4766, 0.4783,\n",
            "        0.4749, 0.4784, 0.4745, 0.4723, 0.4730, 0.4775, 0.4761, 0.4799, 0.4721,\n",
            "        0.4745, 0.4780, 0.4748, 0.4812, 0.4780, 0.4753, 0.4737, 0.4790, 0.4721,\n",
            "        0.4746, 0.4777, 0.4839, 0.4706, 0.4788, 0.4748, 0.4720, 0.4715, 0.4817,\n",
            "        0.4725, 0.4736, 0.4752, 0.4694, 0.4702, 0.4741, 0.4756, 0.4745, 0.4774,\n",
            "        0.4739])\n",
            "Sum of predicted tensor(30.4446)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4740, 0.4762, 0.4684, 0.4722, 0.4781, 0.4742, 0.4799, 0.4750, 0.4786,\n",
            "        0.4722, 0.4761, 0.4752, 0.4729, 0.4774, 0.4795, 0.4720, 0.4732, 0.4757,\n",
            "        0.4794, 0.4747, 0.4726, 0.4762, 0.4754, 0.4720, 0.4746, 0.4762, 0.4734,\n",
            "        0.4790, 0.4752, 0.4757, 0.4733, 0.4787, 0.4766, 0.4694, 0.4716, 0.4767,\n",
            "        0.4757, 0.4726, 0.4743, 0.4744, 0.4772, 0.4738, 0.4750, 0.4770, 0.4749,\n",
            "        0.4712, 0.4752, 0.4786, 0.4743, 0.4735, 0.4753, 0.4736, 0.4746, 0.4731,\n",
            "        0.4757, 0.4778, 0.4759, 0.4758, 0.4764, 0.4779, 0.4765, 0.4769, 0.4771,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4129)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4754, 0.4783, 0.4764, 0.4711, 0.4722, 0.4765, 0.4812, 0.4753, 0.4729,\n",
            "        0.4771, 0.4772, 0.4735, 0.4757, 0.4780, 0.4757, 0.4839, 0.4769, 0.4745,\n",
            "        0.4712, 0.4768, 0.4738, 0.4739, 0.4725, 0.4728, 0.4743, 0.4777, 0.4746,\n",
            "        0.4725, 0.4770, 0.4715, 0.4768, 0.4779, 0.4762, 0.4812, 0.4778, 0.4718,\n",
            "        0.4765, 0.4754, 0.4737, 0.4786, 0.4720, 0.4776, 0.4735, 0.4725, 0.4781,\n",
            "        0.4742, 0.4741, 0.4741, 0.4741, 0.4770, 0.4749, 0.4768, 0.4812, 0.4812,\n",
            "        0.4703, 0.4739, 0.4720, 0.4709, 0.4785, 0.4753, 0.4752, 0.4756, 0.4763,\n",
            "        0.4684])\n",
            "Sum of predicted tensor(30.4240)\n",
            "Loss tensor(0.6907)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4745, 0.4742, 0.4796, 0.4759, 0.4751, 0.4744, 0.4761, 0.4743, 0.4723,\n",
            "        0.4776, 0.4735, 0.4757, 0.4811, 0.4784, 0.4801, 0.4735, 0.4780, 0.4765,\n",
            "        0.4726, 0.4799, 0.4761, 0.4743, 0.4769, 0.4722, 0.4757, 0.4735, 0.4757,\n",
            "        0.4790, 0.4773, 0.4778, 0.4762, 0.4721, 0.4761, 0.4762, 0.4717, 0.4777,\n",
            "        0.4739, 0.4746, 0.4754, 0.4764, 0.4790, 0.4780, 0.4760, 0.4733, 0.4767,\n",
            "        0.4710, 0.4748, 0.4770, 0.4771, 0.4747, 0.4720, 0.4744, 0.4715, 0.4715,\n",
            "        0.4737, 0.4755, 0.4760, 0.4703, 0.4735, 0.4758, 0.4748, 0.4736, 0.4812,\n",
            "        0.4780])\n",
            "Sum of predicted tensor(30.4313)\n",
            "Loss tensor(0.7024)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4763, 0.4724, 0.4743, 0.4769, 0.4697, 0.4770, 0.4751, 0.4768, 0.4735,\n",
            "        0.4697, 0.4769, 0.4761, 0.4727, 0.4725, 0.4770, 0.4742, 0.4745, 0.4749,\n",
            "        0.4725, 0.4748, 0.4712, 0.4753, 0.4776, 0.4715, 0.4722, 0.4754, 0.4759,\n",
            "        0.4761, 0.4733, 0.4785, 0.4747, 0.4775, 0.4757, 0.4813, 0.4767, 0.4760,\n",
            "        0.4767, 0.4694, 0.4740, 0.4749, 0.4741, 0.4741, 0.4733, 0.4766, 0.4738,\n",
            "        0.4772, 0.4751, 0.4694, 0.4709, 0.4694, 0.4735, 0.4803, 0.4780, 0.4746,\n",
            "        0.4728, 0.4734, 0.4793, 0.4790, 0.4775, 0.4724, 0.4711, 0.4722, 0.4720,\n",
            "        0.4763])\n",
            "Sum of predicted tensor(30.3779)\n",
            "Loss tensor(0.6974)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4790, 0.4749, 0.4787, 0.4720, 0.4757, 0.4693, 0.4719, 0.4735, 0.4729,\n",
            "        0.4766, 0.4748, 0.4765, 0.4804, 0.4761, 0.4755, 0.4773, 0.4770, 0.4725,\n",
            "        0.4734, 0.4791, 0.4759, 0.4725, 0.4761, 0.4725, 0.4768, 0.4755, 0.4741,\n",
            "        0.4769, 0.4730, 0.4737, 0.4799, 0.4761, 0.4778, 0.4736, 0.4775, 0.4757,\n",
            "        0.4739, 0.4763, 0.4785, 0.4736, 0.4766, 0.4782, 0.4731, 0.4762, 0.4737,\n",
            "        0.4781, 0.4769, 0.4749, 0.4706, 0.4762, 0.4763, 0.4783, 0.4675, 0.4718,\n",
            "        0.4810, 0.4744, 0.4790, 0.4758, 0.4747, 0.4799, 0.4717, 0.4768, 0.4758,\n",
            "        0.4776])\n",
            "Sum of predicted tensor(30.4324)\n",
            "Loss tensor(0.6998)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4775, 0.4799, 0.4720, 0.4752, 0.4759, 0.4728, 0.4745, 0.4747, 0.4737,\n",
            "        0.4703, 0.4793, 0.4768, 0.4774, 0.4781, 0.4780, 0.4744, 0.4715, 0.4742,\n",
            "        0.4775, 0.4744, 0.4735, 0.4677, 0.4745, 0.4740, 0.4738, 0.4768, 0.4736,\n",
            "        0.4726, 0.4797, 0.4746, 0.4768, 0.4766, 0.4754, 0.4745, 0.4754, 0.4734,\n",
            "        0.4799, 0.4748, 0.4757, 0.4722, 0.4743, 0.4735, 0.4762, 0.4728, 0.4757,\n",
            "        0.4736, 0.4742, 0.4715, 0.4747, 0.4739, 0.4780, 0.4745, 0.4743, 0.4737,\n",
            "        0.4794, 0.4761, 0.4779, 0.4723, 0.4747, 0.4796, 0.4768, 0.4757, 0.4733,\n",
            "        0.4764])\n",
            "Sum of predicted tensor(30.4068)\n",
            "Loss tensor(0.7021)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4721, 0.4787, 0.4749, 0.4754, 0.4764, 0.4732, 0.4731, 0.4715, 0.4763,\n",
            "        0.4708, 0.4754, 0.4769, 0.4735, 0.4803, 0.4743, 0.4797, 0.4760, 0.4786,\n",
            "        0.4776, 0.4747, 0.4711, 0.4799, 0.4766, 0.4764, 0.4719, 0.4761, 0.4768,\n",
            "        0.4699, 0.4739, 0.4755, 0.4799, 0.4752, 0.4780, 0.4725, 0.4736, 0.4750,\n",
            "        0.4714, 0.4798, 0.4754, 0.4781, 0.4797, 0.4770, 0.4733, 0.4799, 0.4720,\n",
            "        0.4743, 0.4726, 0.4779, 0.4747, 0.4748, 0.4737, 0.4723, 0.4759, 0.4744,\n",
            "        0.4754, 0.4725, 0.4810, 0.4720, 0.4749, 0.4795, 0.4736, 0.4725, 0.4745,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4174)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4776, 0.4770, 0.4760, 0.4725, 0.4755, 0.4717, 0.4737, 0.4761, 0.4749,\n",
            "        0.4735, 0.4769, 0.4777, 0.4745, 0.4740, 0.4790, 0.4753, 0.4739, 0.4751,\n",
            "        0.4786, 0.4763, 0.4744, 0.4706, 0.4761, 0.4773, 0.4745, 0.4709, 0.4763,\n",
            "        0.4806, 0.4788, 0.4742, 0.4725, 0.4784, 0.4758, 0.4776, 0.4760, 0.4776,\n",
            "        0.4742, 0.4758, 0.4764, 0.4736, 0.4749, 0.4816, 0.4766, 0.4839, 0.4770,\n",
            "        0.4717, 0.4788, 0.4710, 0.4753, 0.4701, 0.4799, 0.4734, 0.4720, 0.4788,\n",
            "        0.4786, 0.4770, 0.4740, 0.4778, 0.4735, 0.4777, 0.4758, 0.4725, 0.4803,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4469)\n",
            "Loss tensor(0.6950)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4723, 0.4730, 0.4763, 0.4788, 0.4678, 0.4737, 0.4778, 0.4745, 0.4768,\n",
            "        0.4729, 0.4750, 0.4768, 0.4781, 0.4810, 0.4754, 0.4802, 0.4746, 0.4758,\n",
            "        0.4752, 0.4761, 0.4834, 0.4728, 0.4717, 0.4736, 0.4777, 0.4746, 0.4696,\n",
            "        0.4788, 0.4771, 0.4724, 0.4799, 0.4757, 0.4745, 0.4775, 0.4775, 0.4772,\n",
            "        0.4764, 0.4732, 0.4760, 0.4778, 0.4754, 0.4733, 0.4761, 0.4786, 0.4766,\n",
            "        0.4722, 0.4722, 0.4721, 0.4756, 0.4734, 0.4691, 0.4751, 0.4795, 0.4762,\n",
            "        0.4747, 0.4739, 0.4767, 0.4766, 0.4749, 0.4765, 0.4750, 0.4735, 0.4757,\n",
            "        0.4726])\n",
            "Sum of predicted tensor(30.4250)\n",
            "Loss tensor(0.6867)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4752, 0.4771, 0.4783, 0.4773, 0.4760, 0.4733, 0.4758, 0.4766, 0.4805,\n",
            "        0.4750, 0.4727, 0.4806, 0.4764, 0.4705, 0.4799, 0.4758, 0.4737, 0.4760,\n",
            "        0.4727, 0.4711, 0.4722, 0.4760, 0.4774, 0.4757, 0.4748, 0.4738, 0.4776,\n",
            "        0.4745, 0.4738, 0.4793, 0.4757, 0.4698, 0.4759, 0.4807, 0.4778, 0.4736,\n",
            "        0.4813, 0.4788, 0.4708, 0.4757, 0.4754, 0.4770, 0.4752, 0.4751, 0.4706,\n",
            "        0.4760, 0.4759, 0.4794, 0.4705, 0.4756, 0.4767, 0.4768, 0.4751, 0.4780,\n",
            "        0.4798, 0.4763, 0.4784, 0.4753, 0.4777, 0.4771, 0.4687, 0.4709, 0.4794,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4436)\n",
            "Loss tensor(0.7016)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4725, 0.4734, 0.4733, 0.4743, 0.4769, 0.4735, 0.4708, 0.4774, 0.4674,\n",
            "        0.4750, 0.4762, 0.4753, 0.4765, 0.4743, 0.4748, 0.4712, 0.4787, 0.4807,\n",
            "        0.4805, 0.4792, 0.4750, 0.4752, 0.4736, 0.4746, 0.4715, 0.4705, 0.4781,\n",
            "        0.4734, 0.4701, 0.4750, 0.4786, 0.4762, 0.4741, 0.4782, 0.4799, 0.4735,\n",
            "        0.4781, 0.4761, 0.4743, 0.4747, 0.4789, 0.4734, 0.4754, 0.4714, 0.4756,\n",
            "        0.4786, 0.4784, 0.4724, 0.4787, 0.4695, 0.4747, 0.4758, 0.4769, 0.4750,\n",
            "        0.4734, 0.4766, 0.4805, 0.4805, 0.4771, 0.4778, 0.4817, 0.4696, 0.4774,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.4235)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4734, 0.4715, 0.4765, 0.4760, 0.4734, 0.4722, 0.4717, 0.4788, 0.4754,\n",
            "        0.4750, 0.4766, 0.4799, 0.4810, 0.4806, 0.4787, 0.4734, 0.4798, 0.4751,\n",
            "        0.4718, 0.4709, 0.4749, 0.4758, 0.4804, 0.4784, 0.4765, 0.4783, 0.4765,\n",
            "        0.4749, 0.4766, 0.4742, 0.4741, 0.4784, 0.4791, 0.4749, 0.4730, 0.4749,\n",
            "        0.4772, 0.4760, 0.4678, 0.4769, 0.4764, 0.4727, 0.4759, 0.4769, 0.4762,\n",
            "        0.4738, 0.4763, 0.4701, 0.4727, 0.4734, 0.4732, 0.4789, 0.4745, 0.4733,\n",
            "        0.4762, 0.4766, 0.4752, 0.4727, 0.4775, 0.4760, 0.4787, 0.4798, 0.4726,\n",
            "        0.4738])\n",
            "Sum of predicted tensor(30.4338)\n",
            "Loss tensor(0.6990)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4765, 0.4799, 0.4746, 0.4786, 0.4761, 0.4760, 0.4752, 0.4799, 0.4768,\n",
            "        0.4774, 0.4740, 0.4804, 0.4720, 0.4683, 0.4739, 0.4719, 0.4695, 0.4784,\n",
            "        0.4739, 0.4774, 0.4755, 0.4724, 0.4768, 0.4787, 0.4697, 0.4765, 0.4766,\n",
            "        0.4809, 0.4788, 0.4763, 0.4739, 0.4776, 0.4767, 0.4694, 0.4747, 0.4722,\n",
            "        0.4688, 0.4682, 0.4755, 0.4749, 0.4753, 0.4758, 0.4758, 0.4764, 0.4744,\n",
            "        0.4777, 0.4783, 0.4771, 0.4770, 0.4733, 0.4724, 0.4761, 0.4760, 0.4747,\n",
            "        0.4767, 0.4780, 0.4758, 0.4734, 0.4712, 0.4737, 0.4757, 0.4721, 0.4750,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4101)\n",
            "Loss tensor(0.6978)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4742, 0.4747, 0.4721, 0.4798, 0.4760, 0.4770, 0.4744, 0.4757, 0.4708,\n",
            "        0.4774, 0.4783, 0.4759, 0.4769, 0.4744, 0.4745, 0.4778, 0.4774, 0.4711,\n",
            "        0.4698, 0.4770, 0.4755, 0.4776, 0.4817, 0.4753, 0.4794, 0.4771, 0.4766,\n",
            "        0.4733, 0.4745, 0.4739, 0.4759, 0.4760, 0.4766, 0.4728, 0.4774, 0.4712,\n",
            "        0.4749, 0.4732, 0.4732, 0.4805, 0.4722, 0.4767, 0.4760, 0.4769, 0.4751,\n",
            "        0.4773, 0.4808, 0.4725, 0.4754, 0.4760, 0.4749, 0.4775, 0.4712, 0.4805,\n",
            "        0.4799, 0.4755, 0.4683, 0.4760, 0.4710, 0.4727, 0.4734, 0.4808, 0.4810,\n",
            "        0.4805])\n",
            "Sum of predicted tensor(30.4443)\n",
            "Loss tensor(0.6879)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4778, 0.4782, 0.4762, 0.4743, 0.4713, 0.4763, 0.4773, 0.4718, 0.4817,\n",
            "        0.4728, 0.4731, 0.4743, 0.4755, 0.4765, 0.4761, 0.4734, 0.4714, 0.4765,\n",
            "        0.4778, 0.4734, 0.4802, 0.4697, 0.4753, 0.4684, 0.4760, 0.4686, 0.4717,\n",
            "        0.4757, 0.4700, 0.4771, 0.4766, 0.4773, 0.4712, 0.4764, 0.4718, 0.4769,\n",
            "        0.4753, 0.4784, 0.4791, 0.4763, 0.4757, 0.4759, 0.4747, 0.4740, 0.4780,\n",
            "        0.4723, 0.4751, 0.4755, 0.4722, 0.4740, 0.4761, 0.4745, 0.4764, 0.4741,\n",
            "        0.4769, 0.4728, 0.4718, 0.4759, 0.4787, 0.4749, 0.4731, 0.4817, 0.4760,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4013)\n",
            "tensor([0.4786, 0.4740, 0.4753, 0.4714, 0.4764, 0.4745, 0.4731, 0.4762, 0.4766,\n",
            "        0.4786, 0.4799, 0.4754, 0.4754, 0.4724, 0.4754, 0.4754, 0.4750, 0.4788,\n",
            "        0.4764, 0.4749, 0.4773, 0.4739, 0.4744, 0.4798, 0.4731, 0.4761, 0.4734,\n",
            "        0.4701, 0.4739, 0.4723, 0.4700, 0.4757, 0.4780, 0.4698, 0.4745, 0.4794,\n",
            "        0.4754, 0.4768, 0.4763, 0.4784, 0.4751, 0.4754, 0.4750, 0.4717, 0.4769,\n",
            "        0.4789, 0.4733, 0.4769, 0.4766, 0.4758, 0.4779, 0.4781, 0.4807, 0.4721,\n",
            "        0.4760, 0.4800, 0.4763, 0.4723, 0.4788, 0.4759, 0.4782, 0.4688, 0.4765,\n",
            "        0.4800])\n",
            "Sum of predicted tensor(30.4396)\n",
            "Loss tensor(0.6859)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4760, 0.4759, 0.4731, 0.4728, 0.4715, 0.4737, 0.4759, 0.4743, 0.4732,\n",
            "        0.4718, 0.4743, 0.4720, 0.4731, 0.4743, 0.4805, 0.4734, 0.4749, 0.4815,\n",
            "        0.4763, 0.4764, 0.4720, 0.4802, 0.4718, 0.4755, 0.4771, 0.4722, 0.4749,\n",
            "        0.4749, 0.4792, 0.4764, 0.4796, 0.4835, 0.4678, 0.4756, 0.4787, 0.4805,\n",
            "        0.4754, 0.4791, 0.4724, 0.4762, 0.4749, 0.4746, 0.4729, 0.4762, 0.4729,\n",
            "        0.4793, 0.4736, 0.4780, 0.4764, 0.4711, 0.4760, 0.4673, 0.4777, 0.4758,\n",
            "        0.4799, 0.4793, 0.4777, 0.4720, 0.4771, 0.4753, 0.4743, 0.4774, 0.4766,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4277)\n",
            "Loss tensor(0.6887)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4773, 0.4794, 0.4794, 0.4781, 0.4727, 0.4749, 0.4787, 0.4766, 0.4763,\n",
            "        0.4784, 0.4705, 0.4730, 0.4759, 0.4736, 0.4754, 0.4725, 0.4735, 0.4738,\n",
            "        0.4714, 0.4771, 0.4791, 0.4768, 0.4776, 0.4754, 0.4752, 0.4773, 0.4781,\n",
            "        0.4781, 0.4764, 0.4756, 0.4743, 0.4788, 0.4750, 0.4708, 0.4740, 0.4750,\n",
            "        0.4750, 0.4718, 0.4789, 0.4766, 0.4779, 0.4772, 0.4739, 0.4757, 0.4775,\n",
            "        0.4760, 0.4745, 0.4767, 0.4758, 0.4756, 0.4750, 0.4748, 0.4739, 0.4784,\n",
            "        0.4817, 0.4764, 0.4763, 0.4775, 0.4765, 0.4771, 0.4702, 0.4765, 0.4788,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4569)\n",
            "Loss tensor(0.7065)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4747, 0.4759, 0.4758, 0.4729, 0.4739, 0.4743, 0.4780, 0.4739, 0.4761,\n",
            "        0.4771, 0.4712, 0.4751, 0.4760, 0.4763, 0.4755, 0.4805, 0.4697, 0.4740,\n",
            "        0.4755, 0.4740, 0.4765, 0.4737, 0.4756, 0.4768, 0.4817, 0.4766, 0.4728,\n",
            "        0.4787, 0.4739, 0.4765, 0.4747, 0.4746, 0.4748, 0.4683, 0.4743, 0.4763,\n",
            "        0.4811, 0.4732, 0.4746, 0.4771, 0.4783, 0.4735, 0.4765, 0.4747, 0.4766,\n",
            "        0.4796, 0.4756, 0.4773, 0.4744, 0.4752, 0.4793, 0.4799, 0.4694, 0.4728,\n",
            "        0.4713, 0.4778, 0.4722, 0.4750, 0.4819, 0.4746, 0.4760, 0.4817, 0.4749,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4377)\n",
            "Loss tensor(0.6909)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4760, 0.4743, 0.4799, 0.4751, 0.4802, 0.4757, 0.4784, 0.4794, 0.4758,\n",
            "        0.4774, 0.4757, 0.4755, 0.4765, 0.4777, 0.4722, 0.4733, 0.4743, 0.4777,\n",
            "        0.4749, 0.4757, 0.4774, 0.4739, 0.4771, 0.4762, 0.4764, 0.4727, 0.4799,\n",
            "        0.4755, 0.4808, 0.4768, 0.4749, 0.4713, 0.4808, 0.4710, 0.4780, 0.4761,\n",
            "        0.4805, 0.4754, 0.4757, 0.4711, 0.4752, 0.4778, 0.4774, 0.4766, 0.4807,\n",
            "        0.4798, 0.4750, 0.4731, 0.4751, 0.4750, 0.4791, 0.4799, 0.4779, 0.4765,\n",
            "        0.4775, 0.4791, 0.4834, 0.4759, 0.4782, 0.4777, 0.4736, 0.4804, 0.4749,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.5026)\n",
            "Loss tensor(0.6982)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4806, 0.4769, 0.4727, 0.4731, 0.4784, 0.4759, 0.4743, 0.4750, 0.4771,\n",
            "        0.4757, 0.4766, 0.4787, 0.4761, 0.4723, 0.4715, 0.4793, 0.4780, 0.4722,\n",
            "        0.4804, 0.4766, 0.4737, 0.4760, 0.4762, 0.4731, 0.4700, 0.4743, 0.4793,\n",
            "        0.4784, 0.4711, 0.4799, 0.4722, 0.4681, 0.4678, 0.4700, 0.4802, 0.4714,\n",
            "        0.4769, 0.4774, 0.4778, 0.4770, 0.4782, 0.4797, 0.4685, 0.4771, 0.4721,\n",
            "        0.4755, 0.4779, 0.4781, 0.4763, 0.4742, 0.4751, 0.4793, 0.4809, 0.4799,\n",
            "        0.4778, 0.4735, 0.4679, 0.4738, 0.4750, 0.4788, 0.4760, 0.4765, 0.4774,\n",
            "        0.4754])\n",
            "Sum of predicted tensor(30.4373)\n",
            "Loss tensor(0.7043)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4755, 0.4749, 0.4731, 0.4768, 0.4733, 0.4723, 0.4747, 0.4788, 0.4757,\n",
            "        0.4799, 0.4786, 0.4733, 0.4753, 0.4782, 0.4774, 0.4757, 0.4778, 0.4741,\n",
            "        0.4751, 0.4752, 0.4747, 0.4744, 0.4691, 0.4767, 0.4798, 0.4707, 0.4737,\n",
            "        0.4737, 0.4765, 0.4739, 0.4766, 0.4799, 0.4786, 0.4701, 0.4769, 0.4768,\n",
            "        0.4709, 0.4741, 0.4805, 0.4788, 0.4760, 0.4730, 0.4760, 0.4755, 0.4737,\n",
            "        0.4740, 0.4788, 0.4769, 0.4729, 0.4760, 0.4806, 0.4764, 0.4707, 0.4740,\n",
            "        0.4746, 0.4765, 0.4683, 0.4768, 0.4754, 0.4743, 0.4751, 0.4754, 0.4750,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4227)\n",
            "Loss tensor(0.6890)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4779, 0.4771, 0.4730, 0.4720, 0.4763, 0.4759, 0.4805, 0.4765, 0.4746,\n",
            "        0.4763, 0.4764, 0.4745, 0.4745, 0.4715, 0.4710, 0.4743, 0.4799, 0.4793,\n",
            "        0.4770, 0.4740, 0.4771, 0.4723, 0.4765, 0.4762, 0.4751, 0.4768, 0.4707,\n",
            "        0.4782, 0.4702, 0.4790, 0.4744, 0.4712, 0.4765, 0.4758, 0.4736, 0.4755,\n",
            "        0.4766, 0.4730, 0.4821, 0.4771, 0.4750, 0.4753, 0.4782, 0.4735, 0.4754,\n",
            "        0.4766, 0.4763, 0.4736, 0.4780, 0.4773, 0.4713, 0.4745, 0.4768, 0.4730,\n",
            "        0.4748, 0.4752, 0.4738, 0.4748, 0.4796, 0.4808, 0.4740, 0.4758, 0.4762,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.6891)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4709, 0.4791, 0.4761, 0.4768, 0.4780, 0.4770, 0.4762, 0.4721, 0.4750,\n",
            "        0.4783, 0.4753, 0.4770, 0.4772, 0.4758, 0.4738, 0.4762, 0.4748, 0.4794,\n",
            "        0.4715, 0.4817, 0.4804, 0.4761, 0.4754, 0.4714, 0.4756, 0.4729, 0.4762,\n",
            "        0.4758, 0.4753, 0.4760, 0.4765, 0.4746, 0.4744, 0.4771, 0.4756, 0.4736,\n",
            "        0.4821, 0.4734, 0.4767, 0.4742, 0.4768, 0.4708, 0.4783, 0.4744, 0.4754,\n",
            "        0.4751, 0.4775, 0.4731, 0.4710, 0.4715, 0.4751, 0.4752, 0.4753, 0.4731,\n",
            "        0.4762, 0.4697, 0.4749, 0.4735, 0.4771, 0.4777, 0.4765, 0.4769, 0.4736,\n",
            "        0.4778])\n",
            "Sum of predicted tensor(30.4322)\n",
            "Loss tensor(0.6857)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4798, 0.4701, 0.4761, 0.4675, 0.4786, 0.4759, 0.4758, 0.4731, 0.4779,\n",
            "        0.4762, 0.4756, 0.4740, 0.4743, 0.4798, 0.4738, 0.4739, 0.4743, 0.4768,\n",
            "        0.4754, 0.4747, 0.4744, 0.4736, 0.4753, 0.4744, 0.4772, 0.4804, 0.4756,\n",
            "        0.4762, 0.4773, 0.4757, 0.4748, 0.4782, 0.4762, 0.4744, 0.4776, 0.4761,\n",
            "        0.4766, 0.4771, 0.4765, 0.4748, 0.4675, 0.4714, 0.4770, 0.4761, 0.4741,\n",
            "        0.4696, 0.4781, 0.4738, 0.4762, 0.4757, 0.4745, 0.4744, 0.4799, 0.4775,\n",
            "        0.4763, 0.4726, 0.4756, 0.4766, 0.4730, 0.4777, 0.4745, 0.4753, 0.4759,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.4260)\n",
            "Loss tensor(0.6928)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4775, 0.4709, 0.4791, 0.4744, 0.4677, 0.4772, 0.4729, 0.4777, 0.4716,\n",
            "        0.4715, 0.4772, 0.4767, 0.4729, 0.4736, 0.4771, 0.4753, 0.4733, 0.4780,\n",
            "        0.4814, 0.4779, 0.4807, 0.4745, 0.4774, 0.4750, 0.4774, 0.4731, 0.4775,\n",
            "        0.4747, 0.4712, 0.4782, 0.4776, 0.4715, 0.4779, 0.4711, 0.4741, 0.4760,\n",
            "        0.4729, 0.4805, 0.4762, 0.4762, 0.4740, 0.4731, 0.4777, 0.4818, 0.4763,\n",
            "        0.4717, 0.4775, 0.4716, 0.4756, 0.4760, 0.4758, 0.4731, 0.4710, 0.4768,\n",
            "        0.4798, 0.4778, 0.4783, 0.4723, 0.4779, 0.4791, 0.4792, 0.4770, 0.4765,\n",
            "        0.4744])\n",
            "Sum of predicted tensor(30.4421)\n",
            "Loss tensor(0.7027)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4789, 0.4786, 0.4704, 0.4777, 0.4818, 0.4723, 0.4701, 0.4787, 0.4765,\n",
            "        0.4789, 0.4729, 0.4775, 0.4690, 0.4765, 0.4753, 0.4750, 0.4771, 0.4746,\n",
            "        0.4753, 0.4751, 0.4771, 0.4776, 0.4748, 0.4745, 0.4749, 0.4776, 0.4797,\n",
            "        0.4765, 0.4713, 0.4731, 0.4776, 0.4744, 0.4741, 0.4726, 0.4738, 0.4777,\n",
            "        0.4738, 0.4713, 0.4793, 0.4726, 0.4799, 0.4744, 0.4788, 0.4747, 0.4701,\n",
            "        0.4754, 0.4733, 0.4801, 0.4767, 0.4756, 0.4768, 0.4738, 0.4744, 0.4762,\n",
            "        0.4760, 0.4775, 0.4738, 0.4750, 0.4780, 0.4756, 0.4760, 0.4720, 0.4724,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4292)\n",
            "Loss tensor(0.6826)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4786, 0.4730, 0.4755, 0.4766, 0.4775, 0.4751, 0.4735, 0.4765, 0.4742,\n",
            "        0.4791, 0.4729, 0.4757, 0.4745, 0.4698, 0.4762, 0.4763, 0.4748, 0.4772,\n",
            "        0.4751, 0.4777, 0.4738, 0.4817, 0.4708, 0.4795, 0.4756, 0.4785, 0.4786,\n",
            "        0.4762, 0.4698, 0.4802, 0.4777, 0.4782, 0.4718, 0.4813, 0.4741, 0.4738,\n",
            "        0.4731, 0.4766, 0.4736, 0.4768, 0.4705, 0.4723, 0.4753, 0.4757, 0.4784,\n",
            "        0.4744, 0.4768, 0.4761, 0.4726, 0.4780, 0.4729, 0.4762, 0.4775, 0.4745,\n",
            "        0.4765, 0.4791, 0.4769, 0.4764, 0.4772, 0.4756, 0.4762, 0.4771, 0.4806,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4521)\n",
            "Loss tensor(0.6874)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4765, 0.4711, 0.4701, 0.4729, 0.4751, 0.4762, 0.4784, 0.4757, 0.4801,\n",
            "        0.4763, 0.4770, 0.4676, 0.4757, 0.4710, 0.4763, 0.4743, 0.4802, 0.4783,\n",
            "        0.4740, 0.4738, 0.4735, 0.4817, 0.4793, 0.4780, 0.4739, 0.4739, 0.4728,\n",
            "        0.4712, 0.4774, 0.4767, 0.4736, 0.4754, 0.4750, 0.4735, 0.4769, 0.4740,\n",
            "        0.4759, 0.4730, 0.4763, 0.4763, 0.4675, 0.4793, 0.4774, 0.4766, 0.4737,\n",
            "        0.4752, 0.4773, 0.4771, 0.4742, 0.4751, 0.4760, 0.4753, 0.4777, 0.4738,\n",
            "        0.4725, 0.4728, 0.4723, 0.4726, 0.4785, 0.4769, 0.4717, 0.4753, 0.4754,\n",
            "        0.4794])\n",
            "Sum of predicted tensor(30.4121)\n",
            "Loss tensor(0.6963)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4777, 0.4757, 0.4788, 0.4744, 0.4754, 0.4799, 0.4748, 0.4762, 0.4742,\n",
            "        0.4751, 0.4777, 0.4795, 0.4753, 0.4752, 0.4721, 0.4786, 0.4712, 0.4788,\n",
            "        0.4731, 0.4702, 0.4748, 0.4702, 0.4759, 0.4778, 0.4722, 0.4786, 0.4723,\n",
            "        0.4771, 0.4770, 0.4753, 0.4751, 0.4733, 0.4741, 0.4786, 0.4762, 0.4763,\n",
            "        0.4780, 0.4824, 0.4741, 0.4710, 0.4759, 0.4736, 0.4745, 0.4795, 0.4726,\n",
            "        0.4760, 0.4745, 0.4712, 0.4777, 0.4713, 0.4818, 0.4738, 0.4808, 0.4730,\n",
            "        0.4775, 0.4752, 0.4756, 0.4791, 0.4767, 0.4760, 0.4775, 0.4759, 0.4790,\n",
            "        0.4703])\n",
            "Sum of predicted tensor(30.4432)\n",
            "Loss tensor(0.6979)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4740, 0.4773, 0.4744, 0.4751, 0.4745, 0.4753, 0.4709, 0.4779, 0.4775,\n",
            "        0.4713, 0.4746, 0.4751, 0.4740, 0.4769, 0.4746, 0.4782, 0.4747, 0.4773,\n",
            "        0.4786, 0.4740, 0.4775, 0.4747, 0.4776, 0.4774, 0.4803, 0.4758, 0.4712,\n",
            "        0.4783, 0.4779, 0.4715, 0.4760, 0.4738, 0.4788, 0.4762, 0.4778, 0.4762,\n",
            "        0.4732, 0.4765, 0.4725, 0.4767, 0.4794, 0.4805, 0.4773, 0.4786, 0.4749,\n",
            "        0.4758, 0.4753, 0.4798, 0.4748, 0.4737, 0.4766, 0.4818, 0.4756, 0.4804,\n",
            "        0.4739, 0.4713, 0.4762, 0.4714, 0.4768, 0.4738, 0.4744, 0.4775, 0.4775,\n",
            "        0.4765])\n",
            "Sum of predicted tensor(30.4600)\n",
            "Loss tensor(0.7004)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4762, 0.4744, 0.4773, 0.4714, 0.4775, 0.4788, 0.4772, 0.4729, 0.4744,\n",
            "        0.4768, 0.4775, 0.4712, 0.4710, 0.4776, 0.4750, 0.4753, 0.4833, 0.4746,\n",
            "        0.4744, 0.4725, 0.4786, 0.4739, 0.4750, 0.4759, 0.4768, 0.4751, 0.4668,\n",
            "        0.4760, 0.4773, 0.4761, 0.4788, 0.4746, 0.4732, 0.4710, 0.4821, 0.4806,\n",
            "        0.4758, 0.4763, 0.4717, 0.4737, 0.4766, 0.4798, 0.4784, 0.4752, 0.4746,\n",
            "        0.4766, 0.4744, 0.4736, 0.4716, 0.4778, 0.4780, 0.4715, 0.4738, 0.4784,\n",
            "        0.4749, 0.4789, 0.4709, 0.4775, 0.4775, 0.4753, 0.4763, 0.4772, 0.4763,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.4364)\n",
            "Loss tensor(0.7017)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4755, 0.4730, 0.4746, 0.4739, 0.4738, 0.4738, 0.4789, 0.4754, 0.4756,\n",
            "        0.4762, 0.4775, 0.4763, 0.4733, 0.4686, 0.4762, 0.4750, 0.4802, 0.4733,\n",
            "        0.4741, 0.4722, 0.4723, 0.4678, 0.4765, 0.4744, 0.4709, 0.4770, 0.4783,\n",
            "        0.4709, 0.4742, 0.4740, 0.4777, 0.4762, 0.4744, 0.4766, 0.4737, 0.4696,\n",
            "        0.4763, 0.4757, 0.4795, 0.4761, 0.4762, 0.4719, 0.4702, 0.4727, 0.4708,\n",
            "        0.4803, 0.4699, 0.4748, 0.4758, 0.4744, 0.4756, 0.4765, 0.4771, 0.4757,\n",
            "        0.4796, 0.4710, 0.4736, 0.4716, 0.4798, 0.4749, 0.4765, 0.4766, 0.4744,\n",
            "        0.4752])\n",
            "Sum of predicted tensor(30.3846)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4792, 0.4722, 0.4768, 0.4783, 0.4736, 0.4740, 0.4729, 0.4723, 0.4677,\n",
            "        0.4788, 0.4762, 0.4775, 0.4774, 0.4767, 0.4779, 0.4779, 0.4733, 0.4770,\n",
            "        0.4763, 0.4795, 0.4745, 0.4773, 0.4725, 0.4689, 0.4757, 0.4775, 0.4745,\n",
            "        0.4765, 0.4748, 0.4764, 0.4747, 0.4753, 0.4675, 0.4707, 0.4744, 0.4736,\n",
            "        0.4751, 0.4741, 0.4789, 0.4738, 0.4731, 0.4765, 0.4774, 0.4763, 0.4735,\n",
            "        0.4676, 0.4770, 0.4798, 0.4779, 0.4783, 0.4751, 0.4793, 0.4755, 0.4788,\n",
            "        0.4737, 0.4705, 0.4765, 0.4765, 0.4784, 0.4770, 0.4791, 0.4768, 0.4744,\n",
            "        0.4786])\n",
            "Sum of predicted tensor(30.4298)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4773, 0.4727, 0.4789, 0.4744, 0.4772, 0.4763, 0.4756, 0.4718, 0.4699,\n",
            "        0.4725, 0.4711, 0.4732, 0.4774, 0.4755, 0.4823, 0.4747, 0.4765, 0.4747,\n",
            "        0.4747, 0.4747, 0.4739, 0.4758, 0.4758, 0.4748, 0.4736, 0.4784, 0.4753,\n",
            "        0.4773, 0.4723, 0.4730, 0.4726, 0.4741, 0.4785, 0.4729, 0.4760, 0.4761,\n",
            "        0.4784, 0.4728, 0.4743, 0.4751, 0.4739, 0.4745, 0.4717, 0.4748, 0.4767,\n",
            "        0.4769, 0.4806, 0.4773, 0.4714, 0.4776, 0.4789, 0.4787, 0.4708, 0.4745,\n",
            "        0.4749, 0.4690, 0.4743, 0.4823, 0.4735, 0.4802, 0.4714, 0.4797, 0.4737,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4182)\n",
            "Loss tensor(0.6948)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4768, 0.4761, 0.4750, 0.4725, 0.4767, 0.4804, 0.4747, 0.4779, 0.4773,\n",
            "        0.4785, 0.4798, 0.4785, 0.4723, 0.4755, 0.4729, 0.4711, 0.4743, 0.4754,\n",
            "        0.4708, 0.4750, 0.4829, 0.4768, 0.4773, 0.4770, 0.4720, 0.4741, 0.4764,\n",
            "        0.4744, 0.4773, 0.4729, 0.4742, 0.4791, 0.4747, 0.4786, 0.4787, 0.4749,\n",
            "        0.4746, 0.4711, 0.4744, 0.4725, 0.4738, 0.4762, 0.4751, 0.4786, 0.4748,\n",
            "        0.4699, 0.4761, 0.4741, 0.4733, 0.4774, 0.4794, 0.4775, 0.4752, 0.4752,\n",
            "        0.4743, 0.4760, 0.4760, 0.4741, 0.4720, 0.4750, 0.4747, 0.4787, 0.4727,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4345)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4725, 0.4750, 0.4748, 0.4740, 0.4741, 0.4793, 0.4817, 0.4782, 0.4727,\n",
            "        0.4743, 0.4740, 0.4747, 0.4747, 0.4761, 0.4741, 0.4737, 0.4726, 0.4684,\n",
            "        0.4706, 0.4738, 0.4789, 0.4741, 0.4764, 0.4762, 0.4798, 0.4743, 0.4749,\n",
            "        0.4750, 0.4764, 0.4767, 0.4727, 0.4740, 0.4738, 0.4772, 0.4753, 0.4735,\n",
            "        0.4764, 0.4706, 0.4760, 0.4748, 0.4780, 0.4753, 0.4744, 0.4718, 0.4723,\n",
            "        0.4726, 0.4737, 0.4754, 0.4730, 0.4787, 0.4762, 0.4773, 0.4776, 0.4738,\n",
            "        0.4753, 0.4699, 0.4722, 0.4767, 0.4769, 0.4768, 0.4738, 0.4739, 0.4767,\n",
            "        0.4719])\n",
            "Sum of predicted tensor(30.3904)\n",
            "Loss tensor(0.6988)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4720, 0.4756, 0.4751, 0.4780, 0.4775, 0.4772, 0.4741, 0.4745, 0.4745,\n",
            "        0.4695, 0.4766, 0.4728, 0.4798, 0.4744, 0.4752, 0.4764, 0.4829, 0.4773,\n",
            "        0.4786, 0.4753, 0.4744, 0.4760, 0.4727, 0.4696, 0.4747, 0.4762, 0.4794,\n",
            "        0.4811, 0.4751, 0.4789, 0.4773, 0.4727, 0.4722, 0.4719, 0.4765, 0.4806,\n",
            "        0.4741, 0.4730, 0.4744, 0.4726, 0.4731, 0.4728, 0.4802, 0.4762, 0.4720,\n",
            "        0.4747, 0.4744, 0.4765, 0.4741, 0.4785, 0.4721, 0.4754, 0.4727, 0.4735,\n",
            "        0.4720, 0.4750, 0.4707, 0.4754, 0.4760, 0.4787, 0.4787, 0.4731, 0.4722,\n",
            "        0.4746])\n",
            "Sum of predicted tensor(30.4134)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4737, 0.4758, 0.4777, 0.4759, 0.4761, 0.4770, 0.4761, 0.4744, 0.4751,\n",
            "        0.4755, 0.4773, 0.4770, 0.4822, 0.4750, 0.4753, 0.4712, 0.4738, 0.4757,\n",
            "        0.4811, 0.4712, 0.4753, 0.4744, 0.4764, 0.4756, 0.4794, 0.4760, 0.4764,\n",
            "        0.4751, 0.4746, 0.4775, 0.4753, 0.4723, 0.4734, 0.4796, 0.4775, 0.4747,\n",
            "        0.4749, 0.4723, 0.4729, 0.4693, 0.4787, 0.4741, 0.4723, 0.4768, 0.4776,\n",
            "        0.4714, 0.4745, 0.4776, 0.4732, 0.4727, 0.4747, 0.4829, 0.4765, 0.4773,\n",
            "        0.4763, 0.4768, 0.4811, 0.4772, 0.4767, 0.4773, 0.4772, 0.4694, 0.4773,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4472)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4771, 0.4750, 0.4754, 0.4772, 0.4764, 0.4720, 0.4749, 0.4773,\n",
            "        0.4713, 0.4761, 0.4758, 0.4703, 0.4765, 0.4773, 0.4745, 0.4784, 0.4787,\n",
            "        0.4739, 0.4708, 0.4754, 0.4731, 0.4780, 0.4785, 0.4708, 0.4693, 0.4746,\n",
            "        0.4745, 0.4763, 0.4761, 0.4693, 0.4762, 0.4750, 0.4742, 0.4794, 0.4720,\n",
            "        0.4750, 0.4772, 0.4794, 0.4768, 0.4731, 0.4789, 0.4755, 0.4759, 0.4708,\n",
            "        0.4695, 0.4774, 0.4747, 0.4746, 0.4738, 0.4787, 0.4773, 0.4752, 0.4762,\n",
            "        0.4773, 0.4748, 0.4787, 0.4773, 0.4724, 0.4750, 0.4771, 0.4783, 0.4691,\n",
            "        0.4777])\n",
            "Sum of predicted tensor(30.4190)\n",
            "Loss tensor(0.7088)\n",
            "Acc tensor(0.3438)\n",
            "tensor([0.4773, 0.4768, 0.4726, 0.4713, 0.4762, 0.4734, 0.4722, 0.4750, 0.4722,\n",
            "        0.4746, 0.4744, 0.4726, 0.4783, 0.4745, 0.4763, 0.4748, 0.4747, 0.4773,\n",
            "        0.4802, 0.4748, 0.4775, 0.4721, 0.4760, 0.4747, 0.4792, 0.4731, 0.4760,\n",
            "        0.4769, 0.4710, 0.4773, 0.4787, 0.4740, 0.4831, 0.4728, 0.4746, 0.4747,\n",
            "        0.4772, 0.4741, 0.4733, 0.4748, 0.4787, 0.4752, 0.4774, 0.4727, 0.4749,\n",
            "        0.4730, 0.4785, 0.4741, 0.4702, 0.4736, 0.4750, 0.4729, 0.4755, 0.4722,\n",
            "        0.4771, 0.4737, 0.4733, 0.4780, 0.4724, 0.4735, 0.4731, 0.4724, 0.4741,\n",
            "        0.4759])\n",
            "Sum of predicted tensor(30.3977)\n",
            "Loss tensor(0.6962)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4751, 0.4762, 0.4739, 0.4755, 0.4749, 0.4760, 0.4762, 0.4733, 0.4749,\n",
            "        0.4723, 0.4754, 0.4733, 0.4769, 0.4717, 0.4742, 0.4720, 0.4794, 0.4787,\n",
            "        0.4789, 0.4768, 0.4790, 0.4775, 0.4775, 0.4726, 0.4749, 0.4713, 0.4736,\n",
            "        0.4755, 0.4789, 0.4768, 0.4718, 0.4747, 0.4811, 0.4729, 0.4782, 0.4711,\n",
            "        0.4783, 0.4744, 0.4739, 0.4720, 0.4699, 0.4729, 0.4757, 0.4755, 0.4731,\n",
            "        0.4786, 0.4727, 0.4727, 0.4754, 0.4712, 0.4739, 0.4746, 0.4796, 0.4768,\n",
            "        0.4730, 0.4770, 0.4765, 0.4716, 0.4773, 0.4723, 0.4708, 0.4774, 0.4789,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4032)\n",
            "Loss tensor(0.7034)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4746, 0.4743, 0.4718, 0.4751, 0.4714, 0.4805, 0.4763, 0.4811, 0.4728,\n",
            "        0.4730, 0.4752, 0.4797, 0.4687, 0.4764, 0.4742, 0.4812, 0.4752, 0.4755,\n",
            "        0.4747, 0.4741, 0.4771, 0.4699, 0.4802, 0.4763, 0.4789, 0.4747, 0.4744,\n",
            "        0.4720, 0.4811, 0.4740, 0.4740, 0.4760, 0.4777, 0.4749, 0.4757, 0.4752,\n",
            "        0.4789, 0.4755, 0.4760, 0.4700, 0.4738, 0.4695, 0.4761, 0.4750, 0.4791,\n",
            "        0.4750, 0.4744, 0.4802, 0.4806, 0.4751, 0.4705, 0.4757, 0.4741, 0.4757,\n",
            "        0.4741, 0.4775, 0.4794, 0.4699, 0.4711, 0.4768, 0.4743, 0.4728, 0.4737,\n",
            "        0.4735])\n",
            "Sum of predicted tensor(30.4164)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4766, 0.4687, 0.4719, 0.4776, 0.4713, 0.4759, 0.4756, 0.4747,\n",
            "        0.4751, 0.4765, 0.4765, 0.4727, 0.4735, 0.4756, 0.4764, 0.4744, 0.4741,\n",
            "        0.4775, 0.4744, 0.4739, 0.4734, 0.4746, 0.4765, 0.4696, 0.4755, 0.4738,\n",
            "        0.4729, 0.4729, 0.4794, 0.4762, 0.4755, 0.4763, 0.4713, 0.4788, 0.4794,\n",
            "        0.4754, 0.4747, 0.4726, 0.4736, 0.4713, 0.4728, 0.4766, 0.4764, 0.4713,\n",
            "        0.4746, 0.4738, 0.4742, 0.4719, 0.4728, 0.4761, 0.4714, 0.4811, 0.4763,\n",
            "        0.4777, 0.4788, 0.4764, 0.4781, 0.4754, 0.4709, 0.4827, 0.4779, 0.4811,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4090)\n",
            "Loss tensor(0.7067)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4731, 0.4742, 0.4809, 0.4748, 0.4832, 0.4769, 0.4790, 0.4783, 0.4768,\n",
            "        0.4681, 0.4775, 0.4784, 0.4762, 0.4701, 0.4720, 0.4730, 0.4727, 0.4707,\n",
            "        0.4772, 0.4753, 0.4773, 0.4795, 0.4761, 0.4770, 0.4748, 0.4754, 0.4771,\n",
            "        0.4773, 0.4765, 0.4786, 0.4811, 0.4768, 0.4752, 0.4781, 0.4766, 0.4783,\n",
            "        0.4760, 0.4729, 0.4827, 0.4733, 0.4701, 0.4801, 0.4751, 0.4741, 0.4739,\n",
            "        0.4754, 0.4769, 0.4754, 0.4827, 0.4799, 0.4799, 0.4707, 0.4752, 0.4710,\n",
            "        0.4770, 0.4764, 0.4758, 0.4760, 0.4712, 0.4768, 0.4761, 0.4727, 0.4750,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4627)\n",
            "Loss tensor(0.6931)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4770, 0.4743, 0.4776, 0.4740, 0.4705, 0.4765, 0.4768, 0.4760, 0.4747,\n",
            "        0.4764, 0.4692, 0.4792, 0.4764, 0.4773, 0.4713, 0.4744, 0.4727, 0.4761,\n",
            "        0.4804, 0.4786, 0.4782, 0.4754, 0.4769, 0.4785, 0.4726, 0.4762, 0.4747,\n",
            "        0.4748, 0.4719, 0.4719, 0.4788, 0.4811, 0.4784, 0.4804, 0.4713, 0.4755,\n",
            "        0.4723, 0.4804, 0.4830, 0.4742, 0.4736, 0.4750, 0.4736, 0.4713, 0.4773,\n",
            "        0.4744, 0.4737, 0.4757, 0.4721, 0.4746, 0.4773, 0.4804, 0.4762, 0.4708,\n",
            "        0.4716, 0.4748, 0.4804, 0.4733, 0.4748, 0.4783, 0.4747, 0.4729, 0.4783,\n",
            "        0.4712])\n",
            "Sum of predicted tensor(30.4326)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4760, 0.4726, 0.4761, 0.4753, 0.4766, 0.4791, 0.4739, 0.4768, 0.4712,\n",
            "        0.4687, 0.4729, 0.4754, 0.4773, 0.4761, 0.4804, 0.4787, 0.4830, 0.4750,\n",
            "        0.4746, 0.4711, 0.4788, 0.4711, 0.4722, 0.4727, 0.4773, 0.4726, 0.4738,\n",
            "        0.4741, 0.4726, 0.4756, 0.4735, 0.4712, 0.4747, 0.4811, 0.4781, 0.4799,\n",
            "        0.4731, 0.4761, 0.4697, 0.4729, 0.4719, 0.4762, 0.4763, 0.4761, 0.4738,\n",
            "        0.4721, 0.4709, 0.4744, 0.4754, 0.4758, 0.4712, 0.4730, 0.4773, 0.4763,\n",
            "        0.4783, 0.4790, 0.4764, 0.4687, 0.4737, 0.4746, 0.4712, 0.4747, 0.4764,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.3926)\n",
            "Loss tensor(0.6875)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4731, 0.4777, 0.4750, 0.4745, 0.4746, 0.4724, 0.4764, 0.4745, 0.4701,\n",
            "        0.4786, 0.4758, 0.4778, 0.4786, 0.4736, 0.4718, 0.4734, 0.4783, 0.4755,\n",
            "        0.4805, 0.4732, 0.4765, 0.4804, 0.4827, 0.4726, 0.4751, 0.4729, 0.4757,\n",
            "        0.4713, 0.4714, 0.4767, 0.4727, 0.4734, 0.4752, 0.4768, 0.4785, 0.4807,\n",
            "        0.4738, 0.4811, 0.4752, 0.4727, 0.4715, 0.4740, 0.4694, 0.4729, 0.4772,\n",
            "        0.4714, 0.4745, 0.4781, 0.4762, 0.4770, 0.4705, 0.4764, 0.4779, 0.4770,\n",
            "        0.4811, 0.4712, 0.4714, 0.4705, 0.4756, 0.4715, 0.4763, 0.4708, 0.4752,\n",
            "        0.4755])\n",
            "Sum of predicted tensor(30.4041)\n",
            "Loss tensor(0.6964)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4792, 0.4724, 0.4748, 0.4724, 0.4759, 0.4767, 0.4725, 0.4757, 0.4760,\n",
            "        0.4787, 0.4744, 0.4783, 0.4710, 0.4735, 0.4778, 0.4757, 0.4801, 0.4693,\n",
            "        0.4713, 0.4771, 0.4775, 0.4705, 0.4769, 0.4738, 0.4681, 0.4731, 0.4751,\n",
            "        0.4783, 0.4747, 0.4693, 0.4766, 0.4735, 0.4734, 0.4731, 0.4721, 0.4745,\n",
            "        0.4745, 0.4752, 0.4743, 0.4733, 0.4744, 0.4750, 0.4778, 0.4731, 0.4746,\n",
            "        0.4740, 0.4742, 0.4801, 0.4712, 0.4751, 0.4777, 0.4742, 0.4724, 0.4724,\n",
            "        0.4736, 0.4744, 0.4801, 0.4744, 0.4760, 0.4754, 0.4784, 0.4730, 0.4730,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.3800)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4717, 0.4777, 0.4730, 0.4776, 0.4746, 0.4734, 0.4766, 0.4721, 0.4705,\n",
            "        0.4721, 0.4785, 0.4733, 0.4781, 0.4805, 0.4752, 0.4715, 0.4782, 0.4775,\n",
            "        0.4786, 0.4777, 0.4771, 0.4792, 0.4761, 0.4715, 0.4757, 0.4734, 0.4775,\n",
            "        0.4698, 0.4771, 0.4786, 0.4717, 0.4779, 0.4746, 0.4765, 0.4746, 0.4756,\n",
            "        0.4681, 0.4796, 0.4701, 0.4693, 0.4777, 0.4711, 0.4788, 0.4766, 0.4752,\n",
            "        0.4778, 0.4746, 0.4725, 0.4777, 0.4771, 0.4758, 0.4755, 0.4754, 0.4745,\n",
            "        0.4793, 0.4766, 0.4757, 0.4762, 0.4750, 0.4710, 0.4736, 0.4782, 0.4767,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4194)\n",
            "Loss tensor(0.6820)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4747, 0.4765, 0.4709, 0.4811, 0.4758, 0.4717, 0.4760, 0.4699, 0.4811,\n",
            "        0.4743, 0.4765, 0.4748, 0.4720, 0.4731, 0.4766, 0.4737, 0.4758, 0.4785,\n",
            "        0.4801, 0.4765, 0.4775, 0.4767, 0.4724, 0.4725, 0.4698, 0.4705, 0.4761,\n",
            "        0.4759, 0.4701, 0.4733, 0.4745, 0.4744, 0.4805, 0.4744, 0.4726, 0.4746,\n",
            "        0.4751, 0.4756, 0.4724, 0.4752, 0.4750, 0.4750, 0.4731, 0.4721, 0.4731,\n",
            "        0.4730, 0.4775, 0.4726, 0.4714, 0.4775, 0.4775, 0.4736, 0.4726, 0.4744,\n",
            "        0.4775, 0.4777, 0.4752, 0.4732, 0.4778, 0.4686, 0.4726, 0.4744, 0.4771,\n",
            "        0.4750])\n",
            "Sum of predicted tensor(30.3809)\n",
            "Loss tensor(0.6969)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4759, 0.4722, 0.4719, 0.4748, 0.4801, 0.4745, 0.4729, 0.4807, 0.4771,\n",
            "        0.4767, 0.4765, 0.4747, 0.4722, 0.4739, 0.4736, 0.4752, 0.4757, 0.4730,\n",
            "        0.4738, 0.4788, 0.4760, 0.4759, 0.4719, 0.4768, 0.4717, 0.4660, 0.4738,\n",
            "        0.4743, 0.4766, 0.4744, 0.4789, 0.4771, 0.4745, 0.4767, 0.4777, 0.4753,\n",
            "        0.4744, 0.4743, 0.4708, 0.4744, 0.4758, 0.4720, 0.4709, 0.4757, 0.4742,\n",
            "        0.4767, 0.4753, 0.4765, 0.4728, 0.4776, 0.4731, 0.4718, 0.4752, 0.4811,\n",
            "        0.4744, 0.4717, 0.4758, 0.4775, 0.4751, 0.4751, 0.4738, 0.4708, 0.4767,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.3919)\n",
            "Loss tensor(0.6834)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0.4811, 0.4748, 0.4722, 0.4789, 0.4729, 0.4709, 0.4752, 0.4732, 0.4751,\n",
            "        0.4707, 0.4750, 0.4812, 0.4773, 0.4717, 0.4761, 0.4812, 0.4790, 0.4777,\n",
            "        0.4751, 0.4759, 0.4823, 0.4709, 0.4763, 0.4792, 0.4811, 0.4761, 0.4726,\n",
            "        0.4733, 0.4801, 0.4769, 0.4756, 0.4761, 0.4755, 0.4733, 0.4762, 0.4758,\n",
            "        0.4734, 0.4660, 0.4764, 0.4828, 0.4777, 0.4734, 0.4750, 0.4703, 0.4768,\n",
            "        0.4762, 0.4734, 0.4744, 0.4660, 0.4728, 0.4717, 0.4660, 0.4757, 0.4762,\n",
            "        0.4748, 0.4709, 0.4728, 0.4768, 0.4741, 0.4828, 0.4762, 0.4733, 0.4764,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.4146)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4736, 0.4730, 0.4677, 0.4731, 0.4753, 0.4722, 0.4760, 0.4772, 0.4765,\n",
            "        0.4766, 0.4726, 0.4721, 0.4744, 0.4731, 0.4761, 0.4771, 0.4718, 0.4768,\n",
            "        0.4734, 0.4731, 0.4726, 0.4731, 0.4752, 0.4726, 0.4766, 0.4722, 0.4736,\n",
            "        0.4758, 0.4730, 0.4731, 0.4722, 0.4815, 0.4734, 0.4768, 0.4724, 0.4737,\n",
            "        0.4780, 0.4765, 0.4734, 0.4779, 0.4765, 0.4761, 0.4733, 0.4815, 0.4709,\n",
            "        0.4756, 0.4788, 0.4752, 0.4717, 0.4710, 0.4767, 0.4790, 0.4734, 0.4823,\n",
            "        0.4746, 0.4799, 0.4769, 0.4717, 0.4817, 0.4811, 0.4733, 0.4739, 0.4812,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4056)\n",
            "Loss tensor(0.6790)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0.4789, 0.4763, 0.4732, 0.4747, 0.4762, 0.4755, 0.4750, 0.4745, 0.4760,\n",
            "        0.4758, 0.4803])\n",
            "Sum of predicted tensor(5.2364)\n",
            "Loss tensor(0.6734)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.90%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.83%\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3954, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4164, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3934, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3987, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3591, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4125, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4483, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4449, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4132, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4329, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4372, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3762, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3733, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4096, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3898, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3787, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4361, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4283, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4105, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3809, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4269, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4271, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4244, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3255, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4074, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4418, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4370, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4559, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4099, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4498, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4765, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4756, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4049, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4338, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4206, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4406, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3700, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4382, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4554, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4487, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4207, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4144, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4571, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4185, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4208, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4612, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3429, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4548, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3667, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4116, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3682, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3641, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4362, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4580, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4501, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4062, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3097, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4558, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4185, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4203, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3872, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3917, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4526, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3737, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4012, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4426, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4087, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3713, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4512, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4615, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4695, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4250, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3404, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4200, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3532, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4092, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3898, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3866, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4014, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3743, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4429, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4197, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4787, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4395, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3860, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4316, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3991, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4417, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3953, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4428, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4407, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3605, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4504, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4054, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4285, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4194, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3973, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3773, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4172, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3908, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4082, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4083, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3982, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3924, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3691, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3928, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4131, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4316, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4151, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4307, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4584, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4304, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4354, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4353, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3468, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4209, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4367, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4376, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3830, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3917, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4360, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4507, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4376, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4245, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4495, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3977, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4518, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4197, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4559, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4124, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4067, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4470, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3263, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3707, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4075, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4384, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3205, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4284, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3862, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4237, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4314, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4036, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4342, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3656, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4060, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4036, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3830, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3955, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4522, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4217, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4132, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4194, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3906, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4472, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3870, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4333, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4050, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4082, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4530, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4305, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4028, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3869, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4212, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4139, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4001, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4420, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4062, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3935, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4085, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4307, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4160, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3930, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4067, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4522, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4118, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4319, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4522, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3850, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4234, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3652, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4367, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3790, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4246, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4234, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4200, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4244, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4120, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4081, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4234, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4085, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4313, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3895, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3072, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4240, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3823, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4029, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3943, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4139, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4391, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4118, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3260, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4612, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4241, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4251, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4024, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4010, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4160, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4505, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3947, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3837, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4284, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4497, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4076, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4247, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3680, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4690, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4129, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4691, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4211, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3686, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3591, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3793, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4471, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4073, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3936, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3137, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3973, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3959, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4616, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4273, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3446, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3655, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4325, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3958, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4082, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4439, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3928, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4431, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4625, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4395, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4611, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4444, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3500, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4085, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4038, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3817, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4373, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4097, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3988, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4383, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4181, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3991, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4131, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4115, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4341, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4083, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4288, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3759, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4412, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4226, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4175, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4298, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4174, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4132, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4089, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3871, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4400, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4243, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3736, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4084, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3703, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3730, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3810, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4018, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4325, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4103, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4080, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3791, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4254, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4079, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4096, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3969, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4188, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4130, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4526, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3410, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3905, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3924, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4407, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3856, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3967, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4038, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3938, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4410, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3917, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3983, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3612, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4004, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3971, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4147, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4062, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3950, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4047, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4460, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4071, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4194, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4284, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4363, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4074, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4243, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3938, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4538, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4500, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3841, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4226, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3233, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4372, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3988, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4520, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3713, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3792, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4662, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3826, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3876, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3821, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3028, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3936, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3800, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4398, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4141, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4321, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3874, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3977, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3616, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4829, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3292, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3930, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4212, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4280, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4236, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4121, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3762, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4280, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4137, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4304, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4265, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4078, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3747, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4124, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3600, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4366, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4263, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4775, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3889, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4022, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4156, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4329, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4059, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3861, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4013, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4522, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3651, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4130, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4171, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3100, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3577, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4676, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3862, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4240, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3969, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4193, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4295, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4041, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3990, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4070, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3734, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4041, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3993, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3875, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4373, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3887, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4256, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4819, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3614, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4458, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4397, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3637, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4075, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3439, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4462, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4326, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3792, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3813, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4530, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4236, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4069, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3682, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4201, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4278, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4501, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4250, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4019, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4168, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3748, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4451, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4471, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3929, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4086, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4775, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3783, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4125, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4596, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3958, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3757, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3501, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4449, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3558, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3794, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4369, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4069, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4238, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4108, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4112, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4513, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4402, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4632, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4618, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4284, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4276, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4612, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4438, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4032, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4276, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3985, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3276, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4422, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4419, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4367, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4174, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4139, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4443, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4229, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4095, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4021, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4023, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4366, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4155, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4036, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4332, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4321, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4244, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3892, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4469, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4485, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3389, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4432, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4178, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3588, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4249, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4264, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4016, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4007, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3972, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3493, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4314, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3900, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4507, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4164, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4596, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4539, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4305, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4689, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3938, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4581, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4433, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3673, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3944, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4195, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4240, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4397, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3887, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4185, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4366, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4006, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3989, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4219, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3393, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3672, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3618, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4140, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4038, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3333, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4127, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4418, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(11.4183, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4481, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4172, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4218, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4214, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4183, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4070, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3846, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4063, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3235, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3817, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4371, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4171, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4649, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4109, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4190, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4424, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4322, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4036, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4026, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4283, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3700, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4451, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3838, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4163, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3711, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3623, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3548, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3983, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3903, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4176, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4285, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.3998, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4219, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4135, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4049, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4219, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4383, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4499, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.4257, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(30.5011, grad_fn=<SumBackward0>)\n",
            "tensor([0.4724, 0.4748, 0.4738, 0.4735, 0.4737, 0.4741, 0.4717, 0.4744, 0.4728,\n",
            "        0.4768, 0.4749, 0.4741, 0.4721, 0.4746, 0.4741, 0.4739, 0.4741, 0.4744,\n",
            "        0.4743, 0.4741, 0.4714, 0.4746, 0.4739, 0.4714, 0.4762, 0.4746, 0.4741,\n",
            "        0.4707, 0.4707, 0.4721, 0.4742, 0.4733, 0.4744, 0.4721, 0.4741, 0.4727,\n",
            "        0.4741, 0.4741, 0.4714, 0.4764, 0.4744, 0.4725, 0.4699, 0.4751, 0.4728,\n",
            "        0.4718, 0.4756, 0.4755, 0.4754, 0.4735, 0.4713, 0.4721, 0.4750, 0.4741,\n",
            "        0.4722, 0.4737, 0.4762, 0.4728, 0.4741, 0.4741, 0.4718, 0.4744, 0.4741,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.3146)\n",
            "Loss tensor(0.6672)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0.4764, 0.4737, 0.4744, 0.4743, 0.4758, 0.4723, 0.4777, 0.4754, 0.4763,\n",
            "        0.4743, 0.4775, 0.4762, 0.4719, 0.4705, 0.4729, 0.4705, 0.4720, 0.4771,\n",
            "        0.4697, 0.4750, 0.4727, 0.4708, 0.4725, 0.4742, 0.4755, 0.4729, 0.4790,\n",
            "        0.4740, 0.4703, 0.4715, 0.4718, 0.4716, 0.4742, 0.4779, 0.4721, 0.4759,\n",
            "        0.4709, 0.4722, 0.4752, 0.4725, 0.4744, 0.4704, 0.4773, 0.4744, 0.4757,\n",
            "        0.4740, 0.4744, 0.4715, 0.4715, 0.4755, 0.4722, 0.4736, 0.4771, 0.4731,\n",
            "        0.4755, 0.4722, 0.4745, 0.4738, 0.4729, 0.4744, 0.4719, 0.4729, 0.4787,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.3289)\n",
            "Loss tensor(0.6699)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0.4771, 0.4773, 0.4717, 0.4724, 0.4775, 0.4745, 0.4734, 0.4702, 0.4729,\n",
            "        0.4741, 0.4755, 0.4775, 0.4718, 0.4723, 0.4760, 0.4740, 0.4730, 0.4725,\n",
            "        0.4740, 0.4723, 0.4716, 0.4707, 0.4739, 0.4671, 0.4735, 0.4803, 0.4760,\n",
            "        0.4744, 0.4740, 0.4716, 0.4752, 0.4739, 0.4735, 0.4734, 0.4745, 0.4744,\n",
            "        0.4796, 0.4727, 0.4724, 0.4730, 0.4758, 0.4732, 0.4776, 0.4744, 0.4718,\n",
            "        0.4741, 0.4748, 0.4708, 0.4745, 0.4744, 0.4720, 0.4745, 0.4735, 0.4713,\n",
            "        0.4722, 0.4714, 0.4740, 0.4752, 0.4754, 0.4740, 0.4743, 0.4722, 0.4742,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.3231)\n",
            "Loss tensor(0.6818)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4755, 0.4693, 0.4764, 0.4718, 0.4768, 0.4763, 0.4789, 0.4773, 0.4745,\n",
            "        0.4740, 0.4717, 0.4754, 0.4716, 0.4741, 0.4744, 0.4734, 0.4740, 0.4788,\n",
            "        0.4724, 0.4723, 0.4710, 0.4747, 0.4739, 0.4744, 0.4733, 0.4718, 0.4739,\n",
            "        0.4748, 0.4755, 0.4719, 0.4714, 0.4744, 0.4693, 0.4730, 0.4740, 0.4730,\n",
            "        0.4686, 0.4758, 0.4702, 0.4708, 0.4715, 0.4748, 0.4726, 0.4733, 0.4800,\n",
            "        0.4777, 0.4742, 0.4708, 0.4739, 0.4719, 0.4758, 0.4718, 0.4742, 0.4722,\n",
            "        0.4724, 0.4780, 0.4737, 0.4741, 0.4750, 0.4765, 0.4771, 0.4735, 0.4708,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.3246)\n",
            "Loss tensor(0.6674)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0.4760, 0.4709, 0.4728, 0.4743, 0.4736, 0.4712, 0.4726, 0.4748, 0.4717,\n",
            "        0.4753, 0.4751, 0.4728, 0.4745, 0.4752, 0.4733, 0.4765, 0.4721, 0.4704,\n",
            "        0.4738, 0.4768, 0.4716, 0.4738, 0.4730, 0.4772, 0.4705, 0.4746, 0.4721,\n",
            "        0.4740, 0.4746, 0.4819, 0.4751, 0.4763, 0.4782, 0.4716, 0.4720, 0.4772,\n",
            "        0.4742, 0.4738, 0.4724, 0.4739, 0.4693, 0.4757, 0.4747, 0.4739, 0.4746,\n",
            "        0.4750, 0.4705, 0.4762, 0.4742, 0.4746, 0.4766, 0.4729, 0.4733, 0.4722,\n",
            "        0.4742, 0.4757, 0.4740, 0.4734, 0.4671, 0.4725, 0.4737, 0.4744, 0.4738,\n",
            "        0.4708])\n",
            "Sum of predicted tensor(30.3249)\n",
            "Loss tensor(0.6743)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4761, 0.4754, 0.4732, 0.4715, 0.4756, 0.4772, 0.4742, 0.4722, 0.4727,\n",
            "        0.4800, 0.4760, 0.4700, 0.4730, 0.4743, 0.4749, 0.4735, 0.4738, 0.4746,\n",
            "        0.4753, 0.4771, 0.4745, 0.4743, 0.4800, 0.4735, 0.4819, 0.4705, 0.4748,\n",
            "        0.4735, 0.4761, 0.4748, 0.4777, 0.4743, 0.4742, 0.4735, 0.4778, 0.4746,\n",
            "        0.4762, 0.4735, 0.4769, 0.4747, 0.4715, 0.4754, 0.4735, 0.4764, 0.4739,\n",
            "        0.4752, 0.4768, 0.4707, 0.4759, 0.4717, 0.4745, 0.4750, 0.4745, 0.4735,\n",
            "        0.4772, 0.4731, 0.4735, 0.4737, 0.4750, 0.4724, 0.4745, 0.4697, 0.4758,\n",
            "        0.4730])\n",
            "Sum of predicted tensor(30.3745)\n",
            "Loss tensor(0.6824)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0.4711, 0.4731, 0.4760, 0.4771, 0.4713, 0.4717, 0.4732, 0.4742, 0.4705,\n",
            "        0.4716, 0.4762, 0.4778, 0.4743, 0.4738, 0.4746, 0.4746, 0.4763, 0.4807,\n",
            "        0.4777, 0.4778, 0.4753, 0.4703, 0.4700, 0.4745, 0.4763, 0.4772, 0.4705,\n",
            "        0.4738, 0.4695, 0.4817, 0.4761, 0.4750, 0.4726, 0.4772, 0.4718, 0.4767,\n",
            "        0.4698, 0.4738, 0.4763, 0.4760, 0.4730, 0.4753, 0.4746, 0.4711, 0.4732,\n",
            "        0.4768, 0.4740, 0.4738, 0.4738, 0.4735, 0.4705, 0.4694, 0.4718, 0.4757,\n",
            "        0.4717, 0.4755, 0.4715, 0.4758, 0.4752, 0.4727, 0.4760, 0.4748, 0.4753,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3485)\n",
            "Loss tensor(0.6912)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4713, 0.4738, 0.4742, 0.4748, 0.4772, 0.4761, 0.4745, 0.4800, 0.4750,\n",
            "        0.4775, 0.4807, 0.4705, 0.4724, 0.4745, 0.4754, 0.4740, 0.4724, 0.4754,\n",
            "        0.4730, 0.4766, 0.4735, 0.4733, 0.4736, 0.4711, 0.4761, 0.4742, 0.4763,\n",
            "        0.4763, 0.4745, 0.4751, 0.4744, 0.4739, 0.4740, 0.4721, 0.4738, 0.4768,\n",
            "        0.4697, 0.4760, 0.4760, 0.4775, 0.4758, 0.4753, 0.4771, 0.4748, 0.4713,\n",
            "        0.4715, 0.4745, 0.4748, 0.4747, 0.4760, 0.4761, 0.4703, 0.4718, 0.4731,\n",
            "        0.4696, 0.4728, 0.4724, 0.4818, 0.4713, 0.4748, 0.4772, 0.4774, 0.4712,\n",
            "        0.4744])\n",
            "Sum of predicted tensor(30.3678)\n",
            "Loss tensor(0.6870)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4724, 0.4780, 0.4726, 0.4729, 0.4807, 0.4752, 0.4713, 0.4777, 0.4749,\n",
            "        0.4766, 0.4718, 0.4710, 0.4755, 0.4753, 0.4750, 0.4706, 0.4735, 0.4728,\n",
            "        0.4711, 0.4700, 0.4743, 0.4767, 0.4715, 0.4752, 0.4738, 0.4761, 0.4769,\n",
            "        0.4746, 0.4735, 0.4766, 0.4735, 0.4711, 0.4748, 0.4754, 0.4742, 0.4746,\n",
            "        0.4743, 0.4743, 0.4743, 0.4713, 0.4758, 0.4747, 0.4777, 0.4712, 0.4727,\n",
            "        0.4781, 0.4748, 0.4734, 0.4711, 0.4748, 0.4735, 0.4772, 0.4777, 0.4788,\n",
            "        0.4800, 0.4772, 0.4722, 0.4748, 0.4718, 0.4715, 0.4674, 0.4748, 0.4717,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.3562)\n",
            "Loss tensor(0.6907)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4750, 0.4724, 0.4733, 0.4722, 0.4735, 0.4735, 0.4738, 0.4750, 0.4762,\n",
            "        0.4748, 0.4737, 0.4743, 0.4772, 0.4757, 0.4735, 0.4778, 0.4778, 0.4760,\n",
            "        0.4751, 0.4736, 0.4777, 0.4745, 0.4739, 0.4776, 0.4746, 0.4729, 0.4736,\n",
            "        0.4756, 0.4711, 0.4748, 0.4812, 0.4724, 0.4727, 0.4763, 0.4740, 0.4752,\n",
            "        0.4752, 0.4734, 0.4745, 0.4749, 0.4729, 0.4750, 0.4706, 0.4761, 0.4724,\n",
            "        0.4738, 0.4760, 0.4739, 0.4683, 0.4783, 0.4743, 0.4735, 0.4800, 0.4766,\n",
            "        0.4777, 0.4754, 0.4743, 0.4745, 0.4715, 0.4756, 0.4711, 0.4721, 0.4724,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.3682)\n",
            "Loss tensor(0.6792)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0.4735, 0.4713, 0.4740, 0.4731, 0.4752, 0.4702, 0.4740, 0.4832, 0.4749,\n",
            "        0.4832, 0.4753, 0.4772, 0.4724, 0.4749, 0.4740, 0.4757, 0.4748, 0.4718,\n",
            "        0.4745, 0.4755, 0.4774, 0.4700, 0.4735, 0.4748, 0.4746, 0.4759, 0.4731,\n",
            "        0.4752, 0.4728, 0.4738, 0.4740, 0.4700, 0.4736, 0.4736, 0.4712, 0.4708,\n",
            "        0.4686, 0.4769, 0.4686, 0.4743, 0.4738, 0.4753, 0.4723, 0.4747, 0.4722,\n",
            "        0.4713, 0.4775, 0.4728, 0.4686, 0.4749, 0.4739, 0.4709, 0.4718, 0.4731,\n",
            "        0.4748, 0.4717, 0.4736, 0.4766, 0.4760, 0.4733, 0.4761, 0.4737, 0.4750,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.3312)\n",
            "Loss tensor(0.6719)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0.4745, 0.4756, 0.4688, 0.4746, 0.4716, 0.4723, 0.4769, 0.4702, 0.4752,\n",
            "        0.4693, 0.4693, 0.4702, 0.4722, 0.4711, 0.4740, 0.4747, 0.4793, 0.4732,\n",
            "        0.4760, 0.4728, 0.4740, 0.4702, 0.4705, 0.4755, 0.4771, 0.4727, 0.4800,\n",
            "        0.4738, 0.4797, 0.4741, 0.4736, 0.4727, 0.4700, 0.4755, 0.4743, 0.4738,\n",
            "        0.4750, 0.4751, 0.4790, 0.4718, 0.4755, 0.4787, 0.4758, 0.4771, 0.4787,\n",
            "        0.4742, 0.4745, 0.4728, 0.4781, 0.4750, 0.4802, 0.4729, 0.4749, 0.4743,\n",
            "        0.4766, 0.4730, 0.4747, 0.4720, 0.4781, 0.4768, 0.4747, 0.4701, 0.4752,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.3589)\n",
            "Loss tensor(0.6866)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4718, 0.4726, 0.4755, 0.4747, 0.4704, 0.4744, 0.4722, 0.4702, 0.4747,\n",
            "        0.4718, 0.4730, 0.4735, 0.4734, 0.4713, 0.4742, 0.4767, 0.4739, 0.4756,\n",
            "        0.4751, 0.4760, 0.4755, 0.4728, 0.4713, 0.4749, 0.4749, 0.4751, 0.4768,\n",
            "        0.4712, 0.4713, 0.4746, 0.4788, 0.4760, 0.4734, 0.4738, 0.4755, 0.4730,\n",
            "        0.4713, 0.4760, 0.4781, 0.4734, 0.4729, 0.4752, 0.4718, 0.4736, 0.4772,\n",
            "        0.4740, 0.4720, 0.4755, 0.4732, 0.4740, 0.4832, 0.4758, 0.4741, 0.4707,\n",
            "        0.4768, 0.4736, 0.4775, 0.4708, 0.4716, 0.4749, 0.4713, 0.4730, 0.4775,\n",
            "        0.4698])\n",
            "Sum of predicted tensor(30.3389)\n",
            "Loss tensor(0.6748)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4716, 0.4773, 0.4733, 0.4738, 0.4727, 0.4748, 0.4733, 0.4720, 0.4737,\n",
            "        0.4725, 0.4694, 0.4712, 0.4776, 0.4768, 0.4713, 0.4722, 0.4752, 0.4769,\n",
            "        0.4731, 0.4730, 0.4746, 0.4765, 0.4765, 0.4744, 0.4766, 0.4773, 0.4700,\n",
            "        0.4751, 0.4762, 0.4738, 0.4708, 0.4734, 0.4702, 0.4713, 0.4707, 0.4776,\n",
            "        0.4777, 0.4713, 0.4728, 0.4760, 0.4773, 0.4730, 0.4770, 0.4751, 0.4803,\n",
            "        0.4700, 0.4702, 0.4752, 0.4732, 0.4790, 0.4753, 0.4740, 0.4745, 0.4760,\n",
            "        0.4760, 0.4740, 0.4746, 0.4788, 0.4721, 0.4750, 0.4709, 0.4739, 0.4802,\n",
            "        0.4776])\n",
            "Sum of predicted tensor(30.3579)\n",
            "Loss tensor(0.6820)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4729, 0.4757, 0.4728, 0.4736, 0.4744, 0.4713, 0.4793, 0.4781, 0.4788,\n",
            "        0.4751, 0.4745, 0.4724, 0.4760, 0.4755, 0.4775, 0.4721, 0.4729, 0.4738,\n",
            "        0.4765, 0.4769, 0.4751, 0.4713, 0.4775, 0.4713, 0.4750, 0.4731, 0.4745,\n",
            "        0.4744, 0.4744, 0.4737, 0.4739, 0.4741, 0.4749, 0.4702, 0.4736, 0.4753,\n",
            "        0.4775, 0.4720, 0.4760, 0.4763, 0.4725, 0.4705, 0.4768, 0.4793, 0.4805,\n",
            "        0.4788, 0.4756, 0.4744, 0.4692, 0.4729, 0.4753, 0.4767, 0.4693, 0.4729,\n",
            "        0.4762, 0.4756, 0.4735, 0.4757, 0.4746, 0.4713, 0.4718, 0.4765, 0.4751,\n",
            "        0.4794])\n",
            "Sum of predicted tensor(30.3786)\n",
            "Loss tensor(0.6776)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0.4746, 0.4755, 0.4686, 0.4802, 0.4732, 0.4749, 0.4724, 0.4746, 0.4734,\n",
            "        0.4738, 0.4730, 0.4708, 0.4736, 0.4749, 0.4729, 0.4760, 0.4751, 0.4751,\n",
            "        0.4756, 0.4763, 0.4751, 0.4717, 0.4774, 0.4761, 0.4768, 0.4700, 0.4724,\n",
            "        0.4727, 0.4805, 0.4751, 0.4758, 0.4746, 0.4757, 0.4727, 0.4713, 0.4764,\n",
            "        0.4744, 0.4716, 0.4758, 0.4746, 0.4753, 0.4756, 0.4702, 0.4750, 0.4776,\n",
            "        0.4727, 0.4715, 0.4752, 0.4734, 0.4775, 0.4752, 0.4728, 0.4747, 0.4710,\n",
            "        0.4706, 0.4751, 0.4745, 0.4741, 0.4757, 0.4787, 0.4775, 0.4755, 0.4742,\n",
            "        0.4720])\n",
            "Sum of predicted tensor(30.3607)\n",
            "Loss tensor(0.6696)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0.4760, 0.4769, 0.4701, 0.4732, 0.4779, 0.4712, 0.4729, 0.4713, 0.4790,\n",
            "        0.4746, 0.4721, 0.4714, 0.4756, 0.4745, 0.4765, 0.4678, 0.4776, 0.4803,\n",
            "        0.4713, 0.4753, 0.4717, 0.4712, 0.4712, 0.4753, 0.4768, 0.4745, 0.4713,\n",
            "        0.4720, 0.4739, 0.4747, 0.4749, 0.4702, 0.4701, 0.4749, 0.4727, 0.4733,\n",
            "        0.4757, 0.4732, 0.4732, 0.4745, 0.4764, 0.4752, 0.4686, 0.4745, 0.4694,\n",
            "        0.4756, 0.4774, 0.4751, 0.4772, 0.4730, 0.4749, 0.4790, 0.4734, 0.4805,\n",
            "        0.4726, 0.4788, 0.4720, 0.4762, 0.4740, 0.4705, 0.4749, 0.4768, 0.4748,\n",
            "        0.4789])\n",
            "Sum of predicted tensor(30.3509)\n",
            "Loss tensor(0.6758)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0.4748, 0.4742, 0.4787, 0.4733, 0.4744, 0.4693, 0.4738, 0.4712, 0.4760,\n",
            "        0.4758, 0.4700, 0.4719, 0.4752, 0.4737, 0.4738, 0.4727, 0.4723, 0.4709,\n",
            "        0.4760, 0.4756, 0.4755, 0.4686, 0.4749, 0.4743, 0.4757, 0.4723, 0.4790,\n",
            "        0.4758, 0.4776, 0.4716, 0.4790, 0.4752, 0.4713, 0.4739, 0.4751, 0.4742,\n",
            "        0.4769, 0.4713, 0.4713, 0.4701, 0.4740, 0.4686, 0.4713, 0.4755, 0.4790,\n",
            "        0.4734, 0.4768, 0.4742, 0.4715, 0.4778, 0.4708, 0.4805, 0.4686, 0.4760,\n",
            "        0.4752, 0.4753, 0.4718, 0.4741, 0.4726, 0.4761, 0.4765, 0.4747, 0.4787,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.3475)\n",
            "Loss tensor(0.6905)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4727, 0.4752, 0.4725, 0.4775, 0.4713, 0.4800, 0.4694, 0.4721, 0.4733,\n",
            "        0.4814, 0.4717, 0.4797, 0.4778, 0.4787, 0.4765, 0.4728, 0.4757, 0.4735,\n",
            "        0.4752, 0.4724, 0.4721, 0.4658, 0.4764, 0.4778, 0.4761, 0.4765, 0.4757,\n",
            "        0.4790, 0.4742, 0.4746, 0.4769, 0.4709, 0.4741, 0.4781, 0.4721, 0.4765,\n",
            "        0.4731, 0.4754, 0.4736, 0.4726, 0.4753, 0.4699, 0.4768, 0.4775, 0.4768,\n",
            "        0.4765, 0.4726, 0.4713, 0.4744, 0.4694, 0.4736, 0.4718, 0.4729, 0.4723,\n",
            "        0.4719, 0.4700, 0.4709, 0.4723, 0.4749, 0.4749, 0.4733, 0.4759, 0.4862,\n",
            "        0.4712])\n",
            "Sum of predicted tensor(30.3635)\n",
            "Loss tensor(0.6788)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0.4727, 0.4714, 0.4811, 0.4764, 0.4734, 0.4752, 0.4733, 0.4753, 0.4727,\n",
            "        0.4764, 0.4745, 0.4779, 0.4752, 0.4745, 0.4754, 0.4754, 0.4743, 0.4737,\n",
            "        0.4743, 0.4768, 0.4737, 0.4761, 0.4669, 0.4744, 0.4742, 0.4789, 0.4770,\n",
            "        0.4745, 0.4775, 0.4747, 0.4811, 0.4788, 0.4817, 0.4717, 0.4764, 0.4731,\n",
            "        0.4732, 0.4725, 0.4745, 0.4809, 0.4754, 0.4767, 0.4742, 0.4754, 0.4735,\n",
            "        0.4748, 0.4772, 0.4752, 0.4746, 0.4742, 0.4786, 0.4705, 0.4701, 0.4804,\n",
            "        0.4733, 0.4757, 0.4776, 0.4735, 0.4773, 0.4790, 0.4770, 0.4723, 0.4763,\n",
            "        0.4750])\n",
            "Sum of predicted tensor(30.4197)\n",
            "Loss tensor(0.7008)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4725, 0.4753, 0.4727, 0.4770, 0.4727, 0.4722, 0.4764, 0.4727, 0.4764,\n",
            "        0.4749, 0.4751, 0.4724, 0.4802, 0.4761, 0.4791, 0.4817, 0.4783, 0.4781,\n",
            "        0.4723, 0.4725, 0.4748, 0.4761, 0.4774, 0.4807, 0.4757, 0.4764, 0.4767,\n",
            "        0.4743, 0.4725, 0.4755, 0.4743, 0.4738, 0.4727, 0.4743, 0.4748, 0.4754,\n",
            "        0.4752, 0.4751, 0.4755, 0.4783, 0.4763, 0.4792, 0.4785, 0.4811, 0.4766,\n",
            "        0.4749, 0.4752, 0.4753, 0.4743, 0.4795, 0.4725, 0.4764, 0.4786, 0.4795,\n",
            "        0.4774, 0.4754, 0.4740, 0.4773, 0.4764, 0.4766, 0.4710, 0.4764, 0.4722,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4474)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4747, 0.4744, 0.4760, 0.4717, 0.4775, 0.4727, 0.4768, 0.4764, 0.4740,\n",
            "        0.4727, 0.4773, 0.4768, 0.4727, 0.4767, 0.4727, 0.4768, 0.4730, 0.4737,\n",
            "        0.4811, 0.4767, 0.4809, 0.4782, 0.4757, 0.4717, 0.4740, 0.4745, 0.4684,\n",
            "        0.4793, 0.4779, 0.4777, 0.4742, 0.4727, 0.4759, 0.4762, 0.4782, 0.4736,\n",
            "        0.4796, 0.4749, 0.4768, 0.4750, 0.4744, 0.4748, 0.4727, 0.4756, 0.4756,\n",
            "        0.4785, 0.4764, 0.4747, 0.4730, 0.4751, 0.4784, 0.4737, 0.4734, 0.4761,\n",
            "        0.4786, 0.4764, 0.4773, 0.4757, 0.4774, 0.4758, 0.4768, 0.4773, 0.4722,\n",
            "        0.4723])\n",
            "Sum of predicted tensor(30.4315)\n",
            "Loss tensor(0.7020)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4717, 0.4746, 0.4750, 0.4764, 0.4743, 0.4783, 0.4707, 0.4814, 0.4783,\n",
            "        0.4764, 0.4756, 0.4764, 0.4727, 0.4727, 0.4774, 0.4753, 0.4751, 0.4727,\n",
            "        0.4728, 0.4764, 0.4719, 0.4764, 0.4722, 0.4767, 0.4764, 0.4694, 0.4720,\n",
            "        0.4743, 0.4753, 0.4764, 0.4743, 0.4764, 0.4727, 0.4756, 0.4790, 0.4744,\n",
            "        0.4725, 0.4817, 0.4745, 0.4757, 0.4767, 0.4778, 0.4793, 0.4730, 0.4741,\n",
            "        0.4747, 0.4757, 0.4826, 0.4749, 0.4721, 0.4732, 0.4777, 0.4746, 0.4719,\n",
            "        0.4757, 0.4684, 0.4742, 0.4718, 0.4736, 0.4740, 0.4754, 0.4766, 0.4750,\n",
            "        0.4779])\n",
            "Sum of predicted tensor(30.4028)\n",
            "Loss tensor(0.6938)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4773, 0.4739, 0.4755, 0.4746, 0.4756, 0.4764, 0.4734, 0.4733, 0.4755,\n",
            "        0.4717, 0.4769, 0.4764, 0.4717, 0.4771, 0.4762, 0.4728, 0.4754, 0.4761,\n",
            "        0.4742, 0.4727, 0.4750, 0.4748, 0.4777, 0.4782, 0.4705, 0.4720, 0.4761,\n",
            "        0.4734, 0.4757, 0.4757, 0.4757, 0.4753, 0.4714, 0.4775, 0.4774, 0.4756,\n",
            "        0.4768, 0.4751, 0.4743, 0.4739, 0.4722, 0.4755, 0.4753, 0.4740, 0.4720,\n",
            "        0.4758, 0.4750, 0.4720, 0.4684, 0.4753, 0.4779, 0.4768, 0.4787, 0.4747,\n",
            "        0.4795, 0.4759, 0.4727, 0.4793, 0.4747, 0.4741, 0.4792, 0.4778, 0.4738,\n",
            "        0.4731])\n",
            "Sum of predicted tensor(30.4025)\n",
            "Loss tensor(0.7058)\n",
            "Acc tensor(0.3906)\n",
            "tensor([0.4775, 0.4793, 0.4758, 0.4756, 0.4723, 0.4757, 0.4749, 0.4750, 0.4755,\n",
            "        0.4749, 0.4745, 0.4710, 0.4764, 0.4727, 0.4690, 0.4783, 0.4790, 0.4737,\n",
            "        0.4732, 0.4760, 0.4717, 0.4737, 0.4744, 0.4787, 0.4753, 0.4722, 0.4719,\n",
            "        0.4802, 0.4749, 0.4734, 0.4757, 0.4775, 0.4765, 0.4786, 0.4696, 0.4741,\n",
            "        0.4732, 0.4764, 0.4773, 0.4727, 0.4745, 0.4774, 0.4750, 0.4786, 0.4795,\n",
            "        0.4792, 0.4768, 0.4707, 0.4756, 0.4727, 0.4707, 0.4727, 0.4764, 0.4735,\n",
            "        0.4748, 0.4738, 0.4714, 0.4764, 0.4684, 0.4764, 0.4740, 0.4710, 0.4811,\n",
            "        0.4739])\n",
            "Sum of predicted tensor(30.3929)\n",
            "Loss tensor(0.6986)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4739, 0.4756, 0.4749, 0.4755, 0.4707, 0.4754, 0.4735, 0.4707, 0.4724,\n",
            "        0.4746, 0.4764, 0.4764, 0.4751, 0.4764, 0.4784, 0.4761, 0.4692, 0.4757,\n",
            "        0.4758, 0.4766, 0.4771, 0.4727, 0.4733, 0.4701, 0.4732, 0.4777, 0.4746,\n",
            "        0.4802, 0.4753, 0.4752, 0.4793, 0.4752, 0.4743, 0.4747, 0.4764, 0.4739,\n",
            "        0.4754, 0.4760, 0.4764, 0.4743, 0.4811, 0.4774, 0.4740, 0.4714, 0.4749,\n",
            "        0.4753, 0.4764, 0.4786, 0.4750, 0.4706, 0.4747, 0.4755, 0.4781, 0.4764,\n",
            "        0.4792, 0.4792, 0.4755, 0.4758, 0.4717, 0.4764, 0.4747, 0.4741, 0.4789,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.4159)\n",
            "Loss tensor(0.6919)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4779, 0.4792, 0.4684, 0.4722, 0.4786, 0.4765, 0.4727, 0.4727, 0.4764,\n",
            "        0.4725, 0.4727, 0.4720, 0.4760, 0.4793, 0.4767, 0.4757, 0.4734, 0.4719,\n",
            "        0.4782, 0.4727, 0.4703, 0.4772, 0.4758, 0.4752, 0.4710, 0.4719, 0.4727,\n",
            "        0.4745, 0.4731, 0.4748, 0.4765, 0.4750, 0.4732, 0.4763, 0.4727, 0.4757,\n",
            "        0.4727, 0.4734, 0.4745, 0.4745, 0.4705, 0.4728, 0.4727, 0.4727, 0.4761,\n",
            "        0.4767, 0.4740, 0.4758, 0.4757, 0.4764, 0.4707, 0.4743, 0.4770, 0.4737,\n",
            "        0.4764, 0.4725, 0.4753, 0.4728, 0.4740, 0.4727, 0.4782, 0.4752, 0.4764,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3646)\n",
            "Loss tensor(0.6987)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4754, 0.4697, 0.4764, 0.4761, 0.4762, 0.4793, 0.4699, 0.4781, 0.4764,\n",
            "        0.4727, 0.4727, 0.4764, 0.4713, 0.4768, 0.4756, 0.4811, 0.4727, 0.4712,\n",
            "        0.4764, 0.4757, 0.4773, 0.4731, 0.4753, 0.4778, 0.4747, 0.4773, 0.4756,\n",
            "        0.4767, 0.4742, 0.4744, 0.4807, 0.4778, 0.4769, 0.4764, 0.4717, 0.4767,\n",
            "        0.4792, 0.4705, 0.4754, 0.4707, 0.4735, 0.4705, 0.4771, 0.4734, 0.4727,\n",
            "        0.4752, 0.4738, 0.4832, 0.4710, 0.4721, 0.4764, 0.4739, 0.4783, 0.4764,\n",
            "        0.4728, 0.4756, 0.4710, 0.4767, 0.4756, 0.4793, 0.4699, 0.4756, 0.4727,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.4056)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4719, 0.4727, 0.4725, 0.4747, 0.4713, 0.4734, 0.4748, 0.4789, 0.4726,\n",
            "        0.4767, 0.4764, 0.4773, 0.4710, 0.4752, 0.4751, 0.4719, 0.4765, 0.4779,\n",
            "        0.4787, 0.4750, 0.4760, 0.4753, 0.4743, 0.4763, 0.4735, 0.4769, 0.4809,\n",
            "        0.4747, 0.4735, 0.4760, 0.4749, 0.4767, 0.4732, 0.4807, 0.4731, 0.4781,\n",
            "        0.4754, 0.4754, 0.4748, 0.4727, 0.4727, 0.4767, 0.4792, 0.4725, 0.4720,\n",
            "        0.4739, 0.4773, 0.4747, 0.4727, 0.4735, 0.4779, 0.4732, 0.4753, 0.4744,\n",
            "        0.4815, 0.4764, 0.4750, 0.4731, 0.4705, 0.4738, 0.4727, 0.4737, 0.4773,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.3998)\n",
            "Loss tensor(0.6964)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4771, 0.4762, 0.4732, 0.4768, 0.4756, 0.4719, 0.4727, 0.4739, 0.4708,\n",
            "        0.4736, 0.4727, 0.4757, 0.4754, 0.4718, 0.4801, 0.4720, 0.4752, 0.4759,\n",
            "        0.4712, 0.4761, 0.4761, 0.4787, 0.4776, 0.4703, 0.4764, 0.4749, 0.4752,\n",
            "        0.4770, 0.4775, 0.4786, 0.4740, 0.4730, 0.4773, 0.4719, 0.4736, 0.4775,\n",
            "        0.4764, 0.4725, 0.4722, 0.4730, 0.4775, 0.4727, 0.4764, 0.4787, 0.4727,\n",
            "        0.4707, 0.4744, 0.4779, 0.4732, 0.4747, 0.4732, 0.4758, 0.4740, 0.4746,\n",
            "        0.4764, 0.4761, 0.4738, 0.4744, 0.4722, 0.4723, 0.4740, 0.4747, 0.4722,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.3794)\n",
            "Loss tensor(0.6901)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4781, 0.4743, 0.4758, 0.4739, 0.4747, 0.4755, 0.4764, 0.4727, 0.4729,\n",
            "        0.4749, 0.4757, 0.4731, 0.4754, 0.4750, 0.4753, 0.4775, 0.4731, 0.4786,\n",
            "        0.4742, 0.4769, 0.4752, 0.4743, 0.4781, 0.4767, 0.4786, 0.4790, 0.4733,\n",
            "        0.4789, 0.4758, 0.4805, 0.4747, 0.4765, 0.4729, 0.4790, 0.4735, 0.4754,\n",
            "        0.4720, 0.4738, 0.4767, 0.4747, 0.4713, 0.4785, 0.4805, 0.4726, 0.4723,\n",
            "        0.4755, 0.4773, 0.4748, 0.4773, 0.4730, 0.4733, 0.4717, 0.4779, 0.4758,\n",
            "        0.4750, 0.4742, 0.4775, 0.4802, 0.4772, 0.4728, 0.4739, 0.4732, 0.4745,\n",
            "        0.4722])\n",
            "Sum of predicted tensor(30.4257)\n",
            "Loss tensor(0.6973)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4771, 0.4736, 0.4732, 0.4829, 0.4787, 0.4726, 0.4766, 0.4762, 0.4771,\n",
            "        0.4753, 0.4735, 0.4806, 0.4750, 0.4770, 0.4751, 0.4753, 0.4748, 0.4805,\n",
            "        0.4742, 0.4758, 0.4771, 0.4746, 0.4722, 0.4748, 0.4797, 0.4774, 0.4735,\n",
            "        0.4706, 0.4731, 0.4696, 0.4722, 0.4790, 0.4771, 0.4756, 0.4766, 0.4795,\n",
            "        0.4746, 0.4779, 0.4776, 0.4759, 0.4717, 0.4792, 0.4732, 0.4724, 0.4745,\n",
            "        0.4795, 0.4734, 0.4731, 0.4727, 0.4745, 0.4807, 0.4740, 0.4764, 0.4753,\n",
            "        0.4729, 0.4748, 0.4734, 0.4710, 0.4777, 0.4771, 0.4753, 0.4753, 0.4779,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4333)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4762, 0.4771, 0.4771, 0.4736, 0.4791, 0.4783, 0.4718, 0.4771, 0.4722,\n",
            "        0.4771, 0.4733, 0.4738, 0.4746, 0.4757, 0.4786, 0.4717, 0.4759, 0.4753,\n",
            "        0.4756, 0.4771, 0.4752, 0.4791, 0.4706, 0.4712, 0.4822, 0.4755, 0.4756,\n",
            "        0.4762, 0.4784, 0.4758, 0.4757, 0.4756, 0.4738, 0.4748, 0.4772, 0.4746,\n",
            "        0.4770, 0.4742, 0.4747, 0.4771, 0.4757, 0.4744, 0.4742, 0.4783, 0.4759,\n",
            "        0.4760, 0.4732, 0.4797, 0.4779, 0.4749, 0.4755, 0.4771, 0.4750, 0.4736,\n",
            "        0.4722, 0.4755, 0.4750, 0.4752, 0.4717, 0.4765, 0.4772, 0.4734, 0.4734,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4349)\n",
            "Loss tensor(0.6952)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4763, 0.4771, 0.4718, 0.4727, 0.4722, 0.4732, 0.4772, 0.4762, 0.4755,\n",
            "        0.4729, 0.4805, 0.4733, 0.4777, 0.4804, 0.4747, 0.4730, 0.4763, 0.4744,\n",
            "        0.4748, 0.4769, 0.4734, 0.4771, 0.4741, 0.4771, 0.4771, 0.4771, 0.4771,\n",
            "        0.4733, 0.4771, 0.4753, 0.4696, 0.4673, 0.4744, 0.4728, 0.4747, 0.4734,\n",
            "        0.4730, 0.4719, 0.4706, 0.4793, 0.4784, 0.4755, 0.4753, 0.4734, 0.4748,\n",
            "        0.4732, 0.4771, 0.4758, 0.4787, 0.4751, 0.4781, 0.4765, 0.4726, 0.4684,\n",
            "        0.4771, 0.4771, 0.4776, 0.4722, 0.4817, 0.4755, 0.4752, 0.4753, 0.4737,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.4025)\n",
            "Loss tensor(0.6996)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4769, 0.4754, 0.4767, 0.4748, 0.4774, 0.4746, 0.4768, 0.4741, 0.4779,\n",
            "        0.4771, 0.4771, 0.4732, 0.4773, 0.4745, 0.4756, 0.4703, 0.4771, 0.4728,\n",
            "        0.4722, 0.4797, 0.4781, 0.4763, 0.4769, 0.4732, 0.4749, 0.4733, 0.4718,\n",
            "        0.4729, 0.4806, 0.4725, 0.4734, 0.4750, 0.4768, 0.4692, 0.4722, 0.4759,\n",
            "        0.4757, 0.4771, 0.4742, 0.4771, 0.4726, 0.4776, 0.4740, 0.4770, 0.4712,\n",
            "        0.4692, 0.4771, 0.4743, 0.4771, 0.4778, 0.4753, 0.4762, 0.4696, 0.4733,\n",
            "        0.4753, 0.4733, 0.4722, 0.4771, 0.4748, 0.4728, 0.4817, 0.4767, 0.4839,\n",
            "        0.4730])\n",
            "Sum of predicted tensor(30.4114)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4722, 0.4720, 0.4754, 0.4785, 0.4771, 0.4731, 0.4771, 0.4764, 0.4793,\n",
            "        0.4771, 0.4771, 0.4767, 0.4771, 0.4771, 0.4715, 0.4807, 0.4771, 0.4732,\n",
            "        0.4755, 0.4717, 0.4728, 0.4813, 0.4753, 0.4740, 0.4771, 0.4771, 0.4766,\n",
            "        0.4770, 0.4736, 0.4752, 0.4728, 0.4760, 0.4711, 0.4799, 0.4733, 0.4748,\n",
            "        0.4721, 0.4742, 0.4722, 0.4783, 0.4796, 0.4771, 0.4836, 0.4754, 0.4747,\n",
            "        0.4715, 0.4783, 0.4774, 0.4773, 0.4755, 0.4771, 0.4752, 0.4775, 0.4821,\n",
            "        0.4748, 0.4734, 0.4771, 0.4748, 0.4706, 0.4761, 0.4759, 0.4776, 0.4734,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4517)\n",
            "Loss tensor(0.7021)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4789, 0.4756, 0.4745, 0.4757, 0.4753, 0.4751, 0.4763, 0.4726, 0.4744,\n",
            "        0.4741, 0.4771, 0.4756, 0.4772, 0.4723, 0.4746, 0.4735, 0.4712, 0.4748,\n",
            "        0.4772, 0.4707, 0.4739, 0.4769, 0.4767, 0.4758, 0.4774, 0.4760, 0.4843,\n",
            "        0.4706, 0.4775, 0.4741, 0.4771, 0.4722, 0.4749, 0.4724, 0.4753, 0.4771,\n",
            "        0.4779, 0.4771, 0.4778, 0.4777, 0.4763, 0.4755, 0.4750, 0.4771, 0.4722,\n",
            "        0.4765, 0.4733, 0.4710, 0.4771, 0.4771, 0.4750, 0.4760, 0.4733, 0.4768,\n",
            "        0.4732, 0.4771, 0.4791, 0.4707, 0.4752, 0.4748, 0.4706, 0.4722, 0.4752,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4157)\n",
            "Loss tensor(0.6971)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4749, 0.4779, 0.4772, 0.4733, 0.4743, 0.4724, 0.4734, 0.4771, 0.4762,\n",
            "        0.4767, 0.4769, 0.4771, 0.4760, 0.4771, 0.4735, 0.4715, 0.4751, 0.4728,\n",
            "        0.4718, 0.4763, 0.4771, 0.4747, 0.4734, 0.4722, 0.4767, 0.4793, 0.4751,\n",
            "        0.4825, 0.4755, 0.4749, 0.4778, 0.4762, 0.4771, 0.4738, 0.4720, 0.4695,\n",
            "        0.4699, 0.4771, 0.4771, 0.4732, 0.4717, 0.4765, 0.4728, 0.4751, 0.4771,\n",
            "        0.4747, 0.4748, 0.4781, 0.4771, 0.4753, 0.4722, 0.4771, 0.4735, 0.4717,\n",
            "        0.4706, 0.4801, 0.4754, 0.4779, 0.4706, 0.4780, 0.4771, 0.4704, 0.4743,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4032)\n",
            "Loss tensor(0.6951)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4741, 0.4770, 0.4771, 0.4751, 0.4713, 0.4771, 0.4769, 0.4801, 0.4733,\n",
            "        0.4746, 0.4771, 0.4723, 0.4726, 0.4806, 0.4791, 0.4817, 0.4751, 0.4772,\n",
            "        0.4742, 0.4730, 0.4722, 0.4729, 0.4707, 0.4760, 0.4728, 0.4789, 0.4748,\n",
            "        0.4767, 0.4762, 0.4690, 0.4722, 0.4772, 0.4771, 0.4753, 0.4771, 0.4733,\n",
            "        0.4733, 0.4743, 0.4771, 0.4752, 0.4766, 0.4772, 0.4757, 0.4750, 0.4738,\n",
            "        0.4710, 0.4759, 0.4756, 0.4777, 0.4771, 0.4754, 0.4724, 0.4779, 0.4758,\n",
            "        0.4734, 0.4797, 0.4745, 0.4777, 0.4778, 0.4765, 0.4678, 0.4702, 0.4771,\n",
            "        0.4764])\n",
            "Sum of predicted tensor(30.4199)\n",
            "Loss tensor(0.6928)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4750, 0.4794, 0.4760, 0.4754, 0.4779, 0.4746, 0.4771, 0.4781, 0.4706,\n",
            "        0.4798, 0.4754, 0.4712, 0.4789, 0.4744, 0.4722, 0.4771, 0.4743, 0.4707,\n",
            "        0.4730, 0.4789, 0.4748, 0.4771, 0.4722, 0.4706, 0.4741, 0.4791, 0.4858,\n",
            "        0.4777, 0.4735, 0.4705, 0.4787, 0.4754, 0.4771, 0.4744, 0.4740, 0.4725,\n",
            "        0.4793, 0.4723, 0.4756, 0.4751, 0.4768, 0.4771, 0.4688, 0.4748, 0.4732,\n",
            "        0.4821, 0.4714, 0.4771, 0.4734, 0.4760, 0.4753, 0.4771, 0.4812, 0.4755,\n",
            "        0.4775, 0.4771, 0.4730, 0.4771, 0.4746, 0.4792, 0.4754, 0.4753, 0.4787,\n",
            "        0.4722])\n",
            "Sum of predicted tensor(30.4396)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4760, 0.4776, 0.4776, 0.4746, 0.4746, 0.4732, 0.4771, 0.4733, 0.4745,\n",
            "        0.4745, 0.4754, 0.4723, 0.4758, 0.4792, 0.4743, 0.4706, 0.4722, 0.4715,\n",
            "        0.4746, 0.4770, 0.4717, 0.4775, 0.4777, 0.4742, 0.4760, 0.4732, 0.4759,\n",
            "        0.4768, 0.4756, 0.4771, 0.4771, 0.4752, 0.4771, 0.4751, 0.4730, 0.4747,\n",
            "        0.4765, 0.4809, 0.4771, 0.4771, 0.4729, 0.4726, 0.4754, 0.4742, 0.4744,\n",
            "        0.4791, 0.4765, 0.4771, 0.4713, 0.4771, 0.4793, 0.4750, 0.4772, 0.4722,\n",
            "        0.4751, 0.4753, 0.4749, 0.4740, 0.4744, 0.4770, 0.4771, 0.4771, 0.4722,\n",
            "        0.4786])\n",
            "Sum of predicted tensor(30.4256)\n",
            "Loss tensor(0.6970)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4732, 0.4734, 0.4722, 0.4772, 0.4728, 0.4771, 0.4791, 0.4739, 0.4730,\n",
            "        0.4760, 0.4706, 0.4780, 0.4750, 0.4762, 0.4696, 0.4797, 0.4793, 0.4776,\n",
            "        0.4771, 0.4752, 0.4768, 0.4739, 0.4760, 0.4748, 0.4783, 0.4763, 0.4792,\n",
            "        0.4748, 0.4766, 0.4728, 0.4753, 0.4732, 0.4752, 0.4757, 0.4776, 0.4728,\n",
            "        0.4755, 0.4773, 0.4762, 0.4682, 0.4728, 0.4725, 0.4791, 0.4762, 0.4810,\n",
            "        0.4800, 0.4784, 0.4767, 0.4771, 0.4768, 0.4722, 0.4732, 0.4753, 0.4720,\n",
            "        0.4728, 0.4697, 0.4729, 0.4817, 0.4771, 0.4817, 0.4742, 0.4756, 0.4771,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4360)\n",
            "Loss tensor(0.7105)\n",
            "Acc tensor(0.3281)\n",
            "tensor([0.4751, 0.4764, 0.4757, 0.4728, 0.4734, 0.4771, 0.4759, 0.4772, 0.4776,\n",
            "        0.4722, 0.4771, 0.4763, 0.4771, 0.4734, 0.4786, 0.4763, 0.4755, 0.4722,\n",
            "        0.4771, 0.4800, 0.4771, 0.4758, 0.4772, 0.4738, 0.4771, 0.4750, 0.4775,\n",
            "        0.4741, 0.4765, 0.4728, 0.4771, 0.4706, 0.4771, 0.4749, 0.4722, 0.4730,\n",
            "        0.4803, 0.4767, 0.4771, 0.4793, 0.4746, 0.4757, 0.4739, 0.4766, 0.4771,\n",
            "        0.4740, 0.4781, 0.4742, 0.4751, 0.4821, 0.4760, 0.4753, 0.4732, 0.4750,\n",
            "        0.4766, 0.4775, 0.4752, 0.4747, 0.4753, 0.4757, 0.4739, 0.4706, 0.4767,\n",
            "        0.4754])\n",
            "Sum of predicted tensor(30.4447)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4742, 0.4726, 0.4737, 0.4755, 0.4751, 0.4771, 0.4793, 0.4791, 0.4745,\n",
            "        0.4712, 0.4736, 0.4723, 0.4735, 0.4742, 0.4768, 0.4739, 0.4706, 0.4771,\n",
            "        0.4776, 0.4733, 0.4716, 0.4768, 0.4733, 0.4767, 0.4799, 0.4755, 0.4753,\n",
            "        0.4775, 0.4736, 0.4754, 0.4754, 0.4771, 0.4762, 0.4757, 0.4766, 0.4734,\n",
            "        0.4731, 0.4722, 0.4790, 0.4772, 0.4704, 0.4746, 0.4722, 0.4771, 0.4739,\n",
            "        0.4755, 0.4780, 0.4746, 0.4717, 0.4758, 0.4772, 0.4780, 0.4771, 0.4732,\n",
            "        0.4753, 0.4751, 0.4771, 0.4746, 0.4771, 0.4722, 0.4722, 0.4714, 0.4771,\n",
            "        0.4707])\n",
            "Sum of predicted tensor(30.3992)\n",
            "Loss tensor(0.7020)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4771, 0.4771, 0.4775, 0.4767, 0.4757, 0.4771, 0.4762, 0.4752, 0.4785,\n",
            "        0.4732, 0.4771, 0.4772, 0.4762, 0.4771, 0.4753, 0.4715, 0.4731, 0.4721,\n",
            "        0.4748, 0.4771, 0.4756, 0.4754, 0.4771, 0.4806, 0.4771, 0.4771, 0.4771,\n",
            "        0.4725, 0.4731, 0.4732, 0.4746, 0.4732, 0.4771, 0.4745, 0.4772, 0.4722,\n",
            "        0.4695, 0.4785, 0.4769, 0.4768, 0.4756, 0.4742, 0.4763, 0.4749, 0.4753,\n",
            "        0.4771, 0.4771, 0.4731, 0.4722, 0.4771, 0.4803, 0.4704, 0.4701, 0.4752,\n",
            "        0.4757, 0.4712, 0.4771, 0.4732, 0.4747, 0.4732, 0.4749, 0.4771, 0.4715,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.4196)\n",
            "Loss tensor(0.7060)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4763, 0.4771, 0.4771, 0.4748, 0.4760, 0.4746, 0.4771, 0.4771, 0.4731,\n",
            "        0.4790, 0.4762, 0.4767, 0.4742, 0.4759, 0.4771, 0.4772, 0.4707, 0.4729,\n",
            "        0.4733, 0.4793, 0.4771, 0.4811, 0.4795, 0.4770, 0.4772, 0.4740, 0.4748,\n",
            "        0.4771, 0.4722, 0.4732, 0.4771, 0.4811, 0.4752, 0.4766, 0.4765, 0.4722,\n",
            "        0.4766, 0.4722, 0.4750, 0.4768, 0.4773, 0.4761, 0.4795, 0.4696, 0.4816,\n",
            "        0.4736, 0.4771, 0.4771, 0.4801, 0.4748, 0.4722, 0.4771, 0.4746, 0.4771,\n",
            "        0.4777, 0.4754, 0.4776, 0.4725, 0.4725, 0.4764, 0.4771, 0.4730, 0.4762,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4583)\n",
            "Loss tensor(0.6973)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4766, 0.4809, 0.4755, 0.4735, 0.4785, 0.4771, 0.4751, 0.4822, 0.4760,\n",
            "        0.4737, 0.4755, 0.4741, 0.4797, 0.4723, 0.4764, 0.4759, 0.4752, 0.4775,\n",
            "        0.4749, 0.4685, 0.4754, 0.4712, 0.4764, 0.4792, 0.4754, 0.4785, 0.4766,\n",
            "        0.4739, 0.4764, 0.4776, 0.4768, 0.4763, 0.4779, 0.4740, 0.4723, 0.4755,\n",
            "        0.4738, 0.4742, 0.4739, 0.4787, 0.4743, 0.4685, 0.4753, 0.4766, 0.4722,\n",
            "        0.4706, 0.4715, 0.4790, 0.4706, 0.4753, 0.4765, 0.4813, 0.4722, 0.4696,\n",
            "        0.4763, 0.4759, 0.4706, 0.4748, 0.4783, 0.4747, 0.4796, 0.4771, 0.4734,\n",
            "        0.4769])\n",
            "Sum of predicted tensor(30.4244)\n",
            "Loss tensor(0.6894)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4791, 0.4718, 0.4802, 0.4766, 0.4782, 0.4766, 0.4778, 0.4757, 0.4685,\n",
            "        0.4740, 0.4726, 0.4762, 0.4715, 0.4728, 0.4779, 0.4724, 0.4758, 0.4765,\n",
            "        0.4754, 0.4742, 0.4811, 0.4746, 0.4765, 0.4752, 0.4757, 0.4766, 0.4770,\n",
            "        0.4740, 0.4772, 0.4758, 0.4706, 0.4762, 0.4716, 0.4749, 0.4708, 0.4766,\n",
            "        0.4727, 0.4759, 0.4762, 0.4757, 0.4733, 0.4726, 0.4770, 0.4760, 0.4742,\n",
            "        0.4770, 0.4757, 0.4752, 0.4751, 0.4720, 0.4811, 0.4759, 0.4716, 0.4749,\n",
            "        0.4733, 0.4775, 0.4737, 0.4715, 0.4739, 0.4767, 0.4754, 0.4766, 0.4746,\n",
            "        0.4766])\n",
            "Sum of predicted tensor(30.4100)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4776, 0.4731, 0.4715, 0.4764, 0.4743, 0.4770, 0.4739, 0.4734, 0.4766,\n",
            "        0.4752, 0.4770, 0.4718, 0.4757, 0.4748, 0.4766, 0.4749, 0.4794, 0.4761,\n",
            "        0.4730, 0.4757, 0.4753, 0.4762, 0.4754, 0.4754, 0.4770, 0.4741, 0.4775,\n",
            "        0.4734, 0.4743, 0.4766, 0.4746, 0.4748, 0.4766, 0.4719, 0.4766, 0.4748,\n",
            "        0.4722, 0.4715, 0.4702, 0.4766, 0.4760, 0.4685, 0.4766, 0.4771, 0.4764,\n",
            "        0.4775, 0.4760, 0.4741, 0.4773, 0.4754, 0.4760, 0.4754, 0.4787, 0.4735,\n",
            "        0.4753, 0.4752, 0.4756, 0.4773, 0.4744, 0.4779, 0.4701, 0.4761, 0.4765,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.4111)\n",
            "Loss tensor(0.6920)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4775, 0.4766, 0.4725, 0.4802, 0.4747, 0.4685, 0.4759, 0.4766, 0.4753,\n",
            "        0.4676, 0.4757, 0.4761, 0.4775, 0.4717, 0.4718, 0.4723, 0.4766, 0.4767,\n",
            "        0.4812, 0.4765, 0.4765, 0.4761, 0.4766, 0.4769, 0.4750, 0.4685, 0.4771,\n",
            "        0.4766, 0.4754, 0.4766, 0.4741, 0.4735, 0.4743, 0.4752, 0.4746, 0.4766,\n",
            "        0.4751, 0.4753, 0.4740, 0.4742, 0.4761, 0.4727, 0.4685, 0.4685, 0.4763,\n",
            "        0.4704, 0.4766, 0.4726, 0.4760, 0.4764, 0.4757, 0.4766, 0.4703, 0.4779,\n",
            "        0.4718, 0.4721, 0.4766, 0.4784, 0.4840, 0.4757, 0.4766, 0.4778, 0.4776,\n",
            "        0.4710])\n",
            "Sum of predicted tensor(30.3996)\n",
            "Loss tensor(0.7047)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4754, 0.4726, 0.4776, 0.4783, 0.4743, 0.4685, 0.4769, 0.4717, 0.4705,\n",
            "        0.4725, 0.4716, 0.4754, 0.4710, 0.4731, 0.4752, 0.4753, 0.4774, 0.4717,\n",
            "        0.4758, 0.4743, 0.4752, 0.4769, 0.4754, 0.4808, 0.4775, 0.4698, 0.4770,\n",
            "        0.4794, 0.4715, 0.4777, 0.4775, 0.4797, 0.4748, 0.4743, 0.4775, 0.4747,\n",
            "        0.4720, 0.4710, 0.4795, 0.4754, 0.4753, 0.4761, 0.4766, 0.4782, 0.4745,\n",
            "        0.4710, 0.4840, 0.4773, 0.4728, 0.4759, 0.4754, 0.4761, 0.4761, 0.4711,\n",
            "        0.4737, 0.4766, 0.4754, 0.4727, 0.4765, 0.4725, 0.4777, 0.4758, 0.4742,\n",
            "        0.4708])\n",
            "Sum of predicted tensor(30.4026)\n",
            "Loss tensor(0.6977)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4773, 0.4740, 0.4707, 0.4798, 0.4707, 0.4705, 0.4780, 0.4766, 0.4727,\n",
            "        0.4707, 0.4722, 0.4759, 0.4743, 0.4735, 0.4728, 0.4756, 0.4781, 0.4710,\n",
            "        0.4750, 0.4743, 0.4715, 0.4762, 0.4756, 0.4708, 0.4743, 0.4777, 0.4750,\n",
            "        0.4765, 0.4746, 0.4740, 0.4757, 0.4756, 0.4770, 0.4758, 0.4715, 0.4742,\n",
            "        0.4713, 0.4775, 0.4796, 0.4780, 0.4740, 0.4685, 0.4723, 0.4711, 0.4766,\n",
            "        0.4766, 0.4787, 0.4703, 0.4766, 0.4711, 0.4742, 0.4705, 0.4743, 0.4751,\n",
            "        0.4753, 0.4749, 0.4743, 0.4762, 0.4740, 0.4726, 0.4759, 0.4685, 0.4731,\n",
            "        0.4758])\n",
            "Sum of predicted tensor(30.3564)\n",
            "Loss tensor(0.6929)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4779, 0.4762, 0.4794, 0.4749, 0.4771, 0.4775, 0.4761, 0.4776, 0.4766,\n",
            "        0.4756, 0.4760, 0.4749, 0.4726, 0.4776, 0.4756, 0.4735, 0.4763, 0.4757,\n",
            "        0.4754, 0.4751, 0.4776, 0.4752, 0.4786, 0.4721, 0.4784, 0.4766, 0.4792,\n",
            "        0.4759, 0.4780, 0.4753, 0.4723, 0.4771, 0.4712, 0.4690, 0.4766, 0.4766,\n",
            "        0.4766, 0.4731, 0.4739, 0.4814, 0.4744, 0.4742, 0.4758, 0.4766, 0.4749,\n",
            "        0.4794, 0.4759, 0.4702, 0.4793, 0.4766, 0.4763, 0.4685, 0.4738, 0.4727,\n",
            "        0.4776, 0.4766, 0.4757, 0.4734, 0.4753, 0.4766, 0.4716, 0.4752, 0.4727,\n",
            "        0.4795])\n",
            "Sum of predicted tensor(30.4390)\n",
            "Loss tensor(0.7058)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4685, 0.4771, 0.4774, 0.4713, 0.4718, 0.4710, 0.4729, 0.4726, 0.4764,\n",
            "        0.4762, 0.4751, 0.4738, 0.4754, 0.4753, 0.4768, 0.4760, 0.4744, 0.4761,\n",
            "        0.4716, 0.4786, 0.4740, 0.4764, 0.4764, 0.4742, 0.4739, 0.4771, 0.4754,\n",
            "        0.4775, 0.4742, 0.4717, 0.4749, 0.4713, 0.4742, 0.4729, 0.4701, 0.4754,\n",
            "        0.4744, 0.4737, 0.4744, 0.4766, 0.4747, 0.4756, 0.4753, 0.4707, 0.4756,\n",
            "        0.4770, 0.4760, 0.4762, 0.4788, 0.4798, 0.4807, 0.4728, 0.4785, 0.4760,\n",
            "        0.4721, 0.4766, 0.4760, 0.4757, 0.4794, 0.4761, 0.4762, 0.4779, 0.4766,\n",
            "        0.4766])\n",
            "Sum of predicted tensor(30.4082)\n",
            "Loss tensor(0.6897)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4742, 0.4756, 0.4804, 0.4739, 0.4757, 0.4750, 0.4765, 0.4740, 0.4766,\n",
            "        0.4705, 0.4739, 0.4735, 0.4765, 0.4693, 0.4791, 0.4727, 0.4768, 0.4740,\n",
            "        0.4763, 0.4748, 0.4745, 0.4730, 0.4685, 0.4764, 0.4781, 0.4726, 0.4766,\n",
            "        0.4766, 0.4765, 0.4717, 0.4708, 0.4717, 0.4700, 0.4762, 0.4794, 0.4710,\n",
            "        0.4732, 0.4747, 0.4773, 0.4747, 0.4725, 0.4707, 0.4738, 0.4762, 0.4726,\n",
            "        0.4777, 0.4797, 0.4727, 0.4757, 0.4765, 0.4759, 0.4733, 0.4754, 0.4748,\n",
            "        0.4755, 0.4747, 0.4754, 0.4755, 0.4741, 0.4766, 0.4729, 0.4757, 0.4685,\n",
            "        0.4757])\n",
            "Sum of predicted tensor(30.3750)\n",
            "Loss tensor(0.6981)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4770, 0.4754, 0.4730, 0.4776, 0.4780, 0.4709, 0.4766, 0.4757, 0.4777,\n",
            "        0.4808, 0.4705, 0.4763, 0.4739, 0.4742, 0.4808, 0.4840, 0.4762, 0.4760,\n",
            "        0.4752, 0.4765, 0.4762, 0.4763, 0.4756, 0.4708, 0.4743, 0.4717, 0.4809,\n",
            "        0.4752, 0.4753, 0.4749, 0.4746, 0.4795, 0.4751, 0.4755, 0.4765, 0.4714,\n",
            "        0.4762, 0.4755, 0.4802, 0.4808, 0.4750, 0.4775, 0.4786, 0.4716, 0.4794,\n",
            "        0.4765, 0.4765, 0.4756, 0.4766, 0.4766, 0.4835, 0.4761, 0.4743, 0.4726,\n",
            "        0.4738, 0.4741, 0.4756, 0.4740, 0.4696, 0.4746, 0.4740, 0.4748, 0.4761,\n",
            "        0.4719])\n",
            "Sum of predicted tensor(30.4518)\n",
            "Loss tensor(0.6883)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4814, 0.4713, 0.4731, 0.4747, 0.4766, 0.4685, 0.4796, 0.4749, 0.4769,\n",
            "        0.4750, 0.4764, 0.4742, 0.4718, 0.4799, 0.4739, 0.4756, 0.4796, 0.4745,\n",
            "        0.4739, 0.4761, 0.4744, 0.4775, 0.4766, 0.4740, 0.4795, 0.4771, 0.4776,\n",
            "        0.4747, 0.4713, 0.4749, 0.4802, 0.4701, 0.4762, 0.4743, 0.4746, 0.4728,\n",
            "        0.4685, 0.4782, 0.4770, 0.4786, 0.4716, 0.4711, 0.4766, 0.4759, 0.4754,\n",
            "        0.4753, 0.4761, 0.4743, 0.4711, 0.4766, 0.4766, 0.4717, 0.4775, 0.4808,\n",
            "        0.4774, 0.4758, 0.4685, 0.4747, 0.4784, 0.4743, 0.4821, 0.4766, 0.4788,\n",
            "        0.4782])\n",
            "Sum of predicted tensor(30.4316)\n",
            "Loss tensor(0.7082)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0.4714, 0.4738, 0.4749, 0.4754, 0.4742, 0.4735, 0.4727, 0.4755, 0.4797,\n",
            "        0.4786, 0.4709, 0.4742, 0.4732, 0.4717, 0.4766, 0.4740, 0.4740, 0.4802,\n",
            "        0.4746, 0.4756, 0.4758, 0.4735, 0.4798, 0.4760, 0.4783, 0.4770, 0.4748,\n",
            "        0.4754, 0.4814, 0.4792, 0.4729, 0.4747, 0.4744, 0.4685, 0.4779, 0.4769,\n",
            "        0.4739, 0.4767, 0.4754, 0.4766, 0.4777, 0.4713, 0.4787, 0.4763, 0.4743,\n",
            "        0.4755, 0.4766, 0.4777, 0.4774, 0.4733, 0.4775, 0.4753, 0.4762, 0.4735,\n",
            "        0.4750, 0.4758, 0.4735, 0.4755, 0.4760, 0.4752, 0.4745, 0.4768, 0.4706,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4220)\n",
            "Loss tensor(0.6968)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4724, 0.4744, 0.4685, 0.4743, 0.4726, 0.4746, 0.4710, 0.4754, 0.4745,\n",
            "        0.4685, 0.4768, 0.4835, 0.4737, 0.4749, 0.4765, 0.4752, 0.4739, 0.4760,\n",
            "        0.4697, 0.4776, 0.4776, 0.4734, 0.4765, 0.4772, 0.4685, 0.4751, 0.4742,\n",
            "        0.4786, 0.4775, 0.4745, 0.4784, 0.4775, 0.4742, 0.4766, 0.4685, 0.4753,\n",
            "        0.4685, 0.4766, 0.4765, 0.4665, 0.4742, 0.4752, 0.4776, 0.4752, 0.4763,\n",
            "        0.4716, 0.4747, 0.4765, 0.4786, 0.4770, 0.4747, 0.4750, 0.4744, 0.4762,\n",
            "        0.4691, 0.4750, 0.4763, 0.4760, 0.4808, 0.4760, 0.4747, 0.4749, 0.4728,\n",
            "        0.4773])\n",
            "Sum of predicted tensor(30.3860)\n",
            "Loss tensor(0.6916)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4756, 0.4752, 0.4760, 0.4753, 0.4786, 0.4776, 0.4760, 0.4727, 0.4728,\n",
            "        0.4766, 0.4729, 0.4789, 0.4766, 0.4739, 0.4710, 0.4732, 0.4749, 0.4741,\n",
            "        0.4775, 0.4705, 0.4703, 0.4776, 0.4760, 0.4710, 0.4758, 0.4743, 0.4802,\n",
            "        0.4765, 0.4758, 0.4766, 0.4714, 0.4782, 0.4766, 0.4754, 0.4750, 0.4717,\n",
            "        0.4761, 0.4766, 0.4756, 0.4757, 0.4752, 0.4743, 0.4744, 0.4783, 0.4756,\n",
            "        0.4750, 0.4802, 0.4747, 0.4746, 0.4743, 0.4763, 0.4717, 0.4762, 0.4741,\n",
            "        0.4739, 0.4701, 0.4746, 0.4764, 0.4730, 0.4781, 0.4751, 0.4721, 0.4795,\n",
            "        0.4775])\n",
            "Sum of predicted tensor(30.4118)\n",
            "Loss tensor(0.7060)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4723, 0.4739, 0.4759, 0.4737, 0.4725, 0.4753, 0.4772, 0.4751, 0.4756,\n",
            "        0.4741, 0.4800, 0.4760, 0.4758, 0.4752, 0.4739, 0.4722, 0.4763, 0.4752,\n",
            "        0.4757, 0.4751, 0.4751, 0.4702, 0.4784, 0.4753, 0.4710, 0.4775, 0.4723,\n",
            "        0.4709, 0.4835, 0.4738, 0.4742, 0.4764, 0.4740, 0.4750, 0.4723, 0.4775,\n",
            "        0.4760, 0.4717, 0.4764, 0.4746, 0.4748, 0.4708, 0.4695, 0.4745, 0.4766,\n",
            "        0.4766, 0.4756, 0.4717, 0.4761, 0.4685, 0.4741, 0.4727, 0.4737, 0.4765,\n",
            "        0.4725, 0.4766, 0.4755, 0.4808, 0.4752, 0.4757, 0.4735, 0.4741, 0.4742,\n",
            "        0.4714])\n",
            "Sum of predicted tensor(30.3783)\n",
            "Loss tensor(0.6876)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4760, 0.4775, 0.4715, 0.4763, 0.4808, 0.4795, 0.4692, 0.4771, 0.4704,\n",
            "        0.4785, 0.4745, 0.4759, 0.4767, 0.4778, 0.4783, 0.4766, 0.4736, 0.4729,\n",
            "        0.4816, 0.4738, 0.4723, 0.4781, 0.4804, 0.4791, 0.4776, 0.4759, 0.4731,\n",
            "        0.4736, 0.4727, 0.4748, 0.4723, 0.4731, 0.4736, 0.4692, 0.4759, 0.4754,\n",
            "        0.4751, 0.4689, 0.4736, 0.4732, 0.4707, 0.4782, 0.4771, 0.4763, 0.4728,\n",
            "        0.4700, 0.4753, 0.4756, 0.4723, 0.4773, 0.4721, 0.4759, 0.4762, 0.4729,\n",
            "        0.4697, 0.4722, 0.4698, 0.4734, 0.4727, 0.4762, 0.4759, 0.4722, 0.4762,\n",
            "        0.4778])\n",
            "Sum of predicted tensor(30.3851)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4766, 0.4793, 0.4731, 0.4748, 0.4727, 0.4762, 0.4746, 0.4702, 0.4733,\n",
            "        0.4751, 0.4793, 0.4767, 0.4736, 0.4784, 0.4761, 0.4757, 0.4736, 0.4729,\n",
            "        0.4760, 0.4741, 0.4736, 0.4699, 0.4808, 0.4778, 0.4791, 0.4774, 0.4775,\n",
            "        0.4779, 0.4750, 0.4773, 0.4775, 0.4772, 0.4762, 0.4768, 0.4760, 0.4727,\n",
            "        0.4715, 0.4750, 0.4774, 0.4750, 0.4745, 0.4745, 0.4761, 0.4751, 0.4762,\n",
            "        0.4751, 0.4728, 0.4782, 0.4771, 0.4754, 0.4763, 0.4700, 0.4726, 0.4736,\n",
            "        0.4766, 0.4731, 0.4736, 0.4750, 0.4740, 0.4765, 0.4790, 0.4763, 0.4808,\n",
            "        0.4763])\n",
            "Sum of predicted tensor(30.4329)\n",
            "Loss tensor(0.6900)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4781, 0.4757, 0.4757, 0.4748, 0.4739, 0.4782, 0.4739, 0.4728, 0.4757,\n",
            "        0.4821, 0.4786, 0.4742, 0.4758, 0.4759, 0.4785, 0.4736, 0.4728, 0.4775,\n",
            "        0.4734, 0.4745, 0.4761, 0.4772, 0.4744, 0.4786, 0.4761, 0.4734, 0.4785,\n",
            "        0.4756, 0.4750, 0.4818, 0.4752, 0.4757, 0.4779, 0.4775, 0.4775, 0.4776,\n",
            "        0.4796, 0.4744, 0.4722, 0.4753, 0.4782, 0.4743, 0.4741, 0.4793, 0.4779,\n",
            "        0.4742, 0.4776, 0.4786, 0.4745, 0.4730, 0.4758, 0.4772, 0.4774, 0.4739,\n",
            "        0.4712, 0.4758, 0.4707, 0.4670, 0.4760, 0.4808, 0.4774, 0.4766, 0.4783,\n",
            "        0.4780])\n",
            "Sum of predicted tensor(30.4632)\n",
            "Loss tensor(0.6865)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4699, 0.4715, 0.4733, 0.4756, 0.4793, 0.4719, 0.4745, 0.4760, 0.4732,\n",
            "        0.4751, 0.4760, 0.4770, 0.4804, 0.4736, 0.4808, 0.4731, 0.4780, 0.4758,\n",
            "        0.4808, 0.4761, 0.4736, 0.4741, 0.4746, 0.4729, 0.4752, 0.4795, 0.4770,\n",
            "        0.4736, 0.4736, 0.4790, 0.4740, 0.4760, 0.4736, 0.4736, 0.4720, 0.4748,\n",
            "        0.4739, 0.4756, 0.4731, 0.4751, 0.4805, 0.4758, 0.4747, 0.4804, 0.4754,\n",
            "        0.4757, 0.4728, 0.4765, 0.4776, 0.4757, 0.4764, 0.4728, 0.4745, 0.4718,\n",
            "        0.4721, 0.4742, 0.4748, 0.4739, 0.4748, 0.4724, 0.4760, 0.4740, 0.4775,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4117)\n",
            "Loss tensor(0.6996)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4787, 0.4754, 0.4757, 0.4758, 0.4778, 0.4743, 0.4766, 0.4729, 0.4747,\n",
            "        0.4740, 0.4747, 0.4786, 0.4729, 0.4751, 0.4755, 0.4798, 0.4723, 0.4751,\n",
            "        0.4721, 0.4768, 0.4778, 0.4703, 0.4779, 0.4709, 0.4746, 0.4731, 0.4736,\n",
            "        0.4768, 0.4761, 0.4742, 0.4810, 0.4772, 0.4770, 0.4768, 0.4761, 0.4736,\n",
            "        0.4723, 0.4736, 0.4767, 0.4730, 0.4767, 0.4738, 0.4725, 0.4764, 0.4741,\n",
            "        0.4747, 0.4777, 0.4740, 0.4791, 0.4763, 0.4765, 0.4777, 0.4743, 0.4689,\n",
            "        0.4734, 0.4734, 0.4761, 0.4736, 0.4801, 0.4756, 0.4743, 0.4764, 0.4742,\n",
            "        0.4804])\n",
            "Sum of predicted tensor(30.4217)\n",
            "Loss tensor(0.6945)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4721, 0.4744, 0.4710, 0.4741, 0.4785, 0.4753, 0.4789, 0.4762, 0.4749,\n",
            "        0.4744, 0.4734, 0.4781, 0.4744, 0.4738, 0.4736, 0.4794, 0.4793, 0.4733,\n",
            "        0.4742, 0.4761, 0.4751, 0.4733, 0.4729, 0.4761, 0.4751, 0.4727, 0.4728,\n",
            "        0.4749, 0.4751, 0.4732, 0.4738, 0.4748, 0.4725, 0.4746, 0.4714, 0.4784,\n",
            "        0.4761, 0.4780, 0.4755, 0.4783, 0.4758, 0.4778, 0.4747, 0.4749, 0.4780,\n",
            "        0.4751, 0.4793, 0.4760, 0.4722, 0.4705, 0.4743, 0.4789, 0.4740, 0.4761,\n",
            "        0.4759, 0.4769, 0.4755, 0.4777, 0.4801, 0.4778, 0.4738, 0.4754, 0.4746,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4212)\n",
            "Loss tensor(0.6912)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4767, 0.4803, 0.4745, 0.4729, 0.4751, 0.4796, 0.4770, 0.4829, 0.4739,\n",
            "        0.4736, 0.4752, 0.4722, 0.4766, 0.4783, 0.4725, 0.4786, 0.4762, 0.4753,\n",
            "        0.4751, 0.4798, 0.4821, 0.4744, 0.4745, 0.4783, 0.4719, 0.4705, 0.4761,\n",
            "        0.4774, 0.4810, 0.4770, 0.4745, 0.4786, 0.4769, 0.4779, 0.4764, 0.4779,\n",
            "        0.4768, 0.4758, 0.4711, 0.4751, 0.4785, 0.4718, 0.4756, 0.4764, 0.4748,\n",
            "        0.4785, 0.4766, 0.4761, 0.4760, 0.4731, 0.4736, 0.4801, 0.4703, 0.4765,\n",
            "        0.4761, 0.4745, 0.4782, 0.4761, 0.4761, 0.4809, 0.4697, 0.4769, 0.4703,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4637)\n",
            "Loss tensor(0.6876)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4761, 0.4736, 0.4762, 0.4759, 0.4807, 0.4746, 0.4757, 0.4764, 0.4740,\n",
            "        0.4736, 0.4745, 0.4750, 0.4728, 0.4752, 0.4700, 0.4761, 0.4745, 0.4752,\n",
            "        0.4751, 0.4769, 0.4748, 0.4758, 0.4763, 0.4764, 0.4707, 0.4703, 0.4756,\n",
            "        0.4762, 0.4761, 0.4736, 0.4819, 0.4789, 0.4767, 0.4805, 0.4757, 0.4755,\n",
            "        0.4731, 0.4759, 0.4772, 0.4734, 0.4782, 0.4757, 0.4764, 0.4733, 0.4752,\n",
            "        0.4757, 0.4708, 0.4744, 0.4783, 0.4766, 0.4759, 0.4740, 0.4751, 0.4752,\n",
            "        0.4742, 0.4778, 0.4726, 0.4731, 0.4814, 0.4755, 0.4745, 0.4745, 0.4736,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4240)\n",
            "Loss tensor(0.6929)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4785, 0.4753, 0.4728, 0.4736, 0.4752, 0.4763, 0.4765, 0.4747, 0.4786,\n",
            "        0.4774, 0.4757, 0.4749, 0.4833, 0.4710, 0.4759, 0.4724, 0.4736, 0.4716,\n",
            "        0.4739, 0.4786, 0.4722, 0.4728, 0.4749, 0.4829, 0.4792, 0.4777, 0.4790,\n",
            "        0.4782, 0.4743, 0.4755, 0.4736, 0.4733, 0.4817, 0.4759, 0.4777, 0.4722,\n",
            "        0.4779, 0.4765, 0.4764, 0.4786, 0.4775, 0.4758, 0.4736, 0.4771, 0.4758,\n",
            "        0.4736, 0.4776, 0.4697, 0.4736, 0.4747, 0.4745, 0.4758, 0.4739, 0.4772,\n",
            "        0.4743, 0.4745, 0.4763, 0.4753, 0.4783, 0.4753, 0.4747, 0.4728, 0.4772,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4439)\n",
            "Loss tensor(0.6942)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4729, 0.4719, 0.4729, 0.4766, 0.4736, 0.4786, 0.4728, 0.4747, 0.4718,\n",
            "        0.4710, 0.4745, 0.4780, 0.4801, 0.4750, 0.4808, 0.4684, 0.4786, 0.4729,\n",
            "        0.4766, 0.4751, 0.4757, 0.4738, 0.4736, 0.4744, 0.4728, 0.4746, 0.4808,\n",
            "        0.4767, 0.4755, 0.4806, 0.4774, 0.4720, 0.4747, 0.4761, 0.4751, 0.4748,\n",
            "        0.4762, 0.4766, 0.4726, 0.4733, 0.4790, 0.4748, 0.4713, 0.4820, 0.4711,\n",
            "        0.4727, 0.4744, 0.4751, 0.4716, 0.4708, 0.4747, 0.4779, 0.4793, 0.4770,\n",
            "        0.4761, 0.4798, 0.4763, 0.4763, 0.4736, 0.4761, 0.4779, 0.4753, 0.4760,\n",
            "        0.4729])\n",
            "Sum of predicted tensor(30.4160)\n",
            "Loss tensor(0.6895)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4770, 0.4764, 0.4764, 0.4695, 0.4779, 0.4759, 0.4714, 0.4785, 0.4756,\n",
            "        0.4725, 0.4722, 0.4757, 0.4736, 0.4742, 0.4753, 0.4732, 0.4750, 0.4780,\n",
            "        0.4720, 0.4745, 0.4797, 0.4748, 0.4797, 0.4750, 0.4736, 0.4771, 0.4785,\n",
            "        0.4736, 0.4753, 0.4770, 0.4739, 0.4739, 0.4748, 0.4705, 0.4753, 0.4713,\n",
            "        0.4778, 0.4697, 0.4736, 0.4727, 0.4729, 0.4759, 0.4722, 0.4829, 0.4752,\n",
            "        0.4727, 0.4734, 0.4759, 0.4751, 0.4761, 0.4755, 0.4782, 0.4791, 0.4771,\n",
            "        0.4762, 0.4780, 0.4755, 0.4749, 0.4742, 0.4760, 0.4748, 0.4744, 0.4744,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4091)\n",
            "Loss tensor(0.6938)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4778, 0.4700, 0.4766, 0.4752, 0.4734, 0.4772, 0.4768, 0.4736, 0.4766,\n",
            "        0.4811, 0.4771, 0.4761, 0.4718, 0.4753, 0.4767, 0.4772, 0.4779, 0.4734,\n",
            "        0.4760, 0.4703, 0.4785, 0.4736, 0.4734, 0.4751, 0.4736, 0.4762, 0.4777,\n",
            "        0.4788, 0.4736, 0.4719, 0.4697, 0.4716, 0.4726, 0.4750, 0.4761, 0.4745,\n",
            "        0.4764, 0.4729, 0.4736, 0.4763, 0.4744, 0.4732, 0.4741, 0.4806, 0.4740,\n",
            "        0.4774, 0.4763, 0.4782, 0.4759, 0.4764, 0.4731, 0.4774, 0.4721, 0.4731,\n",
            "        0.4783, 0.4746, 0.4755, 0.4751, 0.4783, 0.4759, 0.4795, 0.4796, 0.4725,\n",
            "        0.4784])\n",
            "Sum of predicted tensor(30.4251)\n",
            "Loss tensor(0.6963)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4766, 0.4702, 0.4761, 0.4766, 0.4771, 0.4718, 0.4736, 0.4761, 0.4783,\n",
            "        0.4761, 0.4720, 0.4765, 0.4755, 0.4784, 0.4785, 0.4762, 0.4761, 0.4752,\n",
            "        0.4821, 0.4748, 0.4723, 0.4766, 0.4783, 0.4732, 0.4745, 0.4704, 0.4756,\n",
            "        0.4762, 0.4736, 0.4718, 0.4727, 0.4711, 0.4751, 0.4769, 0.4761, 0.4736,\n",
            "        0.4782, 0.4714, 0.4742, 0.4770, 0.4735, 0.4718, 0.4779, 0.4736, 0.4766,\n",
            "        0.4757, 0.4727, 0.4779, 0.4749, 0.4797, 0.4746, 0.4765, 0.4764, 0.4770,\n",
            "        0.4760, 0.4742, 0.4725, 0.4791, 0.4739, 0.4689, 0.4752, 0.4680, 0.4734,\n",
            "        0.4774])\n",
            "Sum of predicted tensor(30.4042)\n",
            "Loss tensor(0.6911)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4736, 0.4729, 0.4804, 0.4727, 0.4752, 0.4748, 0.4736, 0.4767, 0.4771,\n",
            "        0.4719, 0.4761, 0.4785, 0.4769, 0.4784, 0.4771, 0.4756, 0.4771, 0.4780,\n",
            "        0.4779, 0.4767, 0.4761, 0.4721, 0.4779, 0.4759, 0.4740, 0.4748, 0.4722,\n",
            "        0.4768, 0.4797, 0.4768, 0.4719, 0.4736, 0.4745, 0.4757, 0.4761, 0.4744,\n",
            "        0.4776, 0.4736, 0.4742, 0.4755, 0.4692, 0.4746, 0.4737, 0.4751, 0.4783,\n",
            "        0.4755, 0.4769, 0.4705, 0.4750, 0.4781, 0.4760, 0.4740, 0.4822, 0.4714,\n",
            "        0.4758, 0.4794, 0.4717, 0.4764, 0.4764, 0.4730, 0.4761, 0.4748, 0.4700,\n",
            "        0.4752])\n",
            "Sum of predicted tensor(30.4236)\n",
            "Loss tensor(0.7013)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4780, 0.4741, 0.4767, 0.4736, 0.4737, 0.4785, 0.4771, 0.4745, 0.4746,\n",
            "        0.4709, 0.4769, 0.4753, 0.4729, 0.4763, 0.4736, 0.4781, 0.4752, 0.4766,\n",
            "        0.4758, 0.4754, 0.4766, 0.4710, 0.4781, 0.4773, 0.4759, 0.4795, 0.4760,\n",
            "        0.4742, 0.4780, 0.4763, 0.4761, 0.4724, 0.4750, 0.4761, 0.4714, 0.4761,\n",
            "        0.4765, 0.4745, 0.4744, 0.4786, 0.4762, 0.4779, 0.4797, 0.4782, 0.4747,\n",
            "        0.4776, 0.4746, 0.4760, 0.4737, 0.4791, 0.4817, 0.4760, 0.4745, 0.4779,\n",
            "        0.4742, 0.4714, 0.4783, 0.4761, 0.4756, 0.4727, 0.4773, 0.4750, 0.4751,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4487)\n",
            "Loss tensor(0.6967)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4738, 0.4761, 0.4718, 0.4758, 0.4804, 0.4741, 0.4730, 0.4729, 0.4730,\n",
            "        0.4756, 0.4736, 0.4771, 0.4740, 0.4752, 0.4741, 0.4719, 0.4736, 0.4806,\n",
            "        0.4755, 0.4723, 0.4751, 0.4731, 0.4736, 0.4736, 0.4754, 0.4761, 0.4692,\n",
            "        0.4760, 0.4737, 0.4708, 0.4786, 0.4723, 0.4774, 0.4699, 0.4750, 0.4729,\n",
            "        0.4782, 0.4776, 0.4756, 0.4770, 0.4739, 0.4804, 0.4789, 0.4795, 0.4740,\n",
            "        0.4732, 0.4768, 0.4741, 0.4767, 0.4761, 0.4736, 0.4739, 0.4806, 0.4736,\n",
            "        0.4757, 0.4777, 0.4743, 0.4736, 0.4689, 0.4723, 0.4732, 0.4767, 0.4779,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.3957)\n",
            "Loss tensor(0.7093)\n",
            "Acc tensor(0.3438)\n",
            "tensor([0.4761, 0.4754, 0.4799, 0.4742, 0.4839, 0.4786, 0.4771, 0.4812, 0.4802,\n",
            "        0.4700, 0.4731, 0.4752, 0.4768, 0.4762, 0.4739, 0.4772, 0.4714, 0.4757,\n",
            "        0.4751, 0.4734, 0.4743, 0.4774, 0.4756, 0.4749, 0.4784, 0.4710, 0.4749,\n",
            "        0.4721, 0.4757, 0.4753, 0.4793, 0.4742, 0.4735, 0.4720, 0.4762, 0.4755,\n",
            "        0.4726, 0.4747, 0.4791, 0.4751, 0.4697, 0.4740, 0.4725, 0.4802, 0.4839,\n",
            "        0.4715, 0.4750, 0.4753, 0.4827, 0.4735, 0.4766, 0.4742, 0.4755, 0.4795,\n",
            "        0.4694, 0.4772, 0.4713, 0.4775, 0.4786, 0.4741, 0.4774, 0.4715, 0.4724,\n",
            "        0.4703])\n",
            "Sum of predicted tensor(30.4305)\n",
            "Loss tensor(0.6927)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4770, 0.4758, 0.4724, 0.4781, 0.4761, 0.4768, 0.4774, 0.4748, 0.4775,\n",
            "        0.4713, 0.4675, 0.4753, 0.4726, 0.4675, 0.4761, 0.4793, 0.4744, 0.4747,\n",
            "        0.4735, 0.4817, 0.4766, 0.4760, 0.4694, 0.4774, 0.4741, 0.4795, 0.4722,\n",
            "        0.4784, 0.4748, 0.4774, 0.4803, 0.4776, 0.4801, 0.4739, 0.4730, 0.4747,\n",
            "        0.4717, 0.4762, 0.4720, 0.4711, 0.4772, 0.4764, 0.4718, 0.4761, 0.4768,\n",
            "        0.4733, 0.4775, 0.4748, 0.4717, 0.4738, 0.4720, 0.4732, 0.4720, 0.4747,\n",
            "        0.4737, 0.4745, 0.4748, 0.4706, 0.4708, 0.4766, 0.4733, 0.4755, 0.4714,\n",
            "        0.4788])\n",
            "Sum of predicted tensor(30.3876)\n",
            "Loss tensor(0.6879)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4763, 0.4804, 0.4796, 0.4749, 0.4799, 0.4755, 0.4723, 0.4768, 0.4750,\n",
            "        0.4746, 0.4790, 0.4757, 0.4730, 0.4754, 0.4767, 0.4742, 0.4772, 0.4737,\n",
            "        0.4714, 0.4754, 0.4725, 0.4806, 0.4766, 0.4739, 0.4760, 0.4768, 0.4789,\n",
            "        0.4772, 0.4767, 0.4743, 0.4720, 0.4750, 0.4712, 0.4794, 0.4769, 0.4812,\n",
            "        0.4770, 0.4709, 0.4721, 0.4736, 0.4738, 0.4725, 0.4733, 0.4681, 0.4725,\n",
            "        0.4724, 0.4752, 0.4806, 0.4744, 0.4807, 0.4748, 0.4722, 0.4743, 0.4738,\n",
            "        0.4762, 0.4774, 0.4694, 0.4780, 0.4752, 0.4750, 0.4770, 0.4732, 0.4754,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.4196)\n",
            "Loss tensor(0.6893)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4770, 0.4706, 0.4788, 0.4749, 0.4775, 0.4774, 0.4755, 0.4785, 0.4746,\n",
            "        0.4734, 0.4766, 0.4720, 0.4747, 0.4764, 0.4808, 0.4792, 0.4729, 0.4767,\n",
            "        0.4768, 0.4711, 0.4735, 0.4761, 0.4739, 0.4797, 0.4726, 0.4757, 0.4764,\n",
            "        0.4718, 0.4756, 0.4763, 0.4725, 0.4804, 0.4715, 0.4759, 0.4760, 0.4715,\n",
            "        0.4779, 0.4793, 0.4763, 0.4761, 0.4773, 0.4720, 0.4778, 0.4737, 0.4740,\n",
            "        0.4726, 0.4789, 0.4782, 0.4806, 0.4746, 0.4725, 0.4740, 0.4717, 0.4764,\n",
            "        0.4762, 0.4812, 0.4725, 0.4757, 0.4758, 0.4759, 0.4798, 0.4751, 0.4748,\n",
            "        0.4775])\n",
            "Sum of predicted tensor(30.4431)\n",
            "Loss tensor(0.6921)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4752, 0.4735, 0.4774, 0.4711, 0.4752, 0.4735, 0.4770, 0.4747, 0.4746,\n",
            "        0.4735, 0.4786, 0.4728, 0.4768, 0.4799, 0.4765, 0.4720, 0.4736, 0.4770,\n",
            "        0.4796, 0.4732, 0.4787, 0.4763, 0.4760, 0.4758, 0.4768, 0.4749, 0.4761,\n",
            "        0.4753, 0.4763, 0.4768, 0.4754, 0.4751, 0.4727, 0.4780, 0.4781, 0.4754,\n",
            "        0.4829, 0.4740, 0.4754, 0.4768, 0.4739, 0.4709, 0.4736, 0.4768, 0.4786,\n",
            "        0.4732, 0.4785, 0.4774, 0.4779, 0.4744, 0.4795, 0.4745, 0.4781, 0.4751,\n",
            "        0.4755, 0.4764, 0.4723, 0.4709, 0.4734, 0.4736, 0.4777, 0.4762, 0.4752,\n",
            "        0.4759])\n",
            "Sum of predicted tensor(30.4421)\n",
            "Loss tensor(0.6944)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4806, 0.4758, 0.4799, 0.4684, 0.4697, 0.4760, 0.4742, 0.4743, 0.4732,\n",
            "        0.4747, 0.4720, 0.4727, 0.4728, 0.4774, 0.4740, 0.4738, 0.4780, 0.4757,\n",
            "        0.4720, 0.4724, 0.4782, 0.4730, 0.4767, 0.4744, 0.4743, 0.4770, 0.4751,\n",
            "        0.4765, 0.4724, 0.4766, 0.4757, 0.4763, 0.4722, 0.4763, 0.4749, 0.4765,\n",
            "        0.4759, 0.4779, 0.4772, 0.4756, 0.4765, 0.4741, 0.4799, 0.4768, 0.4739,\n",
            "        0.4760, 0.4775, 0.4740, 0.4721, 0.4806, 0.4755, 0.4746, 0.4741, 0.4764,\n",
            "        0.4786, 0.4741, 0.4739, 0.4757, 0.4778, 0.4762, 0.4739, 0.4803, 0.4759,\n",
            "        0.4757])\n",
            "Sum of predicted tensor(30.4244)\n",
            "Loss tensor(0.6992)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4787, 0.4737, 0.4731, 0.4761, 0.4799, 0.4770, 0.4681, 0.4799, 0.4756,\n",
            "        0.4775, 0.4776, 0.4839, 0.4762, 0.4785, 0.4781, 0.4761, 0.4706, 0.4758,\n",
            "        0.4706, 0.4790, 0.4739, 0.4743, 0.4806, 0.4749, 0.4717, 0.4776, 0.4774,\n",
            "        0.4767, 0.4747, 0.4739, 0.4769, 0.4746, 0.4764, 0.4718, 0.4768, 0.4812,\n",
            "        0.4684, 0.4757, 0.4711, 0.4722, 0.4774, 0.4776, 0.4769, 0.4754, 0.4740,\n",
            "        0.4774, 0.4751, 0.4748, 0.4746, 0.4739, 0.4766, 0.4725, 0.4773, 0.4736,\n",
            "        0.4749, 0.4755, 0.4750, 0.4754, 0.4762, 0.4757, 0.4776, 0.4739, 0.4725,\n",
            "        0.4774])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4754, 0.4766, 0.4808, 0.4764, 0.4794, 0.4795, 0.4747, 0.4744, 0.4763,\n",
            "        0.4760, 0.4721, 0.4773, 0.4741, 0.4827, 0.4766, 0.4812, 0.4764, 0.4703,\n",
            "        0.4759, 0.4751, 0.4716, 0.4752, 0.4726, 0.4759, 0.4759, 0.4766, 0.4783,\n",
            "        0.4749, 0.4784, 0.4745, 0.4723, 0.4730, 0.4775, 0.4761, 0.4799, 0.4721,\n",
            "        0.4745, 0.4780, 0.4748, 0.4812, 0.4780, 0.4753, 0.4737, 0.4790, 0.4721,\n",
            "        0.4746, 0.4777, 0.4839, 0.4706, 0.4788, 0.4748, 0.4720, 0.4715, 0.4817,\n",
            "        0.4725, 0.4736, 0.4752, 0.4694, 0.4702, 0.4741, 0.4756, 0.4745, 0.4774,\n",
            "        0.4739])\n",
            "Sum of predicted tensor(30.4446)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4740, 0.4762, 0.4684, 0.4722, 0.4781, 0.4742, 0.4799, 0.4750, 0.4786,\n",
            "        0.4722, 0.4761, 0.4752, 0.4729, 0.4774, 0.4795, 0.4720, 0.4732, 0.4757,\n",
            "        0.4794, 0.4747, 0.4726, 0.4762, 0.4754, 0.4720, 0.4746, 0.4762, 0.4734,\n",
            "        0.4790, 0.4752, 0.4757, 0.4733, 0.4787, 0.4766, 0.4694, 0.4716, 0.4767,\n",
            "        0.4757, 0.4726, 0.4743, 0.4744, 0.4772, 0.4738, 0.4750, 0.4770, 0.4749,\n",
            "        0.4712, 0.4752, 0.4786, 0.4743, 0.4735, 0.4753, 0.4736, 0.4746, 0.4731,\n",
            "        0.4757, 0.4778, 0.4759, 0.4758, 0.4764, 0.4779, 0.4765, 0.4769, 0.4771,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4129)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4754, 0.4783, 0.4764, 0.4711, 0.4722, 0.4765, 0.4812, 0.4753, 0.4729,\n",
            "        0.4771, 0.4772, 0.4735, 0.4757, 0.4780, 0.4757, 0.4839, 0.4769, 0.4745,\n",
            "        0.4712, 0.4768, 0.4738, 0.4739, 0.4725, 0.4728, 0.4743, 0.4777, 0.4746,\n",
            "        0.4725, 0.4770, 0.4715, 0.4768, 0.4779, 0.4762, 0.4812, 0.4778, 0.4718,\n",
            "        0.4765, 0.4754, 0.4737, 0.4786, 0.4720, 0.4776, 0.4735, 0.4725, 0.4781,\n",
            "        0.4742, 0.4741, 0.4741, 0.4741, 0.4770, 0.4749, 0.4768, 0.4812, 0.4812,\n",
            "        0.4703, 0.4739, 0.4720, 0.4709, 0.4785, 0.4753, 0.4752, 0.4756, 0.4763,\n",
            "        0.4684])\n",
            "Sum of predicted tensor(30.4240)\n",
            "Loss tensor(0.6907)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4745, 0.4742, 0.4796, 0.4759, 0.4751, 0.4744, 0.4761, 0.4743, 0.4723,\n",
            "        0.4776, 0.4735, 0.4757, 0.4811, 0.4784, 0.4801, 0.4735, 0.4780, 0.4765,\n",
            "        0.4726, 0.4799, 0.4761, 0.4743, 0.4769, 0.4722, 0.4757, 0.4735, 0.4757,\n",
            "        0.4790, 0.4773, 0.4778, 0.4762, 0.4721, 0.4761, 0.4762, 0.4717, 0.4777,\n",
            "        0.4739, 0.4746, 0.4754, 0.4764, 0.4790, 0.4780, 0.4760, 0.4733, 0.4767,\n",
            "        0.4710, 0.4748, 0.4770, 0.4771, 0.4747, 0.4720, 0.4744, 0.4715, 0.4715,\n",
            "        0.4737, 0.4755, 0.4760, 0.4703, 0.4735, 0.4758, 0.4748, 0.4736, 0.4812,\n",
            "        0.4780])\n",
            "Sum of predicted tensor(30.4313)\n",
            "Loss tensor(0.7024)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4763, 0.4724, 0.4743, 0.4769, 0.4697, 0.4770, 0.4751, 0.4768, 0.4735,\n",
            "        0.4697, 0.4769, 0.4761, 0.4727, 0.4725, 0.4770, 0.4742, 0.4745, 0.4749,\n",
            "        0.4725, 0.4748, 0.4712, 0.4753, 0.4776, 0.4715, 0.4722, 0.4754, 0.4759,\n",
            "        0.4761, 0.4733, 0.4785, 0.4747, 0.4775, 0.4757, 0.4813, 0.4767, 0.4760,\n",
            "        0.4767, 0.4694, 0.4740, 0.4749, 0.4741, 0.4741, 0.4733, 0.4766, 0.4738,\n",
            "        0.4772, 0.4751, 0.4694, 0.4709, 0.4694, 0.4735, 0.4803, 0.4780, 0.4746,\n",
            "        0.4728, 0.4734, 0.4793, 0.4790, 0.4775, 0.4724, 0.4711, 0.4722, 0.4720,\n",
            "        0.4763])\n",
            "Sum of predicted tensor(30.3779)\n",
            "Loss tensor(0.6974)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4790, 0.4749, 0.4787, 0.4720, 0.4757, 0.4693, 0.4719, 0.4735, 0.4729,\n",
            "        0.4766, 0.4748, 0.4765, 0.4804, 0.4761, 0.4755, 0.4773, 0.4770, 0.4725,\n",
            "        0.4734, 0.4791, 0.4759, 0.4725, 0.4761, 0.4725, 0.4768, 0.4755, 0.4741,\n",
            "        0.4769, 0.4730, 0.4737, 0.4799, 0.4761, 0.4778, 0.4736, 0.4775, 0.4757,\n",
            "        0.4739, 0.4763, 0.4785, 0.4736, 0.4766, 0.4782, 0.4731, 0.4762, 0.4737,\n",
            "        0.4781, 0.4769, 0.4749, 0.4706, 0.4762, 0.4763, 0.4783, 0.4675, 0.4718,\n",
            "        0.4810, 0.4744, 0.4790, 0.4758, 0.4747, 0.4799, 0.4717, 0.4768, 0.4758,\n",
            "        0.4776])\n",
            "Sum of predicted tensor(30.4324)\n",
            "Loss tensor(0.6998)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4775, 0.4799, 0.4720, 0.4752, 0.4759, 0.4728, 0.4745, 0.4747, 0.4737,\n",
            "        0.4703, 0.4793, 0.4768, 0.4774, 0.4781, 0.4780, 0.4744, 0.4715, 0.4742,\n",
            "        0.4775, 0.4744, 0.4735, 0.4677, 0.4745, 0.4740, 0.4738, 0.4768, 0.4736,\n",
            "        0.4726, 0.4797, 0.4746, 0.4768, 0.4766, 0.4754, 0.4745, 0.4754, 0.4734,\n",
            "        0.4799, 0.4748, 0.4757, 0.4722, 0.4743, 0.4735, 0.4762, 0.4728, 0.4757,\n",
            "        0.4736, 0.4742, 0.4715, 0.4747, 0.4739, 0.4780, 0.4745, 0.4743, 0.4737,\n",
            "        0.4794, 0.4761, 0.4779, 0.4723, 0.4747, 0.4796, 0.4768, 0.4757, 0.4733,\n",
            "        0.4764])\n",
            "Sum of predicted tensor(30.4068)\n",
            "Loss tensor(0.7021)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4721, 0.4787, 0.4749, 0.4754, 0.4764, 0.4732, 0.4731, 0.4715, 0.4763,\n",
            "        0.4708, 0.4754, 0.4769, 0.4735, 0.4803, 0.4743, 0.4797, 0.4760, 0.4786,\n",
            "        0.4776, 0.4747, 0.4711, 0.4799, 0.4766, 0.4764, 0.4719, 0.4761, 0.4768,\n",
            "        0.4699, 0.4739, 0.4755, 0.4799, 0.4752, 0.4780, 0.4725, 0.4736, 0.4750,\n",
            "        0.4714, 0.4798, 0.4754, 0.4781, 0.4797, 0.4770, 0.4733, 0.4799, 0.4720,\n",
            "        0.4743, 0.4726, 0.4779, 0.4747, 0.4748, 0.4737, 0.4723, 0.4759, 0.4744,\n",
            "        0.4754, 0.4725, 0.4810, 0.4720, 0.4749, 0.4795, 0.4736, 0.4725, 0.4745,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4174)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4776, 0.4770, 0.4760, 0.4725, 0.4755, 0.4717, 0.4737, 0.4761, 0.4749,\n",
            "        0.4735, 0.4769, 0.4777, 0.4745, 0.4740, 0.4790, 0.4753, 0.4739, 0.4751,\n",
            "        0.4786, 0.4763, 0.4744, 0.4706, 0.4761, 0.4773, 0.4745, 0.4709, 0.4763,\n",
            "        0.4806, 0.4788, 0.4742, 0.4725, 0.4784, 0.4758, 0.4776, 0.4760, 0.4776,\n",
            "        0.4742, 0.4758, 0.4764, 0.4736, 0.4749, 0.4816, 0.4766, 0.4839, 0.4770,\n",
            "        0.4717, 0.4788, 0.4710, 0.4753, 0.4701, 0.4799, 0.4734, 0.4720, 0.4788,\n",
            "        0.4786, 0.4770, 0.4740, 0.4778, 0.4735, 0.4777, 0.4758, 0.4725, 0.4803,\n",
            "        0.4736])\n",
            "Sum of predicted tensor(30.4469)\n",
            "Loss tensor(0.6950)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4723, 0.4730, 0.4763, 0.4788, 0.4678, 0.4737, 0.4778, 0.4745, 0.4768,\n",
            "        0.4729, 0.4750, 0.4768, 0.4781, 0.4810, 0.4754, 0.4802, 0.4746, 0.4758,\n",
            "        0.4752, 0.4761, 0.4834, 0.4728, 0.4717, 0.4736, 0.4777, 0.4746, 0.4696,\n",
            "        0.4788, 0.4771, 0.4724, 0.4799, 0.4757, 0.4745, 0.4775, 0.4775, 0.4772,\n",
            "        0.4764, 0.4732, 0.4760, 0.4778, 0.4754, 0.4733, 0.4761, 0.4786, 0.4766,\n",
            "        0.4722, 0.4722, 0.4721, 0.4756, 0.4734, 0.4691, 0.4751, 0.4795, 0.4762,\n",
            "        0.4747, 0.4739, 0.4767, 0.4766, 0.4749, 0.4765, 0.4750, 0.4735, 0.4757,\n",
            "        0.4726])\n",
            "Sum of predicted tensor(30.4250)\n",
            "Loss tensor(0.6867)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4752, 0.4771, 0.4783, 0.4773, 0.4760, 0.4733, 0.4758, 0.4766, 0.4805,\n",
            "        0.4750, 0.4727, 0.4806, 0.4764, 0.4705, 0.4799, 0.4758, 0.4737, 0.4760,\n",
            "        0.4727, 0.4711, 0.4722, 0.4760, 0.4774, 0.4757, 0.4748, 0.4738, 0.4776,\n",
            "        0.4745, 0.4738, 0.4793, 0.4757, 0.4698, 0.4759, 0.4807, 0.4778, 0.4736,\n",
            "        0.4813, 0.4788, 0.4708, 0.4757, 0.4754, 0.4770, 0.4752, 0.4751, 0.4706,\n",
            "        0.4760, 0.4759, 0.4794, 0.4705, 0.4756, 0.4767, 0.4768, 0.4751, 0.4780,\n",
            "        0.4798, 0.4763, 0.4784, 0.4753, 0.4777, 0.4771, 0.4687, 0.4709, 0.4794,\n",
            "        0.4728])\n",
            "Sum of predicted tensor(30.4436)\n",
            "Loss tensor(0.7016)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4725, 0.4734, 0.4733, 0.4743, 0.4769, 0.4735, 0.4708, 0.4774, 0.4674,\n",
            "        0.4750, 0.4762, 0.4753, 0.4765, 0.4743, 0.4748, 0.4712, 0.4787, 0.4807,\n",
            "        0.4805, 0.4792, 0.4750, 0.4752, 0.4736, 0.4746, 0.4715, 0.4705, 0.4781,\n",
            "        0.4734, 0.4701, 0.4750, 0.4786, 0.4762, 0.4741, 0.4782, 0.4799, 0.4735,\n",
            "        0.4781, 0.4761, 0.4743, 0.4747, 0.4789, 0.4734, 0.4754, 0.4714, 0.4756,\n",
            "        0.4786, 0.4784, 0.4724, 0.4787, 0.4695, 0.4747, 0.4758, 0.4769, 0.4750,\n",
            "        0.4734, 0.4766, 0.4805, 0.4805, 0.4771, 0.4778, 0.4817, 0.4696, 0.4774,\n",
            "        0.4715])\n",
            "Sum of predicted tensor(30.4235)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4734, 0.4715, 0.4765, 0.4760, 0.4734, 0.4722, 0.4717, 0.4788, 0.4754,\n",
            "        0.4750, 0.4766, 0.4799, 0.4810, 0.4806, 0.4787, 0.4734, 0.4798, 0.4751,\n",
            "        0.4718, 0.4709, 0.4749, 0.4758, 0.4804, 0.4784, 0.4765, 0.4783, 0.4765,\n",
            "        0.4749, 0.4766, 0.4742, 0.4741, 0.4784, 0.4791, 0.4749, 0.4730, 0.4749,\n",
            "        0.4772, 0.4760, 0.4678, 0.4769, 0.4764, 0.4727, 0.4759, 0.4769, 0.4762,\n",
            "        0.4738, 0.4763, 0.4701, 0.4727, 0.4734, 0.4732, 0.4789, 0.4745, 0.4733,\n",
            "        0.4762, 0.4766, 0.4752, 0.4727, 0.4775, 0.4760, 0.4787, 0.4798, 0.4726,\n",
            "        0.4738])\n",
            "Sum of predicted tensor(30.4338)\n",
            "Loss tensor(0.6990)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4765, 0.4799, 0.4746, 0.4786, 0.4761, 0.4760, 0.4752, 0.4799, 0.4768,\n",
            "        0.4774, 0.4740, 0.4804, 0.4720, 0.4683, 0.4739, 0.4719, 0.4695, 0.4784,\n",
            "        0.4739, 0.4774, 0.4755, 0.4724, 0.4768, 0.4787, 0.4697, 0.4765, 0.4766,\n",
            "        0.4809, 0.4788, 0.4763, 0.4739, 0.4776, 0.4767, 0.4694, 0.4747, 0.4722,\n",
            "        0.4688, 0.4682, 0.4755, 0.4749, 0.4753, 0.4758, 0.4758, 0.4764, 0.4744,\n",
            "        0.4777, 0.4783, 0.4771, 0.4770, 0.4733, 0.4724, 0.4761, 0.4760, 0.4747,\n",
            "        0.4767, 0.4780, 0.4758, 0.4734, 0.4712, 0.4737, 0.4757, 0.4721, 0.4750,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4101)\n",
            "Loss tensor(0.6978)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4742, 0.4747, 0.4721, 0.4798, 0.4760, 0.4770, 0.4744, 0.4757, 0.4708,\n",
            "        0.4774, 0.4783, 0.4759, 0.4769, 0.4744, 0.4745, 0.4778, 0.4774, 0.4711,\n",
            "        0.4698, 0.4770, 0.4755, 0.4776, 0.4817, 0.4753, 0.4794, 0.4771, 0.4766,\n",
            "        0.4733, 0.4745, 0.4739, 0.4759, 0.4760, 0.4766, 0.4728, 0.4774, 0.4712,\n",
            "        0.4749, 0.4732, 0.4732, 0.4805, 0.4722, 0.4767, 0.4760, 0.4769, 0.4751,\n",
            "        0.4773, 0.4808, 0.4725, 0.4754, 0.4760, 0.4749, 0.4775, 0.4712, 0.4805,\n",
            "        0.4799, 0.4755, 0.4683, 0.4760, 0.4710, 0.4727, 0.4734, 0.4808, 0.4810,\n",
            "        0.4805])\n",
            "Sum of predicted tensor(30.4443)\n",
            "Loss tensor(0.6879)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4778, 0.4782, 0.4762, 0.4743, 0.4713, 0.4763, 0.4773, 0.4718, 0.4817,\n",
            "        0.4728, 0.4731, 0.4743, 0.4755, 0.4765, 0.4761, 0.4734, 0.4714, 0.4765,\n",
            "        0.4778, 0.4734, 0.4802, 0.4697, 0.4753, 0.4684, 0.4760, 0.4686, 0.4717,\n",
            "        0.4757, 0.4700, 0.4771, 0.4766, 0.4773, 0.4712, 0.4764, 0.4718, 0.4769,\n",
            "        0.4753, 0.4784, 0.4791, 0.4763, 0.4757, 0.4759, 0.4747, 0.4740, 0.4780,\n",
            "        0.4723, 0.4751, 0.4755, 0.4722, 0.4740, 0.4761, 0.4745, 0.4764, 0.4741,\n",
            "        0.4769, 0.4728, 0.4718, 0.4759, 0.4787, 0.4749, 0.4731, 0.4817, 0.4760,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4013)\n",
            "tensor([0.4786, 0.4740, 0.4753, 0.4714, 0.4764, 0.4745, 0.4731, 0.4762, 0.4766,\n",
            "        0.4786, 0.4799, 0.4754, 0.4754, 0.4724, 0.4754, 0.4754, 0.4750, 0.4788,\n",
            "        0.4764, 0.4749, 0.4773, 0.4739, 0.4744, 0.4798, 0.4731, 0.4761, 0.4734,\n",
            "        0.4701, 0.4739, 0.4723, 0.4700, 0.4757, 0.4780, 0.4698, 0.4745, 0.4794,\n",
            "        0.4754, 0.4768, 0.4763, 0.4784, 0.4751, 0.4754, 0.4750, 0.4717, 0.4769,\n",
            "        0.4789, 0.4733, 0.4769, 0.4766, 0.4758, 0.4779, 0.4781, 0.4807, 0.4721,\n",
            "        0.4760, 0.4800, 0.4763, 0.4723, 0.4788, 0.4759, 0.4782, 0.4688, 0.4765,\n",
            "        0.4800])\n",
            "Sum of predicted tensor(30.4396)\n",
            "Loss tensor(0.6859)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4760, 0.4759, 0.4731, 0.4728, 0.4715, 0.4737, 0.4759, 0.4743, 0.4732,\n",
            "        0.4718, 0.4743, 0.4720, 0.4731, 0.4743, 0.4805, 0.4734, 0.4749, 0.4815,\n",
            "        0.4763, 0.4764, 0.4720, 0.4802, 0.4718, 0.4755, 0.4771, 0.4722, 0.4749,\n",
            "        0.4749, 0.4792, 0.4764, 0.4796, 0.4835, 0.4678, 0.4756, 0.4787, 0.4805,\n",
            "        0.4754, 0.4791, 0.4724, 0.4762, 0.4749, 0.4746, 0.4729, 0.4762, 0.4729,\n",
            "        0.4793, 0.4736, 0.4780, 0.4764, 0.4711, 0.4760, 0.4673, 0.4777, 0.4758,\n",
            "        0.4799, 0.4793, 0.4777, 0.4720, 0.4771, 0.4753, 0.4743, 0.4774, 0.4766,\n",
            "        0.4734])\n",
            "Sum of predicted tensor(30.4277)\n",
            "Loss tensor(0.6887)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4773, 0.4794, 0.4794, 0.4781, 0.4727, 0.4749, 0.4787, 0.4766, 0.4763,\n",
            "        0.4784, 0.4705, 0.4730, 0.4759, 0.4736, 0.4754, 0.4725, 0.4735, 0.4738,\n",
            "        0.4714, 0.4771, 0.4791, 0.4768, 0.4776, 0.4754, 0.4752, 0.4773, 0.4781,\n",
            "        0.4781, 0.4764, 0.4756, 0.4743, 0.4788, 0.4750, 0.4708, 0.4740, 0.4750,\n",
            "        0.4750, 0.4718, 0.4789, 0.4766, 0.4779, 0.4772, 0.4739, 0.4757, 0.4775,\n",
            "        0.4760, 0.4745, 0.4767, 0.4758, 0.4756, 0.4750, 0.4748, 0.4739, 0.4784,\n",
            "        0.4817, 0.4764, 0.4763, 0.4775, 0.4765, 0.4771, 0.4702, 0.4765, 0.4788,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4569)\n",
            "Loss tensor(0.7065)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4747, 0.4759, 0.4758, 0.4729, 0.4739, 0.4743, 0.4780, 0.4739, 0.4761,\n",
            "        0.4771, 0.4712, 0.4751, 0.4760, 0.4763, 0.4755, 0.4805, 0.4697, 0.4740,\n",
            "        0.4755, 0.4740, 0.4765, 0.4737, 0.4756, 0.4768, 0.4817, 0.4766, 0.4728,\n",
            "        0.4787, 0.4739, 0.4765, 0.4747, 0.4746, 0.4748, 0.4683, 0.4743, 0.4763,\n",
            "        0.4811, 0.4732, 0.4746, 0.4771, 0.4783, 0.4735, 0.4765, 0.4747, 0.4766,\n",
            "        0.4796, 0.4756, 0.4773, 0.4744, 0.4752, 0.4793, 0.4799, 0.4694, 0.4728,\n",
            "        0.4713, 0.4778, 0.4722, 0.4750, 0.4819, 0.4746, 0.4760, 0.4817, 0.4749,\n",
            "        0.4770])\n",
            "Sum of predicted tensor(30.4377)\n",
            "Loss tensor(0.6909)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0.4760, 0.4743, 0.4799, 0.4751, 0.4802, 0.4757, 0.4784, 0.4794, 0.4758,\n",
            "        0.4774, 0.4757, 0.4755, 0.4765, 0.4777, 0.4722, 0.4733, 0.4743, 0.4777,\n",
            "        0.4749, 0.4757, 0.4774, 0.4739, 0.4771, 0.4762, 0.4764, 0.4727, 0.4799,\n",
            "        0.4755, 0.4808, 0.4768, 0.4749, 0.4713, 0.4808, 0.4710, 0.4780, 0.4761,\n",
            "        0.4805, 0.4754, 0.4757, 0.4711, 0.4752, 0.4778, 0.4774, 0.4766, 0.4807,\n",
            "        0.4798, 0.4750, 0.4731, 0.4751, 0.4750, 0.4791, 0.4799, 0.4779, 0.4765,\n",
            "        0.4775, 0.4791, 0.4834, 0.4759, 0.4782, 0.4777, 0.4736, 0.4804, 0.4749,\n",
            "        0.4753])\n",
            "Sum of predicted tensor(30.5026)\n",
            "Loss tensor(0.6982)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4806, 0.4769, 0.4727, 0.4731, 0.4784, 0.4759, 0.4743, 0.4750, 0.4771,\n",
            "        0.4757, 0.4766, 0.4787, 0.4761, 0.4723, 0.4715, 0.4793, 0.4780, 0.4722,\n",
            "        0.4804, 0.4766, 0.4737, 0.4760, 0.4762, 0.4731, 0.4700, 0.4743, 0.4793,\n",
            "        0.4784, 0.4711, 0.4799, 0.4722, 0.4681, 0.4678, 0.4700, 0.4802, 0.4714,\n",
            "        0.4769, 0.4774, 0.4778, 0.4770, 0.4782, 0.4797, 0.4685, 0.4771, 0.4721,\n",
            "        0.4755, 0.4779, 0.4781, 0.4763, 0.4742, 0.4751, 0.4793, 0.4809, 0.4799,\n",
            "        0.4778, 0.4735, 0.4679, 0.4738, 0.4750, 0.4788, 0.4760, 0.4765, 0.4774,\n",
            "        0.4754])\n",
            "Sum of predicted tensor(30.4373)\n",
            "Loss tensor(0.7043)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4755, 0.4749, 0.4731, 0.4768, 0.4733, 0.4723, 0.4747, 0.4788, 0.4757,\n",
            "        0.4799, 0.4786, 0.4733, 0.4753, 0.4782, 0.4774, 0.4757, 0.4778, 0.4741,\n",
            "        0.4751, 0.4752, 0.4747, 0.4744, 0.4691, 0.4767, 0.4798, 0.4707, 0.4737,\n",
            "        0.4737, 0.4765, 0.4739, 0.4766, 0.4799, 0.4786, 0.4701, 0.4769, 0.4768,\n",
            "        0.4709, 0.4741, 0.4805, 0.4788, 0.4760, 0.4730, 0.4760, 0.4755, 0.4737,\n",
            "        0.4740, 0.4788, 0.4769, 0.4729, 0.4760, 0.4806, 0.4764, 0.4707, 0.4740,\n",
            "        0.4746, 0.4765, 0.4683, 0.4768, 0.4754, 0.4743, 0.4751, 0.4754, 0.4750,\n",
            "        0.4747])\n",
            "Sum of predicted tensor(30.4227)\n",
            "Loss tensor(0.6890)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4779, 0.4771, 0.4730, 0.4720, 0.4763, 0.4759, 0.4805, 0.4765, 0.4746,\n",
            "        0.4763, 0.4764, 0.4745, 0.4745, 0.4715, 0.4710, 0.4743, 0.4799, 0.4793,\n",
            "        0.4770, 0.4740, 0.4771, 0.4723, 0.4765, 0.4762, 0.4751, 0.4768, 0.4707,\n",
            "        0.4782, 0.4702, 0.4790, 0.4744, 0.4712, 0.4765, 0.4758, 0.4736, 0.4755,\n",
            "        0.4766, 0.4730, 0.4821, 0.4771, 0.4750, 0.4753, 0.4782, 0.4735, 0.4754,\n",
            "        0.4766, 0.4763, 0.4736, 0.4780, 0.4773, 0.4713, 0.4745, 0.4768, 0.4730,\n",
            "        0.4748, 0.4752, 0.4738, 0.4748, 0.4796, 0.4808, 0.4740, 0.4758, 0.4762,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4372)\n",
            "Loss tensor(0.6891)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0.4709, 0.4791, 0.4761, 0.4768, 0.4780, 0.4770, 0.4762, 0.4721, 0.4750,\n",
            "        0.4783, 0.4753, 0.4770, 0.4772, 0.4758, 0.4738, 0.4762, 0.4748, 0.4794,\n",
            "        0.4715, 0.4817, 0.4804, 0.4761, 0.4754, 0.4714, 0.4756, 0.4729, 0.4762,\n",
            "        0.4758, 0.4753, 0.4760, 0.4765, 0.4746, 0.4744, 0.4771, 0.4756, 0.4736,\n",
            "        0.4821, 0.4734, 0.4767, 0.4742, 0.4768, 0.4708, 0.4783, 0.4744, 0.4754,\n",
            "        0.4751, 0.4775, 0.4731, 0.4710, 0.4715, 0.4751, 0.4752, 0.4753, 0.4731,\n",
            "        0.4762, 0.4697, 0.4749, 0.4735, 0.4771, 0.4777, 0.4765, 0.4769, 0.4736,\n",
            "        0.4778])\n",
            "Sum of predicted tensor(30.4322)\n",
            "Loss tensor(0.6857)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0.4798, 0.4701, 0.4761, 0.4675, 0.4786, 0.4759, 0.4758, 0.4731, 0.4779,\n",
            "        0.4762, 0.4756, 0.4740, 0.4743, 0.4798, 0.4738, 0.4739, 0.4743, 0.4768,\n",
            "        0.4754, 0.4747, 0.4744, 0.4736, 0.4753, 0.4744, 0.4772, 0.4804, 0.4756,\n",
            "        0.4762, 0.4773, 0.4757, 0.4748, 0.4782, 0.4762, 0.4744, 0.4776, 0.4761,\n",
            "        0.4766, 0.4771, 0.4765, 0.4748, 0.4675, 0.4714, 0.4770, 0.4761, 0.4741,\n",
            "        0.4696, 0.4781, 0.4738, 0.4762, 0.4757, 0.4745, 0.4744, 0.4799, 0.4775,\n",
            "        0.4763, 0.4726, 0.4756, 0.4766, 0.4730, 0.4777, 0.4745, 0.4753, 0.4759,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.4260)\n",
            "Loss tensor(0.6928)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0.4775, 0.4709, 0.4791, 0.4744, 0.4677, 0.4772, 0.4729, 0.4777, 0.4716,\n",
            "        0.4715, 0.4772, 0.4767, 0.4729, 0.4736, 0.4771, 0.4753, 0.4733, 0.4780,\n",
            "        0.4814, 0.4779, 0.4807, 0.4745, 0.4774, 0.4750, 0.4774, 0.4731, 0.4775,\n",
            "        0.4747, 0.4712, 0.4782, 0.4776, 0.4715, 0.4779, 0.4711, 0.4741, 0.4760,\n",
            "        0.4729, 0.4805, 0.4762, 0.4762, 0.4740, 0.4731, 0.4777, 0.4818, 0.4763,\n",
            "        0.4717, 0.4775, 0.4716, 0.4756, 0.4760, 0.4758, 0.4731, 0.4710, 0.4768,\n",
            "        0.4798, 0.4778, 0.4783, 0.4723, 0.4779, 0.4791, 0.4792, 0.4770, 0.4765,\n",
            "        0.4744])\n",
            "Sum of predicted tensor(30.4421)\n",
            "Loss tensor(0.7027)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4789, 0.4786, 0.4704, 0.4777, 0.4818, 0.4723, 0.4701, 0.4787, 0.4765,\n",
            "        0.4789, 0.4729, 0.4775, 0.4690, 0.4765, 0.4753, 0.4750, 0.4771, 0.4746,\n",
            "        0.4753, 0.4751, 0.4771, 0.4776, 0.4748, 0.4745, 0.4749, 0.4776, 0.4797,\n",
            "        0.4765, 0.4713, 0.4731, 0.4776, 0.4744, 0.4741, 0.4726, 0.4738, 0.4777,\n",
            "        0.4738, 0.4713, 0.4793, 0.4726, 0.4799, 0.4744, 0.4788, 0.4747, 0.4701,\n",
            "        0.4754, 0.4733, 0.4801, 0.4767, 0.4756, 0.4768, 0.4738, 0.4744, 0.4762,\n",
            "        0.4760, 0.4775, 0.4738, 0.4750, 0.4780, 0.4756, 0.4760, 0.4720, 0.4724,\n",
            "        0.4760])\n",
            "Sum of predicted tensor(30.4292)\n",
            "Loss tensor(0.6826)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4786, 0.4730, 0.4755, 0.4766, 0.4775, 0.4751, 0.4735, 0.4765, 0.4742,\n",
            "        0.4791, 0.4729, 0.4757, 0.4745, 0.4698, 0.4762, 0.4763, 0.4748, 0.4772,\n",
            "        0.4751, 0.4777, 0.4738, 0.4817, 0.4708, 0.4795, 0.4756, 0.4785, 0.4786,\n",
            "        0.4762, 0.4698, 0.4802, 0.4777, 0.4782, 0.4718, 0.4813, 0.4741, 0.4738,\n",
            "        0.4731, 0.4766, 0.4736, 0.4768, 0.4705, 0.4723, 0.4753, 0.4757, 0.4784,\n",
            "        0.4744, 0.4768, 0.4761, 0.4726, 0.4780, 0.4729, 0.4762, 0.4775, 0.4745,\n",
            "        0.4765, 0.4791, 0.4769, 0.4764, 0.4772, 0.4756, 0.4762, 0.4771, 0.4806,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4521)\n",
            "Loss tensor(0.6874)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4765, 0.4711, 0.4701, 0.4729, 0.4751, 0.4762, 0.4784, 0.4757, 0.4801,\n",
            "        0.4763, 0.4770, 0.4676, 0.4757, 0.4710, 0.4763, 0.4743, 0.4802, 0.4783,\n",
            "        0.4740, 0.4738, 0.4735, 0.4817, 0.4793, 0.4780, 0.4739, 0.4739, 0.4728,\n",
            "        0.4712, 0.4774, 0.4767, 0.4736, 0.4754, 0.4750, 0.4735, 0.4769, 0.4740,\n",
            "        0.4759, 0.4730, 0.4763, 0.4763, 0.4675, 0.4793, 0.4774, 0.4766, 0.4737,\n",
            "        0.4752, 0.4773, 0.4771, 0.4742, 0.4751, 0.4760, 0.4753, 0.4777, 0.4738,\n",
            "        0.4725, 0.4728, 0.4723, 0.4726, 0.4785, 0.4769, 0.4717, 0.4753, 0.4754,\n",
            "        0.4794])\n",
            "Sum of predicted tensor(30.4121)\n",
            "Loss tensor(0.6963)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4777, 0.4757, 0.4788, 0.4744, 0.4754, 0.4799, 0.4748, 0.4762, 0.4742,\n",
            "        0.4751, 0.4777, 0.4795, 0.4753, 0.4752, 0.4721, 0.4786, 0.4712, 0.4788,\n",
            "        0.4731, 0.4702, 0.4748, 0.4702, 0.4759, 0.4778, 0.4722, 0.4786, 0.4723,\n",
            "        0.4771, 0.4770, 0.4753, 0.4751, 0.4733, 0.4741, 0.4786, 0.4762, 0.4763,\n",
            "        0.4780, 0.4824, 0.4741, 0.4710, 0.4759, 0.4736, 0.4745, 0.4795, 0.4726,\n",
            "        0.4760, 0.4745, 0.4712, 0.4777, 0.4713, 0.4818, 0.4738, 0.4808, 0.4730,\n",
            "        0.4775, 0.4752, 0.4756, 0.4791, 0.4767, 0.4760, 0.4775, 0.4759, 0.4790,\n",
            "        0.4703])\n",
            "Sum of predicted tensor(30.4432)\n",
            "Loss tensor(0.6979)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4740, 0.4773, 0.4744, 0.4751, 0.4745, 0.4753, 0.4709, 0.4779, 0.4775,\n",
            "        0.4713, 0.4746, 0.4751, 0.4740, 0.4769, 0.4746, 0.4782, 0.4747, 0.4773,\n",
            "        0.4786, 0.4740, 0.4775, 0.4747, 0.4776, 0.4774, 0.4803, 0.4758, 0.4712,\n",
            "        0.4783, 0.4779, 0.4715, 0.4760, 0.4738, 0.4788, 0.4762, 0.4778, 0.4762,\n",
            "        0.4732, 0.4765, 0.4725, 0.4767, 0.4794, 0.4805, 0.4773, 0.4786, 0.4749,\n",
            "        0.4758, 0.4753, 0.4798, 0.4748, 0.4737, 0.4766, 0.4818, 0.4756, 0.4804,\n",
            "        0.4739, 0.4713, 0.4762, 0.4714, 0.4768, 0.4738, 0.4744, 0.4775, 0.4775,\n",
            "        0.4765])\n",
            "Sum of predicted tensor(30.4600)\n",
            "Loss tensor(0.7004)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0.4762, 0.4744, 0.4773, 0.4714, 0.4775, 0.4788, 0.4772, 0.4729, 0.4744,\n",
            "        0.4768, 0.4775, 0.4712, 0.4710, 0.4776, 0.4750, 0.4753, 0.4833, 0.4746,\n",
            "        0.4744, 0.4725, 0.4786, 0.4739, 0.4750, 0.4759, 0.4768, 0.4751, 0.4668,\n",
            "        0.4760, 0.4773, 0.4761, 0.4788, 0.4746, 0.4732, 0.4710, 0.4821, 0.4806,\n",
            "        0.4758, 0.4763, 0.4717, 0.4737, 0.4766, 0.4798, 0.4784, 0.4752, 0.4746,\n",
            "        0.4766, 0.4744, 0.4736, 0.4716, 0.4778, 0.4780, 0.4715, 0.4738, 0.4784,\n",
            "        0.4749, 0.4789, 0.4709, 0.4775, 0.4775, 0.4753, 0.4763, 0.4772, 0.4763,\n",
            "        0.4727])\n",
            "Sum of predicted tensor(30.4364)\n",
            "Loss tensor(0.7017)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4755, 0.4730, 0.4746, 0.4739, 0.4738, 0.4738, 0.4789, 0.4754, 0.4756,\n",
            "        0.4762, 0.4775, 0.4763, 0.4733, 0.4686, 0.4762, 0.4750, 0.4802, 0.4733,\n",
            "        0.4741, 0.4722, 0.4723, 0.4678, 0.4765, 0.4744, 0.4709, 0.4770, 0.4783,\n",
            "        0.4709, 0.4742, 0.4740, 0.4777, 0.4762, 0.4744, 0.4766, 0.4737, 0.4696,\n",
            "        0.4763, 0.4757, 0.4795, 0.4761, 0.4762, 0.4719, 0.4702, 0.4727, 0.4708,\n",
            "        0.4803, 0.4699, 0.4748, 0.4758, 0.4744, 0.4756, 0.4765, 0.4771, 0.4757,\n",
            "        0.4796, 0.4710, 0.4736, 0.4716, 0.4798, 0.4749, 0.4765, 0.4766, 0.4744,\n",
            "        0.4752])\n",
            "Sum of predicted tensor(30.3846)\n",
            "Loss tensor(0.6940)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4792, 0.4722, 0.4768, 0.4783, 0.4736, 0.4740, 0.4729, 0.4723, 0.4677,\n",
            "        0.4788, 0.4762, 0.4775, 0.4774, 0.4767, 0.4779, 0.4779, 0.4733, 0.4770,\n",
            "        0.4763, 0.4795, 0.4745, 0.4773, 0.4725, 0.4689, 0.4757, 0.4775, 0.4745,\n",
            "        0.4765, 0.4748, 0.4764, 0.4747, 0.4753, 0.4675, 0.4707, 0.4744, 0.4736,\n",
            "        0.4751, 0.4741, 0.4789, 0.4738, 0.4731, 0.4765, 0.4774, 0.4763, 0.4735,\n",
            "        0.4676, 0.4770, 0.4798, 0.4779, 0.4783, 0.4751, 0.4793, 0.4755, 0.4788,\n",
            "        0.4737, 0.4705, 0.4765, 0.4765, 0.4784, 0.4770, 0.4791, 0.4768, 0.4744,\n",
            "        0.4786])\n",
            "Sum of predicted tensor(30.4298)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0.4773, 0.4727, 0.4789, 0.4744, 0.4772, 0.4763, 0.4756, 0.4718, 0.4699,\n",
            "        0.4725, 0.4711, 0.4732, 0.4774, 0.4755, 0.4823, 0.4747, 0.4765, 0.4747,\n",
            "        0.4747, 0.4747, 0.4739, 0.4758, 0.4758, 0.4748, 0.4736, 0.4784, 0.4753,\n",
            "        0.4773, 0.4723, 0.4730, 0.4726, 0.4741, 0.4785, 0.4729, 0.4760, 0.4761,\n",
            "        0.4784, 0.4728, 0.4743, 0.4751, 0.4739, 0.4745, 0.4717, 0.4748, 0.4767,\n",
            "        0.4769, 0.4806, 0.4773, 0.4714, 0.4776, 0.4789, 0.4787, 0.4708, 0.4745,\n",
            "        0.4749, 0.4690, 0.4743, 0.4823, 0.4735, 0.4802, 0.4714, 0.4797, 0.4737,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4182)\n",
            "Loss tensor(0.6948)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4768, 0.4761, 0.4750, 0.4725, 0.4767, 0.4804, 0.4747, 0.4779, 0.4773,\n",
            "        0.4785, 0.4798, 0.4785, 0.4723, 0.4755, 0.4729, 0.4711, 0.4743, 0.4754,\n",
            "        0.4708, 0.4750, 0.4829, 0.4768, 0.4773, 0.4770, 0.4720, 0.4741, 0.4764,\n",
            "        0.4744, 0.4773, 0.4729, 0.4742, 0.4791, 0.4747, 0.4786, 0.4787, 0.4749,\n",
            "        0.4746, 0.4711, 0.4744, 0.4725, 0.4738, 0.4762, 0.4751, 0.4786, 0.4748,\n",
            "        0.4699, 0.4761, 0.4741, 0.4733, 0.4774, 0.4794, 0.4775, 0.4752, 0.4752,\n",
            "        0.4743, 0.4760, 0.4760, 0.4741, 0.4720, 0.4750, 0.4747, 0.4787, 0.4727,\n",
            "        0.4787])\n",
            "Sum of predicted tensor(30.4345)\n",
            "Loss tensor(0.6959)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0.4725, 0.4750, 0.4748, 0.4740, 0.4741, 0.4793, 0.4817, 0.4782, 0.4727,\n",
            "        0.4743, 0.4740, 0.4747, 0.4747, 0.4761, 0.4741, 0.4737, 0.4726, 0.4684,\n",
            "        0.4706, 0.4738, 0.4789, 0.4741, 0.4764, 0.4762, 0.4798, 0.4743, 0.4749,\n",
            "        0.4750, 0.4764, 0.4767, 0.4727, 0.4740, 0.4738, 0.4772, 0.4753, 0.4735,\n",
            "        0.4764, 0.4706, 0.4760, 0.4748, 0.4780, 0.4753, 0.4744, 0.4718, 0.4723,\n",
            "        0.4726, 0.4737, 0.4754, 0.4730, 0.4787, 0.4762, 0.4773, 0.4776, 0.4738,\n",
            "        0.4753, 0.4699, 0.4722, 0.4767, 0.4769, 0.4768, 0.4738, 0.4739, 0.4767,\n",
            "        0.4719])\n",
            "Sum of predicted tensor(30.3904)\n",
            "Loss tensor(0.6988)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0.4720, 0.4756, 0.4751, 0.4780, 0.4775, 0.4772, 0.4741, 0.4745, 0.4745,\n",
            "        0.4695, 0.4766, 0.4728, 0.4798, 0.4744, 0.4752, 0.4764, 0.4829, 0.4773,\n",
            "        0.4786, 0.4753, 0.4744, 0.4760, 0.4727, 0.4696, 0.4747, 0.4762, 0.4794,\n",
            "        0.4811, 0.4751, 0.4789, 0.4773, 0.4727, 0.4722, 0.4719, 0.4765, 0.4806,\n",
            "        0.4741, 0.4730, 0.4744, 0.4726, 0.4731, 0.4728, 0.4802, 0.4762, 0.4720,\n",
            "        0.4747, 0.4744, 0.4765, 0.4741, 0.4785, 0.4721, 0.4754, 0.4727, 0.4735,\n",
            "        0.4720, 0.4750, 0.4707, 0.4754, 0.4760, 0.4787, 0.4787, 0.4731, 0.4722,\n",
            "        0.4746])\n",
            "Sum of predicted tensor(30.4134)\n",
            "Loss tensor(0.7029)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4737, 0.4758, 0.4777, 0.4759, 0.4761, 0.4770, 0.4761, 0.4744, 0.4751,\n",
            "        0.4755, 0.4773, 0.4770, 0.4822, 0.4750, 0.4753, 0.4712, 0.4738, 0.4757,\n",
            "        0.4811, 0.4712, 0.4753, 0.4744, 0.4764, 0.4756, 0.4794, 0.4760, 0.4764,\n",
            "        0.4751, 0.4746, 0.4775, 0.4753, 0.4723, 0.4734, 0.4796, 0.4775, 0.4747,\n",
            "        0.4749, 0.4723, 0.4729, 0.4693, 0.4787, 0.4741, 0.4723, 0.4768, 0.4776,\n",
            "        0.4714, 0.4745, 0.4776, 0.4732, 0.4727, 0.4747, 0.4829, 0.4765, 0.4773,\n",
            "        0.4763, 0.4768, 0.4811, 0.4772, 0.4767, 0.4773, 0.4772, 0.4694, 0.4773,\n",
            "        0.4772])\n",
            "Sum of predicted tensor(30.4472)\n",
            "Loss tensor(0.6954)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4771, 0.4750, 0.4754, 0.4772, 0.4764, 0.4720, 0.4749, 0.4773,\n",
            "        0.4713, 0.4761, 0.4758, 0.4703, 0.4765, 0.4773, 0.4745, 0.4784, 0.4787,\n",
            "        0.4739, 0.4708, 0.4754, 0.4731, 0.4780, 0.4785, 0.4708, 0.4693, 0.4746,\n",
            "        0.4745, 0.4763, 0.4761, 0.4693, 0.4762, 0.4750, 0.4742, 0.4794, 0.4720,\n",
            "        0.4750, 0.4772, 0.4794, 0.4768, 0.4731, 0.4789, 0.4755, 0.4759, 0.4708,\n",
            "        0.4695, 0.4774, 0.4747, 0.4746, 0.4738, 0.4787, 0.4773, 0.4752, 0.4762,\n",
            "        0.4773, 0.4748, 0.4787, 0.4773, 0.4724, 0.4750, 0.4771, 0.4783, 0.4691,\n",
            "        0.4777])\n",
            "Sum of predicted tensor(30.4190)\n",
            "Loss tensor(0.7088)\n",
            "Acc tensor(0.3438)\n",
            "tensor([0.4773, 0.4768, 0.4726, 0.4713, 0.4762, 0.4734, 0.4722, 0.4750, 0.4722,\n",
            "        0.4746, 0.4744, 0.4726, 0.4783, 0.4745, 0.4763, 0.4748, 0.4747, 0.4773,\n",
            "        0.4802, 0.4748, 0.4775, 0.4721, 0.4760, 0.4747, 0.4792, 0.4731, 0.4760,\n",
            "        0.4769, 0.4710, 0.4773, 0.4787, 0.4740, 0.4831, 0.4728, 0.4746, 0.4747,\n",
            "        0.4772, 0.4741, 0.4733, 0.4748, 0.4787, 0.4752, 0.4774, 0.4727, 0.4749,\n",
            "        0.4730, 0.4785, 0.4741, 0.4702, 0.4736, 0.4750, 0.4729, 0.4755, 0.4722,\n",
            "        0.4771, 0.4737, 0.4733, 0.4780, 0.4724, 0.4735, 0.4731, 0.4724, 0.4741,\n",
            "        0.4759])\n",
            "Sum of predicted tensor(30.3977)\n",
            "Loss tensor(0.6962)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4751, 0.4762, 0.4739, 0.4755, 0.4749, 0.4760, 0.4762, 0.4733, 0.4749,\n",
            "        0.4723, 0.4754, 0.4733, 0.4769, 0.4717, 0.4742, 0.4720, 0.4794, 0.4787,\n",
            "        0.4789, 0.4768, 0.4790, 0.4775, 0.4775, 0.4726, 0.4749, 0.4713, 0.4736,\n",
            "        0.4755, 0.4789, 0.4768, 0.4718, 0.4747, 0.4811, 0.4729, 0.4782, 0.4711,\n",
            "        0.4783, 0.4744, 0.4739, 0.4720, 0.4699, 0.4729, 0.4757, 0.4755, 0.4731,\n",
            "        0.4786, 0.4727, 0.4727, 0.4754, 0.4712, 0.4739, 0.4746, 0.4796, 0.4768,\n",
            "        0.4730, 0.4770, 0.4765, 0.4716, 0.4773, 0.4723, 0.4708, 0.4774, 0.4789,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4032)\n",
            "Loss tensor(0.7034)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4746, 0.4743, 0.4718, 0.4751, 0.4714, 0.4805, 0.4763, 0.4811, 0.4728,\n",
            "        0.4730, 0.4752, 0.4797, 0.4687, 0.4764, 0.4742, 0.4812, 0.4752, 0.4755,\n",
            "        0.4747, 0.4741, 0.4771, 0.4699, 0.4802, 0.4763, 0.4789, 0.4747, 0.4744,\n",
            "        0.4720, 0.4811, 0.4740, 0.4740, 0.4760, 0.4777, 0.4749, 0.4757, 0.4752,\n",
            "        0.4789, 0.4755, 0.4760, 0.4700, 0.4738, 0.4695, 0.4761, 0.4750, 0.4791,\n",
            "        0.4750, 0.4744, 0.4802, 0.4806, 0.4751, 0.4705, 0.4757, 0.4741, 0.4757,\n",
            "        0.4741, 0.4775, 0.4794, 0.4699, 0.4711, 0.4768, 0.4743, 0.4728, 0.4737,\n",
            "        0.4735])\n",
            "Sum of predicted tensor(30.4164)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4793, 0.4766, 0.4687, 0.4719, 0.4776, 0.4713, 0.4759, 0.4756, 0.4747,\n",
            "        0.4751, 0.4765, 0.4765, 0.4727, 0.4735, 0.4756, 0.4764, 0.4744, 0.4741,\n",
            "        0.4775, 0.4744, 0.4739, 0.4734, 0.4746, 0.4765, 0.4696, 0.4755, 0.4738,\n",
            "        0.4729, 0.4729, 0.4794, 0.4762, 0.4755, 0.4763, 0.4713, 0.4788, 0.4794,\n",
            "        0.4754, 0.4747, 0.4726, 0.4736, 0.4713, 0.4728, 0.4766, 0.4764, 0.4713,\n",
            "        0.4746, 0.4738, 0.4742, 0.4719, 0.4728, 0.4761, 0.4714, 0.4811, 0.4763,\n",
            "        0.4777, 0.4788, 0.4764, 0.4781, 0.4754, 0.4709, 0.4827, 0.4779, 0.4811,\n",
            "        0.4745])\n",
            "Sum of predicted tensor(30.4090)\n",
            "Loss tensor(0.7067)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0.4731, 0.4742, 0.4809, 0.4748, 0.4832, 0.4769, 0.4790, 0.4783, 0.4768,\n",
            "        0.4681, 0.4775, 0.4784, 0.4762, 0.4701, 0.4720, 0.4730, 0.4727, 0.4707,\n",
            "        0.4772, 0.4753, 0.4773, 0.4795, 0.4761, 0.4770, 0.4748, 0.4754, 0.4771,\n",
            "        0.4773, 0.4765, 0.4786, 0.4811, 0.4768, 0.4752, 0.4781, 0.4766, 0.4783,\n",
            "        0.4760, 0.4729, 0.4827, 0.4733, 0.4701, 0.4801, 0.4751, 0.4741, 0.4739,\n",
            "        0.4754, 0.4769, 0.4754, 0.4827, 0.4799, 0.4799, 0.4707, 0.4752, 0.4710,\n",
            "        0.4770, 0.4764, 0.4758, 0.4760, 0.4712, 0.4768, 0.4761, 0.4727, 0.4750,\n",
            "        0.4762])\n",
            "Sum of predicted tensor(30.4627)\n",
            "Loss tensor(0.6931)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4770, 0.4743, 0.4776, 0.4740, 0.4705, 0.4765, 0.4768, 0.4760, 0.4747,\n",
            "        0.4764, 0.4692, 0.4792, 0.4764, 0.4773, 0.4713, 0.4744, 0.4727, 0.4761,\n",
            "        0.4804, 0.4786, 0.4782, 0.4754, 0.4769, 0.4785, 0.4726, 0.4762, 0.4747,\n",
            "        0.4748, 0.4719, 0.4719, 0.4788, 0.4811, 0.4784, 0.4804, 0.4713, 0.4755,\n",
            "        0.4723, 0.4804, 0.4830, 0.4742, 0.4736, 0.4750, 0.4736, 0.4713, 0.4773,\n",
            "        0.4744, 0.4737, 0.4757, 0.4721, 0.4746, 0.4773, 0.4804, 0.4762, 0.4708,\n",
            "        0.4716, 0.4748, 0.4804, 0.4733, 0.4748, 0.4783, 0.4747, 0.4729, 0.4783,\n",
            "        0.4712])\n",
            "Sum of predicted tensor(30.4326)\n",
            "Loss tensor(0.7025)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0.4760, 0.4726, 0.4761, 0.4753, 0.4766, 0.4791, 0.4739, 0.4768, 0.4712,\n",
            "        0.4687, 0.4729, 0.4754, 0.4773, 0.4761, 0.4804, 0.4787, 0.4830, 0.4750,\n",
            "        0.4746, 0.4711, 0.4788, 0.4711, 0.4722, 0.4727, 0.4773, 0.4726, 0.4738,\n",
            "        0.4741, 0.4726, 0.4756, 0.4735, 0.4712, 0.4747, 0.4811, 0.4781, 0.4799,\n",
            "        0.4731, 0.4761, 0.4697, 0.4729, 0.4719, 0.4762, 0.4763, 0.4761, 0.4738,\n",
            "        0.4721, 0.4709, 0.4744, 0.4754, 0.4758, 0.4712, 0.4730, 0.4773, 0.4763,\n",
            "        0.4783, 0.4790, 0.4764, 0.4687, 0.4737, 0.4746, 0.4712, 0.4747, 0.4764,\n",
            "        0.4771])\n",
            "Sum of predicted tensor(30.3926)\n",
            "Loss tensor(0.6875)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4731, 0.4777, 0.4750, 0.4745, 0.4746, 0.4724, 0.4764, 0.4745, 0.4701,\n",
            "        0.4786, 0.4758, 0.4778, 0.4786, 0.4736, 0.4718, 0.4734, 0.4783, 0.4755,\n",
            "        0.4805, 0.4732, 0.4765, 0.4804, 0.4827, 0.4726, 0.4751, 0.4729, 0.4757,\n",
            "        0.4713, 0.4714, 0.4767, 0.4727, 0.4734, 0.4752, 0.4768, 0.4785, 0.4807,\n",
            "        0.4738, 0.4811, 0.4752, 0.4727, 0.4715, 0.4740, 0.4694, 0.4729, 0.4772,\n",
            "        0.4714, 0.4745, 0.4781, 0.4762, 0.4770, 0.4705, 0.4764, 0.4779, 0.4770,\n",
            "        0.4811, 0.4712, 0.4714, 0.4705, 0.4756, 0.4715, 0.4763, 0.4708, 0.4752,\n",
            "        0.4755])\n",
            "Sum of predicted tensor(30.4041)\n",
            "Loss tensor(0.6964)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4792, 0.4724, 0.4748, 0.4724, 0.4759, 0.4767, 0.4725, 0.4757, 0.4760,\n",
            "        0.4787, 0.4744, 0.4783, 0.4710, 0.4735, 0.4778, 0.4757, 0.4801, 0.4693,\n",
            "        0.4713, 0.4771, 0.4775, 0.4705, 0.4769, 0.4738, 0.4681, 0.4731, 0.4751,\n",
            "        0.4783, 0.4747, 0.4693, 0.4766, 0.4735, 0.4734, 0.4731, 0.4721, 0.4745,\n",
            "        0.4745, 0.4752, 0.4743, 0.4733, 0.4744, 0.4750, 0.4778, 0.4731, 0.4746,\n",
            "        0.4740, 0.4742, 0.4801, 0.4712, 0.4751, 0.4777, 0.4742, 0.4724, 0.4724,\n",
            "        0.4736, 0.4744, 0.4801, 0.4744, 0.4760, 0.4754, 0.4784, 0.4730, 0.4730,\n",
            "        0.4749])\n",
            "Sum of predicted tensor(30.3800)\n",
            "Loss tensor(0.6939)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0.4717, 0.4777, 0.4730, 0.4776, 0.4746, 0.4734, 0.4766, 0.4721, 0.4705,\n",
            "        0.4721, 0.4785, 0.4733, 0.4781, 0.4805, 0.4752, 0.4715, 0.4782, 0.4775,\n",
            "        0.4786, 0.4777, 0.4771, 0.4792, 0.4761, 0.4715, 0.4757, 0.4734, 0.4775,\n",
            "        0.4698, 0.4771, 0.4786, 0.4717, 0.4779, 0.4746, 0.4765, 0.4746, 0.4756,\n",
            "        0.4681, 0.4796, 0.4701, 0.4693, 0.4777, 0.4711, 0.4788, 0.4766, 0.4752,\n",
            "        0.4778, 0.4746, 0.4725, 0.4777, 0.4771, 0.4758, 0.4755, 0.4754, 0.4745,\n",
            "        0.4793, 0.4766, 0.4757, 0.4762, 0.4750, 0.4710, 0.4736, 0.4782, 0.4767,\n",
            "        0.4743])\n",
            "Sum of predicted tensor(30.4194)\n",
            "Loss tensor(0.6820)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0.4747, 0.4765, 0.4709, 0.4811, 0.4758, 0.4717, 0.4760, 0.4699, 0.4811,\n",
            "        0.4743, 0.4765, 0.4748, 0.4720, 0.4731, 0.4766, 0.4737, 0.4758, 0.4785,\n",
            "        0.4801, 0.4765, 0.4775, 0.4767, 0.4724, 0.4725, 0.4698, 0.4705, 0.4761,\n",
            "        0.4759, 0.4701, 0.4733, 0.4745, 0.4744, 0.4805, 0.4744, 0.4726, 0.4746,\n",
            "        0.4751, 0.4756, 0.4724, 0.4752, 0.4750, 0.4750, 0.4731, 0.4721, 0.4731,\n",
            "        0.4730, 0.4775, 0.4726, 0.4714, 0.4775, 0.4775, 0.4736, 0.4726, 0.4744,\n",
            "        0.4775, 0.4777, 0.4752, 0.4732, 0.4778, 0.4686, 0.4726, 0.4744, 0.4771,\n",
            "        0.4750])\n",
            "Sum of predicted tensor(30.3809)\n",
            "Loss tensor(0.6969)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0.4759, 0.4722, 0.4719, 0.4748, 0.4801, 0.4745, 0.4729, 0.4807, 0.4771,\n",
            "        0.4767, 0.4765, 0.4747, 0.4722, 0.4739, 0.4736, 0.4752, 0.4757, 0.4730,\n",
            "        0.4738, 0.4788, 0.4760, 0.4759, 0.4719, 0.4768, 0.4717, 0.4660, 0.4738,\n",
            "        0.4743, 0.4766, 0.4744, 0.4789, 0.4771, 0.4745, 0.4767, 0.4777, 0.4753,\n",
            "        0.4744, 0.4743, 0.4708, 0.4744, 0.4758, 0.4720, 0.4709, 0.4757, 0.4742,\n",
            "        0.4767, 0.4753, 0.4765, 0.4728, 0.4776, 0.4731, 0.4718, 0.4752, 0.4811,\n",
            "        0.4744, 0.4717, 0.4758, 0.4775, 0.4751, 0.4751, 0.4738, 0.4708, 0.4767,\n",
            "        0.4768])\n",
            "Sum of predicted tensor(30.3919)\n",
            "Loss tensor(0.6834)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0.4811, 0.4748, 0.4722, 0.4789, 0.4729, 0.4709, 0.4752, 0.4732, 0.4751,\n",
            "        0.4707, 0.4750, 0.4812, 0.4773, 0.4717, 0.4761, 0.4812, 0.4790, 0.4777,\n",
            "        0.4751, 0.4759, 0.4823, 0.4709, 0.4763, 0.4792, 0.4811, 0.4761, 0.4726,\n",
            "        0.4733, 0.4801, 0.4769, 0.4756, 0.4761, 0.4755, 0.4733, 0.4762, 0.4758,\n",
            "        0.4734, 0.4660, 0.4764, 0.4828, 0.4777, 0.4734, 0.4750, 0.4703, 0.4768,\n",
            "        0.4762, 0.4734, 0.4744, 0.4660, 0.4728, 0.4717, 0.4660, 0.4757, 0.4762,\n",
            "        0.4748, 0.4709, 0.4728, 0.4768, 0.4741, 0.4828, 0.4762, 0.4733, 0.4764,\n",
            "        0.4756])\n",
            "Sum of predicted tensor(30.4146)\n",
            "Loss tensor(0.6888)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0.4736, 0.4730, 0.4677, 0.4731, 0.4753, 0.4722, 0.4760, 0.4772, 0.4765,\n",
            "        0.4766, 0.4726, 0.4721, 0.4744, 0.4731, 0.4761, 0.4771, 0.4718, 0.4768,\n",
            "        0.4734, 0.4731, 0.4726, 0.4731, 0.4752, 0.4726, 0.4766, 0.4722, 0.4736,\n",
            "        0.4758, 0.4730, 0.4731, 0.4722, 0.4815, 0.4734, 0.4768, 0.4724, 0.4737,\n",
            "        0.4780, 0.4765, 0.4734, 0.4779, 0.4765, 0.4761, 0.4733, 0.4815, 0.4709,\n",
            "        0.4756, 0.4788, 0.4752, 0.4717, 0.4710, 0.4767, 0.4790, 0.4734, 0.4823,\n",
            "        0.4746, 0.4799, 0.4769, 0.4717, 0.4817, 0.4811, 0.4733, 0.4739, 0.4812,\n",
            "        0.4741])\n",
            "Sum of predicted tensor(30.4056)\n",
            "Loss tensor(0.6790)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0.4789, 0.4763, 0.4732, 0.4747, 0.4762, 0.4755, 0.4750, 0.4745, 0.4760,\n",
            "        0.4758, 0.4803])\n",
            "Sum of predicted tensor(5.2364)\n",
            "Loss tensor(0.6734)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.87%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsampled Strategy 2"
      ],
      "metadata": {
        "id": "L2dJClqaX9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_full(model, test_loader, device='cpu'):\n",
        "    '''\n",
        "    Accepts the current best model and evaluates\n",
        "    the test dataset. Printing test accuracy and\n",
        "    an sklearn confusion matrix report.\n",
        "    \n",
        "    Inputs:\n",
        "    - model: PyTorch model object, the current best model.\n",
        "    - test_loader: an iterator with test data.\n",
        "    - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "    \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        Prints test accuracy.\n",
        "        Prints an Accuracy / F1 Report (sklearn)\n",
        "        Prints a Confusion Matrix (sklearn & matplotlib)\n",
        "    '''\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for ((text, text_len),labels) in test_loader:           \n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "            text_len = text_len.to(device)\n",
        "            output = model(text, text_len)   \n",
        "            output = torch.round(output)\n",
        "            # torch.max(output, axis=1).indices\n",
        "            #.indices\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "\n",
        "    acc = 0.0\n",
        "    for i, y in enumerate(y_pred):\n",
        "        if y == y_true[i]:\n",
        "            acc += 1.0\n",
        "    \n",
        "    acc = acc / len(y_pred)\n",
        "    \n",
        "    \n",
        "    print('Test Accuracy: ', acc)\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['1','0'])\n",
        "    ax.yaxis.set_ticklabels(['1','0'])\n",
        "    \n",
        "    return None"
      ],
      "metadata": {
        "id": "qJLkmIllJGuT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "    '''\n",
        "    Used in train_model() to save the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - save_path: string, where to save model specs.\n",
        "        - model: a PyTorch model object.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - train_loader: the iterator with training data.\n",
        "        - valid_loss: float, current model loss.\n",
        "        \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's state dictionary to designated\n",
        "        file path as 'model.pt'\n",
        "    '''\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer, device='cpu'):\n",
        "    '''\n",
        "    Used in evaluate() to load the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - load_path: string, where to save model specs.\n",
        "        - model: a PyTorch model object.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "        \n",
        "    Returns: float, the models last validation loss.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Loads the saved state at load_path\n",
        "        into the current model object\n",
        "    '''\n",
        "    if load_path==None:\n",
        "        return\n",
        "\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "    '''\n",
        "    Used in train_model() to save the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - save_path: string, where to save model specs.\n",
        "        - train_loss_list: list of float, current model training losses.\n",
        "        - valid_loss_list: list of float, current model validation losses.\n",
        "        - global_steps_list: list of loss function calculation steps.\n",
        "        \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's loss history designated\n",
        "        file path as 'metrics.pt'\n",
        "    '''\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path, device='cpu'):\n",
        "    '''\n",
        "    Used in evaluate() to load the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - load_path: string, where find model specs.\n",
        "        - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "    Returns: tuple (train_loss_list, valid_loss_list, global_steps_list)\n",
        "        - train_loss_list: list of float, current model training losses.\n",
        "        - valid_loss_list: list of float, current model validation losses.\n",
        "        - global_steps_list: list of loss function calculation steps.\n",
        "    '''\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return (state_dict['train_loss_list'], \n",
        "            state_dict['valid_loss_list'], \n",
        "            state_dict['global_steps_list'])\n",
        "        "
      ],
      "metadata": {
        "id": "WdKALlcEAE6K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                optimizer,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                criterion = nn.CrossEntropyLoss(),\n",
        "                num_epochs = 7,\n",
        "                file_path = './data',\n",
        "                best_valid_loss = float(\"Inf\"),\n",
        "                device='cpu'):\n",
        "    '''\n",
        "    Runs training and evaluation on validation data loop over specified number of epochs.\n",
        "    \n",
        "    Inputs:\n",
        "        - model: a PyTorch model object. In this case, the one from lstm.py.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - train_loader: the iterator with training data.\n",
        "        - valid_loader: the iterator with validation data.\n",
        "        - criterion: a PyTorch loss function instance.\n",
        "        - num_epochs: int, how many epochs to train over.\n",
        "        - file_path: string, where to save model specs.\n",
        "        - best_valid_loss: float, defaults to infinity to start training from scratch, \n",
        "            but if continuing from a prior run and wish to only save better outcomes,\n",
        "            you can pass the last best model's loss.\n",
        "        - device: string, 'cpu' or 'cuda' if running on google colab.\n",
        "    \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's state dictionary to designated\n",
        "        file path as 'model.pt'\n",
        "        - Saves loss history to designated file path as '/metrics.pt'\n",
        "    '''\n",
        "    \n",
        "    eval_every = len(train_loader) // 2\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for ((text, text_len), labels) in train_loader:           \n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "            text_len = text_len.to(device)\n",
        "            output = model(text, text_len).squeeze()\n",
        "            loss = criterion(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  for ((text, text_len), labels) in valid_loader:\n",
        "                        labels = labels.to(device)\n",
        "                        text = text.to(device)\n",
        "                        text_len = text_len.to(device)\n",
        "                        output = model(text, text_len).squeeze()\n",
        "                        loss = criterion(output, labels)\n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    print(best_valid_loss)\n",
        "                    save_checkpoint('/model.pt', model, optimizer, best_valid_loss)\n",
        "                    save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print(train_loss_list, valid_loss_list, global_steps_list)\n",
        "    save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "    \n",
        "    return None"
      ],
      "metadata": {
        "id": "90YadA77AEk8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "SAVE_PATH = './data'\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "train_model(model=model,\n",
        "            optimizer=optimizer,\n",
        "            train_loader=train_iterator,\n",
        "            valid_loader=valid_iterator,\n",
        "            criterion = criterion,\n",
        "            num_epochs = 20,\n",
        "            file_path = SAVE_PATH,\n",
        "            device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeSf8_9cPc-n",
        "outputId": "38613a96-7b67-4f0b-be00-482b2fd808a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [325/13000], Train Loss: 0.2055, Valid Loss: 0.0815\n",
            "0.0814746913408661\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [1/20], Step [650/13000], Train Loss: 0.0650, Valid Loss: 0.0416\n",
            "0.04159677991951217\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [2/20], Step [975/13000], Train Loss: 0.0256, Valid Loss: 0.0334\n",
            "0.0334215342497503\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [2/20], Step [1300/13000], Train Loss: 0.0194, Valid Loss: 0.0242\n",
            "0.024183453950016493\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [3/20], Step [1625/13000], Train Loss: 0.0076, Valid Loss: 0.0232\n",
            "0.023232126249786232\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [3/20], Step [1950/13000], Train Loss: 0.0085, Valid Loss: 0.0197\n",
            "0.01971637272976555\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [4/20], Step [2275/13000], Train Loss: 0.0067, Valid Loss: 0.0261\n",
            "Epoch [4/20], Step [2600/13000], Train Loss: 0.0049, Valid Loss: 0.0197\n",
            "0.01967115882858447\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [5/20], Step [2925/13000], Train Loss: 0.0019, Valid Loss: 0.0187\n",
            "0.018746791361835286\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [5/20], Step [3250/13000], Train Loss: 0.0037, Valid Loss: 0.0554\n",
            "Epoch [6/20], Step [3575/13000], Train Loss: 0.0028, Valid Loss: 0.0187\n",
            "Epoch [6/20], Step [3900/13000], Train Loss: 0.0007, Valid Loss: 0.0189\n",
            "Epoch [7/20], Step [4225/13000], Train Loss: 0.0008, Valid Loss: 0.0158\n",
            "0.015754014105102344\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [7/20], Step [4550/13000], Train Loss: 0.0014, Valid Loss: 0.0190\n",
            "Epoch [8/20], Step [4875/13000], Train Loss: 0.0009, Valid Loss: 0.0207\n",
            "Epoch [8/20], Step [5200/13000], Train Loss: 0.0006, Valid Loss: 0.0197\n",
            "Epoch [9/20], Step [5525/13000], Train Loss: 0.0003, Valid Loss: 0.0245\n",
            "Epoch [9/20], Step [5850/13000], Train Loss: 0.0009, Valid Loss: 0.0200\n",
            "Epoch [10/20], Step [6175/13000], Train Loss: 0.0008, Valid Loss: 0.0234\n",
            "Epoch [10/20], Step [6500/13000], Train Loss: 0.0002, Valid Loss: 0.0243\n",
            "Epoch [11/20], Step [6825/13000], Train Loss: 0.0004, Valid Loss: 0.0321\n",
            "Epoch [11/20], Step [7150/13000], Train Loss: 0.0007, Valid Loss: 0.0258\n",
            "Epoch [12/20], Step [7475/13000], Train Loss: 0.0005, Valid Loss: 0.0291\n",
            "Epoch [12/20], Step [7800/13000], Train Loss: 0.0004, Valid Loss: 0.0218\n",
            "Epoch [13/20], Step [8125/13000], Train Loss: 0.0084, Valid Loss: 0.0198\n",
            "Epoch [13/20], Step [8450/13000], Train Loss: 0.0022, Valid Loss: 0.0171\n",
            "Epoch [14/20], Step [8775/13000], Train Loss: 0.0008, Valid Loss: 0.0201\n",
            "Epoch [14/20], Step [9100/13000], Train Loss: 0.0006, Valid Loss: 0.0180\n",
            "Epoch [15/20], Step [9425/13000], Train Loss: 0.0006, Valid Loss: 0.0193\n",
            "Epoch [15/20], Step [9750/13000], Train Loss: 0.0002, Valid Loss: 0.0201\n",
            "Epoch [16/20], Step [10075/13000], Train Loss: 0.0003, Valid Loss: 0.0204\n",
            "Epoch [16/20], Step [10400/13000], Train Loss: 0.0005, Valid Loss: 0.0201\n",
            "Epoch [17/20], Step [10725/13000], Train Loss: 0.0003, Valid Loss: 0.0214\n",
            "Epoch [17/20], Step [11050/13000], Train Loss: 0.0003, Valid Loss: 0.0194\n",
            "Epoch [18/20], Step [11375/13000], Train Loss: 0.0003, Valid Loss: 0.0219\n",
            "Epoch [18/20], Step [11700/13000], Train Loss: 0.0003, Valid Loss: 0.0209\n",
            "Epoch [19/20], Step [12025/13000], Train Loss: 0.0001, Valid Loss: 0.0230\n",
            "Epoch [19/20], Step [12350/13000], Train Loss: 0.0005, Valid Loss: 0.0212\n",
            "Epoch [20/20], Step [12675/13000], Train Loss: 0.0003, Valid Loss: 0.0301\n",
            "Epoch [20/20], Step [13000/13000], Train Loss: 0.0003, Valid Loss: 0.0327\n",
            "[0.20553174828394102, 0.06503794685149422, 0.025641717093531042, 0.019429252074064256, 0.007559814086304798, 0.008481285473882542, 0.006726792573205714, 0.004949657168494573, 0.001916580095824499, 0.0037247697224274115, 0.002822380983147573, 0.0006858233869150774, 0.0007999892391615923, 0.0014316791219439787, 0.0009310189675516785, 0.0005771351017371601, 0.0002583419114671415, 0.0009347588814440454, 0.0008473035325124119, 0.00016726879001103392, 0.0004248099077531501, 0.0006626116663771707, 0.00047374502879043013, 0.0003510373504618568, 0.008360644706516495, 0.002154550823207291, 0.0008097281022576051, 0.0005902916979770313, 0.0006124012183317972, 0.0001956686463717443, 0.00027087761583935877, 0.0004764251615740879, 0.00034892232685059264, 0.00032212186753667995, 0.00034579003596893297, 0.0003201081576462282, 8.114593862252115e-05, 0.0005182875486187956, 0.0002963389227668446, 0.0003045323271423355] [0.0814746913408661, 0.04159677991951217, 0.0334215342497503, 0.024183453950016493, 0.023232126249786232, 0.01971637272976555, 0.02608103605827117, 0.01967115882858447, 0.018746791361835286, 0.0554044361484817, 0.01874835462159743, 0.018907749512728906, 0.015754014105102344, 0.0189955871003154, 0.020726188426033333, 0.0197142354539486, 0.024546820219472108, 0.020045243696910933, 0.02339682106061543, 0.024308435678642583, 0.03210473774151694, 0.025759876494707002, 0.029063338291173375, 0.021792176934532392, 0.01981594639210016, 0.01709555366101735, 0.02005389936987285, 0.018025502789833225, 0.019250555599585693, 0.020119522582254083, 0.020363245168705038, 0.020053951348929948, 0.021357802261373342, 0.019426902774422876, 0.021928342216602913, 0.020882413450925088, 0.023021656759463126, 0.02123969754996184, 0.030132698783692544, 0.03267005228553061] [325, 650, 975, 1300, 1625, 1950, 2275, 2600, 2925, 3250, 3575, 3900, 4225, 4550, 4875, 5200, 5525, 5850, 6175, 6500, 6825, 7150, 7475, 7800, 8125, 8450, 8775, 9100, 9425, 9750, 10075, 10400, 10725, 11050, 11375, 11700, 12025, 12350, 12675, 13000]\n",
            "Model saved to ==> /metrics.pt\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "best_model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, TEXT.vocab, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)\n",
        "\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=0.00001)\n",
        "\n",
        "load_checkpoint('/model.pt', best_model, optimizer)\n",
        "evaluate_full(best_model, test_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "TH5Nm9aTRqT0",
        "outputId": "43eaa78c-b001-4303-eb8a-7e678e4c7c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /model.pt\n",
            "Test Accuracy:  0.9968574635241302\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.9944    0.9993    0.9969      4461\n",
            "           0     0.9993    0.9944    0.9968      4449\n",
            "\n",
            "    accuracy                         0.9969      8910\n",
            "   macro avg     0.9969    0.9969    0.9969      8910\n",
            "weighted avg     0.9969    0.9969    0.9969      8910\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEWCAYAAACZnQc8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwVxb338c93QEQRFBWNQbxiRCNqRKK4IHGLiGguRo27Io+5ZIGoiYn7I1ejXpcYo3HJQyKKuxg1opIgcQnu4EJUROI8EiOIYgRxQZGB3/2ja/Awzpw5A2fmzDTfN69+cbq6urp6xN+pqa6uUkRgZmb5UFXpCpiZWfk4qJuZ5YiDuplZjjiom5nliIO6mVmOOKibmeWIg7qtMklrSbpf0kJJd61COcdIeqicdasESX+WNLTS9bDVk4P6akTS0ZKek/SxpLkp+OxRhqIPAzYGNoiI761sIRFxa0QMLEN9ViBpL0kh6d466Tuk9MdKLOe/Jd3SWL6IOCAixq5kdc1WiYP6akLSz4DfABeRBeDNgGuBIWUo/j+Af0RETRnKai7vAbtJ2qAgbSjwj3JdQBn/P2UV5X+AqwFJ6wLnAyMi4p6I+CQilkTE/RHxi5RnTUm/kfR22n4jac10bC9JsyWdKmleauUPS8fOA84Fjki/AZxYt0UrafPUIm6f9k+Q9IakjyTNknRMQfoTBeftLmlq6taZKmn3gmOPSfqlpCdTOQ9J2rDIj+Fz4E/Aken8dsARwK11flZXSnpL0oeSnpc0IKUPAs4quM+/F9TjQklPAouALVLa99Px6yTdXVD+JZIelqSS/wOaNYGD+uphN6AjcG+RPGcDuwJ9gB2AfsA5Bce/AqwLdAdOBK6R1DUiRpG1/u+MiHUi4vpiFZHUCbgKOCAiOgO7A9Pqybc+8GDKuwHwa+DBOi3to4FhwEZAB+Dnxa4N3AQcnz7vD7wCvF0nz1Syn8H6wG3AXZI6RsRf6tznDgXnHAcMBzoDb9Yp71Rg+/SFNYDsZzc0PD+HNRMH9dXDBsC/G+keOQY4PyLmRcR7wHlkwarWknR8SURMAD4Gtl7J+iwDtpO0VkTMjYjp9eQ5EHg9Im6OiJqIuB14DfhOQZ4bIuIfEfEpMI4sGDcoIp4C1pe0NVlwv6mePLdExPvpmpcDa9L4fd4YEdPTOUvqlLeI7Of4a+AW4CcRMbuR8sxWmoP66uF9YMPa7o8GfJUVW5lvprTlZdT5UlgErNPUikTEJ2TdHj8E5kp6UNLXS6hPbZ26F+y/sxL1uRkYCexNPb+5SPq5pBmpy+cDst9OinXrALxV7GBEPAu8AYjsy8es2Tiorx6eBhYDBxfJ8zbZA89am/HlrolSfQKsXbD/lcKDETExIvYDNiFrff++hPrU1mnOStap1s3Aj4EJqRW9XOoeOQ04HOgaEesBC8mCMUBDXSZFu1IkjSBr8b+dyjdrNg7qq4GIWEj2MPMaSQdLWlvSGpIOkHRpynY7cI6kbumB47lk3QUrYxrwLUmbpYe0Z9YekLSxpCGpb30xWTfOsnrKmABslYZhtpd0BNAbeGAl6wRARMwC9iR7hlBXZ6CGbKRMe0nnAl0Kjr8LbN6UES6StgIuAI4l64Y5TVLRbiKzVeGgvppI/cM/I3v4+R5Zl8FIshEhkAWe54CXgJeBF1LaylxrEnBnKut5VgzEVakebwPzyQLsj+op433gILIHje+TtXAPioh/r0yd6pT9RETU91vIROAvZMMc3wQ+Y8WuldoXq96X9EJj10ndXbcAl0TE3yPidbIRNDfXjiwyKzf5IbyZWX64pW5mliMO6mZmOeKgbmaWIw7qZmY5UuxllIpaa8eRfoJrX7Jg6tWVroK1Qh3bs8pz6TQl5nz64tWtdu4et9TNzHKk1bbUzcxaVE5mTXZQNzMDqGpX6RqUhYO6mRlATqa4d1A3MwN3v5iZ5Ypb6mZmOeKWuplZjrilbmaWIx79YmaWI+5+MTPLEXe/mJnliFvqZmY54qBuZpYj7fyg1MwsP9ynbmaWI+5+MTPLEbfUzcxyxC11M7McyUlLPR9fTWZmq6qqXelbCSS1k/SipAfSfk9Jz0qqlnSnpA4pfc20X52Ob15Qxpkpfaak/Uu6jSbfuJlZHqmq9K00JwMzCvYvAa6IiC2BBcCJKf1EYEFKvyLlQ1Jv4EhgW2AQcK2kRr9RHNTNzCDrfil1a7QobQocCPwh7QvYB/hjyjIWODh9HpL2Scf3TfmHAHdExOKImAVUA/0au7aDupkZNKmlLmm4pOcKtuF1SvsNcBqwLO1vAHwQETVpfzbQPX3uDrwFkI4vTPmXp9dzToP8oNTMDJo0+iUiRgOj6y1GOgiYFxHPS9qrPJUrnYO6mRmUcz71/sB/ShoMdAS6AFcC60lqn1rjmwJzUv45QA9gtqT2wLrA+wXptQrPafg2ynUXZmZtWpn61CPizIjYNCI2J3vQ+UhEHAM8ChyWsg0F7kufx6d90vFHIiJS+pFpdExPoBcwpbHbcEvdzAxa4uWj04E7JF0AvAhcn9KvB26WVA3MJ/siICKmSxoHvArUACMiYmljF3FQNzODZnn5KCIeAx5Ln9+gntErEfEZ8L0Gzr8QuLAp13RQNzMDlJM3Sh3UzcxwUDczyxVVOaibmeWGW+pmZjnioG5mliMO6mZmeZKPmO6gbmYGbqmbmeVKVVU+Zk1xUDczwy11M7N8yUdMd1A3MwO31M3McsVB3cwsRzxNgJlZjuSlpZ6PMTxmZqtIUslbI+V0lDRF0t8lTZd0Xkq/UdIsSdPS1ielS9JVkqolvSSpb0FZQyW9nrahDV2zkFvqZmaUtaW+GNgnIj6WtAbwhKQ/p2O/iIg/1sl/ANlSdb2AXYDrgF0krQ+MAnYCAnhe0viIWFDs4m6pm5lRvpZ6ZD5Ou2ukLYqcMgS4KZ33DNkC1ZsA+wOTImJ+CuSTgEGN3YeDupkZZOPUS9wkDZf0XME2fIWipHaSpgHzyALzs+nQhamL5QpJa6a07sBbBafPTmkNpRfl7hczM5o2TUBEjAZGFzm+FOgjaT3gXknbAWcC7wAd0rmnA+evSp3r45a6mRnl634pFBEfAI8CgyJibupiWQzcwBeLUM8BehSctmlKayi9KAd1MzNoUvdL0WKkbqmFjqS1gP2A11I/Ocq+FQ4GXkmnjAeOT6NgdgUWRsRcYCIwUFJXSV2BgSmtKHe/VFBVlXjy1tN4e95CDj35d8vTLz/tMI4fshvd+p8KwLHf2YWLfnowb89bCMDv7vwbN977NAAXnjyEQQO2o0rikWdf49RL6z5Yt7xZvHgxw44/hiWff07N0qXsN3B/fjzypEpXq80r4+iXTYCxktqRNZzHRcQDkh6R1I3sa2Ea8MOUfwIwGKgGFgHDACJivqRfAlNTvvMjYn5jF3dQr6CRR+/NzFnv0rlTx+VpfXtvxnqd1/5S3rsnvsBPL7lrhbRdd+jJbn22YOfDLwLgkRt+xoBv9uLx519v3opbRXXo0IE/jBnL2p06sWTJEk447mj2GPAtvrFDn0pXrU0rV1CPiJeAHetJ36eB/AGMaODYGGBMU67v7pcK6b7RegzaY1tuuPep5WlVVeKiUw7m7Cv/VFIZEbBmhzXosEZ71uzQnvbt2zFv/ofNVWVrJSSxdqdOANTU1FBTUwM5eRuykpqjT70S3FKvkMt+cShnX/kn1ln7i1b6j47Ykwf/9jLv/PvLgXnIvn3o33dLqv81j9N+dTez3/2AZ1+axeTnXmfWpAsR4nd3TmbmrHdb8jasQpYuXcpR3zuEf/3rXxxx1NF84xs7VLpKbV5e5n5p8Za6pGFFji0f+1nz7+ktWa0WdcCA7Zg3/yNenPHFENRNuq3LIfvtyLV3/O1L+SdMfoWvHziKfkf8Dw8/8xq/P/84ALbosSFb99yYLfc/h6/tfzZ79duK/jt+rcXuwyqnXbt2jLvnPh565G+88vJLvP76PypdpTYvLy31SnS/nNfQgYgYHRE7RcRO7TfctiXr1KJ267MFB+25Pa89eB43XTyMvXbeiuf/eDZb9OjG9PGjeO3B81i74xq8ct8oAOYv/ITPl9QAcMO9T7HjNpsBMGTvHZjy8j/55NPP+eTTz5n45HR2+UbPit2XtbwuXbqwc79deOqJxytdlTYvL0G9WbpfJL3U0CFg4+a4Zlty7m/Hc+5vxwMw4Ju9OOX4fVcY/QLw3pOXs92Q7PvvKxt2Wd4lc9Ce2zNz1jsAvPXOAoYdsjuXjalCggF9e3H1bY+24J1YJcyfP5/27dvTpUsXPvvsM555+imGnfhfla5Wm9fKY3XJmqtPfWOyeQvqTjwj4KkvZ7difnzUXhy45/bULF3KgoWL+K9RtwBwz19fZM+dt+K5cWcRBJOemsGEya80Upq1df9+bx7nnHUGy5YtZdmyYOD+g9hzr70rXa02r7W3wEulbDRNmQuVrgduiIgn6jl2W0Qc3VgZa+04svwVszZvwdSrK10Fa4U6tl/1FUa3Pn1iyTFn5iX7t9pvgGZpqUfEiUWONRrQzcxaWk4a6h7SaGYG2XsieeCgbmaGW+pmZrmSlwelDupmZrilbmaWK01ZJKM1c1A3M8MtdTOzXMlLn3o+ft8wM1tFUulb8XLUUdIUSX+XNF3SeSm9p6RnJVVLulNSh5S+ZtqvTsc3LyjrzJQ+U9L+pdyHg7qZGWWd0GsxsE9E7AD0AQalZeouAa6IiC3JplCpfUnzRGBBSr8i5UNSb+BIYFtgEHBtWk2pKAd1MzPK11JPi0t/nHbXSFsA+wC1602OJVunFGBI2icd3zetYzoEuCMiFkfELLLl7moXq26Qg7qZGdkbpaVujZHUTtI0YB4wCfj/wAcRUZOyzAa6p8/dgbcA0vGFwAaF6fWc0/B9lHS3ZmY515Tul8IFfdI2vLCsiFgaEX2ATcla119vqfvw6BczM5o2pDEiRgOjS8j3gaRHgd2A9SS1T63xTYE5KdscoAcwW1J7YF3g/YL0WoXnNMgtdTMzyvegVFI3Seulz2sB+wEzgEeBw1K2ocB96fP4tE86/khkc6KPB45Mo2N6Ar2AKY3dh1vqZmaU9eWjTYCxaaRKFTAuIh6Q9Cpwh6QLgBeB61P+64GbJVUD88lGvBAR0yWNA14FaoAREbG0sYs7qJuZUb6pdyPiJWDHetLfoJ7RKxHxGfC9Bsq6ELiwKdd3UDczIz9vlDqom5nhoG5mlis5iekO6mZm4Ja6mVmu5CSmO6ibmUF+Fp5u9OUjSSdL6qLM9ZJekDSwJSpnZtZSqqSSt9aslDdK/09EfAgMBLoCxwEXN2utzMxaWLlmaay0Urpfam9hMHBzesupld+WmVnT5CWslRLUn5f0ENATOFNSZ2BZ81bLzKxl5aRLvaSgfiLZ6h1vRMQiSRsAw5q3WmZmLSsvD0obDOqS+tZJ2iIvv56YmdUl8hHfirXULy9yrHZpJjOzXMhJQ73hoB4Re7dkRczMKikvPRGljFNfW9I5kkan/V6SDmr+qpmZtZy8DGksZZz6DcDnwO5pfw5wQbPVyMysAlanl4++FhGXAksAImIR5OSJgplZUlWlkrdiJPWQ9KikVyVNl3RySv9vSXMkTUvb4IJzzpRULWmmpP0L0geltGpJZ5RyH6UMafw8rbMX6SJfAxaXUriZWVtRxgZ4DXBqRLyQ3ut5XtKkdOyKiPjVitdVb7Il7LYFvgr8VdJW6fA1ZGuczgamShofEa8Wu3gpQX0U8Begh6Rbgf7ACSXdmplZG1GubpWImAvMTZ8/kjQD6F7klCHAHRGxGJiV1iqtXfauOi2Dh6Q7Ut6iQb3R7peImAQcQhbIbwd2iojHGjvPzKwtUVM2abik5wq24fWWKW1Otl7psylppKSXJI2R1DWldQfeKjhtdkprKL2oUvrUAfYE9gX2BgaUeI6ZWZshqeQtIkZHxE4F2+h6ylsHuBs4JU2KeB3wNbI39OdS/F2gldZo94uka4EtyVrpAD+Q9O2IGNEcFTIzq4RyvnwkaQ2ygH5rRNwDEBHvFhz/PfBA2p0D9Cg4fdOURpH0BpXSp74PsE1E1D4oHQtML+E8M7M2o1xzv6RZbK8HZkTErwvSN0n97QDfBV5Jn8cDt0n6NdmD0l7AFLKenl6SepIF8yOBoxu7filBvRrYDHgz7fdIaWZmuVHGN0r7k6078bKkaSntLOAoSX3IRhL+E/gBQJrOfBzZA9AaYERELE11GglMBNoBYyKi0QZ1sQm97k8X7wzMkDQl7e9C9i1iZpYb5ep+iYgnqP9dnglFzrkQuLCe9AnFzqtPsZb6r4ocMzPLlbzM/VJsQq+/tWRFzMwqKR8hvbQJvXaVNFXSx5I+l7RU0octUTkzs5bSrkolb61ZKQ9KryZ76noXsBNwPLBV0TPMzNqYvHS/lPTyUURUA+0iYmlE3AAMat5qmZm1rLxMvVtKS32RpA7ANEmXkr0JVeqbqGZmbUJrn1K3VKUE5+NSvpHAJ2Tj1A9pzkqZmbW01aalHhG1Lx19BpwHIOlO4IhmrBfzp1zdnMVbG9W130mVroK1Qp++cNUql5GXPvVSul/qs1tZa2FmVmHtVvOgbmaWK618pGLJik0T0LehQ8AazVMdM7PKyH1Qp/hcv6+VuyJmZpWU+z71iNi7JStiZlZJq0NL3cxstZGThrqDupkZQPucRHW/GWpmRvlePpLUQ9Kjkl6VNF3SySl9fUmTJL2e/u6a0iXpKknVaVHqvgVlDU35X5c0tJT7KGWWRkk6VtK5aX8zSf1KKdzMrK2okkreGlEDnBoRvYFdgRGSegNnAA9HRC/g4bQPcADZEna9gOFkC1QjaX1gFNnCRP2AUbVfBEXvo4R7vZbsZaOj0v5HwDUlnGdm1maUq6UeEXMj4oX0+SNgBtAdGAKMTdnGAgenz0OAmyLzDLCepE2A/YFJETE/IhYAkyhhMsVS+tR3iYi+kl5MlVyQJvgyM8uNpox+kTScrFVda3REjK4n3+bAjsCzwMYFC0+/A2ycPncH3io4bXZKayi9qFKC+hJJ7cjWJ0VSN2BZCeeZmbUZTVn8IgXwLwXxQpLWAe4GTomIDwvHwUdESIqVrGpRpXS/XAXcC2wk6ULgCeCi5qiMmVmlVKn0rTGS1iAL6LdGxD0p+d3UrUL6e15Kn0M2+22tTVNaQ+nF76OxDBFxK3Aa8D9kc6kfHBF3NXaemVlboib8KVpO1iS/HpgREb8uODQeqB3BMhS4ryD9+DQoZVdgYeqmmQgMlNQ1PSAdmNKKarT7RdJmwCLg/sK0iPhXY+eambUVZXyjtD/ZOhQvS5qW0s4CLgbGSToReBM4PB2bAAwGqsli7TCAiJgv6ZfA1JTv/IiY39jFS+lTf5CsP11AR6AnMBPYtoRzzczahHIF9Yh4Ahpszu9bT/4ARjRQ1hhgTFOuX8oiGdsX7qeB8T9uykXMzFq73E/o1ZCIeEHSLs1RGTOzSmmXk/frS+lT/1nBbhXQF3i72WpkZlYBeVl4upSWeueCzzVkfex3N091zMwqY7WYeje9dNQ5In7eQvUxM6uInDTUiy5n1z4iaiT1b8kKmZlVQlUj48/bimIt9Slk/efTJI0H7gI+qT1Y8JaUmVmbl/uWeoGOwPvAPnwxXj0AB3Uzy432OelULxbUN0ojX17hi2Beq1kmojEzq5TVoaXeDliH+t+MclA3s1xZHYY0zo2I81usJmZmFZSTmF40qOfkFs3MGpeTF0qLBvUvTTxjZpZXue9+KWWKRzOzvMh9UDczW53kI6Q7qJuZAfl5UJqXZwNmZqtEUslbCWWNkTRP0isFaf8taY6kaWkbXHDsTEnVkmZK2r8gfVBKq5Z0Rin34aBuZkYWDEvdSnAjMKie9Csiok/aJgBI6g0cSbaa3CDgWknt0oSK1wAHAL2Bo1Leotz9YmZGeR+URsRkSZuXmH0IcEdELAZmSaoG+qVj1RHxBoCkO1LeV4sV5pa6mRlN636RNFzScwXb8BIvM1LSS6l7pmtK6w68VZBndkprKL0oB3UzM5rW/RIRoyNip4JtdAmXuA74GtAHmAtcXv67cPeLmRnQ/AtPR8S7Bdf6PfBA2p0D9CjIumlKo0h6g9xSNzMjG6de6rZS5UubFOx+l2wGXIDxwJGS1pTUE+hFtp7FVKCXpJ6SOpA9TB3f2HXcUjczA9qVsaUu6XZgL2BDSbOBUcBekvqQzXL7T+AHABExXdI4sgegNcCIiFiayhkJTCSbNXdMRExv7NoO6mZmlPflo4g4qp7k64vkvxC4sJ70CcCEplzbQd3MDFBOJgpwUDczIz/TBDiom5kBVW6pm5nlh1vqZmY54vnUzcxypCofMd1B3cwMPPrFzCxXctL74mkCWpt35s7l+8OO45D/HMwhQw7k1pvHAnDdNb9lv30GcPihQzj80CE8PvlvFa6pNZeqKvH0badx95UrTvx3+S8O5b0nLlu+f9Ixe/PCH89iyp2nM+F3I9hsk64r5O/cqSPVfz6fK04/rEXq3dapCX9aM7fUW5l27dtx6i/OYJve2/LJJx9z1OGHsuvu/QE49rgTGDrsxArX0JrbyKP2Yuasd+i8TsflaX236cF6XdZeId+0mbPpf+xlfPrZEv7rsD248OQhHHfGjcuPj/rRYJ54obqlqt3m5aVP3S31VqZbt43Ypve2AHTqtA5bbLEF8959t5GzLC+6b7Qegwb05oY/Pb08rapKXHTKwZx95X0r5J383Ot8+tkSAKa8/E+6b7Te8mM7btODjTbozF+fea1lKp4DVVLJW2vWbEFd0tclnS7pqrSdLmmb5rpeHs2ZM5vXZsxg+2/sAMAdt9/K9777HUadcyYfLlxY4dpZc7js54dw9pXjWbYslqf96Ihv8eDkl3nn3x82eN4JB+/KxCezBXEkcfFPD+bMK+5rML99WXPP0thSmiWoSzoduIPs/qekTcDtxRZPLVxN5Po/lDLnfH4tWvQJP//pSfzi9LNYZ511OPyIo3jgz5O48+772LDbRlx+2cWVrqKV2QEDtmXe/I94ccYXi91ssmEXDvl2H669Y3KD5x05eCf69t6MK256BIAfHL4HE598lTnzPmj2OudJXlrqiojGczW1UOkfwLYRsaROegdgekT0aqyMT5dQ/oq1EUuWLOGkET9k9/57cNzQYV86PmfObE4a8UPu/tMD9Zydb+vvclKlq9Bszh/5HY4+cGdqli5lzQ5r0KVTRxYvqWHx5zUs/jz7X6nHV7oya877bDfklwDs3W8rfn36YQz8/lW8t+BjAG644Hj677gFyyLotNaadFijPaPvepz/+9v7K3Zvze3TF65a5Uj7TPUHJcecXbdcr9VG9uZ6ULoM+CrwZp30TdIxa0BEcN65Z9Nziy1WCOjvvTePbt02AuCRh//Klls2+r1obcy5V9/PuVdngXfAN7fklOP34dCTV/yN9b0nLlse0HfYelOuPvtI/nPkdcsDOsCwc25a/vnY7/Tjm703y3VAL5tWG6abprmC+inAw5Je54uFUzcDtgRGNtM1c2Hai8/zwP330avXVhx+6BAAfnLyz/jLhAeYOfM1BHy1e3fOGXV+ZStqFXfRKUPotHYHbr00+/J/650FfO+nv69wrdqucnarSBoDHATMi4jtUtr6wJ3A5mSLZBweEQuUraN3JTAYWAScEBEvpHOGAuekYi+IiLGNXrs5ul9SZaqAfnyx+vUcYGrtih6NWZ27X6xhee5+sZVXju6XqW8sLDnm7LzFukWvJ+lbwMfATQVB/VJgfkRcnJ4tdo2I0yUNBn5CFtR3Aa6MiF3Sl8BzwE5kqyU9D3wzIhYUu3azjVOPiGXAM81VvplZWZV35aPJkjavkzyEbIk7gLHAY8DpKf2myFrYz0haL61nuhcwKSLmA0iaBAwCbi92bY9TNzOjaW+UFo7US9vwxq/AxhExN31+B9g4fe7OF93UALNTWkPpRfmNUjMzmjb3S0SMBlZ63HVEhKRm6WJ2S93MjBZ5+ejd1K1C+nteSp8D9CjIt2lKayi9KAd1MzOyN3FL3VbSeGBo+jwUuK8g/XhldgUWpm6aicBASV0ldQUGprSi3P1iZkZ5p96VdDvZg84NJc0GRgEXA+MknUj2Ds/hKfsEspEv1WRDGocBRMR8Sb8EpqZ859c+NC3GQd3MjPK+exQRRzVwaN968gYwooFyxgBjmnJtB3UzM/AbpWZmedLaF78olYO6mRn5Wc7OQd3MDAd1M7NccfeLmVmOuKVuZpYjOYnpDupmZkBuorqDupkZ5V0ko5Ic1M3MyE1D3UHdzAzITVR3UDczw0MazcxyJSdd6g7qZmaQm94XB3UzM2BVFr9oVbzykZkZWfdLqVvjZemfkl6WNE3ScyltfUmTJL2e/u6a0iXpKknVkl6S1HdV7sNB3cyMZlmjdO+I6BMRO6X9M4CHI6IX8HDaBzgA6JW24cB1q3IfDupmZtASK08PAcamz2OBgwvSb4rMM8B6tQtUrwwHdTMzsiGNpf4pQQAPSXpe0vCUtnFaUBrgHWDj9Lk78FbBubNT2krxg1IzM5o2pDEF6uEFSaMjYnTB/h4RMUfSRsAkSa8Vnh8RISlWpb4NcVA3MwOqmhDUUwAfXeT4nPT3PEn3Av2AdyVtEhFzU/fKvJR9DtCj4PRNU9pKcfeLmRlQrk51SZ0kda79DAwEXgHGA0NTtqHAfenzeOD4NApmV2BhQTdNk7mlbmZGWd8o3Ri4N417bw/cFhF/kTQVGCfpROBN4PCUfwIwGKgGFgHDVuXiDupmZpTvjdKIeAPYoZ7094F960kPYESZLu+gbmYGnvvFzCxX8jJNgIO6mRme0MvMLFdy0lB3UDczAy+SYWaWL/mI6Q7qZmaQm5juoG5mBlCVk051B3UzM/LzoNRzv5iZ5Yhb6mZm5Kel7qBuZoaHNJqZ5Ypb6mZmOeKgbmaWI+5+MTPLEbfUzcxyJCcx3UHdzAzITVR3UDczIz/TBChbHs9aM0nDI2J0pethrYv/XVh9PE1A2zC80hWwVsn/LuxLHNTNzHLEQd3MLEcc1NsG95taffzvwr7ED0rNzHLELXUzsxxxUDczy22rVVkAAARBSURBVBEH9VZM0hhJ8yS9Uum6WOsiaZCkmZKqJZ1R6fpY6+Gg3rrdCAyqdCWsdZHUDrgGOADoDRwlqXdla2WthYN6KxYRk4H5la6HtTr9gOqIeCMiPgfuAIZUuE7WSjiom7U93YG3CvZnpzQzB3UzszxxUDdre+YAPQr2N01pZg7qZm3QVKCXpJ6SOgBHAuMrXCdrJRzUWzFJtwNPA1tLmi3pxErXySovImqAkcBEYAYwLiKmV7ZW1lp4mgAzsxxxS93MLEcc1M3McsRB3cwsRxzUzcxyxEHdzCxHHNRtBZKWSpom6RVJd0laexXKulHSYenzH4pNOiVpL0m7r8Q1/ilpw1LTGyjjBElXl+O6ZpXmoG51fRoRfSJiO+Bz4IeFByW1X5lCI+L7EfFqkSx7AU0O6ma2Igd1K+ZxYMvUin5c0njgVUntJF0maaqklyT9AECZq9M8338FNqotSNJjknZKnwdJekHS3yU9LGlzsi+Pn6bfEgZI6ibp7nSNqZL6p3M3kPSQpOmS/gCo1JuR1E/S05JelPSUpK0LDvdIdXxd0qiCc46VNCXV6/+laW8Ly+wk6cF0L69IOqKJP2OzslqpVpflX2qRHwD8JSX1BbaLiFmShgMLI2JnSWsCT0p6CNgR2Jpsju+NgVeBMXXK7Qb8HvhWKmv9iJgv6XfAxxHxq5TvNuCKiHhC0mZkb09uA4wCnoiI8yUdCDTlLdvXgAERUSPp28BFwKHpWD9gO2ARMFXSg8AnwBFA/4hYIula4BjgpoIyBwFvR8SBqd7rNqE+ZmXnoG51rSVpWvr8OHA9WbfIlIiYldIHAt+o7S8H1gV6Ad8Cbo+IpcDbkh6pp/xdgcm1ZUVEQ/PFfxvoLS1viHeRtE66xiHp3AclLWjCva0LjJXUCwhgjYJjkyLifQBJ9wB7ADXAN8mCPMBawLw6Zb4MXC7pEuCBiHi8CfUxKzsHdavr04joU5iQAtonhUnATyJiYp18g8tYjypg14j4rJ66rKxfAo9GxHdTl89jBcfqzpcRZPc5NiLObKjAiPiHpL7AYOACSQ9HxPmrUkmzVeE+dVsZE4EfSVoDQNJWkjoBk4EjUp/7JsDe9Zz7DPAtST3Tueun9I+AzgX5HgJ+UrsjqfaLZjJwdEo7AOjahHqvyxdT1J5Q59h+ktaXtBZwMPAk8DBwmKSNausq6T8KT5L0VWBRRNwCXEbWTWVWMW6p28r4A7A58IKypvN7ZIHwXmAfsr70f5HNMLmCiHgv9cnfI6mKrDtjP+B+4I+ShpAF85OAayS9RPbvdDLZw9TzgNslTQeeStdpyEuSlqXP44BLybpfzgEerJN3CnA32dzkt0TEcwAp70OprkuAEcCbBedtD1yWrrME+FGR+pg1O8/SaGaWI+5+MTPLEQd1M7MccVA3M8sRB3UzsxxxUDczyxEHdTOzHHFQNzPLkf8F63BIM197lDIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eD73vA6nMRyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oERkAKwB_7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep Original"
      ],
      "metadata": {
        "id": "CjH6qFA61hVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"hate_train.csv\")\n",
        "df_test = pd.read_csv(\"hate_test.csv\")\n",
        "df_val = pd.read_csv(\"hate_val.csv\")\n",
        "\n",
        "print(df_train.shape)\n",
        "df_train.head()\n",
        "df_train = df_train[df_train['clean_tweet'].notna()]\n",
        "df_test = df_test[df_test['clean_tweet'].notna()]\n",
        "df_val = df_val[df_val['clean_tweet'].notna()]\n",
        "\n",
        "df_train[[\"clean_tweet\", \"label\"]].to_csv(\"train.csv\")\n",
        "df_test[[\"clean_tweet\", \"label\"]].to_csv(\"test.csv\")\n",
        "df_val[[\"clean_tweet\", \"label\"]].to_csv(\"val.csv\")\n",
        "df_train[[\"clean_tweet\", \"label\"]]\n",
        "\n",
        "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "TEXT = Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = LabelField(sequential=False, use_vocab=False, dtype = torch.float,batch_first=True)\n",
        "fields = [(None, None), ('clean_tweet',TEXT),('label', LABEL)]\n",
        "train, val, test = TabularDataset.splits(path = \"/content/\", train='train.csv',validation='val.csv',\n",
        "                                         test='test.csv', format='csv',\n",
        "                                         fields= fields, skip_header=True)\n",
        "\n",
        "print(vars(train.examples[8]))\n",
        "TEXT.build_vocab(train,min_freq=3,vectors = \"glove.twitter.27B.100d\")  \n",
        "# LABEL.build_vocab(train)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "# print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "# print(LABEL.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "#set batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load an iterator\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.clean_tweet),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "LxYmFMzu_6Um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb11b272-1d79-4c9f-fecb-1d08785be003"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22373, 9)\n",
            "{'clean_tweet': ['i', 'm', 'london', 'tomorrow', 'blogtacular', 'full', 'planning', 'prep', 'blogger'], 'label': '0'}\n",
            "Size of TEXT vocabulary: 7559\n",
            "[('love', 1886), ('nt', 1708), ('day', 1617), ('happy', 1163), ('i', 1027), ('m', 839), ('life', 810), ('time', 804), ('like', 737), ('today', 708)]\n",
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7fccd58efe90>>, {'<unk>': 0, '<pad>': 1, 'love': 2, 'nt': 3, 'day': 4, 'happy': 5, 'i': 6, 'm': 7, 'life': 8, 'time': 9, 'like': 10, 'today': 11, 'new': 12, 'thankful': 13, 'positive': 14, 'get': 15, 'bihday': 16, 'people': 17, 'good': 18, 'ca': 19, 'one': 20, 'see': 21, 'fathers': 22, 'do': 23, 'smile': 24, 's': 25, 'go': 26, 'want': 27, 'take': 28, 'work': 29, 'healthy': 30, 'weekend': 31, 'fun': 32, 'friday': 33, 'got': 34, 'summer': 35, 'family': 36, 'make': 37, 'great': 38, 'best': 39, 'us': 40, 'need': 41, 'way': 42, 'beautiful': 43, 'bull': 44, 'first': 45, 'friends': 46, 'days': 47, 'ur': 48, 'really': 49, 'going': 50, 'wait': 51, 'music': 52, 'morning': 53, 'world': 54, 'you': 55, 'back': 56, 'orlando': 57, 'cute': 58, 'know': 59, 'week': 60, 'fathersday': 61, 'sad': 62, 'happiness': 63, 'never': 64, 'tomorrow': 65, 'night': 66, 're': 67, 'model': 68, 'much': 69, 'sunday': 70, 'home': 71, 'blog': 72, 'right': 73, 'think': 74, 'well': 75, 'feel': 76, 'trump': 77, 'affirmation': 78, 've': 79, 'girl': 80, 'come': 81, 'always': 82, 'finally': 83, 'next': 84, 'even': 85, 'live': 86, 'last': 87, 'would': 88, 'ready': 89, 'still': 90, 'look': 91, 'thank': 92, 'follow': 93, 'gold': 94, 'silver': 95, 'selfie': 96, 'iam': 97, 'thanks': 98, 'via': 99, 'amazing': 100, 'altwaystoheal': 101, 'little': 102, 'tonight': 103, 'year': 104, 'things': 105, 'find': 106, 'makes': 107, 'free': 108, 'many': 109, 'dad': 110, 'forex': 111, 'getting': 112, 'pay': 113, 'another': 114, 'blessed': 115, 'ever': 116, '2016': 117, 'watch': 118, 'city': 119, 'man': 120, 'feeling': 121, 'everyone': 122, 'girls': 123, 'looking': 124, 'sta': 125, 'climb': 126, 'news': 127, 'black': 128, 'food': 129, 'peace': 130, 'game': 131, 'hope': 132, 'god': 133, 'show': 134, 'bear': 135, 'enjoy': 136, 'old': 137, 'saturday': 138, 'sun': 139, 'say': 140, 'made': 141, 'that': 142, 'instagood': 143, 'stop': 144, 'help': 145, 'dog': 146, 'might': 147, 'every': 148, 'kids': 149, 'keep': 150, 'whatever': 151, 'friend': 152, 'hate': 153, 'angry': 154, 'may': 155, 'baby': 156, 'polar': 157, 'big': 158, 'na': 159, 'coming': 160, 'excited': 161, 'better': 162, 'funny': 163, 'nice': 164, 'someone': 165, 'believe': 166, 'guys': 167, 'video': 168, 'white': 169, 'beach': 170, 'father': 171, 'grateful': 172, 'healing': 173, 'around': 174, 'motivation': 175, 'school': 176, 'lol': 177, 'nothing': 178, 'awesome': 179, 'two': 180, 'yes': 181, 'cool': 182, 'let': 183, 'found': 184, 'long': 185, 'years': 186, 'true': 187, 'wedding': 188, 'could': 189, 'done': 190, 'hea': 191, 'please': 192, 'proud': 193, 'play': 194, 'soon': 195, 'waiting': 196, 'attack': 197, 'did': 198, 'holiday': 199, 'real': 200, 'without': 201, 'lost': 202, 'team': 203, 'bad': 204, 'direct': 205, 'quote': 206, 'watching': 207, 'america': 208, 'dominate': 209, 'travel': 210, 'change': 211, 'rip': 212, 'yeah': 213, 'thing': 214, 'women': 215, 'photooftheday': 216, 'something': 217, 'away': 218, 'miss': 219, 'monday': 220, 'strong': 221, 'weeks': 222, 'face': 223, 'fashion': 224, 'june': 225, 'left': 226, 'person': 227, 'followme': 228, 'end': 229, 'forward': 230, 'sex': 231, 'does': 232, 'hot': 233, 'making': 234, 'mindset': 235, 'place': 236, 'wish': 237, 'yet': 238, 'check': 239, 'racing': 240, 'twitter': 241, 'boy': 242, 'lovely': 243, 'oh': 244, 'playing': 245, 'bing': 246, 'libtard': 247, 'bong': 248, 'wo': 249, 'lot': 250, 'give': 251, 'hair': 252, 'sleep': 253, 'tbt': 254, 'book': 255, 'gon': 256, 'moment': 257, 'dads': 258, 'everything': 259, 'hard': 260, 'health': 261, 'house': 262, 'living': 263, 'gorilla': 264, 'listen': 265, 'money': 266, 'gay': 267, 'joy': 268, 'pretty': 269, 'shooting': 270, 'stay': 271, 'win': 272, 'inspiration': 273, 'london': 274, 'poetry': 275, 'says': 276, 'couple': 277, 'ill': 278, 'tear': 279, 'tweets': 280, 'young': 281, 'racist': 282, 'sexy': 283, 'buffalo': 284, 'environment': 285, 'leave': 286, 'lets': 287, 'omg': 288, 'already': 289, 'fitness': 290, 'job': 291, 'loved': 292, 'politics': 293, 'success': 294, 'woman': 295, 'body': 296, 'head': 297, 'pa': 298, '10': 299, 'call': 300, 'hours': 301, 'thought': 302, 'perfect': 303, 'wow': 304, 'euro2016': 305, 'full': 306, 'lt3': 307, 'mind': 308, 'mom': 309, 'obama': 310, 'quotes': 311, 'read': 312, 'adapt': 313, 'beauty': 314, 'conference': 315, 'depression': 316, 'guy': 317, 'simulator': 318, 'try': 319, 'what': 320, 'words': 321, 'men': 322, 'dance': 323, 'song': 324, 'use': 325, 'looks': 326, 'truth': 327, 'working': 328, 'dream': 329, 'fuck': 330, 'he': 331, 'porn': 332, 'said': 333, 'shit': 334, 'super': 335, 'business': 336, 'retweet': 337, 'season': 338, 'thursday': 339, 'yay': 340, 'times': 341, 'cat': 342, 'cold': 343, 'month': 344, 'mood': 345, 'together': 346, 'also': 347, 'dead': 348, 'must': 349, 'open': 350, 'others': 351, 'race': 352, 'relax': 353, 'simulation': 354, 'buy': 355, 'children': 356, 'country': 357, 'came': 358, 'flowers': 359, 'gone': 360, 'hear': 361, 'nude': 362, 'since': 363, 'story': 364, 'wonderful': 365, 'enough': 366, 'hey': 367, 'is': 368, 'rest': 369, 'hour': 370, 'movie': 371, 'put': 372, 'run': 373, 'shop': 374, 'smiles': 375, 'd': 376, 'media': 377, 'photo': 378, 'ppl': 379, 'prayfororlando': 380, 'share': 381, 'sunshine': 382, 'anything': 383, 'comes': 384, 'meet': 385, 'wrong': 386, 'coffee': 387, 'wednesday': 388, 'allahsoil': 389, 'gift': 390, 'lives': 391, 'months': 392, 'post': 393, 'sick': 394, 'single': 395, 'thoughts': 396, 'till': 397, 'almost': 398, 'else': 399, 'hu': 400, 'join': 401, 'lifestyle': 402, 'remember': 403, 'tell': 404, 'there': 405, 'trip': 406, 'tuesday': 407, 'birds': 408, 'care': 409, 'celebrate': 410, 'empty': 411, 'lgbt': 412, 'far': 413, 'kind': 414, 'liberal': 415, 'needs': 416, 'seeing': 417, 'tickets': 418, 'trying': 419, 'bed': 420, 'car': 421, 'crazy': 422, 'delete': 423, 'gym': 424, 'hardcore': 425, 'photography': 426, 'suppo': 427, 'sure': 428, 'todays': 429, 'vacation': 430, '1st': 431, 'alone': 432, 'favorite': 433, 'forever': 434, 'gun': 435, 'loving': 436, 'tweet': 437, 'usa': 438, 'useful': 439, 'blue': 440, 'child': 441, 'evening': 442, 'nervous': 443, 'saw': 444, 'shopping': 445, 'tgif': 446, 'visit': 447, 'went': 448, 'yesterday': 449, 'anyone': 450, 'aww': 451, 'daddy': 452, 'future': 453, 'goes': 454, 'order': 455, 'tired': 456, 'uk': 457, 'actually': 458, 'all': 459, 'fans': 460, 'nature': 461, 'social': 462, 'sweet': 463, 'toptags': 464, 'y': 465, 'anxiety': 466, 'complete': 467, 'deletetweets': 468, 'gets': 469, 'power': 470, 'running': 471, 'talk': 472, 'booked': 473, 'daily': 474, 'fact': 475, 'football': 476, 'goodmorning': 477, 'hello': 478, 'high': 479, 'miami': 480, 'places': 481, 'they': 482, 'victims': 483, 'matter': 484, 'sorry': 485, 'ago': 486, 'death': 487, 'early': 488, 'followers': 489, 'latest': 490, 'mean': 491, 'picoftheday': 492, 'smiling': 493, 'son': 494, 'wishing': 495, 'depressed': 496, 'florida': 497, 'green': 498, 'happened': 499, 'lunch': 500, 'side': 501, 'cantwait': 502, 'families': 503, 'history': 504, 'impoant': 505, 'instagram': 506, 'learn': 507, 'police': 508, 'seen': 509, 'become': 510, 'close': 511, 'everyday': 512, 'killed': 513, 'late': 514, 'less': 515, 'pm': 516, 'pray': 517, 'racism': 518, 'reading': 519, 'seems': 520, 'style': 521, '20': 522, 'act': 523, 'boys': 524, 'cry': 525, 'leads': 526, 'lucky': 527, 'ok': 528, 'safe': 529, 'save': 530, 'sjw': 531, 'smh': 532, 'update': 533, 'able': 534, 'arrived': 535, 'damn': 536, 'event': 537, 'freedom': 538, 'guess': 539, 'laugh': 540, 'parents': 541, 'set': 542, 'special': 543, 'stas': 544, 'sunny': 545, 'target': 546, 'vine': 547, 'brexit': 548, 'bring': 549, 'control': 550, 'daughter': 551, 'finished': 552, 'organizations': 553, 'sea': 554, 'sometimes': 555, 'talking': 556, 'used': 557, 'vs': 558, 'welcome': 559, 'whole': 560, 'afternoon': 561, 'bless': 562, 'boyfriend': 563, 'broken': 564, 'chill': 565, 'cultureofdevelopment': 566, 'date': 567, 'education': 568, 'flag': 569, 'fresh': 570, 'goals': 571, 'later': 572, 'mad': 573, 'pic': 574, 'prayers': 575, 'rain': 576, 'reached': 577, 'reason': 578, 'saying': 579, 'staing': 580, 'tv': 581, 'word': 582, 'app': 583, 'bday': 584, 'fan': 585, 'film': 586, 'snapchat': 587, 'anymore': 588, 'bit': 589, 'despite': 590, 'dinner': 591, 'disney': 592, 'gop': 593, 'hill': 594, 'means': 595, 'nyc': 596, 'point': 597, 'youtube': 598, '2017': 599, '30': 600, '50': 601, 'break': 602, 'comments': 603, 'dear': 604, 'll': 605, 'mountains': 606, 'photos': 607, 'president': 608, 'survive': 609, 'though': 610, 'understand': 611, 'walk': 612, 'xxx': 613, 'yo': 614, '100': 615, '12': 616, 'affirmations': 617, 'ahead': 618, 'animals': 619, 'ass': 620, 'brother': 621, 'cause': 622, 'choose': 623, 'conce': 624, 'easy': 625, 'enjoying': 626, 'gbp': 627, 'happening': 628, 'reach': 629, 'shows': 630, 'thankyou': 631, 'top': 632, 'wanted': 633, 'cake': 634, 'can': 635, 'feels': 636, 'finding': 637, 'friendship': 638, 'fucking': 639, 'homes': 640, 'lawofattraction': 641, 'meeting': 642, 'name': 643, 'not': 644, 'park': 645, 'ramadan': 646, 'taking': 647, 'voice': 648, '2nd': 649, '3d': 650, 'bc': 651, 'community': 652, 'crying': 653, 'eat': 654, 'eyes': 655, 'garden': 656, 'glad': 657, 'gt': 658, 'guns': 659, 'have': 660, 'heal': 661, 'ht': 662, 'july': 663, 'kid': 664, 'kill': 665, 'list': 666, 'lover': 667, 'missing': 668, 'officially': 669, 'tears': 670, 'are': 671, 'bought': 672, 'called': 673, 'club': 674, 'color': 675, 'group': 676, 'makeup': 677, 'maybe': 678, 'rock': 679, 'service': 680, 'she': 681, 'simple': 682, 'state': 683, 'ta': 684, 'thinking': 685, 'tragedy': 686, 'training': 687, 'truly': 688, 'yoga': 689, 'england': 690, 'final': 691, 'half': 692, 'happen': 693, 'heard': 694, 'hell': 695, 'leadership': 696, 'mother': 697, 'picture': 698, 'red': 699, 'stuff': 700, 'vote': 701, 'wanna': 702, 'wants': 703, 'worst': 704, 'beer': 705, 'blonde': 706, 'design': 707, 'fantastic': 708, 'forget': 709, 'fridayfeeling': 710, 'human': 711, 'least': 712, 'light': 713, 'memories': 714, 'move': 715, 'ones': 716, 'phone': 717, 'rooster': 718, 'soul': 719, 'task': 720, 'tragic': 721, 'we': 722, 'wife': 723, 'actor': 724, 'drink': 725, 'expanse': 726, 'giving': 727, 'gorgeous': 728, 'htt': 729, 'instamood': 730, 'lighttherapy': 731, 'lonely': 732, 'moments': 733, 'nbafinals': 734, 'orlandoshooting': 735, 'pain': 736, 'room': 737, 'scared': 738, 'send': 739, 'series': 740, 'stomping': 741, 'vast': 742, 'vicinity': 743, 'view': 744, 'violence': 745, 'who': 746, 'wine': 747, 'account': 748, 'agree': 749, 'anniversary': 750, 'dancing': 751, 'die': 752, 'disappointed': 753, 'dj': 754, 'dreams': 755, 'flight': 756, 'grow': 757, 'instalike': 758, 'internet': 759, 'line': 760, 'lose': 761, 'lots': 762, 'mine': 763, 'ride': 764, 'small': 765, 'source': 766, 'suppoers': 767, 'takes': 768, 'tech': 769, 'using': 770, 'wake': 771, 'water': 772, 'wet': 773, 'wishes': 774, 'york': 775, '15': 776, 'along': 777, 'americans': 778, 'ask': 779, 'breakfast': 780, 'different': 781, 'festival': 782, 'haha': 783, 'hit': 784, 'huge': 785, 'loves': 786, 'mass': 787, 'minutes': 788, 'national': 789, 'naughty': 790, 'pathetic': 791, 'pink': 792, 'rather': 793, 'respect': 794, 'slut': 795, 'street': 796, 'teen': 797, 'american': 798, 'behind': 799, 'chase': 800, 'died': 801, 'dogs': 802, 'hatred': 803, 'leaving': 804, 'met': 805, 'present': 806, 'self': 807, 'staff': 808, 'three': 809, 'united': 810, 'videos': 811, 'woh': 812, 'wonder': 813, 'blur': 814, 'congrats': 815, 'course': 816, 'customer': 817, 'facebook': 818, 'fear': 819, 'fight': 820, 'fit': 821, 'india': 822, 'joke': 823, 'king': 824, 'la': 825, 'missed': 826, 'moving': 827, 'muslim': 828, 'naked': 829, 'nasty': 830, 'needed': 831, 'pool': 832, 'poor': 833, 'problem': 834, 'queen': 835, 'shame': 836, 'sky': 837, 'staed': 838, 'star': 839, 'udtapunjab': 840, 'walking': 841, 'weather': 842, 'wtf': 843, 'busy': 844, 'college': 845, 'dark': 846, 'especially': 847, 'exciting': 848, 'fall': 849, 'hands': 850, 'heres': 851, 'horny': 852, 'husband': 853, 'inside': 854, 'jobs': 855, 'kinky': 856, 'local': 857, 'past': 858, 'pop': 859, 'probably': 860, 'puppy': 861, 'pussy': 862, 'sho': 863, 'shot': 864, 'shy': 865, 'sister': 866, 'students': 867, 'sunset': 868, 'absolutely': 869, 'babies': 870, 'calm': 871, 'congratulations': 872, 'e32016': 873, 'episode': 874, 'everybody': 875, 'experience': 876, 'fire': 877, 'flower': 878, 'following': 879, 'france': 880, 'gave': 881, 'happier': 882, 'hi': 883, 'hillary': 884, 'leakage': 885, 'marriage': 886, 'office': 887, 'online': 888, 'opening': 889, 'page': 890, 'pizza': 891, 'speak': 892, 'stupid': 893, 'told': 894, 'took': 895, 'vibes': 896, 'ways': 897, 'ai': 898, 'aist': 899, 'due': 900, 'favourite': 901, 'feelings': 902, 'folks': 903, 'instadaily': 904, 'listening': 905, 'loss': 906, 'mr': 907, 'official': 908, 'spos': 909, 'successful': 910, 'surprise': 911, 'trust': 912, 'vegas': 913, 'adventure': 914, 'album': 915, 'alive': 916, 'apple': 917, 'christmas': 918, 'content': 919, 'countdown': 920, 'drinks': 921, 'ending': 922, 'energy': 923, 'faith': 924, 'female': 925, 'games': 926, 'given': 927, 'inspirational': 928, 'literally': 929, 'luck': 930, 'nails': 931, 'newyork': 932, 'oitnb': 933, 'reality': 934, 'sale': 935, 'seriously': 936, 'spend': 937, 'states': 938, 'towards': 939, 'war': 940, 'writing': 941, 'yummy': 942, 'bike': 943, 'camp': 944, 'cavs': 945, 'celebrating': 946, 'choice': 947, 'dress': 948, 'fly': 949, 'foodporn': 950, 'germany': 951, 'goodvibes': 952, 'graduation': 953, 'heabroken': 954, 'hungry': 955, 'ice': 956, 'instead': 957, 'launch': 958, 'like4like': 959, 'lovelife': 960, 'melancholy': 961, 'nobody': 962, 'political': 963, 'pride': 964, 'question': 965, 'realize': 966, 'received': 967, 'ripchristina': 968, 'road': 969, 'sense': 970, 'stand': 971, 'test': 972, 'tips': 973, 'wisdom': 974, 'workout': 975, 'xx': 976, 'beat': 977, 'behappy': 978, 'blm': 979, 'blogger': 980, 'campaign': 981, 'cats': 982, 'church': 983, 'decided': 984, 'diet': 985, 'entire': 986, 'gratitude': 987, 'hand': 988, 'ladies': 989, 'lady': 990, 'legend': 991, 'longer': 992, 'married': 993, 'oil': 994, 'okay': 995, 'previous': 996, 'release': 997, 'sign': 998, 'spain': 999, 'step': 1000, 'turn': 1001, 'weak': 1002, 'website': 1003, 'wild': 1004, 'within': 1005, 'august': 1006, 'books': 1007, 'born': 1008, 'card': 1009, 'celebration': 1010, 'class': 1011, 'confused': 1012, 'e3': 1013, 'either': 1014, 'emotional': 1015, 'emotions': 1016, 'finger': 1017, 'hispanic': 1018, 'horrible': 1019, 'journey': 1020, 'losing': 1021, 'lyrics': 1022, 'nation': 1023, 'passed': 1024, 'punjab': 1025, 'relaxing': 1026, 'sing': 1027, 'space': 1028, 'spent': 1029, 'stage': 1030, 'stories': 1031, 'town': 1032, 'trending': 1033, 'wakeup': 1034, 'youth': 1035, 'add': 1036, 'asian': 1037, 'bitch': 1038, 'bjp': 1039, 'calling': 1040, 'cheers': 1041, 'clean': 1042, 'deserve': 1043, 'dory': 1044, 'fake': 1045, 'gif': 1046, 'girlfriend': 1047, 'heaven': 1048, 'inshot': 1049, 'islam': 1050, 'nofilter': 1051, 'orange': 1052, 'paid': 1053, 'paris': 1054, 'praying': 1055, 'pulse': 1056, 'review': 1057, 'snapshot': 1058, 'tampa': 1059, 'university': 1060, 'upset': 1061, 'was': 1062, 'yrs': 1063, 'age': 1064, 'analytics': 1065, 'animal': 1066, 'anime': 1067, 'australia': 1068, 'begins': 1069, 'chance': 1070, 'changes': 1071, 'deep': 1072, 'donald': 1073, 'drawing': 1074, 'ff': 1075, 'finish': 1076, 'happens': 1077, 'heading': 1078, 'issue': 1079, 'melancholymusic': 1080, 'minute': 1081, 'ness': 1082, 'positivity': 1083, 'reasons': 1084, 'st': 1085, 'tea': 1086, 'temple': 1087, 'terrorism': 1088, 'tho': 1089, 'til': 1090, 'train': 1091, 'tried': 1092, 'vegan': 1093, 'web': 1094, '25': 1095, 'action': 1096, 'allow': 1097, 'available': 1098, 'bar': 1099, 'benefits': 1100, 'canada': 1101, 'case': 1102, 'christina': 1103, 'company': 1104, 'cover': 1105, 'creative': 1106, 'eah': 1107, 'essentialoils': 1108, 'fail': 1109, 'fast': 1110, 'hero': 1111, 'igers': 1112, 'japan': 1113, 'kitty': 1114, 'knew': 1115, 'link': 1116, 'lord': 1117, 'low': 1118, 'nba': 1119, 'nearly': 1120, 'number': 1121, 'offer': 1122, 'often': 1123, 'passion': 1124, 'pics': 1125, 'plan': 1126, 'public': 1127, 'relationship': 1128, 'secret': 1129, 'shoot': 1130, 'singing': 1131, 'songs': 1132, 'south': 1133, 'stuck': 1134, 'worse': 1135, 'anger': 1136, 'blacks': 1137, 'box': 1138, 'brand': 1139, 'bride': 1140, 'count': 1141, 'create': 1142, 'culture': 1143, 'cut': 1144, 'definitely': 1145, 'disgusting': 1146, 'fo': 1147, 'keeping': 1148, 'key': 1149, 'knows': 1150, 'likes': 1151, 'pig': 1152, 'planning': 1153, 'played': 1154, 'powerful': 1155, 'quoteoftheday': 1156, 'republican': 1157, 'ripchristinagrimmie': 1158, 'round': 1159, 'second': 1160, 'spring': 1161, 'student': 1162, 'studio': 1163, 'supposed': 1164, 'swimming': 1165, 'tattoo': 1166, 'treat': 1167, '11': 1168, '14': 1169, 'advice': 1170, 'altright': 1171, 'bigot': 1172, 'bliss': 1173, 'breaking': 1174, 'bro': 1175, 'calgary': 1176, 'clients': 1177, 'current': 1178, 'customers': 1179, 'deal': 1180, 'delicious': 1181, 'english': 1182, 'euro': 1183, 'events': 1184, 'exactly': 1185, 'expect': 1186, 'goodnight': 1187, 'google': 1188, 'growing': 1189, 'helping': 1190, 'hotel': 1191, 'idea': 1192, 'idiot': 1193, 'idwp': 1194, 'ignorance': 1195, 'imagine': 1196, 'ireland': 1197, 'lake': 1198, 'law': 1199, 'muslims': 1200, 'nights': 1201, 'outside': 1202, 'pictures': 1203, 'pls': 1204, 'session': 1205, 'shoes': 1206, 'stream': 1207, 'stress': 1208, 'texas': 1209, 'theatre': 1210, 'totally': 1211, 'turned': 1212, 'worry': 1213, 'yum': 1214, '16': 1215, 'accept': 1216, 'adult': 1217, 'answer': 1218, 'attention': 1219, 'awards': 1220, 'bag': 1221, 'ball': 1222, 'boricua': 1223, 'boss': 1224, 'brilliant': 1225, 'brokers': 1226, 'california': 1227, 'cleveland': 1228, 'clothes': 1229, 'cnn': 1230, 'colombia': 1231, 'comedy': 1232, 'double': 1233, 'dude': 1234, 'edm': 1235, 'exercise': 1236, 'fat': 1237, 'fed': 1238, 'front': 1239, 'gamer': 1240, 'grace': 1241, 'growth': 1242, 'happiest': 1243, 'heabreaking': 1244, 'holidays': 1245, 'hopefully': 1246, 'humor': 1247, 'inlove': 1248, 'jesus': 1249, 'lack': 1250, 'learning': 1251, 'lies': 1252, 'message': 1253, 'ocean': 1254, 'ootd': 1255, 'peaceful': 1256, 'pick': 1257, 'piece': 1258, 'plans': 1259, 'princess': 1260, 'product': 1261, 'radio': 1262, 'response': 1263, 'scary': 1264, 'schools': 1265, 'simply': 1266, 'society': 1267, 'store': 1268, 'straight': 1269, '24': 1270, 'announce': 1271, 'appreciate': 1272, 'area': 1273, 'bbc': 1274, 'bestfriend': 1275, 'blame': 1276, 'bus': 1277, 'cards': 1278, 'chicago': 1279, 'childhood': 1280, 'closer': 1281, 'cock': 1282, 'de': 1283, 'diy': 1284, 'door': 1285, 'download': 1286, 'f4f': 1287, 'fab': 1288, 'faces': 1289, 'findingdory': 1290, 'forgot': 1291, 'generation': 1292, 'gives': 1293, 'greatest': 1294, 'hold': 1295, 'humanity': 1296, 'lead': 1297, 'logins': 1298, 'male': 1299, 'match': 1300, 'matters': 1301, 'member': 1302, 'michael': 1303, 'moon': 1304, 'movies': 1305, 'mum': 1306, 'natural': 1307, 'near': 1308, 'netflix': 1309, 'notice': 1310, 'opinion': 1311, 'packing': 1312, 'pet': 1313, 'possible': 1314, 'project': 1315, 'rainbow': 1316, 'russia': 1317, 'sadly': 1318, 'sent': 1319, 'shine': 1320, 'skin': 1321, 'squad': 1322, 'staup': 1323, 'system': 1324, 'teambts': 1325, 'ticket': 1326, 'tour': 1327, 'watched': 1328, 'weird': 1329, 'winner': 1330, 'woke': 1331, 'works': 1332, 'xoxo': 1333, 'abt': 1334, 'acting': 1335, 'air': 1336, 'apparently': 1337, 'astrologer': 1338, 'beginning': 1339, 'bet': 1340, 'beyond': 1341, 'biggest': 1342, 'bird': 1343, 'blackandwhite': 1344, 'bouncingbaby': 1345, 'broke': 1346, 'clear': 1347, 'coach': 1348, 'cooking': 1349, 'counting': 1350, 'dies': 1351, 'drinking': 1352, 'euros': 1353, 'everywhere': 1354, 'ex': 1355, 'factory': 1356, 'feet': 1357, 'felt': 1358, 'filled': 1359, 'fl': 1360, 'focus': 1361, 'foodie': 1362, 'gaming': 1363, 'goodtimes': 1364, 'handsome': 1365, 'holding': 1366, 'hoping': 1367, 'humpday': 1368, 'ibiza': 1369, 'inspired': 1370, 'interview': 1371, 'keeps': 1372, 'lebron': 1373, 'level': 1374, 'loa': 1375, 'loser': 1376, 'loveit': 1377, 'maga': 1378, 'magic': 1379, 'nightclub': 1380, 'performance': 1381, 'positivevibes': 1382, 'religion': 1383, 'sharing': 1384, 'sikh': 1385, 'study': 1386, 'terrorist': 1387, 'therapy': 1388, 'tonights': 1389, 'usgtgtgt': 1390, 'warm': 1391, 'warriors': 1392, 'waste': 1393, 'wear': 1394, 'wearing': 1395, 'airpo': 1396, 'award': 1397, 'bestfriends': 1398, 'bestie': 1399, 'bff': 1400, 'bitches': 1401, 'board': 1402, 'bout': 1403, 'bye': 1404, 'click': 1405, 'clinton': 1406, 'comment': 1407, 'condemns': 1408, 'congress': 1409, 'cream': 1410, 'decision': 1411, 'details': 1412, 'evil': 1413, 'fabulous': 1414, 'fitfam': 1415, 'flagday2016': 1416, 'flying': 1417, 'goal': 1418, 'grimmie': 1419, 'heas': 1420, 'injured': 1421, 'islamic': 1422, 'killing': 1423, 'kiss': 1424, 'knowing': 1425, 'leaders': 1426, 'leaves': 1427, 'lil': 1428, 'loveislove': 1429, 'major': 1430, 'neverump': 1431, 'niggas': 1432, 'noh': 1433, 'none': 1434, 'nursery': 1435, 'ordered': 1436, 'papa': 1437, 'plus': 1438, 'press': 1439, 'price': 1440, 'quality': 1441, 'reaction': 1442, 'realized': 1443, 'released': 1444, 'remain': 1445, 'repost': 1446, 'result': 1447, 'robe': 1448, 'role': 1449, 'science': 1450, 'screen': 1451, 'shift': 1452, 'shootings': 1453, 'shout': 1454, 'singer': 1455, 'sound': 1456, 'stars': 1457, 'strength': 1458, 'summeime': 1459, 'taken': 1460, 'teacher': 1461, 'telling': 1462, 'toddler': 1463, 'total': 1464, 'trumps': 1465, 'ugh': 1466, 'unfounately': 1467, 'vandalised': 1468, 'weight': 1469, 'weightloss': 1470, 'wellness': 1471, 'whites': 1472, 'worked': 1473, 'wso': 1474, 'ya': 1475, 'accepted': 1476, 'adorable': 1477, 'afraid': 1478, 'amwriting': 1479, 'anne': 1480, 'apps': 1481, 'arrive': 1482, 'band': 1483, 'begin': 1484, 'boom': 1485, 'calls': 1486, 'catch': 1487, 'chocolate': 1488, 'collection': 1489, 'cuz': 1490, 'da': 1491, 'dangerous': 1492, 'david': 1493, 'depressing': 1494, 'difficult': 1495, 'driving': 1496, 'em': 1497, 'essential': 1498, 'europe': 1499, 'eye': 1500, 'feliz': 1501, 'fighting': 1502, 'fine': 1503, 'gamedev': 1504, 'golf': 1505, 'ha': 1506, 'hearing': 1507, 'hrs': 1508, 'humans': 1509, 'hype': 1510, 'ignored': 1511, 'john': 1512, 'l4l': 1513, 'latepost': 1514, 'le': 1515, 'liar': 1516, 'lifeisgood': 1517, 'lmao': 1518, 'meditation': 1519, 'mins': 1520, 'misogynist': 1521, 'misogyny': 1522, 'organic': 1523, 'painting': 1524, 'peoples': 1525, 'piano': 1526, 'politicians': 1527, 'profile': 1528, 'questions': 1529, 'rhymes': 1530, 'rich': 1531, 'san': 1532, 'search': 1533, 'shi': 1534, 'sma': 1535, 'speech': 1536, 'spo': 1537, 'storm': 1538, 'suppoing': 1539, 'talented': 1540, 'tool': 1541, 'tough': 1542, 'track': 1543, 'updates': 1544, 'writer': 1545, '70': 1546, 'across': 1547, 'amen': 1548, 'attitude': 1549, 'bae': 1550, 'bank': 1551, 'bigger': 1552, 'blessings': 1553, 'boston': 1554, 'bs': 1555, 'build': 1556, 'chat': 1557, 'cheer': 1558, 'classic': 1559, 'completely': 1560, 'cow': 1561, 'creating': 1562, 'crime': 1563, 'cup': 1564, 'eating': 1565, 'email': 1566, 'entrepreneur': 1567, 'eve': 1568, 'familytime': 1569, 'fave': 1570, 'feminism': 1571, 'fix': 1572, 'funeral': 1573, 'gig': 1574, 'glass': 1575, 'goodbye': 1576, 'goodday': 1577, 'greece': 1578, 'hits': 1579, 'involved': 1580, 'iphone': 1581, 'isis': 1582, 'island': 1583, 'justice': 1584, 'kick': 1585, 'kinda': 1586, 'kitchen': 1587, 'lie': 1588, 'likeforlike': 1589, 'luxury': 1590, 'lying': 1591, 'main': 1592, 'market': 1593, 'marketing': 1594, 'materia': 1595, 'material': 1596, 'members': 1597, 'memes': 1598, 'mindfulness': 1599, 'mylove': 1600, 'plz': 1601, 'podcast': 1602, 'posted': 1603, 'problems': 1604, 'progress': 1605, 'ps4': 1606, 'pure': 1607, 'purpose': 1608, 'quick': 1609, 'quiet': 1610, 'rally': 1611, 'rape': 1612, 'record': 1613, 'repo': 1614, 'resist': 1615, 'restaurant': 1616, 'rules': 1617, 'sat': 1618, 'september': 1619, 'signed': 1620, 'silly': 1621, 'sitting': 1622, 'snow': 1623, 'station': 1624, 'surprised': 1625, 'terror': 1626, 'text': 1627, 'tiny': 1628, 'traveling': 1629, 'turning': 1630, 'twice': 1631, 'unhappy': 1632, 'wales': 1633, 'winter': 1634, '13': 1635, '1gabba': 1636, 'anton': 1637, 'asked': 1638, 'babe': 1639, 'becoming': 1640, 'bf': 1641, 'bihdaygirl': 1642, 'boat': 1643, 'bored': 1644, 'brain': 1645, 'bright': 1646, 'building': 1647, 'cancer': 1648, 'clearly': 1649, 'colors': 1650, 'cycling': 1651, 'dc': 1652, 'dr': 1653, 'draw': 1654, 'driver': 1655, 'dumb': 1656, 'dying': 1657, 'election': 1658, 'electronic': 1659, 'emiratis': 1660, 'emo': 1661, 'ended': 1662, 'epic': 1663, 'excellent': 1664, 'excitement': 1665, 'expected': 1666, 'fair': 1667, 'fav': 1668, 'fish': 1669, 'foods': 1670, 'fox': 1671, 'friyay': 1672, 'frustrated': 1673, 'genocide': 1674, 'gods': 1675, 'grandpa': 1676, 'hahaha': 1677, 'innocent': 1678, 'judge': 1679, 'kkk': 1680, 'known': 1681, 'laughing': 1682, 'madrid': 1683, 'manchester': 1684, 'mexico': 1685, 'miles': 1686, 'nationalbestfriendsday': 1687, 'nigga': 1688, 'oils': 1689, 'paner': 1690, 'pc': 1691, 'poland': 1692, 'practice': 1693, 'process': 1694, 'registered': 1695, 'relationships': 1696, 'remedies': 1697, 'return': 1698, 'returns': 1699, 'rights': 1700, 'ring': 1701, 'roll': 1702, 'romance': 1703, 'security': 1704, 'sell': 1705, 'senseless': 1706, 'shower': 1707, 'sisters': 1708, 'size': 1709, 'somebody': 1710, 'someones': 1711, 'stronger': 1712, 'stunning': 1713, 'suicide': 1714, 'sundayfunday': 1715, 'swim': 1716, 'table': 1717, 'teams': 1718, 'terrible': 1719, 'thousands': 1720, 'throwback': 1721, 'tuned': 1722, 'typical': 1723, 'unbelievable': 1724, 'vk': 1725, 'wall': 1726, 'weekends': 1727, 'west': 1728, 'winning': 1729, 'workshop': 1730, 'yellow': 1731, '11th': 1732, '21': 1733, '3rd': 1734, '90': 1735, 'africa': 1736, 'african': 1737, 'am': 1738, 'amazon': 1739, 'annoyed': 1740, 'awake': 1741, 'based': 1742, 'basketball': 1743, 'bbq': 1744, 'beard': 1745, 'berlin': 1746, 'bestseller': 1747, 'bigotry': 1748, 'blacklivesmatter': 1749, 'bloggers': 1750, 'bloody': 1751, 'bts': 1752, 'buddy': 1753, 'changed': 1754, 'channel': 1755, 'cheese': 1756, 'christian': 1757, 'christinagrimmie': 1758, 'client': 1759, 'clouds': 1760, 'coldplay': 1761, 'copy': 1762, 'cou': 1763, 'cousins': 1764, 'crew': 1765, 'decide': 1766, 'delivery': 1767, 'drive': 1768, 'easily': 1769, 'ends': 1770, 'eu': 1771, 'exams': 1772, 'fasting': 1773, 'five': 1774, 'fruit': 1775, 'fucked': 1776, 'fuher': 1777, 'glasses': 1778, 'government': 1779, 'govt': 1780, 'holy': 1781, 'hug': 1782, 'hugs': 1783, 'indeed': 1784, 'indiedev': 1785, 'industry': 1786, 'insane': 1787, 'international': 1788, 'issues': 1789, 'italy': 1790, 'jersey': 1791, 'jo': 1792, 'jokes': 1793, 'kindness': 1794, 'laughter': 1795, 'letting': 1796, 'looked': 1797, 'loveyou': 1798, 'luv': 1799, 'malevote': 1800, 'mall': 1801, 'mark': 1802, 'massage': 1803, 'massive': 1804, 'materi': 1805, 'mental': 1806, 'mexican': 1807, 'mobile': 1808, 'motivated': 1809, 'negative': 1810, 'nerd': 1811, 'nominee': 1812, 'note': 1813, 'packed': 1814, 'paint': 1815, 'paladino': 1816, 'plants': 1817, 'poem': 1818, 'posts': 1819, 'quite': 1820, 'recent': 1821, 'reminder': 1822, 'republicans': 1823, 'sales': 1824, 'scotland': 1825, 'setting': 1826, 'shooter': 1827, 'should': 1828, 'signs': 1829, 'silence': 1830, 'sir': 1831, 'slow': 1832, 'sold': 1833, 'sons': 1834, 'spending': 1835, 'starbucks': 1836, 'sushi': 1837, 'tells': 1838, 'ten': 1839, 'thailand': 1840, 'thinks': 1841, 'touch': 1842, 'travelling': 1843, 'tune': 1844, 'turns': 1845, 'ugly': 1846, 'upcoming': 1847, 'voters': 1848, 'vsco': 1849, 'weddings': 1850, 'wednesdaywisdom': 1851, 'yr': 1852, 'zen': 1853, '17': 1854, '22': 1855, '40': 1856, 'aap': 1857, 'acts': 1858, 'ad': 1859, 'aicle': 1860, 'allowed': 1861, 'antiracism': 1862, 'anybody': 1863, 'anywhere': 1864, 'assault': 1865, 'author': 1866, 'balance': 1867, 'bitter': 1868, 'bogota': 1869, 'boring': 1870, 'boycott': 1871, 'brings': 1872, 'brothers': 1873, 'brought': 1874, 'brown': 1875, 'candidate': 1876, 'career': 1877, 'cares': 1878, 'ceain': 1879, 'centre': 1880, 'childrens': 1881, 'christians': 1882, 'classes': 1883, 'code': 1884, 'competition': 1885, 'confident': 1886, 'continue': 1887, 'drama': 1888, 'drunk': 1889, 'easier': 1890, 'eid': 1891, 'enteainment': 1892, 'facts': 1893, 'fb': 1894, 'feelgood': 1895, 'feminismiscancer': 1896, 'feminismisterrorism': 1897, 'feminismmuktbharat': 1898, 'filter': 1899, 'followed': 1900, 'force': 1901, 'four': 1902, 'fraud': 1903, 'gb': 1904, 'gifts': 1905, 'global': 1906, 'grand': 1907, 'hamilton': 1908, 'hang': 1909, 'heat': 1910, 'hospital': 1911, 'hubby': 1912, 'hump': 1913, 'idiots': 1914, 'ig': 1915, 'ignorant': 1916, 'illustration': 1917, 'index': 1918, 'info': 1919, 'kingdom': 1920, 'knowledge': 1921, 'liberals': 1922, 'lit': 1923, 'location': 1924, 'meant': 1925, 'middle': 1926, 'million': 1927, 'mirror': 1928, 'murder': 1929, 'network': 1930, 'notmypresident': 1931, 'nowplaying': 1932, 'path': 1933, 'period': 1934, 'photoshoot': 1935, 'preparing': 1936, 'productive': 1937, 'products': 1938, 'proof': 1939, 'putting': 1940, 'radical': 1941, 'reply': 1942, 'research': 1943, 'resources': 1944, 'revolution': 1945, 'risk': 1946, 'rose': 1947, 'row': 1948, 'salad': 1949, 'season4': 1950, 'sending': 1951, 'serious': 1952, 'sit': 1953, 'sleepy': 1954, 'snap': 1955, 'socialmedia': 1956, 'sounds': 1957, 'speaking': 1958, 'spread': 1959, 'statement': 1960, 'tcot': 1961, 'third': 1962, 'thx': 1963, 'value': 1964, 'village': 1965, 'vintage': 1966, 'wa': 1967, 'wins': 1968, 'woohoo': 1969, 'wrote': 1970, 'xenophobia': 1971, 'yolo': 1972, '18': 1973, '200': 1974, '2016in4words': 1975, 'affected': 1976, 'agenda': 1977, 'alex': 1978, 'ali': 1979, 'anal': 1980, 'android': 1981, 'angel': 1982, 'animation': 1983, 'announces': 1984, 'anyway': 1985, 'asking': 1986, 'badly': 1987, 'bags': 1988, 'basis': 1989, 'bath': 1990, 'batman': 1991, 'brighton': 1992, 'brunch': 1993, 'buzzing': 1994, 'carl': 1995, 'cars': 1996, 'caught': 1997, 'charity': 1998, 'cinema': 1999, 'clothing': 2000, 'co': 2001, 'colorful': 2002, 'continues': 2003, 'countries': 2004, 'crap': 2005, 'currently': 2006, 'daughters': 2007, 'delayed': 2008, 'delivered': 2009, 'desperate': 2010, 'difference': 2011, 'downtown': 2012, 'dublin': 2013, 'edinburgh': 2014, 'effo': 2015, 'en': 2016, 'etsy': 2017, 'european': 2018, 'everyones': 2019, 'expectations': 2020, 'fam': 2021, 'fantasy': 2022, 'fell': 2023, 'fool': 2024, 'funday': 2025, 'gel': 2026, 'gtgt': 2027, 'handmade': 2028, 'held': 2029, 'hide': 2030, 'highest': 2031, 'hollywood': 2032, 'homophobic': 2033, 'hopeful': 2034, 'horror': 2035, 'hrc': 2036, 'humble': 2037, 'ideas': 2038, 'impression': 2039, 'incredible': 2040, 'incredibly': 2041, 'inspire': 2042, 'instapic': 2043, 'jim': 2044, 'jobsearch': 2045, 'keshi': 2046, 'ki': 2047, 'kicks': 2048, 'kitten': 2049, 'laws': 2050, 'league': 2051, 'lifecoach': 2052, 'loveyourself': 2053, 'lt': 2054, 'majority': 2055, 'melbourne': 2056, 'memory': 2057, 'michelle': 2058, 'military': 2059, 'milk': 2060, 'normal': 2061, 'ny': 2062, 'officers': 2063, 'oppounity': 2064, 'option': 2065, 'original': 2066, 'peeps': 2067, 'personalised': 2068, 'plane': 2069, 'players': 2070, 'plays': 2071, 'poems': 2072, 'poster': 2073, 'potus': 2074, 'precious': 2075, 'program': 2076, 'pumped': 2077, 'raise': 2078, 'random': 2079, 'rat': 2080, 'relaxed': 2081, 'retail': 2082, 'romantic': 2083, 'route': 2084, 'sa': 2085, 'seashepherd': 2086, 'selfies': 2087, 'serve': 2088, 'shepherd': 2089, 'shocked': 2090, 'site': 2091, 'situation': 2092, 'six': 2093, 'smoke': 2094, 'soccer': 2095, 'sydney': 2096, 'tag': 2097, 'taste': 2098, 'teens': 2099, 'teeth': 2100, 'term': 2101, 'thursdaythoughts': 2102, 'title': 2103, 'tony': 2104, 'tshi': 2105, 'twitch': 2106, 'ty': 2107, 'ukraine': 2108, 'version': 2109, 'views': 2110, 'vscocam': 2111, 'waking': 2112, 'window': 2113, 'workers': 2114, 'worldwide': 2115, 'write': 2116, 'yelchin': 2117, '9th': 2118, 'abuse': 2119, 'actual': 2120, 'ah': 2121, 'allah': 2122, 'alligator': 2123, 'argument': 2124, 'ashamed': 2125, 'ate': 2126, 'attacks': 2127, 'attempt': 2128, 'attending': 2129, 'barely': 2130, 'basic': 2131, 'bikini': 2132, 'bill': 2133, 'bio': 2134, 'blocked': 2135, 'breaks': 2136, 'btw': 2137, 'bunch': 2138, 'buying': 2139, 'caturday': 2140, 'censorship': 2141, 'challenge': 2142, 'character': 2143, 'charge': 2144, 'choices': 2145, 'christ': 2146, 'cocktails': 2147, 'common': 2148, 'computer': 2149, 'condolences': 2150, 'confidence': 2151, 'curry': 2152, 'daddys': 2153, 'data': 2154, 'debate': 2155, 'delighted': 2156, 'destiny': 2157, 'detoxdiet': 2158, 'dis': 2159, 'dm': 2160, 'doplants': 2161, 'doubt': 2162, 'east': 2163, 'ego': 2164, 'emails': 2165, 'employees': 2166, 'eng': 2167, 'engaged': 2168, 'equality': 2169, 'escape': 2170, 'except': 2171, 'excit': 2172, 'fascism': 2173, 'figure': 2174, 'finals': 2175, 'fired': 2176, 'focused': 2177, 'gains': 2178, 'geek': 2179, 'gin': 2180, 'guitar': 2181, 'hanging': 2182, 'has': 2183, 'hat': 2184, 'hateful': 2185, 'hathaway': 2186, 'hawaii': 2187, 'herbal': 2188, 'horrific': 2189, 'hus': 2190, 'interest': 2191, 'interesting': 2192, 'israel': 2193, 'it': 2194, 'japanese': 2195, 'jealous': 2196, 'joe': 2197, 'joytrain': 2198, 'jump': 2199, 'kicking': 2200, 'kisses': 2201, 'korea': 2202, 'land': 2203, 'laptop': 2204, 'lasvegas': 2205, 'leader': 2206, 'lip': 2207, 'log': 2208, 'lovemylife': 2209, 'magnettherapy': 2210, 'mail': 2211, 'massacre': 2212, 'masterkeyexperience': 2213, 'mention': 2214, 'mode': 2215, 'mommy': 2216, 'moms': 2217, 'mouth': 2218, 'murdered': 2219, 'nigeria': 2220, 'obsessed': 2221, 'obviously': 2222, 'orlandonightclubshooting': 2223, 'paper': 2224, 'parenting': 2225, 'pass': 2226, 'pets': 2227, 'photographer': 2228, 'possibility': 2229, 'praise': 2230, 'preordered': 2231, 'presents': 2232, 'presidential': 2233, 'priceless': 2234, 'protect': 2235, 'pub': 2236, 'purchase': 2237, 'rainy': 2238, 'recipe': 2239, 'reso': 2240, 'responsible': 2241, 'results': 2242, 'reunited': 2243, 'rid': 2244, 'rn': 2245, 'roadtrip': 2246, 'rugby': 2247, 'sadness': 2248, 'seem': 2249, 'sets': 2250, 'shocking': 2251, 'showing': 2252, 'shut': 2253, 'singapore': 2254, 'sis': 2255, 'smell': 2256, 'sohappy': 2257, 'solve': 2258, 'somewhere': 2259, 'soundcloud': 2260, 'speakers': 2261, 'spin': 2262, 'spirit': 2263, 'staying': 2264, 'stephen': 2265, 'stoked': 2266, 'sundaymorning': 2267, 'sunnyday': 2268, 'systemic': 2269, 'talent': 2270, 'tan': 2271, 'terrorists': 2272, 'theme': 2273, 'thru': 2274, 'toronto': 2275, 'tupac': 2276, 'turkey': 2277, 'victory': 2278, 'visiting': 2279, 'waited': 2280, 'webcam': 2281, 'whe': 2282, 'whenever': 2283, 'whoop': 2284, '19': 2285, '28': 2286, '49': 2287, 'actions': 2288, 'activities': 2289, 'actress': 2290, 'afford': 2291, 'ages': 2292, 'alarm': 2293, 'ale': 2294, 'amsterdam': 2295, 'angels': 2296, 'announcement': 2297, 'annual': 2298, 'antisemitism': 2299, 'april': 2300, 'avoid': 2301, 'barcelona': 2302, 'base': 2303, 'baseball': 2304, 'bernie': 2305, 'besties': 2306, 'binge': 2307, 'block': 2308, 'blowing': 2309, 'blues': 2310, 'bond': 2311, 'bonding': 2312, 'bottle': 2313, 'british': 2314, 'bubbles': 2315, 'camping': 2316, 'cardiff': 2317, 'caused': 2318, 'challenges': 2319, 'changing': 2320, 'chick': 2321, 'chosen': 2322, 'claim': 2323, 'claims': 2324, 'coldplaywembley': 2325, 'comfo': 2326, 'comics': 2327, 'commercial': 2328, 'connect': 2329, 'cookies': 2330, 'cox': 2331, 'created': 2332, 'cross': 2333, 'crowd': 2334, 'cucumbers': 2335, 'danske': 2336, 'dat': 2337, 'democracy': 2338, 'describe': 2339, 'destroyed': 2340, 'determined': 2341, 'dick': 2342, 'disease': 2343, 'donaldtrump': 2344, 'dropping': 2345, 'drug': 2346, 'entry': 2347, 'etc': 2348, 'eurusd': 2349, 'excuse': 2350, 'express': 2351, 'famous': 2352, 'flights': 2353, 'form': 2354, 'former': 2355, 'fought': 2356, 'frank': 2357, 'freaking': 2358, 'friendly': 2359, 'gender': 2360, 'gf': 2361, 'goodness': 2362, 'grind': 2363, 'ground': 2364, 'groups': 2365, 'gutted': 2366, 'hardly': 2367, 'heritage': 2368, 'hilarious': 2369, 'hockey': 2370, 'honest': 2371, 'honestly': 2372, 'how': 2373, 'hv': 2374, 'icecream': 2375, 'idol': 2376, 'impact': 2377, 'independent': 2378, 'indiegamedev': 2379, 'instacool': 2380, 'intelligence': 2381, 'islamophobia': 2382, 'james': 2383, 'jewelry': 2384, 'jews': 2385, 'kevin': 2386, 'kicked': 2387, 'killer': 2388, 'kills': 2389, 'large': 2390, 'las': 2391, 'launched': 2392, 'learned': 2393, 'lifes': 2394, 'lights': 2395, 'likely': 2396, 'lines': 2397, 'menu': 2398, 'mi': 2399, 'millions': 2400, 'mixed': 2401, 'motivate': 2402, 'moved': 2403, 'mp': 2404, 'multiple': 2405, 'murdering': 2406, 'names': 2407, 'nations': 2408, 'nephew': 2409, 'newcastle': 2410, 'newyear': 2411, 'niece': 2412, 'nightout': 2413, 'nose': 2414, 'noticed': 2415, 'november': 2416, 'nutrition': 2417, 'optimistic': 2418, 'outrage': 2419, 'pack': 2420, 'paradise': 2421, 'performing': 2422, 'periscope': 2423, 'planet': 2424, 'playlist': 2425, 'plenty': 2426, 'porait': 2427, 'posting': 2428, 'preorder': 2429, 'prepare': 2430, 'prince': 2431, 'promote': 2432, 'protesting': 2433, 'puntohost': 2434, 'raining': 2435, 'ramadhan': 2436, 'rate': 2437, 'raw': 2438, 'referendum': 2439, 'refugees': 2440, 'remains': 2441, 'reveal': 2442, 'rise': 2443, 'roof': 2444, 'roses': 2445, 'russian': 2446, 'satisfied': 2447, 'schedule': 2448, 'score': 2449, 'screaming': 2450, 'seats': 2451, 'seconds': 2452, 'secrets': 2453, 'selflove': 2454, 'services': 2455, 'sexual': 2456, 'shake': 2457, 'shall': 2458, 'shitty': 2459, 'shock': 2460, 'shoutout': 2461, 'sight': 2462, 'sirf': 2463, 'skills': 2464, 'sleeps': 2465, 'slowly': 2466, 'smiley': 2467, 'spa': 2468, 'spiritual': 2469, 'stick': 2470, 'stone': 2471, 'stopped': 2472, 'strawberries': 2473, 'suck': 2474, 'sucks': 2475, 'sukhbir': 2476, 'summit': 2477, 'sundays': 2478, 'swear': 2479, 'taught': 2480, 'tb': 2481, 'teach': 2482, 'teamsuperjunior': 2483, 'terms': 2484, 'tokyo': 2485, 'tomorrows': 2486, 'trailer': 2487, 'trapped': 2488, 'treason': 2489, 'trees': 2490, 'troll': 2491, 'type': 2492, 'uni': 2493, 'unleashyourjoy': 2494, 'unless': 2495, 'voting': 2496, 'waves': 2497, 'wee': 2498, 'wembley': 2499, 'wh': 2500, 'whilst': 2501, 'written': 2502, 'youtuber': 2503, 'zone': 2504, '2008': 2505, '20th': 2506, '31': 2507, '40404': 2508, 'access': 2509, 'achieve': 2510, 'acne': 2511, 'active': 2512, 'added': 2513, 'address': 2514, 'adveising': 2515, 'af': 2516, 'ag': 2517, 'agreed': 2518, 'ahhh': 2519, 'americas': 2520, 'anti': 2521, 'anxious': 2522, 'appreciation': 2523, 'arm': 2524, 'arms': 2525, 'army': 2526, 'ashiq': 2527, 'attend': 2528, 'awful': 2529, 'babygirl': 2530, 'background': 2531, 'ban': 2532, 'basically': 2533, 'battle': 2534, 'bay': 2535, 'bears': 2536, 'beats': 2537, 'beutiful': 2538, 'bih': 2539, 'bihdays': 2540, 'blast': 2541, 'brave': 2542, 'bridge': 2543, 'bullying': 2544, 'caoon': 2545, 'carlpaladino': 2546, 'cast': 2547, 'cd': 2548, 'cedm': 2549, 'celebrations': 2550, 'center': 2551, 'chapter': 2552, 'charging': 2553, 'cheap': 2554, 'checked': 2555, 'chicken': 2556, 'china': 2557, 'closing': 2558, 'cloudy': 2559, 'comic': 2560, 'comingsoon': 2561, 'committed': 2562, 'completed': 2563, 'con': 2564, 'confirmed': 2565, 'cost': 2566, 'courage': 2567, 'cousin': 2568, 'coz': 2569, 'creativity': 2570, 'credit': 2571, 'cried': 2572, 'cutest': 2573, 'cutie': 2574, 'decades': 2575, 'definition': 2576, 'denial': 2577, 'development': 2578, 'di': 2579, 'director': 2580, 'disgusted': 2581, 'disneyland': 2582, 'dogsarejoy': 2583, 'dope': 2584, 'dropped': 2585, 'dwd': 2586, 'edc': 2587, 'edit': 2588, 'embrace': 2589, 'emergency': 2590, 'ep': 2591, 'euros2016': 2592, 'exam': 2593, 'exist': 2594, 'expensive': 2595, 'extremely': 2596, 'f1': 2597, 'factsguide': 2598, 'falling': 2599, 'false': 2600, 'faster': 2601, 'feelinggood': 2602, 'fml': 2603, 'follow4follow': 2604, 'follower': 2605, 'foot': 2606, 'fr': 2607, 'french': 2608, 'frm': 2609, 'fuhered': 2610, 'fully': 2611, 'gain': 2612, 'game7': 2613, 'gameofthrones': 2614, 'gator': 2615, 'gbpusd': 2616, 'genuine': 2617, 'gewitter': 2618, 'giant': 2619, 'glitter': 2620, 'goodmood': 2621, 'groom': 2622, 'habits': 2623, 'haters': 2624, 'hating': 2625, 'headed': 2626, 'ignore': 2627, 'immigrants': 2628, 'immigration': 2629, 'increasing': 2630, 'inspiring': 2631, 'instamoment': 2632, 'interested': 2633, 'jack': 2634, 'jan': 2635, 'ji': 2636, 'karen': 2637, 'kpop': 2638, 'launching': 2639, 'lawyer': 2640, 'lee': 2641, 'lesson': 2642, 'lgbtq': 2643, 'library': 2644, 'lincoln': 2645, 'liverpool': 2646, 'logic': 2647, 'looki': 2648, 'lovemyjob': 2649, 'lovers': 2650, 'lovinglife': 2651, 'lush': 2652, 'magical': 2653, 'mama': 2654, 'manager': 2655, 'mate': 2656, 'mature': 2657, 'meal': 2658, 'medical': 2659, 'mess': 2660, 'midweek': 2661, 'minds': 2662, 'minecraft': 2663, 'mini': 2664, 'mondaymotivation': 2665, 'musicvideo': 2666, 'nowadays': 2667, 'np': 2668, 'ohio': 2669, 'oscar': 2670, 'overwatch': 2671, 'pair': 2672, 'pants': 2673, 'parking': 2674, 'paul': 2675, 'paying': 2676, 'peppa': 2677, 'pharrell': 2678, 'picked': 2679, 'piggies': 2680, 'player': 2681, 'pleased': 2682, 'policy': 2683, 'position': 2684, 'pougal': 2685, 'pr': 2686, 'prayersfororlando': 2687, 'pregnancy': 2688, 'presentation': 2689, 'prize': 2690, 'pro': 2691, 'producer': 2692, 'production': 2693, 'promise': 2694, 'pulsenightclub': 2695, 'puppies': 2696, 'purchased': 2697, 'queens': 2698, 'quiz': 2699, 'rage': 2700, 'realise': 2701, 'recording': 2702, 'recovery': 2703, 'regret': 2704, 'relate': 2705, 'remind': 2706, 'retweets': 2707, 'river': 2708, 'rude': 2709, 'sandy': 2710, 'saved': 2711, 'schwandorfchwandorf': 2712, 'scum': 2713, 'selfish': 2714, 'selling': 2715, 'sessions': 2716, 'sexist': 2717, 'skies': 2718, 'sleeping': 2719, 'sms': 2720, 'soo': 2721, 'southafrica': 2722, 'speed': 2723, 'spot': 2724, 'standing': 2725, 'starkes': 2726, 'steak': 2727, 'summer2016': 2728, 'superb': 2729, 'superhero': 2730, 'swag': 2731, 'swing': 2732, 'switzerland': 2733, 'talks': 2734, 'th': 2735, 'theconjuring2': 2736, 'trade': 2737, 'treated': 2738, 'treats': 2739, 'trolling': 2740, 'truck': 2741, 'union': 2742, 'upon': 2743, 'usdcad': 2744, 'usually': 2745, 'venusexchange': 2746, 'victim': 2747, 'vinyl': 2748, 'vinyls': 2749, 'vlicobs': 2750, 'warnung': 2751, 'wattpad': 2752, 'were': 2753, 'wetter': 2754, 'wetterwarnung': 2755, 'william': 2756, 'williams': 2757, 'wonthey': 2758, 'wood': 2759, 'woot': 2760, 'worship': 2761, 'yep': 2762, 'yup': 2763, 'zero': 2764, '23': 2765, '27': 2766, '2b': 2767, '8': 2768, 'advanced': 2769, 'adventures': 2770, 'aists': 2771, 'ally': 2772, 'amarinder': 2773, 'amodu': 2774, 'among': 2775, 'andrew': 2776, 'anna': 2777, 'appletstag': 2778, 'arkansas': 2779, 'atherapy': 2780, 'audusd': 2781, 'awaiting': 2782, 'awork': 2783, 'badal': 2784, 'bbuk': 2785, 'becomes': 2786, 'bees': 2787, 'begun': 2788, 'bell': 2789, 'blind': 2790, 'blogging': 2791, 'blood': 2792, 'bluesky': 2793, 'bob': 2794, 'bollywood': 2795, 'boo': 2796, 'bound': 2797, 'bringing': 2798, 'brunette': 2799, 'bt': 2800, 'bullshit': 2801, 'burn': 2802, 'butt': 2803, 'candles': 2804, 'cared': 2805, 'cheesy': 2806, 'chilling': 2807, 'clock': 2808, 'closed': 2809, 'cloud': 2810, 'colours': 2811, 'complain': 2812, 'concept': 2813, 'connecting': 2814, 'conservative': 2815, 'cosplay': 2816, 'couples': 2817, 'covered': 2818, 'crack': 2819, 'crafts': 2820, 'croatia': 2821, 'cruel': 2822, 'cuddles': 2823, 'deserves': 2824, 'desire': 2825, 'dey': 2826, 'disaster': 2827, 'disneygatorattack': 2828, 'distance': 2829, 'diversity': 2830, 'documentary': 2831, 'dogsofinstagram': 2832, 'dubai': 2833, 'dust': 2834, 'earrings': 2835, 'ebook': 2836, 'educationfest': 2837, 'emoji': 2838, 'emotion': 2839, 'enemy': 2840, 'environmental': 2841, 'episodes': 2842, 'euref': 2843, 'example': 2844, 'excite': 2845, 'exhausted': 2846, 'fa': 2847, 'faraz': 2848, 'fascist': 2849, 'fashionblogger': 2850, 'feed': 2851, 'fellow': 2852, 'feminist': 2853, 'field': 2854, 'films': 2855, 'flat': 2856, 'follows': 2857, 'forecasts': 2858, 'forest': 2859, 'forgotten': 2860, 'ft': 2861, 'fundraising': 2862, 'furniture': 2863, 'galib': 2864, 'gays': 2865, 'golden': 2866, 'graduated': 2867, 'grandma': 2868, 'grass': 2869, 'guilty': 2870, 'gunviolence': 2871, 'haircut': 2872, 'harassment': 2873, 'hashtag': 2874, 'hbd': 2875, 'heavy': 2876, 'hoes': 2877, 'holds': 2878, 'hole': 2879, 'horse': 2880, 'host': 2881, 'houston': 2882, 'hr': 2883, 'idk': 2884, 'iloveyou': 2885, 'images': 2886, 'imagination': 2887, 'improve': 2888, 'indians': 2889, 'ink': 2890, 'instafood': 2891, 'instagay': 2892, 'internalize': 2893, 'iqbal': 2894, 'irish': 2895, 'istanbul': 2896, 'italian': 2897, 'johnny': 2898, 'kejriwal': 2899, 'keys': 2900, 'kit': 2901, 'lately': 2902, 'lazy': 2903, 'leading': 2904, 'leftright': 2905, 'lessons': 2906, 'lied': 2907, 'liked': 2908, 'lineup': 2909, 'lion': 2910, 'lipstick': 2911, 'lmfao': 2912, 'lo': 2913, 'longhair': 2914, 'losangeles': 2915, 'loud': 2916, 'loveisland': 2917, 'lucy': 2918, 'mac': 2919, 'managed': 2920, 'manga': 2921, 'mans': 2922, 'marathon': 2923, 'marvel': 2924, 'master': 2925, 'masters': 2926, 'mat': 2927, 'mcdonalds': 2928, 'meme': 2929, 'mens': 2930, 'mercy': 2931, 'messages': 2932, 'miscegenation': 2933, 'miserable': 2934, 'mistakes': 2935, 'mohsin': 2936, 'momlife': 2937, 'monkey': 2938, 'mrs': 2939, 'musical': 2940, 'musictherapy': 2941, 'mylife': 2942, 'newmusic': 2943, 'nike': 2944, 'nomnom': 2945, 'nonsense': 2946, 'notes': 2947, 'nottingham': 2948, 'nzdusd': 2949, 'obamas': 2950, 'opened': 2951, 'opposition': 2952, 'oppressive': 2953, 'ordinary': 2954, 'outdoor': 2955, 'outdoors': 2956, 'outfit': 2957, 'outrageous': 2958, 'overweight': 2959, 'owner': 2960, 'pageant': 2961, 'pakistan': 2962, 'passing': 2963, 'perhaps': 2964, 'personal': 2965, 'personally': 2966, 'philippines': 2967, 'phones': 2968, 'placed': 2969, 'placement': 2970, 'planned': 2971, 'playa': 2972, 'pleasure': 2973, 'plymouth': 2974, 'points': 2975, 'polarisation': 2976, 'polls': 2977, 'popular': 2978, 'potd': 2979, 'povey': 2980, 'prayer': 2981, 'prayfoheworld': 2982, 'premier': 2983, 'premiere': 2984, 'premium': 2985, 'pretending': 2986, 'prime': 2987, 'print': 2988, 'prints': 2989, 'prom': 2990, 'prove': 2991, 'proverb': 2992, 'pull': 2993, 'putinschoice': 2994, 'puts': 2995, 'quickly': 2996, 'rains': 2997, 'raised': 2998, 'rant': 2999, 'rd': 3000, 'recognition': 3001, 'reflect': 3002, 'reflections': 3003, 'regrann': 3004, 'reinventimpossible': 3005, 'relationshipgoals': 3006, 'relaxation': 3007, 'relevant': 3008, 'remembered': 3009, 'reminds': 3010, 'remix': 3011, 'remove': 3012, 'request': 3013, 'rhetoric': 3014, 'rome': 3015, 'rule': 3016, 'runs': 3017, 'safety': 3018, 'santa': 3019, 'saturdays': 3020, 'scream': 3021, 'seeklearning': 3022, 'seeks': 3023, 'shairi': 3024, 'shameful': 3025, 'shark': 3026, 'signing': 3027, 'silent': 3028, 'sites': 3029, 'sketch': 3030, 'smoking': 3031, 'somehow': 3032, 'stadium': 3033, 'staer': 3034, 'stafresh': 3035, 'stella': 3036, 'steph': 3037, 'steps': 3038, 'stocks': 3039, 'stopracism': 3040, 'strange': 3041, 'studying': 3042, 'suicidal': 3043, 'sunrise': 3044, 'sway': 3045, 'tagsforlikes': 3046, 'tale': 3047, 'tattoosleeves': 3048, 'technology': 3049, 'theater': 3050, 'theresistance': 3051, 'throw': 3052, 'tip': 3053, 'tom': 3054, 'tonyawards': 3055, 'trails': 3056, 'traitor': 3057, 'transformation': 3058, 'transition': 3059, 'trash': 3060, 'tree': 3061, 'trial': 3062, 'tumblr': 3063, 'tyler': 3064, 'underway': 3065, 'upload': 3066, 'uselections2016': 3067, 'useless': 3068, 'values': 3069, 'vanilla': 3070, 'vehicle': 3071, 'vendor': 3072, 'venue': 3073, 'vids': 3074, 'violent': 3075, 'voted': 3076, 'votes': 3077, 'wars': 3078, 'wasi': 3079, 'weddingplanning': 3080, 'wellbeing': 3081, 'wells': 3082, 'willing': 3083, 'windows10': 3084, 'wise': 3085, 'wit': 3086, 'woo': 3087, 'worried': 3088, 'xbox': 3089, 'xd': 3090, 'yoy': 3091, '14th': 3092, '21st': 3093, '2day': 3094, '35': 3095, '360': 3096, '500': 3097, '53': 3098, 'abrahamhicks': 3099, 'academy': 3100, 'accident': 3101, 'according': 3102, 'ace': 3103, 'adam': 3104, 'adele': 3105, 'adults': 3106, 'advocate': 3107, 'alcohol': 3108, 'aloha': 3109, 'amount': 3110, 'anthems': 3111, 'appointment': 3112, 'arabic': 3113, 'archives': 3114, 'asia': 3115, 'asks': 3116, 'asshole': 3117, 'atlanta': 3118, 'atm': 3119, 'auspol': 3120, 'average': 3121, 'aw': 3122, 'bali': 3123, 'balls': 3124, 'bankai': 3125, 'beings': 3126, 'belgium': 3127, 'believes': 3128, 'beyou': 3129, 'bills': 3130, 'bitcoin': 3131, 'bite': 3132, 'blamed': 3133, 'blessing': 3134, 'blueeyes': 3135, 'boots': 3136, 'bottles': 3137, 'bottom': 3138, 'breath': 3139, 'brithday': 3140, 'broadway': 3141, 'bunny': 3142, 'cafe': 3143, 'cali': 3144, 'camera': 3145, 'canadian': 3146, 'cancel': 3147, 'cancelled': 3148, 'candidates': 3149, 'candy': 3150, 'casting': 3151, 'catching': 3152, 'cc': 3153, 'chain': 3154, 'chip': 3155, 'choosing': 3156, 'chris': 3157, 'christianity': 3158, 'citizens': 3159, 'classroom': 3160, 'clue': 3161, 'cm': 3162, 'collage': 3163, 'colour': 3164, 'conces': 3165, 'confirmation': 3166, 'consumer': 3167, 'contain': 3168, 'cook': 3169, 'corruption': 3170, 'cricket': 3171, 'criminal': 3172, 'crisis': 3173, 'crossed': 3174, 'cuties': 3175, 'dancer': 3176, 'darling': 3177, 'dates': 3178, 'deals': 3179, 'decisions': 3180, 'deeply': 3181, 'defense': 3182, 'degree': 3183, 'delhi': 3184, 'demand': 3185, 'demo': 3186, 'denim': 3187, 'destroy': 3188, 'direction': 3189, 'disturbing': 3190, 'divorce': 3191, 'dna': 3192, 'domestic': 3193, 'dragon': 3194, 'drop': 3195, 'drops': 3196, 'earlier': 3197, 'editing': 3198, 'effect': 3199, 'eggs': 3200, 'engineer': 3201, 'enjoylife': 3202, 'eurgbp': 3203, 'everytime': 3204, 'exo': 3205, 'experiences': 3206, 'explain': 3207, 'exploring': 3208, 'extremism': 3209, 'failed': 3210, 'failing': 3211, 'fairytail': 3212, 'familyfun': 3213, 'familys': 3214, 'fascinating': 3215, 'featured': 3216, 'feedback': 3217, 'females': 3218, 'finale': 3219, 'finn': 3220, 'fitnessaddict': 3221, 'flagday': 3222, 'founate': 3223, 'fred': 3224, 'fridays': 3225, 'fuckin': 3226, 'gallery': 3227, 'garage': 3228, 'gardening': 3229, 'general': 3230, 'george': 3231, 'giftideas': 3232, 'giphy': 3233, 'glasgow': 3234, 'glow': 3235, 'goodlife': 3236, 'gr8': 3237, 'grandmother': 3238, 'greed': 3239, 'grey': 3240, 'grief': 3241, 'gross': 3242, 'grown': 3243, 'guidance': 3244, 'harm': 3245, 'harry': 3246, 'herbalremedies': 3247, 'hip': 3248, 'hiphop': 3249, 'homemade': 3250, 'homophobia': 3251, 'honor': 3252, 'hood': 3253, 'hook': 3254, 'ideal': 3255, 'ideology': 3256, 'image': 3257, 'included': 3258, 'including': 3259, 'increase': 3260, 'independence': 3261, 'indigenous': 3262, 'industrial': 3263, 'infinite': 3264, 'infographic': 3265, 'ing': 3266, 'inner': 3267, 'instasize': 3268, 'insults': 3269, 'irl': 3270, 'jackson': 3271, 'january': 3272, 'jocox': 3273, 'ka': 3274, 'language': 3275, 'largest': 3276, 'lean': 3277, 'letsgo': 3278, 'lifted': 3279, 'limited': 3280, 'livelypics': 3281, 'loads': 3282, 'logo': 3283, 'lounge': 3284, 'luis': 3285, 'lyon': 3286, 'macbook': 3287, 'machine': 3288, 'mam': 3289, 'manhattan': 3290, 'marijuana': 3291, 'marine': 3292, 'maui': 3293, 'mccain': 3294, 'meanwhile': 3295, 'meat': 3296, 'mensfashion': 3297, 'mentalhealth': 3298, 'mentalillness': 3299, 'metal': 3300, 'mike': 3301, 'mil': 3302, 'min': 3303, 'mindsconsole': 3304, 'mo': 3305, 'mornings': 3306, 'moron': 3307, 'mothers': 3308, 'moves': 3309, 'msnbc': 3310, 'mud': 3311, 'muhammadali': 3312, 'mustread': 3313, 'namaste': 3314, 'nashville': 3315, 'nationalist': 3316, 'nazi': 3317, 'nazis': 3318, 'necklace': 3319, 'negativity': 3320, 'net': 3321, 'newest': 3322, 'newswithed': 3323, 'nohern': 3324, 'noise': 3325, 'noone': 3326, 'novel': 3327, 'nowhere': 3328, 'nra': 3329, 'oakland': 3330, 'officebearers': 3331, 'officer': 3332, 'older': 3333, 'ole': 3334, 'orientation': 3335, 'orlandohorror': 3336, 'orlandounited': 3337, 'otherwise': 3338, 'overcome': 3339, 'owners': 3340, 'pages': 3341, 'palette': 3342, 'panda': 3343, 'pas': 3344, 'payday': 3345, 'peek': 3346, 'pen': 3347, 'per': 3348, 'perform': 3349, 'picking': 3350, 'piss': 3351, 'pissed': 3352, 'pitch': 3353, 'plain': 3354, 'po': 3355, 'poet': 3356, 'polishgirl': 3357, 'poll': 3358, 'pos': 3359, 'possibly': 3360, 'potential': 3361, 'potter': 3362, 'pounds': 3363, 'preach': 3364, 'prep': 3365, 'preview': 3366, 'prison': 3367, 'professional': 3368, 'profiling': 3369, 'projects': 3370, 'promotion': 3371, 'prosecco': 3372, 'protection': 3373, 'pulseshooting': 3374, 'ran': 3375, 'range': 3376, 'react': 3377, 'realtalk': 3378, 'rebeccas': 3379, 'receive': 3380, 'rehearsal': 3381, 'rejected': 3382, 'repeat': 3383, 'repoer': 3384, 'resign': 3385, 'resistance': 3386, 'retro': 3387, 'reverse': 3388, 'rg': 3389, 'ridiculous': 3390, 'rolling': 3391, 'runner': 3392, 'salute': 3393, 'sam': 3394, 'sand': 3395, 'sanders': 3396, 'sandiego': 3397, 'saysomething': 3398, 'scene': 3399, 'scenes': 3400, 'scotiabank': 3401, 'screwed': 3402, 'sculpted': 3403, 'seat': 3404, 'shane': 3405, 'shed': 3406, 'shoe': 3407, 'showed': 3408, 'sides': 3409, 'sigh': 3410, 'skincare': 3411, 'slavery': 3412, 'sneak': 3413, 'soed': 3414, 'sources': 3415, 'spanish': 3416, 'speaks': 3417, 'split': 3418, 'spotify': 3419, 'steal': 3420, 'stray': 3421, 'stupidity': 3422, 'suit': 3423, 'sunglasses': 3424, 'sup': 3425, 'suppoer': 3426, 'survey': 3427, 'suspended': 3428, 'sweden': 3429, 'taylor': 3430, 'teachers': 3431, 'throwing': 3432, 'thursdays': 3433, 'tick': 3434, 'tower': 3435, 'toy': 3436, 'toys': 3437, 'travels': 3438, 'treatment': 3439, 'trek': 3440, 'trend': 3441, 'tries': 3442, 'tweeted': 3443, 'tweeting': 3444, 'ukrunchat': 3445, 'un': 3446, 'unforgettable': 3447, 'unique': 3448, 'upside': 3449, 'urdu': 3450, 'usdjpy': 3451, 'uses': 3452, 'usual': 3453, 'vijay': 3454, 'vile': 3455, 'vip': 3456, 'voter': 3457, 'vr': 3458, 'vscogood': 3459, 'walked': 3460, 'warcraft': 3461, 'wasted': 3462, 'weed': 3463, 'where': 3464, 'whether': 3465, 'whisky': 3466, 'whoever': 3467, 'witness': 3468, 'womens': 3469, 'worlds': 3470, 'wwdc': 3471, 'wwdc2016': 3472, 'xboxe3': 3473, 'yea': 3474, 'zelda': 3475, '01': 3476, '02': 3477, '1000': 3478, '1306': 3479, '1600': 3480, '1900': 3481, '2012': 3482, '2015': 3483, '247': 3484, '29': 3485, '3': 3486, '4th': 3487, '5': 3488, '5th': 3489, '60': 3490, '6th': 3491, '8th': 3492, '95': 3493, 'absolute': 3494, 'acceptable': 3495, 'accessories': 3496, 'accounts': 3497, 'actorslife': 3498, 'addition': 3499, 'admit': 3500, 'affect': 3501, 'aim': 3502, 'alaska': 3503, 'albea': 3504, 'almighty': 3505, 'although': 3506, 'amateur': 3507, 'ambassador': 3508, 'announced': 3509, 'antonyelchin': 3510, 'apply': 3511, 'argentina': 3512, 'asians': 3513, 'assume': 3514, 'attacked': 3515, 'attractive': 3516, 'audition': 3517, 'augusta': 3518, 'aunt': 3519, 'aus': 3520, 'b4': 3521, 'baba': 3522, 'backed': 3523, 'backyard': 3524, 'balloons': 3525, 'bastards': 3526, 'belief': 3527, 'bentley': 3528, 'beside': 3529, 'bestsellers': 3530, 'beware': 3531, 'bffs': 3532, 'bigots': 3533, 'bihdaysway': 3534, 'billion': 3535, 'billy': 3536, 'birmingham': 3537, 'bk': 3538, 'blicqer': 3539, 'bodrum': 3540, 'bomb': 3541, 'bonus': 3542, 'bowling': 3543, 'brazil': 3544, 'bread': 3545, 'breakup': 3546, 'brighten': 3547, 'bringiton': 3548, 'bron': 3549, 'brooklyn': 3550, 'browning': 3551, 'bruh': 3552, 'brussels': 3553, 'bubble': 3554, 'bucket': 3555, 'bummer': 3556, 'burger': 3557, 'burning': 3558, 'cage': 3559, 'cap': 3560, 'cape': 3561, 'captain': 3562, 'caribbean': 3563, 'caring': 3564, 'carnival': 3565, 'carry': 3566, 'carrying': 3567, 'celebrated': 3568, 'cereal': 3569, 'chair': 3570, 'champions': 3571, 'cheat': 3572, 'cheeky': 3573, 'cheering': 3574, 'cherry': 3575, 'chile': 3576, 'chinese': 3577, 'chips': 3578, 'circle': 3579, 'citation': 3580, 'civil': 3581, 'clarity': 3582, 'classy': 3583, 'cleaning': 3584, 'clown': 3585, 'coaching': 3586, 'coast': 3587, 'coffe': 3588, 'colonialism': 3589, 'com': 3590, 'conjuring': 3591, 'connected': 3592, 'considering': 3593, 'contact': 3594, 'continuous': 3595, 'corner': 3596, 'correct': 3597, 'coupon': 3598, 'cracked': 3599, 'craft': 3600, 'creates': 3601, 'criticism': 3602, 'crooked': 3603, 'cruise': 3604, 'cuddle': 3605, 'cuteness': 3606, 'daddysgirl': 3607, 'dadsavailable': 3608, 'dallas': 3609, 'danger': 3610, 'dare': 3611, 'datenight': 3612, 'dawn': 3613, 'dayoff': 3614, 'deaths': 3615, 'decor': 3616, 'dedicated': 3617, 'degrees': 3618, 'democratic': 3619, 'dems': 3620, 'designed': 3621, 'desse': 3622, 'destination': 3623, 'destruction': 3624, 'determination': 3625, 'detox': 3626, 'dev': 3627, 'diego': 3628, 'differently': 3629, 'discrimination': 3630, 'disgrace': 3631, 'disgraceful': 3632, 'dive': 3633, 'divide': 3634, 'doggy': 3635, 'dogsoftwitter': 3636, 'donkey': 3637, 'doors': 3638, 'drake': 3639, 'dreamcatcher': 3640, 'ducks': 3641, 'dudes': 3642, 'duty': 3643, 'eac': 3644, 'earn': 3645, 'earned': 3646, 'ebay': 3647, 'ecstatic': 3648, 'effective': 3649, 'eh': 3650, 'empathy': 3651, 'emtec': 3652, 'encouragement': 3653, 'enemies': 3654, 'enjoyed': 3655, 'et': 3656, 'evenings': 3657, 'evidence': 3658, 'exclusive': 3659, 'excuses': 3660, 'exhibition': 3661, 'explore': 3662, 'extra': 3663, 'faced': 3664, 'fails': 3665, 'fakenews': 3666, 'falls': 3667, 'fancy': 3668, 'fargo': 3669, 'farmers': 3670, 'fate': 3671, 'fever': 3672, 'ffs': 3673, 'fill': 3674, 'filming': 3675, 'financial': 3676, 'fingers': 3677, 'finishing': 3678, 'fitbit': 3679, 'flies': 3680, 'floor': 3681, 'folk': 3682, 'followback': 3683, 'forced': 3684, 'forgiveness': 3685, 'fotokuapp': 3686, 'fri': 3687, 'frog': 3688, 'frustrating': 3689, 'fuel': 3690, 'gamers': 3691, 'gang': 3692, 'gangs': 3693, 'gayboy': 3694, 'gearing': 3695, 'gemini': 3696, 'genuinely': 3697, 'ghost': 3698, 'girlpower': 3699, 'girly': 3700, 'giveaway': 3701, 'glamping': 3702, 'glastonbury': 3703, 'goat': 3704, 'goodluck': 3705, 'goodread': 3706, 'goofy': 3707, 'gopro': 3708, 'grab': 3709, 'graduating': 3710, 'granted': 3711, 'grave': 3712, 'greatday': 3713, 'greatful': 3714, 'greetings': 3715, 'gs': 3716, 'guide': 3717, 'guncontrol': 3718, 'guncontrolnow': 3719, 'hall': 3720, 'hd': 3721, 'heaache': 3722, 'heabreak': 3723, 'healthandfitness': 3724, 'helps': 3725, 'heroes': 3726, 'higher': 3727, 'hoe': 3728, 'homeopathic': 3729, 'honored': 3730, 'horses': 3731, 'hoshi': 3732, 'hs': 3733, 'huh': 3734, 'hulk': 3735, 'humanrights': 3736, 'hurry': 3737, 'hustle': 3738, 'illegal': 3739, 'imwithher': 3740, 'inc': 3741, 'incident': 3742, 'includes': 3743, 'increased': 3744, 'indian': 3745, 'indonesia': 3746, 'information': 3747, 'inn': 3748, 'insights': 3749, 'instahappy': 3750, 'interracial': 3751, 'intolerance': 3752, 'introduction': 3753, 'invest': 3754, 'invite': 3755, 'invited': 3756, 'ios': 3757, 'iron': 3758, 'jam': 3759, 'jamaica': 3760, 'jeff': 3761, 'jeffsessions': 3762, 'jew': 3763, 'jewellery': 3764, 'jewish': 3765, 'jihad': 3766, 'jimmy': 3767, 'johnnydepp': 3768, 'joyful': 3769, 'judges': 3770, 'justify': 3771, 'justin': 3772, 'justsaying': 3773, 'kindle': 3774, 'kings': 3775, 'knitting': 3776, 'knocked': 3777, 'ku': 3778, 'labour': 3779, 'landed': 3780, 'landscape': 3781, 'latina': 3782, 'laughs': 3783, 'launches': 3784, 'leak': 3785, 'leather': 3786, 'led': 3787, 'leeds': 3788, 'leg': 3789, 'legal': 3790, 'lemans24': 3791, 'letter': 3792, 'levels': 3793, 'lightroom': 3794, 'lions': 3795, 'listened': 3796, 'lists': 3797, 'livelife': 3798, 'lock': 3799, 'loneliness': 3800, 'lookin': 3801, 'loop': 3802, 'loose': 3803, 'loosing': 3804, 'louis': 3805, 'lows': 3806, 'lumpy': 3807, 'lunchtime': 3808, 'majesty': 3809, 'makeuptransformation': 3810, 'management': 3811, 'mantra': 3812, 'mask': 3813, 'matt': 3814, 'mc': 3815, 'meets': 3816, 'mentioned': 3817, 'mickey': 3818, 'mindful': 3819, 'minions': 3820, 'mission': 3821, 'mistake': 3822, 'modelling': 3823, 'modern': 3824, 'momma': 3825, 'mon': 3826, 'mondays': 3827, 'monthly': 3828, 'motorcycle': 3829, 'motto': 3830, 'mountain': 3831, 'movingon': 3832, 'mug': 3833, 'natalie': 3834, 'netherlands': 3835, 'neverforget': 3836, 'newlook': 3837, 'newproject': 3838, 'nick': 3839, 'nightmares': 3840, 'ninja': 3841, 'non': 3842, 'nowlinkup': 3843, 'nurses': 3844, 'nut': 3845, 'october': 3846, 'odd': 3847, 'offered': 3848, 'offers': 3849, 'oj': 3850, 'olathe': 3851, 'olds': 3852, 'opera': 3853, 'oprah': 3854, 'oven': 3855, 'palestinian': 3856, 'passionate': 3857, 'pencil': 3858, 'performer': 3859, 'phase': 3860, 'pie': 3861, 'pixar': 3862, 'plant': 3863, 'plate': 3864, 'pll': 3865, 'policies': 3866, 'pops': 3867, 'pout': 3868, 'pre': 3869, 'pregnant': 3870, 'pres': 3871, 'presidents': 3872, 'pretend': 3873, 'prices': 3874, 'private': 3875, 'promises': 3876, 'promoting': 3877, 'protest': 3878, 'proved': 3879, 'proven': 3880, 'proves': 3881, 'punjabis': 3882, 'pup': 3883, 'push': 3884, 'putin': 3885, 'qotd': 3886, 'quack': 3887, 'quantities': 3888, 'queenat90': 3889, 'quit': 3890, 'racial': 3891, 'rad': 3892, 'raghuramrajan': 3893, 'rap': 3894, 'rapist': 3895, 'realestate': 3896, 'recorded': 3897, 'refs': 3898, 'reggae': 3899, 'removed': 3900, 'represents': 3901, 'respected': 3902, 'respond': 3903, 'restinpeace': 3904, 'revealed': 3905, 'rewrite': 3906, 'rings': 3907, 'rob': 3908, 'roles': 3909, 'rooms': 3910, 'routine': 3911, 'royal': 3912, 'rss': 3913, 'ruined': 3914, 'sabbath': 3915, 'sake': 3916, 'saturdaymorning': 3917, 'saturdaynight': 3918, 'sauce': 3919, 'script': 3920, 'seaside': 3921, 'seattle': 3922, 'sees': 3923, 'selected': 3924, 'selena': 3925, 'selfietime': 3926, 'semester': 3927, 'semitic': 3928, 'sentence': 3929, 'sept': 3930, 'shades': 3931, 'shape': 3932, 'shopalyssas': 3933, 'shopthemint': 3934, 'shots': 3935, 'shuaibu': 3936, 'skate': 3937, 'slice': 3938, 'smilepowerday': 3939, 'smith': 3940, 'smoothie': 3941, 'solid': 3942, 'sooo': 3943, 'soooo': 3944, 'spark': 3945, 'spoilers': 3946, 'spoke': 3947, 'square': 3948, 'standard': 3949, 'stayed': 3950, 'stays': 3951, 'staytuned': 3952, 'strangers': 3953, 'stressfree': 3954, 'strikes': 3955, 'struggle': 3956, 'stylish': 3957, 'suddenly': 3958, 'suffer': 3959, 'suppos': 3960, 'surely': 3961, 'swift': 3962, 'syfy': 3963, 'tall': 3964, 'tanks': 3965, 'tattoos': 3966, 'teaching': 3967, 'terribly': 3968, 'testing': 3969, 'tests': 3970, 'thomas': 3971, 'thrilled': 3972, 'throwbackthursday': 3973, 'thrown': 3974, 'thug': 3975, 'tiger': 3976, 'tlc': 3977, 'topic': 3978, 'topless': 3979, 'touches': 3980, 'tournament': 3981, 'traditions': 3982, 'traffic': 3983, 'trends': 3984, 'tribute': 3985, 'troubles': 3986, 'truestory': 3987, 'tt': 3988, 'tube': 3989, 'tuesdaymotivation': 3990, 'tule': 3991, 'twins': 3992, 'uae': 3993, 'ultimate': 3994, 'unable': 3995, 'unfounate': 3996, 'unknown': 3997, 'unreal': 3998, 'upbeat': 3999, 'updated': 4000, 'user': 4001, 'vancouver': 4002, 'venice': 4003, 'vibe': 4004, 'videogames': 4005, 'vlog': 4006, 'voiceover': 4007, 'walker': 4008, 'warning': 4009, 'warren': 4010, 'wash': 4011, 'wealth': 4012, 'wealthy': 4013, 'weapon': 4014, 'western': 4015, 'whitepeople': 4016, 'whitesupremacy': 4017, 'whose': 4018, 'wi': 4019, 'wildlife': 4020, 'wimbledon': 4021, 'wing': 4022, 'wings': 4023, 'winners': 4024, 'wooden': 4025, 'workplace': 4026, 'worrying': 4027, 'wrap': 4028, 'wth': 4029, 'wwe': 4030, 'xboxone': 4031, 'yogi': 4032, 'younger': 4033, 'yu': 4034, 'zoo': 4035, '05': 4036, '18th': 4037, '19th': 4038, '2014': 4039, '2pac': 4040, '30th': 4041, '35th': 4042, '45': 4043, '50th': 4044, '72': 4045, '79': 4046, '80': 4047, '911': 4048, 'abandoned': 4049, 'acceptance': 4050, 'ache': 4051, 'actors': 4052, 'addiction': 4053, 'adidas': 4054, 'admin': 4055, 'adrenaline': 4056, 'ads': 4057, 'advantage': 4058, 'ahhhh': 4059, 'aka': 4060, 'aliens': 4061, 'alps': 4062, 'alright': 4063, 'alumni': 4064, 'amendment': 4065, 'andreas': 4066, 'anf': 4067, 'ankara': 4068, 'annoying': 4069, 'answered': 4070, 'answers': 4071, 'anyways': 4072, 'apa': 4073, 'apaheid': 4074, 'archangels': 4075, 'architecture': 4076, 'arianagrandedrawing': 4077, 'arriving': 4078, 'arsenal': 4079, 'asleep': 4080, 'assholes': 4081, 'attraction': 4082, 'audience': 4083, 'aug': 4084, 'austin': 4085, 'aware': 4086, 'awesomeness': 4087, 'awhile': 4088, 'awww': 4089, 'babes': 4090, 'bachelorette': 4091, 'backwards': 4092, 'bacon': 4093, 'badger': 4094, 'baking': 4095, 'baku': 4096, 'baltimore': 4097, 'bangkok': 4098, 'banned': 4099, 'barn': 4100, 'bashful': 4101, 'bastard': 4102, 'beachbody': 4103, 'beachlife': 4104, 'bedroom': 4105, 'bee': 4106, 'beers': 4107, 'behavior': 4108, 'bel': 4109, 'believing': 4110, 'beloved': 4111, 'ben': 4112, 'benidorm': 4113, 'bestoftheday': 4114, 'bittersweet': 4115, 'blanket': 4116, 'blatant': 4117, 'blow': 4118, 'blowjob': 4119, 'bn': 4120, 'boating': 4121, 'bodies': 4122, 'bombs': 4123, 'bones': 4124, 'boob': 4125, 'brains': 4126, 'branding': 4127, 'brands': 4128, 'breathe': 4129, 'breeze': 4130, 'brian': 4131, 'bridal': 4132, 'browser': 4133, 'brutality': 4134, 'budget': 4135, 'burgers': 4136, 'butterflies': 4137, 'butterfly': 4138, 'button': 4139, 'camden': 4140, 'cameron': 4141, 'campus': 4142, 'cannon': 4143, 'capitalstb': 4144, 'cases': 4145, 'cash': 4146, 'causes': 4147, 'causing': 4148, 'cdnpoli': 4149, 'celebrity': 4150, 'century': 4151, 'ceo': 4152, 'ceremony': 4153, 'champagne': 4154, 'championship': 4155, 'chaos': 4156, 'charles': 4157, 'charming': 4158, 'checking': 4159, 'cheerful': 4160, 'chef': 4161, 'chelsea': 4162, 'chicks': 4163, 'chief': 4164, 'chilled': 4165, 'chose': 4166, 'clip': 4167, 'clueless': 4168, 'cochair': 4169, 'cocktail': 4170, 'colourful': 4171, 'combo': 4172, 'comedians': 4173, 'comfoable': 4174, 'comfy': 4175, 'commenting': 4176, 'commercials': 4177, 'complaint': 4178, 'conclusion': 4179, 'condition': 4180, 'confirm': 4181, 'conflict': 4182, 'connection': 4183, 'constant': 4184, 'constantly': 4185, 'constitution': 4186, 'construction': 4187, 'contract': 4188, 'convention': 4189, 'conversations': 4190, 'convinced': 4191, 'coolestlifehack': 4192, 'cooper': 4193, 'corporate': 4194, 'cos': 4195, 'council': 4196, 'coverage': 4197, 'covering': 4198, 'cows': 4199, 'coybig': 4200, 'cpi': 4201, 'creations': 4202, 'criminals': 4203, 'crown': 4204, 'crush': 4205, 'cum': 4206, 'custom': 4207, 'customerservice': 4208, 'cyprus': 4209, 'dan': 4210, 'dancers': 4211, 'danny': 4212, 'dating': 4213, 'debut': 4214, 'del': 4215, 'delta': 4216, 'dem': 4217, 'democrat': 4218, 'depament': 4219, 'depth': 4220, 'dese': 4221, 'digging': 4222, 'directly': 4223, 'disabled': 4224, 'disappointment': 4225, 'discover': 4226, 'discovered': 4227, 'display': 4228, 'disrespect': 4229, 'dissapointed': 4230, 'division': 4231, 'dnaquotes': 4232, 'doctor': 4233, 'doin': 4234, 'dollar': 4235, 'dreaming': 4236, 'dressed': 4237, 'drove': 4238, 'drugs': 4239, 'dry': 4240, 'duck': 4241, 'eastcoast': 4242, 'economy': 4243, 'ed': 4244, 'edits': 4245, 'educate': 4246, 'eight': 4247, 'elegant': 4248, 'elephants': 4249, 'ella': 4250, 'elses': 4251, 'emea': 4252, 'employment': 4253, 'endorsed': 4254, 'engineering': 4255, 'engrus': 4256, 'epcot': 4257, 'equal': 4258, 'error': 4259, 'expats': 4260, 'experienced': 4261, 'explains': 4262, 'extremists': 4263, 'eyed': 4264, 'factor': 4265, 'failure': 4266, 'fang': 4267, 'fashionillustration': 4268, 'fatty': 4269, 'fault': 4270, 'favorites': 4271, 'fbi': 4272, 'feat': 4273, 'feeding': 4274, 'feelthebern': 4275, 'fifa17': 4276, 'figured': 4277, 'finest': 4278, 'fingerscrossed': 4279, 'fitmom': 4280, 'fits': 4281, 'fixed': 4282, 'flash': 4283, 'flip': 4284, 'floating': 4285, 'floral': 4286, 'florence': 4287, 'floridashooting': 4288, 'flourish': 4289, 'ford': 4290, 'fouh': 4291, 'freemilo': 4292, 'funding': 4293, 'gandhi': 4294, 'gas': 4295, 'gate': 4296, 'gators': 4297, 'gaypride': 4298, 'generations': 4299, 'genius': 4300, 'gg': 4301, 'ghana': 4302, 'ghostbusters': 4303, 'giggles': 4304, 'glam': 4305, 'glastofest': 4306, 'glorious': 4307, 'gm': 4308, 'goodies': 4309, 'goodtime': 4310, 'gordie': 4311, 'gotten': 4312, 'grade': 4313, 'grads': 4314, 'grandfather': 4315, 'gray': 4316, 'greens': 4317, 'grill': 4318, 'gsw': 4319, 'gypsy': 4320, 'hack': 4321, 'hacking': 4322, 'handle': 4323, 'hangout': 4324, 'harder': 4325, 'harmony': 4326, 'harvest': 4327, 'hated': 4328, 'hates': 4329, 'headlines': 4330, 'healthyeating': 4331, 'healthyliving': 4332, 'helped': 4333, 'helpful': 4334, 'herbalife': 4335, 'highlight': 4336, 'highlights': 4337, 'hippie': 4338, 'hometown': 4339, 'honey': 4340, 'hong': 4341, 'hongkong': 4342, 'hopes': 4343, 'hottweets': 4344, 'however': 4345, 'huing': 4346, 'hurryup': 4347, 'hyped': 4348, 'ibiza2016': 4349, 'iconic': 4350, 'identity': 4351, 'ie': 4352, 'ignores': 4353, 'ignoring': 4354, 'immature': 4355, 'imperfections': 4356, 'impoance': 4357, 'impressed': 4358, 'impressive': 4359, 'inauguration': 4360, 'indie': 4361, 'indieauthor': 4362, 'inequality': 4363, 'injustice': 4364, 'instaaoftheday': 4365, 'insure': 4366, 'integral': 4367, 'investment': 4368, 'invited2jive': 4369, 'ion': 4370, 'ipad': 4371, 'israeli': 4372, 'items': 4373, 'jacksons': 4374, 'jacksonville': 4375, 'jazz': 4376, 'jeeplife': 4377, 'jerry': 4378, 'jessica': 4379, 'jivemap': 4380, 'joining': 4381, 'joking': 4382, 'journalism': 4383, 'juan': 4384, 'juice': 4385, 'jumping': 4386, 'jus': 4387, 'kajal': 4388, 'kanye': 4389, 'karaoke': 4390, 'kent': 4391, 'kentucky': 4392, 'kept': 4393, 'kg': 4394, 'kh': 4395, 'kharkiv': 4396, 'kharkivgram': 4397, 'kidding': 4398, 'kim': 4399, 'kits': 4400, 'klan': 4401, 'km': 4402, 'kscrashcorrectors': 4403, 'lab': 4404, 'lads': 4405, 'ladyboy': 4406, 'lame': 4407, 'lang': 4408, 'laundry': 4409, 'lay': 4410, 'laziness': 4411, 'lb': 4412, 'leaked': 4413, 'legit': 4414, 'letsdothis': 4415, 'liam': 4416, 'liars': 4417, 'lifegoals': 4418, 'lifehacks': 4419, 'lips': 4420, 'lived': 4421, 'lokiday': 4422, 'longweekend': 4423, 'losers': 4424, 'loyal': 4425, 'luke': 4426, 'lure': 4427, 'lyft': 4428, 'madness': 4429, 'magazine': 4430, 'mallorca': 4431, 'mar': 4432, 'march': 4433, 'maria': 4434, 'mars': 4435, 'mateen': 4436, 'megan': 4437, 'messi': 4438, 'mfs': 4439, 'minority': 4440, 'missyou': 4441, 'mix': 4442, 'mm': 4443, 'mma': 4444, 'modeling': 4445, 'monsoon': 4446, 'mount': 4447, 'mouse': 4448, 'movement': 4449, 'mps': 4450, 'msg': 4451, 'mt': 4452, 'mumbai': 4453, 'myhappycapture': 4454, 'nah': 4455, 'nailed': 4456, 'named': 4457, 'narrative': 4458, 'nationalbestfriendday': 4459, 'nationals': 4460, 'native': 4461, 'naturally': 4462, 'nda': 4463, 'neighborhood': 4464, 'neighbors': 4465, 'neither': 4466, 'newjob': 4467, 'nicola': 4468, 'nicole': 4469, 'nigger': 4470, 'nite': 4471, 'nope': 4472, 'nuascannan': 4473, 'numb': 4474, 'nuts': 4475, 'nz': 4476, 'obvious': 4477, 'ojmadeinamerica': 4478, 'olives': 4479, 'onelove': 4480, 'oomf': 4481, 'oops': 4482, 'opposite': 4483, 'optimism': 4484, 'organised': 4485, 'organization': 4486, 'originally': 4487, 'orlandostrong': 4488, 'osaka': 4489, 'otaku': 4490, 'ouch': 4491, 'outta': 4492, 'ove': 4493, 'overall': 4494, 'overwhelmed': 4495, 'owls': 4496, 'oxford': 4497, 'package': 4498, 'paies': 4499, 'parent': 4500, 'patience': 4501, 'payintheusa': 4502, 'pb': 4503, 'pearl': 4504, 'peh': 4505, 'perfectly': 4506, 'personality': 4507, 'perspective': 4508, 'petty': 4509, 'phelps': 4510, 'philly': 4511, 'picnic': 4512, 'pigs': 4513, 'pinoy': 4514, 'pit': 4515, 'pittsburgh': 4516, 'pocket': 4517, 'pokemon': 4518, 'politician': 4519, 'poman': 4520, 'postive': 4521, 'potato': 4522, 'prayingfororlando': 4523, 'prefer': 4524, 'prejudice': 4525, 'prepped': 4526, 'prevents': 4527, 'primary': 4528, 'principle': 4529, 'privilege': 4530, 'productivity': 4531, 'professionals': 4532, 'profit': 4533, 'programme': 4534, 'propaganda': 4535, 'properly': 4536, 'protecting': 4537, 'protein': 4538, 'psychology': 4539, 'puff': 4540, 'purple': 4541, 'pushing': 4542, 'quotestags': 4543, 'quotestagsapp': 4544, 'ra': 4545, 'races': 4546, 'racists': 4547, 'rational': 4548, 'rbc': 4549, 'reagan': 4550, 'recently': 4551, 'recognize': 4552, 'recommend': 4553, 'recovered': 4554, 'refuse': 4555, 'regarding': 4556, 'regardless': 4557, 'region': 4558, 'relatable': 4559, 'releases': 4560, 'religious': 4561, 'remarks': 4562, 'replace': 4563, 'replies': 4564, 'represent': 4565, 'restore': 4566, 'returned': 4567, 'retweeted': 4568, 'reunion': 4569, 'rider': 4570, 'ridiculously': 4571, 'riding': 4572, 'rnc': 4573, 'romania': 4574, 'root': 4575, 'ross': 4576, 'rough': 4577, 'royalascot': 4578, 'ruin': 4579, 'ryan': 4580, 'saddened': 4581, 'saddest': 4582, 'samsunggalaxys2': 4583, 'sanfrancisco': 4584, 'sarcasm': 4585, 'sassy': 4586, 'saterday': 4587, 'scifi': 4588, 'scores': 4589, 'scott': 4590, 'scratch': 4591, 'scumbag': 4592, 'se': 4593, 'seasons': 4594, 'seemed': 4595, 'selca': 4596, 'selfharm': 4597, 'selfporait': 4598, 'semi': 4599, 'seniors': 4600, 'shared': 4601, 'sharks': 4602, 'sharpens': 4603, 'shining': 4604, 'shis': 4605, 'shofilm': 4606, 'shooters': 4607, 'showtime': 4608, 'shrm16': 4609, 'siblings': 4610, 'sic': 4611, 'sitges': 4612, 'skinny': 4613, 'slammed': 4614, 'smallthings': 4615, 'smaphone': 4616, 'smash': 4617, 'smells': 4618, 'snatched': 4619, 'sneezy': 4620, 'soap': 4621, 'sober': 4622, 'sociopath': 4623, 'socks': 4624, 'sofa': 4625, 'soft': 4626, 'solo': 4627, 'solutions': 4628, 'sooooo': 4629, 'sorrow': 4630, 'souls': 4631, 'southbeach': 4632, 'spaces': 4633, 'speechless': 4634, 'spirituality': 4635, 'stages': 4636, 'stands': 4637, 'starring': 4638, 'starwars': 4639, 'statements': 4640, 'staups': 4641, 'steve': 4642, 'stl': 4643, 'stock': 4644, 'stood': 4645, 'str': 4646, 'stranger': 4647, 'strawberry': 4648, 'stroller': 4649, 'stumble': 4650, 'subject': 4651, 'subtitles': 4652, 'sucked': 4653, 'suits': 4654, 'sunbathing': 4655, 'sunsets': 4656, 'suppoive': 4657, 'surgery': 4658, 'surrounded': 4659, 'sweat': 4660, 'sweetie': 4661, 'swiftly': 4662, 'taco': 4663, 'talked': 4664, 'tarot': 4665, 'tax': 4666, 'teamwork': 4667, 'techno': 4668, 'teddy': 4669, 'tennessee': 4670, 'tennis': 4671, 'terrific': 4672, 'tha': 4673, 'thai': 4674, 'thalaivaa': 4675, 'thebest': 4676, 'thevoice': 4677, 'thi': 4678, 'thinkbigsundaywithmarsha': 4679, 'thousand': 4680, 'tight': 4681, 'tolerance': 4682, 'tomato': 4683, 'torn': 4684, 'toxic': 4685, 'tradition': 4686, 'tragedies': 4687, 'trainhard': 4688, 'transformed': 4689, 'traveler': 4690, 'triggered': 4691, 'tropical': 4692, 'trusted': 4693, 'truths': 4694, 'tunes': 4695, 'types': 4696, 'ukraineblog': 4697, 'um': 4698, 'unchanged': 4699, 'unite': 4700, 'unity': 4701, 'universe': 4702, 'unlimited': 4703, 'unpresidented': 4704, 'unwanted': 4705, 'ups': 4706, 'utterly': 4707, 'va': 4708, 'vacations': 4709, 'verse': 4710, 'vibrant': 4711, 'victoria': 4712, 'viral': 4713, 'virgin': 4714, 'virginia': 4715, 'vividsydney': 4716, 'voices': 4717, 'volatile': 4718, 'wakeuppeopl3': 4719, 'walls': 4720, 'wannabe': 4721, 'wanting': 4722, 'warcraftmovie': 4723, 'warrior': 4724, 'washington': 4725, 'watermelon': 4726, 'weapons': 4727, 'webcammodel': 4728, 'weekly': 4729, 'wenger': 4730, 'wheel': 4731, 'whitegenocide': 4732, 'whiteprivilege': 4733, 'whoa': 4734, 'whores': 4735, 'willow': 4736, 'witch': 4737, 'wohy': 4738, 'xmas': 4739, 'ye': 4740, 'yen': 4741, 'yesterdays': 4742, 'yyc': 4743, 'zara': 4744, 'zealand': 4745, '03': 4746, '04': 4747, '06': 4748, '0806': 4749, '1': 4750, '10th': 4751, '13th': 4752, '1500': 4753, '15th': 4754, '17th': 4755, '1q': 4756, '2000': 4757, '2013': 4758, '25th': 4759, '26th': 4760, '2k': 4761, '2years': 4762, '34': 4763, '37': 4764, '4': 4765, '42': 4766, '43': 4767, '48': 4768, '4wd': 4769, '600': 4770, '63': 4771, '64': 4772, '642': 4773, '6pcs': 4774, '75': 4775, '85': 4776, '899': 4777, '90th': 4778, 'a': 4779, 'aampe': 4780, 'abba': 4781, 'abc': 4782, 'abundance': 4783, 'abusive': 4784, 'accent': 4785, 'accused': 4786, 'achievements': 4787, 'acknowledge': 4788, 'acted': 4789, 'activists': 4790, 'adding': 4791, 'adds': 4792, 'advance': 4793, 'affair': 4794, 'afghan': 4795, 'aftr': 4796, 'airlines': 4797, 'al': 4798, 'alabama': 4799, 'alarms': 4800, 'allen': 4801, 'allies': 4802, 'alligators': 4803, 'allows': 4804, 'allsmiles': 4805, 'alternative': 4806, 'amazed': 4807, 'amo': 4808, 'amour': 4809, 'amreading': 4810, 'ana': 4811, 'anc': 4812, 'ancient': 4813, 'andor': 4814, 'anniversaries': 4815, 'antiaging': 4816, 'antiblackness': 4817, 'ape': 4818, 'apologies': 4819, 'appearing': 4820, 'appears': 4821, 'appreciated': 4822, 'appropriate': 4823, 'approx': 4824, 'arabs': 4825, 'archery': 4826, 'areas': 4827, 'arguing': 4828, 'arrival': 4829, 'aruba': 4830, 'asap': 4831, 'ascot': 4832, 'aside': 4833, 'asses': 4834, 'assessment': 4835, 'athlete': 4836, 'athletes': 4837, 'auction': 4838, 'australian': 4839, 'avocado': 4840, 'awareness': 4841, 'awasome': 4842, 'awkward': 4843, 'az': 4844, 'backlash': 4845, 'badge': 4846, 'badminton': 4847, 'bailey': 4848, 'bake': 4849, 'baker': 4850, 'balanced': 4851, 'balloon': 4852, 'bambi': 4853, 'bands': 4854, 'bang': 4855, 'banks': 4856, 'barry': 4857, 'bash': 4858, 'basket': 4859, 'bathroom': 4860, 'bbh': 4861, 'bdayspl': 4862, 'bean': 4863, 'bearlovestravel': 4864, 'beast': 4865, 'beaten': 4866, 'beauties': 4867, 'beginner': 4868, 'bei': 4869, 'belfast': 4870, 'believed': 4871, 'belong': 4872, 'bi': 4873, 'bible': 4874, 'biden': 4875, 'bigly': 4876, 'bihay': 4877, 'bihdayboy': 4878, 'bihdaykazuki': 4879, 'biher': 4880, 'biking': 4881, 'bingewatching': 4882, 'blackpool': 4883, 'blacktwitter': 4884, 'blaming': 4885, 'blissful': 4886, 'blocks': 4887, 'blooms': 4888, 'blunt': 4889, 'bnz': 4890, 'bo3': 4891, 'boj': 4892, 'bold': 4893, 'booking': 4894, 'bookstagram': 4895, 'boot': 4896, 'booty': 4897, 'bora': 4898, 'bordercollie': 4899, 'bother': 4900, 'bounce': 4901, 'bowl': 4902, 'bracelets': 4903, 'brainwashed': 4904, 'breathing': 4905, 'bremain': 4906, 'brentwood': 4907, 'brick': 4908, 'bridesmaid': 4909, 'bridetobe': 4910, 'brief': 4911, 'bristol': 4912, 'brokenhea': 4913, 'brokenquotes': 4914, 'bros': 4915, 'brows': 4916, 'bubbly': 4917, 'bulgaria': 4918, 'bullies': 4919, 'bully': 4920, 'buried': 4921, 'businesses': 4922, 'buttons': 4923, 'bw': 4924, 'caer': 4925, 'calico': 4926, 'camps': 4927, 'canadiangp': 4928, 'cannabis': 4929, 'caoons': 4930, 'capable': 4931, 'captured': 4932, 'careful': 4933, 'carol': 4934, 'carving': 4935, 'casino': 4936, 'casual': 4937, 'catlover': 4938, 'cbs': 4939, 'celebrities': 4940, 'cell': 4941, 'challenged': 4942, 'channels': 4943, 'characters': 4944, 'charleston': 4945, 'charlotte': 4946, 'cheating': 4947, 'checkout': 4948, 'chennai': 4949, 'cherish': 4950, 'cheryl': 4951, 'chickens': 4952, 'childish': 4953, 'childs': 4954, 'chillin': 4955, 'chills': 4956, 'chronicpain': 4957, 'cincinnati': 4958, 'circumstances': 4959, 'cities': 4960, 'clannad': 4961, 'clap': 4962, 'cleaneating': 4963, 'cleared': 4964, 'closest': 4965, 'closet': 4966, 'cloudchaser': 4967, 'coal': 4968, 'cologne': 4969, 'colorado': 4970, 'comeonengland': 4971, 'compare': 4972, 'compassion': 4973, 'compilation': 4974, 'complaining': 4975, 'concern': 4976, 'conditioning': 4977, 'conduct': 4978, 'conman': 4979, 'connecticut': 4980, 'considered': 4981, 'constitutional': 4982, 'contagious': 4983, 'continued': 4984, 'conversion': 4985, 'cop': 4986, 'cope': 4987, 'cops': 4988, 'covers': 4989, 'cowboy': 4990, 'craftbeer': 4991, 'crash': 4992, 'craving': 4993, 'creator': 4994, 'credibility': 4995, 'crib': 4996, 'crochet': 4997, 'crystals': 4998, 'cupcakes': 4999, 'curb': 5000, 'cure': 5001, 'curly': 5002, 'cycle': 5003, 'czech': 5004, 'daddies': 5005, 'daisy': 5006, 'dances': 5007, 'daniel': 5008, 'dave': 5009, 'davis': 5010, 'daze': 5011, 'dealing': 5012, 'dec': 5013, 'deck': 5014, 'declared': 5015, 'decorative': 5016, 'decors': 5017, 'defeat': 5018, 'defeated': 5019, 'defend': 5020, 'define': 5021, 'delegates': 5022, 'deleted': 5023, 'deliver': 5024, 'delusional': 5025, 'dementeddonny': 5026, 'democraticpay': 5027, 'democrats': 5028, 'den': 5029, 'denounce': 5030, 'dentist': 5031, 'deplorable': 5032, 'desk': 5033, 'despise': 5034, 'destroying': 5035, 'detroit': 5036, 'developer': 5037, 'devil': 5038, 'devops': 5039, 'dicks': 5040, 'dictionary': 5041, 'digital': 5042, 'dining': 5043, 'dinosaur': 5044, 'dishonest': 5045, 'dismantle': 5046, 'disneys': 5047, 'disorder': 5048, 'disrespectful': 5049, 'distress': 5050, 'district': 5051, 'dnc': 5052, 'doggie': 5053, 'donthecon': 5054, 'doodle': 5055, 'doom': 5056, 'dorm': 5057, 'downloaded': 5058, 'drag': 5059, 'dragged': 5060, 'dramatic': 5061, 'draymond': 5062, 'dresses': 5063, 'dressing': 5064, 'drives': 5065, 'drumpf': 5066, 'drums': 5067, 'dumptrump': 5068, 'duper': 5069, 'dutch': 5070, 'eagles': 5071, 'earp': 5072, 'eatclean': 5073, 'eats': 5074, 'economic': 5075, 'eek': 5076, 'el': 5077, 'elected': 5078, 'elections': 5079, 'electric': 5080, 'electro': 5081, 'eli': 5082, 'elizabeth': 5083, 'em2016': 5084, 'emotionally': 5085, 'empire': 5086, 'employed': 5087, 'encourage': 5088, 'endless': 5089, 'endorsement': 5090, 'energetic': 5091, 'enter': 5092, 'episode13': 5093, 'equally': 5094, 'equals': 5095, 'equate': 5096, 'era': 5097, 'essex': 5098, 'eva': 5099, 'everythings': 5100, 'excess': 5101, 'exit': 5102, 'expand': 5103, 'experiencing': 5104, 'exposed': 5105, 'expressing': 5106, 'expression': 5107, 'extinction': 5108, 'eyebrow': 5109, 'facial': 5110, 'facility': 5111, 'facing': 5112, 'fallen': 5113, 'fallschurch': 5114, 'familia': 5115, 'familytrip': 5116, 'farm': 5117, 'fatherday': 5118, 'fathersday2016': 5119, 'fathersdayquotes': 5120, 'fatloss': 5121, 'faves': 5122, 'favs': 5123, 'fearing': 5124, 'feelin': 5125, 'fees': 5126, 'felling': 5127, 'fest': 5128, 'fetish': 5129, 'fianc': 5130, 'figgity': 5131, 'fighter': 5132, 'filibuster': 5133, 'finds': 5134, 'finland': 5135, 'firefly': 5136, 'fireworks': 5137, 'firsttime': 5138, 'fishing': 5139, 'fitting': 5140, 'flame': 5141, 'flashbackfriday': 5142, 'flourishing': 5143, 'focusing': 5144, 'folding': 5145, 'fomc': 5146, 'fooled': 5147, 'footage': 5148, 'forecast': 5149, 'foreign': 5150, 'forum': 5151, 'freddie': 5152, 'freespirit': 5153, 'freetime': 5154, 'freshsta': 5155, 'fries': 5156, 'frozen': 5157, 'fucks': 5158, 'fuming': 5159, 'funtimes': 5160, 'furry': 5161, 'gained': 5162, 'gameplay': 5163, 'gamergirl': 5164, 'gameshow': 5165, 'garbage': 5166, 'gardens': 5167, 'gawa': 5168, 'gd': 5169, 'gear': 5170, 'gem': 5171, 'georgia': 5172, 'german': 5173, 'gimme': 5174, 'girltime': 5175, 'goa': 5176, 'godbless': 5177, 'godisgood': 5178, 'goin': 5179, 'goldenretriever': 5180, 'goodbook': 5181, 'gosh': 5182, 'gov': 5183, 'governments': 5184, 'governor': 5185, 'gp': 5186, 'graceful': 5187, 'graders': 5188, 'greek': 5189, 'greg': 5190, 'grew': 5191, 'grins': 5192, 'grounded': 5193, 'grove': 5194, 'grows': 5195, 'growthwithhubspot': 5196, 'grp': 5197, 'grunge': 5198, 'gud': 5199, 'gum': 5200, 'gunna': 5201, 'gurl': 5202, 'had': 5203, 'halfway': 5204, 'handbag': 5205, 'hap': 5206, 'happ': 5207, 'hardest': 5208, 'harmonious': 5209, 'haul': 5210, 'hav': 5211, 'havefun': 5212, 'hawaiian': 5213, 'hc': 5214, 'headache': 5215, 'headline': 5216, 'heads': 5217, 'heavenly': 5218, 'heck': 5219, 'hedgehog': 5220, 'heels': 5221, 'hehe': 5222, 'henry': 5223, 'hepburn': 5224, 'heros': 5225, 'hiding': 5226, 'highly': 5227, 'highs': 5228, 'hijacked': 5229, 'hiking': 5230, 'hint': 5231, 'historic': 5232, 'ho': 5233, 'holes': 5234, 'homegrown': 5235, 'homesweethome': 5236, 'homies': 5237, 'homophobe': 5238, 'honour': 5239, 'hoo': 5240, 'hoodies': 5241, 'hosting': 5242, 'hosts': 5243, 'hudson': 5244, 'hung': 5245, 'hunger': 5246, 'hunter': 5247, 'ibar': 5248, 'icons': 5249, 'icymi': 5250, 'igersbnw': 5251, 'ii': 5252, 'illinois': 5253, 'impo': 5254, 'impossible': 5255, 'incest': 5256, 'independenceday': 5257, 'inflation': 5258, 'innocence': 5259, 'innovation': 5260, 'insta': 5261, 'instaboy': 5262, 'instafollow': 5263, 'instafun': 5264, 'instagramers': 5265, 'installation': 5266, 'instatraveling': 5267, 'intellectual': 5268, 'intelligent': 5269, 'interiordesign': 5270, 'intro': 5271, 'introduce': 5272, 'ios10': 5273, 'iphoneonly': 5274, 'iq': 5275, 'iraq': 5276, 'irony': 5277, 'irrelevant': 5278, 'isaac': 5279, 'islamist': 5280, 'isolated': 5281, 'item': 5282, 'itunes': 5283, 'jacob': 5284, 'jaitley': 5285, 'jakaa': 5286, 'jcpenny': 5287, 'jeep': 5288, 'jeepmafia': 5289, 'jocoxmp': 5290, 'johnson': 5291, 'joshua': 5292, 'jp': 5293, 'jr': 5294, 'junior': 5295, 'justinb': 5296, 'kareem': 5297, 'karma': 5298, 'ke': 5299, 'kickstaer': 5300, 'kik': 5301, 'kinds': 5302, 'ko': 5303, 'koala': 5304, 'koreans': 5305, 'kro': 5306, 'kudos': 5307, 'lane': 5308, 'languages': 5309, 'larry': 5310, 'lasted': 5311, 'latino': 5312, 'latte': 5313, 'lauren': 5314, 'lbj': 5315, 'lecture': 5316, 'leftist': 5317, 'legacy': 5318, 'legally': 5319, 'legs': 5320, 'leicester': 5321, 'lemonade': 5322, 'leo': 5323, 'letters': 5324, 'lfc': 5325, 'lgbti': 5326, 'lightweight': 5327, 'likescam': 5328, 'lingerie': 5329, 'lite': 5330, 'loaded': 5331, 'lobby': 5332, 'lola': 5333, 'lollipop': 5334, 'longest': 5335, 'loses': 5336, 'loudly': 5337, 'loveher': 5338, 'loveme': 5339, 'loveofmylife': 5340, 'lower': 5341, 'lowest': 5342, 'luckygirl': 5343, 'lyric': 5344, 'maam': 5345, 'maccosmetics': 5346, 'maialas': 5347, 'maine': 5348, 'makemoney': 5349, 'malaysia': 5350, 'males': 5351, 'managing': 5352, 'manor': 5353, 'marble': 5354, 'marcus': 5355, 'marks': 5356, 'marseille': 5357, 'marx': 5358, 'massachusetts': 5359, 'mast': 5360, 'maternity': 5361, 'math': 5362, 'mayor': 5363, 'mcflurry': 5364, 'meaning': 5365, 'meetings': 5366, 'melania': 5367, 'membership': 5368, 'mentions': 5369, 'mentor': 5370, 'meow': 5371, 'mercedes': 5372, 'mere': 5373, 'merely': 5374, 'messed': 5375, 'microsoft': 5376, 'midnight': 5377, 'mighty': 5378, 'migraine': 5379, 'milano': 5380, 'mile': 5381, 'millennials': 5382, 'milo': 5383, 'minorities': 5384, 'mis': 5385, 'misogynistic': 5386, 'mk': 5387, 'mmm': 5388, 'models': 5389, 'modi': 5390, 'momtips': 5391, 'monetary': 5392, 'monster': 5393, 'montana': 5394, 'monthsary': 5395, 'montreal': 5396, 'moose': 5397, 'morn': 5398, 'mornin': 5399, 'motivational': 5400, 'moto': 5401, 'mourning': 5402, 'msm': 5403, 'mtb': 5404, 'mummy': 5405, 'munich': 5406, 'murders': 5407, 'muscles': 5408, 'museum': 5409, 'mybihday': 5410, 'mydubai': 5411, 'mytraining': 5412, 'myworld': 5413, 'nab': 5414, 'naoyuki': 5415, 'nascar': 5416, 'natsu': 5417, 'navy': 5418, 'nbafinals2016': 5419, 'nbc': 5420, 'nemo': 5421, 'nevergiveup': 5422, 'newday': 5423, 'newyearseve': 5424, 'newyorkcity': 5425, 'nhl': 5426, 'nhs': 5427, 'nightmare': 5428, 'nigth': 5429, 'nintendo': 5430, 'nomnomnom': 5431, 'nonton': 5432, 'norfolk': 5433, 'norman': 5434, 'nothappy': 5435, 'noticing': 5436, 'notmypres': 5437, 'nov': 5438, 'nudist': 5439, 'nxt': 5440, 'occur': 5441, 'oceans': 5442, 'offensive': 5443, 'offline': 5444, 'ohwell': 5445, 'oitnbchat': 5446, 'oitnbseason4': 5447, 'ojb': 5448, 'oldest': 5449, 'oldschool': 5450, 'omar': 5451, 'omfg': 5452, 'omw': 5453, 'onda': 5454, 'ongoing': 5455, 'onto': 5456, 'ooh': 5457, 'opener': 5458, 'openly': 5459, 'opens': 5460, 'oppa': 5461, 'optimist': 5462, 'options': 5463, 'orangeisthenewblack': 5464, 'orders': 5465, 'ottawa': 5466, 'outlook': 5467, 'owl': 5468, 'p2': 5469, 'packaging': 5470, 'paicipate': 5471, 'paicularly': 5472, 'palace': 5473, 'palm': 5474, 'pampered': 5475, 'panel': 5476, 'panic': 5477, 'papers': 5478, 'parade': 5479, 'paranormal': 5480, 'parliament': 5481, 'password': 5482, 'pastor': 5483, 'pastry': 5484, 'paths': 5485, 'patiently': 5486, 'patio': 5487, 'pays': 5488, 'pearly': 5489, 'pedophilia': 5490, 'pepper': 5491, 'perfection': 5492, 'perfume': 5493, 'perry': 5494, 'persons': 5495, 'peru': 5496, 'pharrellwilliams': 5497, 'phd': 5498, 'philadelphia': 5499, 'phuket': 5500, 'pieces': 5501, 'pineapple': 5502, 'pineapples': 5503, 'pipe': 5504, 'pissedoff': 5505, 'pjs': 5506, 'platform': 5507, 'playground': 5508, 'plot': 5509, 'pole': 5510, 'policebrutality': 5511, 'pond': 5512, 'poop': 5513, 'popcorn': 5514, 'pose': 5515, 'positivethinking': 5516, 'prayed': 5517, 'preparation': 5518, 'prepared': 5519, 'presence': 5520, 'presented': 5521, 'pressure': 5522, 'prez': 5523, 'printed': 5524, 'prob': 5525, 'probe': 5526, 'professions': 5527, 'progressive': 5528, 'propey': 5529, 'prosperity': 5530, 'provide': 5531, 'provides': 5532, 'psychological': 5533, 'pt': 5534, 'publishing': 5535, 'pueorico': 5536, 'punishment': 5537, 'punjabi': 5538, 'qualified': 5539, 'raging': 5540, 'rahulgandhi': 5541, 'rainforest': 5542, 'rainyday': 5543, 'ramsey': 5544, 'rates': 5545, 'rave': 5546, 'reaching': 5547, 'reader': 5548, 'recall': 5549, 'receiving': 5550, 'recognizing': 5551, 'records': 5552, 'recovers': 5553, 'redhead': 5554, 'redlips': 5555, 'ref': 5556, 'reflection': 5557, 'regular': 5558, 'related': 5559, 'releasing': 5560, 'relieved': 5561, 'remembering': 5562, 'renaissance': 5563, 'rental': 5564, 'repoed': 5565, 'repostapp': 5566, 'resignation': 5567, 'retirement': 5568, 'reveals': 5569, 'revenge': 5570, 'reviews': 5571, 'rewards': 5572, 'rice': 5573, 'rick': 5574, 'riclswtravelbook': 5575, 'rifle': 5576, 'rio': 5577, 'ripantonyelchin': 5578, 'risks': 5579, 'rite': 5580, 'roads': 5581, 'robin': 5582, 'rocks': 5583, 'rohingya': 5584, 'rope': 5585, 'rotterdam': 5586, 'rottweiler': 5587, 'sabon': 5588, 'sacrifice': 5589, 'sarah': 5590, 'saving': 5591, 'scam': 5592, 'scenery': 5593, 'scout': 5594, 'screens': 5595, 'scrubs': 5596, 'searching': 5597, 'seasonal': 5598, 'secure': 5599, 'seed': 5600, 'segment': 5601, 'select': 5602, 'senator': 5603, 'sensitive': 5604, 'separate': 5605, 'server': 5606, 'serves': 5607, 'seth': 5608, 'settle': 5609, 'seven': 5610, 'several': 5611, 'sewing': 5612, 'sexism': 5613, 'sexuality': 5614, 'shady': 5615, 'shakur': 5616, 'shattered': 5617, 'shave': 5618, 'shaved': 5619, 'sheboutit': 5620, 'sheffield': 5621, 'shell': 5622, 'shideism': 5623, 'shiless': 5624, 'shill': 5625, 'shipped': 5626, 'shipping': 5627, 'showcase': 5628, 'shown': 5629, 'similar': 5630, 'singlelife': 5631, 'sketchbook': 5632, 'slander': 5633, 'slave': 5634, 'slimmingworld': 5635, 'smaller': 5636, 'smilemore': 5637, 'sneaks': 5638, 'snug': 5639, 'software': 5640, 'solidarity': 5641, 'solution': 5642, 'somerset': 5643, 'sophie': 5644, 'sore': 5645, 'soros': 5646, 'sorrynotsorry': 5647, 'soulmate': 5648, 'soulmates': 5649, 'sounding': 5650, 'southern': 5651, 'sp': 5652, 'sparks': 5653, 'speaker': 5654, 'specials': 5655, 'spectacle': 5656, 'speeches': 5657, 'spiderman': 5658, 'spike': 5659, 'spinning': 5660, 'spoiled': 5661, 'spoilt': 5662, 'spokesperson': 5663, 'spotlight': 5664, 'spree': 5665, 'springfield': 5666, 'squirrel': 5667, 'starek': 5668, 'status': 5669, 'staystrong': 5670, 'steam': 5671, 'stem': 5672, 'stepping': 5673, 'stereotype': 5674, 'stopping': 5675, 'stoptheviolence': 5676, 'stores': 5677, 'streaming': 5678, 'streets': 5679, 'strip': 5680, 'strolling': 5681, 'struggling': 5682, 'suarezs': 5683, 'subs': 5684, 'suffering': 5685, 'sugar': 5686, 'suicides': 5687, 'sums': 5688, 'sundaymood': 5689, 'suppose': 5690, 'surf': 5691, 'surprises': 5692, 'sustainable': 5693, 'suv': 5694, 'swastika': 5695, 'sync': 5696, 'syria': 5697, 'systems': 5698, 'tablet': 5699, 'tackling': 5700, 'tacloban': 5701, 'tacos': 5702, 'tactics': 5703, 'taiwan': 5704, 'tanhai': 5705, 'tap': 5706, 'tasty': 5707, 'taxpayers': 5708, 'tds': 5709, 'te': 5710, 'teammates': 5711, 'teamshide': 5712, 'tease': 5713, 'teaser': 5714, 'technical': 5715, 'teenagers': 5716, 'teleprompter': 5717, 'temecula': 5718, 'temporary': 5719, 'tenerife': 5720, 'tent': 5721, 'terrified': 5722, 'thankfulthursday': 5723, 'thanking': 5724, 'thanx': 5725, 'thoughtful': 5726, 'thread': 5727, 'thrive': 5728, 'tht': 5729, 'thugs': 5730, 'tie': 5731, 'tiffany': 5732, 'tim': 5733, 'tlot': 5734, 'toe': 5735, 'toes': 5736, 'toilet': 5737, 'tomhiddleston': 5738, 'tommorow': 5739, 'ton': 5740, 'toptweeters': 5741, 'tourism': 5742, 'toward': 5743, 'tracks': 5744, 'tracy': 5745, 'traditional': 5746, 'trail': 5747, 'trainers': 5748, 'tranny': 5749, 'travelled': 5750, 'traveltuesday': 5751, 'trouble': 5752, 'trucks': 5753, 'truelove': 5754, 'trumptrain': 5755, 'trumpuniversity': 5756, 'truthful': 5757, 'tu': 5758, 'tummy': 5759, 'turnup': 5760, 'tym': 5761, 'udta': 5762, 'ukip': 5763, 'uks': 5764, 'ukulele': 5765, 'unemployment': 5766, 'unfair': 5767, 'unfit': 5768, 'unicorn': 5769, 'unleashed': 5770, 'updating': 5771, 'upgrade': 5772, 'uploading': 5773, 'upsetting': 5774, 'upsideofflorida': 5775, 'usb': 5776, 'users': 5777, 'vacay': 5778, 'vaccines': 5779, 'van': 5780, 'vandals': 5781, 'various': 5782, 'vegetables': 5783, 'veteran': 5784, 'veterans': 5785, 'viernes': 5786, 'vietnam': 5787, 'vino': 5788, 'vital': 5789, 'vocal': 5790, 'vocals': 5791, 'voltaire': 5792, 'volunteer': 5793, 'volunteering': 5794, 'voteleave': 5795, 'wage': 5796, 'wakes': 5797, 'wakow': 5798, 'walks': 5799, 'wallet': 5800, 'wallpaper': 5801, 'wanderlust': 5802, 'ward': 5803, 'wardrobe': 5804, 'warns': 5805, 'wasting': 5806, 'watson': 5807, 'wave': 5808, 'wcw': 5809, 'weaker': 5810, 'weddinganniversary': 5811, 'weddingdress': 5812, 'welsh': 5813, 'wendy': 5814, 'whack': 5815, 'wholesome': 5816, 'wid': 5817, 'wigs': 5818, 'wilson': 5819, 'winnipeg': 5820, 'winterfashion': 5821, 'wisconsin': 5822, 'woah': 5823, 'wohless': 5824, 'womans': 5825, 'womenempowerment': 5826, 'wondering': 5827, 'wordpress': 5828, 'worldginday': 5829, 'worldoceansday': 5830, 'worries': 5831, 'wrestling': 5832, 'writerslife': 5833, 'wud': 5834, 'xenophobic': 5835, 'xians': 5836, 'yass': 5837, 'yayyyy': 5838, 'yell': 5839, 'zionazis': 5840, 'zionism': 5841, '061116': 5842, '0612': 5843, '08': 5844, '100000': 5845, '101': 5846, '10alltypespos': 5847, '10k': 5848, '110': 5849, '1200': 5850, '12000': 5851, '123': 5852, '1299': 5853, '12th': 5854, '140': 5855, '150': 5856, '1999': 5857, '1day': 5858, '2': 5859, '2010': 5860, '201617': 5861, '2018': 5862, '24th': 5863, '26': 5864, '28th': 5865, '2a': 5866, '2days': 5867, '2k16': 5868, '2nite': 5869, '2the': 5870, '2yearold': 5871, '300': 5872, '33': 5873, '36': 5874, '38': 5875, '40th': 5876, '44': 5877, '46': 5878, '47pm': 5879, '4k': 5880, '4o4o4': 5881, '4u': 5882, '5000': 5883, '50islamicinfo': 5884, '51': 5885, '52': 5886, '55': 5887, '59': 5888, '5k': 5889, '6': 5890, '60s': 5891, '69': 5892, '700': 5893, '73': 5894, '77': 5895, '799': 5896, '7s': 5897, '7th': 5898, '80s': 5899, '87': 5900, '9': 5901, '90s': 5902, '999': 5903, '99c99p': 5904, 'ab': 5905, 'ableg': 5906, 'aboutlastnight': 5907, 'absence': 5908, 'abstract': 5909, 'abstracta': 5910, 'ac': 5911, 'acab': 5912, 'acc': 5913, 'accepting': 5914, 'accidentally': 5915, 'accountable': 5916, 'accusations': 5917, 'achieved': 5918, 'aching': 5919, 'acoustic': 5920, 'acquainted': 5921, 'acquired': 5922, 'activist': 5923, 'acu': 5924, 'additional': 5925, 'adopted': 5926, 'adrienne': 5927, 'adultery': 5928, 'affects': 5929, 'afghanistan': 5930, 'africans': 5931, 'agencies': 5932, 'agr': 5933, 'ahfodtour': 5934, 'ahh': 5935, 'aicles': 5936, 'aims': 5937, 'aired': 5938, 'aj': 5939, 'ako': 5940, 'akrotiri': 5941, 'aladdin': 5942, 'albums': 5943, 'aldub47thweeksary': 5944, 'alhamdulillah': 5945, 'alice': 5946, 'alicia': 5947, 'alkalamba': 5948, 'allday': 5949, 'alleged': 5950, 'alllivesmatter': 5951, 'alohafriday': 5952, 'alongside': 5953, 'alot': 5954, 'alt': 5955, 'alternatively': 5956, 'amanda': 5957, 'amazingpeople': 5958, 'amber': 5959, 'ambition': 5960, 'amid': 5961, 'amigos': 5962, 'amjoy': 5963, 'amongst': 5964, 'amounts': 5965, 'ampamp': 5966, 'ampfamilies': 5967, 'angst': 5968, 'angus': 5969, 'animated': 5970, 'annefrank': 5971, 'announcements': 5972, 'anonymous': 5973, 'antisemite': 5974, 'antisemitic': 5975, 'antonio': 5976, 'anytime': 5977, 'anz': 5978, 'ap': 5979, 'apament': 5980, 'apaments': 5981, 'apologize': 5982, 'appearance': 5983, 'apples': 5984, 'application': 5985, 'appreciating': 5986, 'appt': 5987, 'aquarium': 5988, 'arab': 5989, 'arff2016': 5990, 'argue': 5991, 'aria': 5992, 'arrest': 5993, 'arrested': 5994, 'arrests': 5995, 'arrives': 5996, 'asf': 5997, 'ash': 5998, 'asianladyboy': 5999, 'asshat': 6000, 'astonished': 6001, 'athena': 6002, 'atl': 6003, 'attacking': 6004, 'attest': 6005, 'attract': 6006, 'attracted': 6007, 'audio': 6008, 'authorities': 6009, 'autism': 6010, 'automatic': 6011, 'autumn': 6012, 'aworks': 6013, 'awwww': 6014, 'babyboy': 6015, 'babys': 6016, 'badass': 6017, 'badday': 6018, 'baked': 6019, 'bakery': 6020, 'balakrishna': 6021, 'bankruptcy': 6022, 'barack': 6023, 'barber': 6024, 'barbiets93': 6025, 'barney': 6026, 'bars': 6027, 'bashing': 6028, 'bats': 6029, 'battlefield': 6030, 'battlefield1': 6031, 'bbw': 6032, 'bck': 6033, 'bd': 6034, 'beachday': 6035, 'beatz': 6036, 'beau': 6037, 'became': 6038, 'beeroclock': 6039, 'begs': 6040, 'behaving': 6041, 'beinlife': 6042, 'beirut': 6043, 'belated': 6044, 'belgian': 6045, 'belize': 6046, 'belongs': 6047, 'bent': 6048, 'bernies': 6049, 'berniesanders': 6050, 'besides': 6051, 'bestday': 6052, 'bestfriendsday': 6053, 'bestrong': 6054, 'bethechange': 6055, 'bethesda': 6056, 'betterlife': 6057, 'bewitched': 6058, 'bhagwantmann': 6059, 'bhakt': 6060, 'bicycle': 6061, 'bieber': 6062, 'bigbang': 6063, 'bigday': 6064, 'bigender': 6065, 'bigoted': 6066, 'bihdaycake': 6067, 'bihdayweekend': 6068, 'bin': 6069, 'biological': 6070, 'bisexual': 6071, 'bites': 6072, 'bits': 6073, 'blackampwhite': 6074, 'blackheathstandard': 6075, 'blah': 6076, 'blatantly': 6077, 'bleed': 6078, 'blend': 6079, 'blocking': 6080, 'blogginggals': 6081, 'blond': 6082, 'blondie': 6083, 'bloom': 6084, 'blu': 6085, 'bluegrass': 6086, 'bluelivesmatter': 6087, 'blueskies': 6088, 'boanoite': 6089, 'boarding': 6090, 'boards': 6091, 'bobby': 6092, 'boe': 6093, 'bogotadc': 6094, 'bombing': 6095, 'bonjour': 6096, 'boogie': 6097, 'boomerang': 6098, 'boost': 6099, 'booze': 6100, 'bopanna': 6101, 'bothered': 6102, 'botox': 6103, 'boxes': 6104, 'boycottdelta': 6105, 'boyfriends': 6106, 'bracelet': 6107, 'branch': 6108, 'braves': 6109, 'brazilian': 6110, 'breakingnews': 6111, 'breezy': 6112, 'bretagne': 6113, 'bribes': 6114, 'brides': 6115, 'broadcast': 6116, 'brochure': 6117, 'brooks': 6118, 'browns': 6119, 'brutal': 6120, 'bucks': 6121, 'buddies': 6122, 'builders': 6123, 'buildings': 6124, 'bullet': 6125, 'bum': 6126, 'bump': 6127, 'bunnies': 6128, 'burns': 6129, 'burnt': 6130, 'burst': 6131, 'bury': 6132, 'bush': 6133, 'businessoppounity': 6134, 'bust': 6135, 'busty': 6136, 'buyer': 6137, 'cabin': 6138, 'calories': 6139, 'canal': 6140, 'canceled': 6141, 'candid': 6142, 'cannotwait': 6143, 'cantsleep': 6144, 'cantstopsmiling': 6145, 'canvas': 6146, 'capetown': 6147, 'capitalism': 6148, 'cappuccino': 6149, 'caption': 6150, 'capture': 6151, 'cardinals': 6152, 'cardio': 6153, 'careers': 6154, 'carefree': 6155, 'carmineryderrr': 6156, 'carolina': 6157, 'carpet': 6158, 'carwash': 6159, 'cassie': 6160, 'castle': 6161, 'catsoftwitter': 6162, 'catstagram': 6163, 'cavsnation': 6164, 'cbd': 6165, 'cecily': 6166, 'cedarrapids': 6167, 'celebrates': 6168, 'celebs': 6169, 'celinedion': 6170, 'central': 6171, 'champ': 6172, 'champion': 6173, 'chanel': 6174, 'chaplin': 6175, 'charlie': 6176, 'chaser': 6177, 'chasing': 6178, 'chats': 6179, 'cheaper': 6180, 'cheated': 6181, 'chest': 6182, 'chester': 6183, 'chihuahua': 6184, 'childfree': 6185, 'childrensbook': 6186, 'christmaseve': 6187, 'churchill': 6188, 'cirquedusoleil': 6189, 'claimed': 6190, 'clash': 6191, 'classism': 6192, 'cleanse': 6193, 'clever': 6194, 'clicking': 6195, 'climate': 6196, 'climatechange': 6197, 'climbing': 6198, 'cmon': 6199, 'coaches': 6200, 'coc': 6201, 'cocoa': 6202, 'coconut': 6203, 'cod': 6204, 'coke': 6205, 'colbe': 6206, 'collab': 6207, 'colleagues': 6208, 'collected': 6209, 'coloring': 6210, 'commerzbank': 6211, 'commitment': 6212, 'communal': 6213, 'communities': 6214, 'commute': 6215, 'competing': 6216, 'complicated': 6217, 'comprehensive': 6218, 'concerns': 6219, 'condemn': 6220, 'condemned': 6221, 'conferences': 6222, 'confetti': 6223, 'conjuring2': 6224, 'conscious': 6225, 'conservatives': 6226, 'consider': 6227, 'conspiracy': 6228, 'cont': 6229, 'contented': 6230, 'contestant': 6231, 'contour': 6232, 'contracts': 6233, 'contribute': 6234, 'conveible': 6235, 'conversation': 6236, 'converse': 6237, 'convince': 6238, 'copaamerica': 6239, 'core': 6240, 'cornwall': 6241, 'corrupt': 6242, 'costarica': 6243, 'costume': 6244, 'cotd': 6245, 'county': 6246, 'couture': 6247, 'cowards': 6248, 'coworkers': 6249, 'cracks': 6250, 'crain': 6251, 'crappy': 6252, 'craziestpeoples': 6253, 'craziness': 6254, 'creamy': 6255, 'credentials': 6256, 'creepy': 6257, 'critical': 6258, 'criticize': 6259, 'critics': 6260, 'cro': 6261, 'crude': 6262, 'crushed': 6263, 'cruz': 6264, 'css': 6265, 'cubs': 6266, 'cud': 6267, 'cult': 6268, 'cultures': 6269, 'cunt': 6270, 'cuppa': 6271, 'cups': 6272, 'curiosity': 6273, 'cushions': 6274, 'customised': 6275, 'cyclists': 6276, 'dairy': 6277, 'damage': 6278, 'damaged': 6279, 'danbury': 6280, 'dang': 6281, 'dank': 6282, 'dapper': 6283, 'darker': 6284, 'darkest': 6285, 'darkknight': 6286, 'day1': 6287, 'dazzling': 6288, 'deadliest': 6289, 'deadpool': 6290, 'dearest': 6291, 'dearly': 6292, 'dears': 6293, 'decade': 6294, 'decides': 6295, 'deciding': 6296, 'declare': 6297, 'decorating': 6298, 'dedicating': 6299, 'dee': 6300, 'deepest': 6301, 'default': 6302, 'defending': 6303, 'definitions': 6304, 'delegaterevolt': 6305, 'denied': 6306, 'depaures': 6307, 'deplorables': 6308, 'depp': 6309, 'derogatory': 6310, 'deserved': 6311, 'designer': 6312, 'desses': 6313, 'destress': 6314, 'detour': 6315, 'devastating': 6316, 'develop': 6317, 'devoted': 6318, 'dex': 6319, 'dhoni': 6320, 'dia': 6321, 'diabetes': 6322, 'dictator': 6323, 'dictatorial': 6324, 'dignity': 6325, 'dimples': 6326, 'din': 6327, 'dip': 6328, 'diploma': 6329, 'directed': 6330, 'disappointing': 6331, 'disco': 6332, 'discount': 6333, 'disgust': 6334, 'dislike': 6335, 'dismiss': 6336, 'dispose': 6337, 'diss': 6338, 'divisive': 6339, 'divulgaoeparceria': 6340, 'djlife': 6341, 'dl2016': 6342, 'dnt': 6343, 'doc': 6344, 'dogslife': 6345, 'dogwalking': 6346, 'domain': 6347, 'donated': 6348, 'doublewin': 6349, 'dovish': 6350, 'doyou': 6351, 'dragonfly': 6352, 'dragons': 6353, 'drags': 6354, 'dread': 6355, 'dreamteam': 6356, 'dri': 6357, 'driven': 6358, 'drugaddicts': 6359, 'drum': 6360, 'dt': 6361, 'dub': 6362, 'duke': 6363, 'dukhi': 6364, 'duncan': 6365, 'dustin': 6366, 'dvr': 6367, 'dynamic': 6368, 'dysfunction': 6369, 'dystopian': 6370, 'ea': 6371, 'ear': 6372, 'ears': 6373, 'eco': 6374, 'ecuador': 6375, 'edition': 6376, 'editorial': 6377, 'educational': 6378, 'effects': 6379, 'effos': 6380, 'egg': 6381, 'eia': 6382, 'elder': 6383, 'elderly': 6384, 'electricity': 6385, 'elementary': 6386, 'elements': 6387, 'eliminate': 6388, 'elitist': 6389, 'ellen': 6390, 'elliot': 6391, 'embarrassing': 6392, 'employee': 6393, 'enchanting': 6394, 'endof2016': 6395, 'endorses': 6396, 'engage': 6397, 'engaging': 6398, 'engerland': 6399, 'engineers': 6400, 'enjoyinglife': 6401, 'entering': 6402, 'enthusiasm': 6403, 'entirely': 6404, 'entitled': 6405, 'entrance': 6406, 'entrepreneurs': 6407, 'equestrian': 6408, 'escaped': 6409, 'esco': 6410, 'esp': 6411, 'est': 6412, 'estate': 6413, 'eugenics': 6414, 'euro16': 6415, 'evans': 6416, 'evolution': 6417, 'excellence': 6418, 'exci': 6419, 'excitem': 6420, 'existed': 6421, 'exists': 6422, 'expat': 6423, 'expecting': 6424, 'experiment': 6425, 'explosions': 6426, 'expos': 6427, 'extended': 6428, 'extends': 6429, 'external': 6430, 'extreme': 6431, 'eyebrows': 6432, 'facepalm': 6433, 'faded': 6434, 'familiar': 6435, 'fanboy': 6436, 'fandom': 6437, 'fare': 6438, 'farming': 6439, 'fashiondesign': 6440, 'fashionista': 6441, 'fathersdaymessage': 6442, 'favor': 6443, 'favored': 6444, 'fck': 6445, 'fd': 6446, 'fearless': 6447, 'feast': 6448, 'featuring': 6449, 'fedup': 6450, 'felicidad': 6451, 'felicidade': 6452, 'fellas': 6453, 'felony': 6454, 'festivals': 6455, 'feta': 6456, 'fewer': 6457, 'fiestar': 6458, 'fighters': 6459, 'filling': 6460, 'filmmaking': 6461, 'fing': 6462, 'firenze': 6463, 'firing': 6464, 'firstworldproblems': 6465, 'fk': 6466, 'flats': 6467, 'flavor': 6468, 'flightofalifetime': 6469, 'flood': 6470, 'flooding': 6471, 'flow': 6472, 'flowerlove': 6473, 'flyers': 6474, 'fm': 6475, 'fn': 6476, 'followusoninstagram': 6477, 'foodblogger': 6478, 'foodies': 6479, 'foodstagram': 6480, 'footy': 6481, 'forces': 6482, 'forgivers': 6483, 'format': 6484, 'formentera': 6485, 'forming': 6486, 'foxnews': 6487, 'fp': 6488, 'fra': 6489, 'frame': 6490, 'francisco': 6491, 'frankly': 6492, 'frarou': 6493, 'frenchie': 6494, 'fridayfun': 6495, 'fridge': 6496, 'fruits': 6497, 'fuckmylife': 6498, 'fucktrump': 6499, 'fuelled': 6500, 'fulfilled': 6501, 'fund': 6502, 'funimate': 6503, 'funnehanever': 6504, 'funtime': 6505, 'furbabies': 6506, 'furious': 6507, 'fuss': 6508, 'futureofwork': 6509, 'ga': 6510, 'gahbrooks': 6511, 'game6': 6512, 'gamesofthrones': 6513, 'gap': 6514, 'gary': 6515, 'gayguy': 6516, 'gazal': 6517, 'gbpjpy': 6518, 'gentleman': 6519, 'gentlemen': 6520, 'geography': 6521, 'ger': 6522, 'gettin': 6523, 'gettingfit': 6524, 'ghuggi': 6525, 'giddy': 6526, 'gifted': 6527, 'girlsholiday': 6528, 'glamorous': 6529, 'glamorousiam': 6530, 'glee': 6531, 'glory': 6532, 'glover': 6533, 'gloves': 6534, 'glowing': 6535, 'glutenfree': 6536, 'gmr': 6537, 'gn': 6538, 'goats': 6539, 'goldenstatewarriors': 6540, 'gomez': 6541, 'goodvibesonly': 6542, 'goody': 6543, 'gplay': 6544, 'grabbing': 6545, 'graco': 6546, 'grad': 6547, 'grandad': 6548, 'grandparents': 6549, 'graphic': 6550, 'greater': 6551, 'greatness': 6552, 'greatquotes': 6553, 'greet': 6554, 'grieve': 6555, 'grilled': 6556, 'grimmies': 6557, 'grin': 6558, 'grooming': 6559, 'growingup': 6560, 'growthhacking': 6561, 'grumpy': 6562, 'grumpycat': 6563, 'gtgtgt': 6564, 'gtlt': 6565, 'guard': 6566, 'guest': 6567, 'gummy': 6568, 'gunman': 6569, 'guru': 6570, 'gutless': 6571, 'gweh': 6572, 'gymlife': 6573, 'hacked': 6574, 'haford': 6575, 'hahahaha': 6576, 'hai': 6577, 'haiku': 6578, 'handled': 6579, 'happyholidays': 6580, 'harbaugh': 6581, 'hardworking': 6582, 'harrypotter': 6583, 'hatecrime': 6584, 'hatespeech': 6585, 'hats': 6586, 'hbo': 6587, 'headphones': 6588, 'heals': 6589, 'healthylife': 6590, 'healthylifestyle': 6591, 'heh': 6592, 'helpless': 6593, 'helpme': 6594, 'henderson': 6595, 'heyy': 6596, 'highfashion': 6597, 'highfive': 6598, 'highlycontested': 6599, 'highschool': 6600, 'hijab': 6601, 'hijacking': 6602, 'hike': 6603, 'hillaryclinton': 6604, 'hindu': 6605, 'hippy': 6606, 'hire': 6607, 'historical': 6608, 'historyancient': 6609, 'hm': 6610, 'hmm': 6611, 'holland': 6612, 'holly': 6613, 'hols': 6614, 'homedecor': 6615, 'homeless': 6616, 'homeowners': 6617, 'homesick': 6618, 'homework': 6619, 'homosexuality': 6620, 'honesty': 6621, 'honeymoon': 6622, 'hoodie': 6623, 'hooker': 6624, 'hooray': 6625, 'hopping': 6626, 'hormone': 6627, 'horribly': 6628, 'housemusic': 6629, 'hsp': 6630, 'html': 6631, 'humiliated': 6632, 'hunt': 6633, 'hutchings': 6634, 'hyper': 6635, 'hypocrisy': 6636, 'hypocrite': 6637, 'ian': 6638, 'ibooks': 6639, 'iced': 6640, 'icon': 6641, 'iftaar': 6642, 'iftar': 6643, 'il': 6644, 'illusion': 6645, 'iloveit': 6646, 'ily': 6647, 'immediately': 6648, 'imperialism': 6649, 'impress': 6650, 'incense': 6651, 'incite': 6652, 'incompetent': 6653, 'indecent': 6654, 'indiegame': 6655, 'inevitable': 6656, 'inferior': 6657, 'influence': 6658, 'inhumane': 6659, 'inked': 6660, 'inmates': 6661, 'instaa': 6662, 'instaday': 6663, 'instafashion': 6664, 'instagirl': 6665, 'instantly': 6666, 'instatags4likes': 6667, 'instatravel': 6668, 'institution': 6669, 'instrumental': 6670, 'insult': 6671, 'integrity': 6672, 'intense': 6673, 'intention': 6674, 'interests': 6675, 'internetmarketing': 6676, 'internship': 6677, 'inwoo': 6678, 'ipsy': 6679, 'iran': 6680, 'irresponsible': 6681, 'isle': 6682, 'ita': 6683, 'itsmybihday': 6684, 'ivy': 6685, 'jackblair': 6686, 'jacket': 6687, 'jackie': 6688, 'jammin': 6689, 'jar': 6690, 'jay': 6691, 'jeans': 6692, 'jeepnation': 6693, 'jehovah': 6694, 'joined': 6695, 'joint': 6696, 'jordan': 6697, 'josh': 6698, 'journal': 6699, 'journalists': 6700, 'juiceplus': 6701, 'jumper': 6702, 'junaricrm': 6703, 'june16': 6704, 'juneteenth': 6705, 'justification': 6706, 'justinbieber': 6707, 'justme': 6708, 'kansas': 6709, 'kanyewest': 6710, 'keen': 6711, 'keisha': 6712, 'kennel': 6713, 'kenya': 6714, 'kerala': 6715, 'keshis': 6716, 'keynote': 6717, 'keywest': 6718, 'kia': 6719, 'kiev': 6720, 'killers': 6721, 'kimbo': 6722, 'knees': 6723, 'knock': 6724, 'knossos': 6725, 'knowledgeable': 6726, 'kong': 6727, 'kylie': 6728, 'kyrie': 6729, 'label': 6730, 'labels': 6731, 'lacking': 6732, 'lad': 6733, 'laid': 6734, 'lancaster': 6735, 'landholding': 6736, 'landing': 6737, 'lapride': 6738, 'lastnight': 6739, 'latenights': 6740, 'latin': 6741, 'latvia': 6742, 'laughable': 6743, 'laura': 6744, 'laying': 6745, 'leaf': 6746, 'legendary': 6747, 'legends': 6748, 'lemans': 6749, 'lemons': 6750, 'lessismore': 6751, 'letsplay': 6752, 'lettuce': 6753, 'leukemia': 6754, 'li': 6755, 'libey': 6756, 'libtards': 6757, 'lichfield': 6758, 'lifeisbeautiful': 6759, 'lifetime': 6760, 'lightning': 6761, 'likeit': 6762, 'liking': 6763, 'limiting': 6764, 'liners': 6765, 'linkedin': 6766, 'linstagram': 6767, 'linzy': 6768, 'liquid': 6769, 'liquor': 6770, 'lisa': 6771, 'litter': 6772, 'littlemix': 6773, 'liv': 6774, 'liveandletlive': 6775, 'livemusic': 6776, 'livinglife': 6777, 'liye': 6778, 'lmaoo': 6779, 'load': 6780, 'loading': 6781, 'lobster': 6782, 'lois': 6783, 'lolol': 6784, 'losses': 6785, 'lovebts': 6786, 'loveconquershate': 6787, 'lovehim': 6788, 'lovelovelove': 6789, 'lovemeinstagoodfollowphotooftheday': 6790, 'lovemyfamily': 6791, 'lovequotes': 6792, 'lovewins': 6793, 'lovin': 6794, 'lowenergy': 6795, 'ludicrous': 6796, 'luggage': 6797, 'macklemore': 6798, 'macos': 6799, 'madly': 6800, 'mai': 6801, 'majestic': 6802, 'maker': 6803, 'makers': 6804, 'makeupaddict': 6805, 'malcolm': 6806, 'malign': 6807, 'mango': 6808, 'manipulation': 6809, 'manipur': 6810, 'manner': 6811, 'mansplaining': 6812, 'margarita': 6813, 'marina': 6814, 'marry': 6815, 'mary': 6816, 'maryland': 6817, 'masses': 6818, 'mater': 6819, 'materials': 6820, 'max': 6821, 'meaningless': 6822, 'measure': 6823, 'measured': 6824, 'measures': 6825, 'mebeforeyou': 6826, 'medal': 6827, 'medicine': 6828, 'medium': 6829, 'meds': 6830, 'mega': 6831, 'melee': 6832, 'melted': 6833, 'memorable': 6834, 'mentality': 6835, 'mentally': 6836, 'mentors': 6837, 'messing': 6838, 'metro': 6839, 'mic': 6840, 'michaels': 6841, 'microaggressions': 6842, 'micropoetry': 6843, 'milan': 6844, 'minime': 6845, 'minneapolis': 6846, 'minsk': 6847, 'miracle': 6848, 'misses': 6849, 'mitb': 6850, 'mob': 6851, 'moda': 6852, 'modes': 6853, 'mods': 6854, 'mogage': 6855, 'moi': 6856, 'mojo': 6857, 'momentum': 6858, 'moods': 6859, 'moody': 6860, 'morelove': 6861, 'morocco': 6862, 'mostly': 6863, 'motherfucker': 6864, 'motivationmonday': 6865, 'motive': 6866, 'moveon': 6867, 'ms': 6868, 'msgs': 6869, 'mtg': 6870, 'mua': 6871, 'mubarak': 6872, 'muchlove': 6873, 'muffins': 6874, 'muhammad': 6875, 'muller': 6876, 'multivitamins': 6877, 'mummersparade': 6878, 'mums': 6879, 'muna': 6880, 'murderer': 6881, 'musica': 6882, 'musicians': 6883, 'musicislife': 6884, 'myday': 6885, 'myfavorite': 6886, 'mygirl': 6887, 'myhappyplace': 6888, 'myhea': 6889, 'mykonos': 6890, 'myle': 6891, 'mymood': 6892, 'mystery': 6893, 'mytime': 6894, 'nail': 6895, 'nap': 6896, 'narcissist': 6897, 'naruto': 6898, 'nationalroseday': 6899, 'natureperfection': 6900, 'nbjc': 6901, 'nd': 6902, 'needa': 6903, 'needahug': 6904, 'neighbor': 6905, 'neighborhoods': 6906, 'neko': 6907, 'networking': 6908, 'neutral': 6909, 'neutrality': 6910, 'newbook': 6911, 'newcar': 6912, 'newhair': 6913, 'newly': 6914, 'newsletter': 6915, 'newstar': 6916, 'nextchapter': 6917, 'ni': 6918, 'nicest': 6919, 'nickjonas': 6920, 'niggers': 6921, 'nightshift': 6922, 'nikkei': 6923, 'nikki': 6924, 'nikon': 6925, 'nina': 6926, 'nitro': 6927, 'nj': 6928, 'no1': 6929, 'nohampton': 6930, 'nohate': 6931, 'noheast': 6932, 'nonracist': 6933, 'noon': 6934, 'noooo': 6935, 'norwich': 6936, 'notcool': 6937, 'notfunny': 6938, 'notifications': 6939, 'notreally': 6940, 'nuclear': 6941, 'numbers': 6942, 'nurse': 6943, 'nursing': 6944, 'nyman': 6945, 'nyt': 6946, 'objective': 6947, 'obs': 6948, 'obsession': 6949, 'obstruction': 6950, 'occasion': 6951, 'occurred': 6952, 'ocd': 6953, 'offended': 6954, 'offering': 6955, 'oi': 6956, 'oiler': 6957, 'oitnb4': 6958, 'omits': 6959, 'onion': 6960, 'onpoint': 6961, 'ontario': 6962, 'ontheredcarpet': 6963, 'oooh': 6964, 'ooooh': 6965, 'operation': 6966, 'opponents': 6967, 'oppose': 6968, 'oppounities': 6969, 'oral': 6970, 'oregon': 6971, 'orlandolove': 6972, 'oscarpistorius': 6973, 'oscarpretorious': 6974, 'outburst': 6975, 'outcome': 6976, 'outlets': 6977, 'overnight': 6978, 'owe': 6979, 'owlashop': 6980, 'ownership': 6981, 'oysters': 6982, 'pad': 6983, 'paddington': 6984, 'page3': 6985, 'paleo': 6986, 'palmbeach': 6987, 'pamper': 6988, 'panama': 6989, 'pancakes': 6990, 'paperback': 6991, 'par': 6992, 'paranoid': 6993, 'passes': 6994, 'passpo': 6995, 'pastel': 6996, 'pat': 6997, 'patch': 6998, 'patients': 6999, 'pattaya': 7000, 'pause': 7001, 'paycheck': 7002, 'payet': 7003, 'paypal': 7004, 'paz': 7005, 'pdf': 7006, 'pe': 7007, 'peak': 7008, 'peaked': 7009, 'peanut': 7010, 'peds': 7011, 'peepz': 7012, 'peonies': 7013, 'perma': 7014, 'personaldevelopment': 7015, 'personaltrainer': 7016, 'peter': 7017, 'phew': 7018, 'phoenix': 7019, 'photobooth': 7020, 'photograph': 7021, 'photographs': 7022, 'pin': 7023, 'pint': 7024, 'pisses': 7025, 'pity': 7026, 'pl': 7027, 'playstation': 7028, 'pllseason7': 7029, 'poc': 7030, 'poi': 7031, 'pointless': 7032, 'pokemonsunmoon': 7033, 'polaroid': 7034, 'polite': 7035, 'politically': 7036, 'polo': 7037, 'poolside': 7038, 'poorly': 7039, 'popped': 7040, 'posing': 7041, 'potatoes': 7042, 'pound': 7043, 'pov': 7044, 'powers': 7045, 'pple': 7046, 'ppt': 7047, 'prague': 7048, 'prank': 7049, 'precisely': 7050, 'preparations': 7051, 'presentations': 7052, 'presidentelect': 7053, 'pressed': 7054, 'presses': 7055, 'pressured': 7056, 'presumptive': 7057, 'prevail': 7058, 'preventable': 7059, 'previously': 7060, 'principles': 7061, 'prior': 7062, 'prizes': 7063, 'probs': 7064, 'producers': 7065, 'productions': 7066, 'professor': 7067, 'progressives': 7068, 'promised': 7069, 'proper': 7070, 'prospect': 7071, 'prostitutes': 7072, 'protected': 7073, 'protesters': 7074, 'protests': 7075, 'proudmom': 7076, 'proving': 7077, 'provoking': 7078, 'ps': 7079, 'psycho': 7080, 'ptsd': 7081, 'publicity': 7082, 'publicly': 7083, 'puglife': 7084, 'pugs': 7085, 'pundits': 7086, 'punished': 7087, 'punk': 7088, 'puppylove': 7089, 'pursue': 7090, 'python': 7091, 'qampa': 7092, 'quaer': 7093, 'que': 7094, 'quebec': 7095, 'quietly': 7096, 'quillen': 7097, 'quotesoftheday': 7098, 'radiohead': 7099, 'rainnyday': 7100, 'raising': 7101, 'ramadankareem': 7102, 'ramon': 7103, 'rappers': 7104, 'rare': 7105, 'raspberry': 7106, 'rating': 7107, 'ray': 7108, 'reaches': 7109, 'realised': 7110, 'realizing': 7111, 'rebellion': 7112, 'recharged': 7113, 'reddit': 7114, 'refreshing': 7115, 'refused': 7116, 'refuses': 7117, 'regards': 7118, 'relatives': 7119, 'relief': 7120, 'remake': 7121, 'remark': 7122, 'reminded': 7123, 'remotely': 7124, 'removal': 7125, 'removes': 7126, 'removing': 7127, 'renovation': 7128, 'renowned': 7129, 'rent': 7130, 'rep': 7131, 'repeating': 7132, 'replaced': 7133, 'replay': 7134, 'representation': 7135, 'required': 7136, 'rescued': 7137, 'researchers': 7138, 'residents': 7139, 'resolution': 7140, 'resolve': 7141, 'resolved': 7142, 'retailtherapy': 7143, 'reward': 7144, 'rhonda': 7145, 'richard': 7146, 'riddance': 7147, 'rifles': 7148, 'ringing': 7149, 'rising': 7150, 'riskoff': 7151, 'ritzcarlton': 7152, 'ro': 7153, 'robbie': 7154, 'rocking': 7155, 'rocknroll': 7156, 'roger': 7157, 'ron': 7158, 'rooting': 7159, 'rotten': 7160, 'roy': 7161, 'royalty': 7162, 'royaltyfreemusic': 7163, 'ruby': 7164, 'rus': 7165, 'rwnj': 7166, 'sach': 7167, 'sacramento': 7168, 'sailing': 7169, 'salt': 7170, 'salut': 7171, 'sample': 7172, 'samples': 7173, 'sandwich': 7174, 'sandyhook': 7175, 'satellite': 7176, 'saudi': 7177, 'sc': 7178, 'scenic': 7179, 'schoolofrock': 7180, 'sciencefiction': 7181, 'scientists': 7182, 'screening': 7183, 'scuba': 7184, 'sd': 7185, 'seal': 7186, 'sealed': 7187, 'season7': 7188, 'seasoned': 7189, 'seaworld': 7190, 'secondlife': 7191, 'section': 7192, 'seek': 7193, 'seeking': 7194, 'selection': 7195, 'selfesteem': 7196, 'selfhelp': 7197, 'selfrespect': 7198, 'sends': 7199, 'senior': 7200, 'sentiments': 7201, 'seoul': 7202, 'sephora': 7203, 'sequence': 7204, 'serenity': 7205, 'serving': 7206, 'setup': 7207, 'sew': 7208, 'sexualpredator': 7209, 'sh': 7210, 'shade': 7211, 'shadow': 7212, 'shakes': 7213, 'shares': 7214, 'sheets': 7215, 'shld': 7216, 'shocker': 7217, 'shoplocal': 7218, 'shopper': 7219, 'shops': 7220, 'shoulder': 7221, 'showcases': 7222, 'shuttle': 7223, 'si': 7224, 'signal': 7225, 'silve': 7226, 'simon': 7227, 'simplest': 7228, 'simplicity': 7229, 'sin': 7230, 'sisterhood': 7231, 'sixty': 7232, 'ski': 7233, 'skiing': 7234, 'skill': 7235, 'skilled': 7236, 'sl': 7237, 'slap': 7238, 'slaughter': 7239, 'slaughtering': 7240, 'sleepless': 7241, 'sliding': 7242, 'slim': 7243, 'slimmer': 7244, 'slip': 7245, 'smaccdub': 7246, 'smallbusiness': 7247, 'smashed': 7248, 'smdh': 7249, 'smileyface': 7250, 'smooth': 7251, 'smores': 7252, 'snacks': 7253, 'snd': 7254, 'sneaky': 7255, 'snooze': 7256, 'snowball': 7257, 'soa': 7258, 'soar': 7259, 'socialanxiety': 7260, 'socially': 7261, 'sole': 7262, 'someday': 7263, 'somethin': 7264, 'songwriter': 7265, 'soooooo': 7266, 'sour': 7267, 'spade': 7268, 'spagetti': 7269, 'spare': 7270, 'sparkle': 7271, 'species': 7272, 'spectrum': 7273, 'spew': 7274, 'spice': 7275, 'splendid': 7276, 'sponsoring': 7277, 'sponsors': 7278, 'spoonie': 7279, 'spoy': 7280, 'spray': 7281, 'stance': 7282, 'stanleycup': 7283, 'starlight': 7284, 'steady': 7285, 'steals': 7286, 'stepdad': 7287, 'stephcurrys': 7288, 'sterling': 7289, 'stickers': 7290, 'stockmusic': 7291, 'stole': 7292, 'stomach': 7293, 'stopgunviolence': 7294, 'stopthehate': 7295, 'storage': 7296, 'strategy': 7297, 'stressed': 7298, 'strike': 7299, 'stripes': 7300, 'strips': 7301, 'striving': 7302, 'stud': 7303, 'studies': 7304, 'stylehausboutique': 7305, 'stylistic': 7306, 'su': 7307, 'submitted': 7308, 'subtle': 7309, 'succubus': 7310, 'suffolk': 7311, 'suga': 7312, 'sum': 7313, 'summerfair': 7314, 'summers': 7315, 'sunderland': 7316, 'sunflowers': 7317, 'sunnies': 7318, 'superman': 7319, 'suppoed': 7320, 'suppression': 7321, 'surface': 7322, 'surprising': 7323, 'surround': 7324, 'surviving': 7325, 'survivor': 7326, 'suspect': 7327, 'suspected': 7328, 'swearing': 7329, 'sweater': 7330, 'sweets': 7331, 'swimsuit': 7332, 'swimwear': 7333, 'switch': 7334, 'switching': 7335, 'symptoms': 7336, 'tabarez': 7337, 'tackle': 7338, 'tags': 7339, 'tagsforlikebeautifulselfieg': 7340, 'tail': 7341, 'tank': 7342, 'tanned': 7343, 'tapas': 7344, 'tdl': 7345, 'tedatibm': 7346, 'tedtalks': 7347, 'tee': 7348, 'teletubbiesusa': 7349, 'temper': 7350, 'tense': 7351, 'tequila': 7352, 'testimony': 7353, 'texts': 7354, 'tgifriday': 7355, 'tgifridays': 7356, 'thanksgiving': 7357, 'thankyoustudio': 7358, 'thefosters': 7359, 'thegreatest': 7360, 'themusketeers': 7361, 'theron': 7362, 'thewiam2016': 7363, 'thieves': 7364, 'thin': 7365, 'thirst': 7366, 'threatening': 7367, 'threw': 7368, 'thrill': 7369, 'thriller': 7370, 'throat': 7371, 'thrones': 7372, 'throughout': 7373, 'thumb': 7374, 'thunder': 7375, 'ties': 7376, 'tin': 7377, 'titanic': 7378, 'titles': 7379, 'tits': 7380, 'tl': 7381, 'tmrw': 7382, 'tn': 7383, 'toa': 7384, 'toast': 7385, 'todo': 7386, 'togetherstronger': 7387, 'tolstoy': 7388, 'tongue': 7389, 'tools': 7390, 'toothbrush': 7391, 'top10': 7392, 'topics': 7393, 'tops': 7394, 'touched': 7395, 'tragically': 7396, 'trained': 7397, 'trains': 7398, 'trampled': 7399, 'trans': 7400, 'travelgram': 7401, 'travelingram': 7402, 'travis': 7403, 'treasure': 7404, 'treasures': 7405, 'trendy': 7406, 'trim': 7407, 'trims': 7408, 'trio': 7409, 'triste': 7410, 'trophy': 7411, 'trump2016': 7412, 'trumpism': 7413, 'trumpsamerica': 7414, 'trumpusa': 7415, 'tryna': 7416, 'tsa': 7417, 'tshis': 7418, 'tules': 7419, 'turner': 7420, 'tweegram': 7421, 'tweeps': 7422, 'twenty': 7423, 'twilight': 7424, 'twin': 7425, 'twinklatinboys': 7426, 'twist': 7427, 'twisted': 7428, 'tx': 7429, 'tyranny': 7430, 'uber': 7431, 'ultra': 7432, 'unacceptable': 7433, 'unbreakable': 7434, 'uncle': 7435, 'unextraordinary': 7436, 'uniform': 7437, 'unifying': 7438, 'uniteblue': 7439, 'unitedstates': 7440, 'unjust': 7441, 'unlikely': 7442, 'unmasking': 7443, 'unstable': 7444, 'unwavering': 7445, 'upcm': 7446, 'upcycled': 7447, 'ure': 7448, 'urself': 7449, 'usher': 7450, 'ut': 7451, 'vacuum': 7452, 'valentines': 7453, 'valid': 7454, 'valued': 7455, 'vape': 7456, 'vegetarian': 7457, 'veggie': 7458, 'vehicles': 7459, 'veiled': 7460, 'venezuela': 7461, 'venues': 7462, 'verified': 7463, 'versa': 7464, 'verysad': 7465, 'vets': 7466, 'videoclip': 7467, 'viewer': 7468, 'viewing': 7469, 'vincent': 7470, 'vindication': 7471, 'visited': 7472, 'vitiating': 7473, 'vitorr': 7474, 'void': 7475, 'volunteers': 7476, 'volunteersweek': 7477, 'vox': 7478, 'vscodaily': 7479, 'vscolove': 7480, 'vscoph': 7481, 'vscoturkey': 7482, 'wade': 7483, 'wah': 7484, 'wakeupamerica': 7485, 'wal': 7486, 'wallpapers': 7487, 'walt': 7488, 'wang': 7489, 'washed': 7490, 'washing': 7491, 'watchthisspace': 7492, 'watering': 7493, 'wayne': 7494, 'weakness': 7495, 'weareorlando': 7496, 'webdesign': 7497, 'webster': 7498, 'weddingday': 7499, 'weeksary': 7500, 'weighed': 7501, 'welfare': 7502, 'westend': 7503, 'westpac': 7504, 'whatajoke': 7505, 'whatsapp': 7506, 'whatson': 7507, 'wheels': 7508, 'whining': 7509, 'whiskey': 7510, 'whitesupremacist': 7511, 'wicked': 7512, 'wide': 7513, 'wifi': 7514, 'wil': 7515, 'wiley': 7516, 'winding': 7517, 'winelover': 7518, 'wines': 7519, 'winfrey': 7520, 'wip': 7521, 'wisely': 7522, 'wished': 7523, 'witnessing': 7524, 'woe': 7525, 'woes': 7526, 'wohwhile': 7527, 'woods': 7528, 'woori': 7529, 'worker': 7530, 'workshops': 7531, 'wounded': 7532, 'wounds': 7533, 'wrapped': 7534, 'wrapping': 7535, 'wreck': 7536, 'wtf2016': 7537, 'wypipo': 7538, 'xboxones': 7539, 'xo': 7540, 'yeahhh': 7541, 'yegfood': 7542, 'yesssss': 7543, 'yg': 7544, 'yogalove': 7545, 'yousuck': 7546, 'youthday2016': 7547, 'youuu': 7548, 'yul': 7549, 'yumy': 7550, 'yur': 7551, 'yyt': 7552, 'zaynmalik': 7553, 'zimbabwe': 7554, 'zombie': 7555, 'zoological': 7556, 'zootopia': 7557, 'zurich': 7558})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_vocab = len(TEXT.vocab)\n",
        "\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 50\n",
        "num_output_nodes = 1\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "#instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, TEXT.vocab, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)\n",
        "#architecture\n",
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Initialize the pretrained embedding\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.00001)\n",
        "criterion = nn.BCELoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbVrltdZ14pQ",
        "outputId": "5f0fb1a6-f2d3-4599-9244-1de7a6de2401"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(7559, 100)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (lstm): LSTM(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 877,601 trainable parameters\n",
            "torch.Size([7559, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy 1"
      ],
      "metadata": {
        "id": "iFJ_xmbG56n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "#set the model in training phase\n",
        "model.train()\n",
        "\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.00001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzgSs5A655-7",
        "outputId": "c91501b4-b920-49b4-eb63-0403b9cccae7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss tensor(0.4037)\n",
            "Acc tensor(0.9219)\n",
            "tensor([5.6725e-06, 9.1102e-04, 3.0967e-06, 3.9884e-06, 3.9884e-06, 2.7521e-05,\n",
            "        3.9884e-06, 1.9166e-06, 9.8267e-02, 1.1486e-03, 1.4945e-02, 1.5329e-05,\n",
            "        3.4614e-05, 1.5296e-06, 1.2075e-04, 4.0146e-03, 8.8695e-06, 2.8773e-05,\n",
            "        9.9980e-01, 1.6847e-06, 1.0145e-01, 7.4588e-02, 3.9884e-06, 5.0449e-05,\n",
            "        1.0808e-04, 3.9759e-06, 1.1167e-04, 1.4458e-05, 5.7012e-04, 2.2709e-06,\n",
            "        1.1033e-05, 1.5072e-05, 2.2221e-05, 7.0071e-06, 7.1465e-06, 5.5575e-05,\n",
            "        1.1353e-04, 8.2842e-05, 9.1041e-05, 1.0792e-05, 1.7982e-05, 1.1557e-03,\n",
            "        1.1216e-06, 3.3066e-06, 1.1796e-05, 1.3180e-04, 1.4799e-05, 1.1284e-06,\n",
            "        1.0294e-06, 3.9884e-06, 3.9884e-06, 5.2341e-06, 4.1567e-05, 2.1545e-03,\n",
            "        3.9293e-06, 2.5396e-06, 3.9884e-06, 7.7134e-06, 5.8068e-06, 2.5641e-06,\n",
            "        2.6783e-05, 4.5633e-03, 4.8740e-05, 3.1724e-04])\n",
            "Sum of predicted tensor(1.3052)\n",
            "Loss tensor(0.2235)\n",
            "Acc tensor(0.9688)\n",
            "tensor([4.2681e-05, 1.0730e-04, 1.5921e-06, 3.9488e-06, 3.9306e-06, 4.2415e-05,\n",
            "        1.2198e-01, 9.7895e-01, 6.2525e-02, 9.0643e-01, 4.2063e-05, 1.3666e-06,\n",
            "        3.9629e-05, 6.0929e-06, 2.8659e-06, 4.0722e-06, 3.8747e-05, 1.5867e-05,\n",
            "        1.0343e-05, 4.4113e-05, 6.0497e-05, 1.2321e-06, 1.9099e-04, 2.0262e-04,\n",
            "        1.5906e-05, 5.0294e-06, 2.1131e-06, 7.7334e-05, 2.8602e-05, 1.2201e-06,\n",
            "        2.3825e-06, 5.1017e-03, 7.0712e-06, 7.2586e-05, 1.4961e-03, 9.7226e-07,\n",
            "        7.9832e-06, 8.4919e-05, 2.4794e-04, 5.8393e-06, 3.9884e-06, 6.4643e-05,\n",
            "        1.7147e-03, 2.4337e-05, 4.4010e-05, 3.4564e-06, 3.9884e-06, 9.9983e-01,\n",
            "        1.3977e-05, 9.9983e-01, 2.3230e-06, 8.6424e-06, 1.0159e-06, 3.9884e-06,\n",
            "        8.2066e-04, 2.3574e-06, 6.7231e-05, 9.9995e-01, 4.4858e-05, 9.9983e-01,\n",
            "        7.6882e-01, 2.8544e-06, 1.1048e-03, 8.2662e-01])\n",
            "Sum of predicted tensor(7.6767)\n",
            "Loss tensor(0.3300)\n",
            "Acc tensor(0.9219)\n",
            "tensor([1.9065e-06, 9.9714e-01, 5.1536e-06, 5.5793e-06, 9.9947e-01, 2.5098e-02,\n",
            "        1.2133e-05, 1.5107e-05, 5.8716e-06, 5.9736e-06, 3.6290e-06, 1.0278e-05,\n",
            "        9.3900e-06, 1.5484e-06, 5.1688e-06, 1.5655e-06, 2.9673e-05, 1.4702e-04,\n",
            "        9.7371e-07, 4.2971e-06, 6.8154e-06, 3.5438e-05, 3.5436e-04, 1.5517e-06,\n",
            "        1.1033e-04, 3.0907e-06, 3.7065e-06, 1.0388e-03, 9.9954e-01, 9.9546e-01,\n",
            "        5.7870e-04, 9.4323e-01, 1.2415e-06, 1.2321e-06, 6.0850e-06, 1.2075e-02,\n",
            "        1.0213e-05, 2.2977e-05, 3.9639e-06, 1.4276e-05, 8.3146e-07, 3.6308e-06,\n",
            "        9.9947e-01, 1.6885e-04, 3.6039e-05, 9.9541e-06, 2.2395e-05, 5.5571e-05,\n",
            "        1.5590e-05, 9.3900e-06, 1.4575e-05, 3.1445e-06, 3.3255e-04, 2.0981e-06,\n",
            "        3.8040e-06, 4.8391e-06, 3.3567e-04, 7.9459e-06, 3.9349e-06, 9.2602e-05,\n",
            "        1.5683e-04, 1.0936e-05, 1.1723e-05, 5.7048e-05])\n",
            "Sum of predicted tensor(5.9753)\n",
            "Loss tensor(0.1298)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.7447e-06, 1.8839e-06, 5.8451e-05, 4.3464e-02, 3.1473e-01, 7.2174e-04,\n",
            "        1.4563e-06, 2.0391e-06, 2.0455e-04, 2.1522e-06, 1.3223e-05, 8.1777e-06,\n",
            "        3.6680e-04, 8.8913e-04, 1.1267e-05, 4.1691e-06, 5.9330e-06, 2.4751e-06,\n",
            "        8.8882e-06, 4.5242e-06, 1.5569e-05, 2.2357e-05, 9.9947e-01, 1.0966e-03,\n",
            "        9.8854e-01, 2.6562e-05, 4.4082e-06, 2.2346e-06, 9.9954e-01, 2.5821e-06,\n",
            "        1.5117e-04, 3.1976e-06, 9.2057e-06, 9.9901e-01, 4.4884e-06, 1.5349e-06,\n",
            "        3.7626e-06, 4.2636e-05, 4.5550e-05, 3.1473e-01, 7.5252e-06, 1.3350e-04,\n",
            "        2.2338e-05, 2.1312e-06, 1.3582e-05, 8.1920e-01, 3.3433e-02, 6.4568e-05,\n",
            "        1.9107e-05, 1.0402e-02, 1.6860e-04, 1.3048e-03, 4.6184e-06, 1.7229e-06,\n",
            "        4.6312e-05, 7.1812e-03, 2.5229e-01, 1.4967e-06, 9.2749e-06, 9.8671e-06,\n",
            "        9.7033e-06, 2.6562e-05, 1.3930e-03, 4.0291e-06])\n",
            "Sum of predicted tensor(5.7890)\n",
            "Loss tensor(0.1773)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.3619e-05, 2.2614e-03, 9.9548e-01, 4.8919e-06, 8.2617e-06, 2.4264e-06,\n",
            "        7.9834e-06, 5.8412e-06, 9.4323e-06, 2.8472e-04, 2.5617e-05, 4.5406e-05,\n",
            "        3.8494e-04, 3.6461e-05, 8.8220e-06, 8.7491e-06, 6.7438e-06, 1.5662e-05,\n",
            "        5.5323e-05, 2.7359e-06, 1.2321e-06, 3.1473e-01, 2.7154e-05, 1.9616e-05,\n",
            "        4.9137e-05, 1.1838e-04, 3.8900e-03, 3.9474e-05, 3.3074e-06, 2.0391e-06,\n",
            "        4.2972e-01, 1.6624e-04, 8.7991e-06, 2.7666e-03, 5.1607e-06, 5.8718e-06,\n",
            "        3.1083e-06, 4.2613e-06, 1.1339e-04, 1.5448e-02, 2.0197e-05, 9.9954e-01,\n",
            "        1.0246e-05, 2.4867e-06, 1.1389e-05, 1.4169e-06, 2.9588e-05, 1.1053e-05,\n",
            "        3.9639e-06, 2.2801e-05, 3.2711e-06, 2.4600e-06, 9.0067e-06, 9.9954e-01,\n",
            "        2.8725e-01, 5.1381e-06, 1.9385e-04, 9.9984e-01, 1.9320e-06, 2.0391e-06,\n",
            "        1.8721e-06, 9.9721e-01, 3.3953e-06, 1.0107e-03])\n",
            "Sum of predicted tensor(6.0505)\n",
            "Loss tensor(0.1245)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.6411e-07, 1.4249e-05, 1.8924e-05, 1.8568e-05, 1.5655e-06, 1.2985e-02,\n",
            "        3.2431e-05, 8.1628e-06, 3.2048e-05, 1.9342e-06, 2.9747e-05, 2.5965e-06,\n",
            "        3.2046e-06, 8.6554e-06, 6.9853e-06, 5.2645e-02, 1.1754e-03, 8.6592e-03,\n",
            "        7.2396e-06, 6.3462e-06, 2.7679e-01, 3.9270e-05, 8.1997e-01, 1.4979e-05,\n",
            "        9.9903e-01, 3.6290e-06, 1.0062e-05, 2.6020e-05, 1.1370e-03, 5.5337e-06,\n",
            "        7.2733e-05, 2.2981e-03, 2.2903e-06, 3.4450e-05, 2.6306e-06, 7.2747e-06,\n",
            "        2.6537e-04, 3.3973e-06, 6.6484e-04, 6.4942e-06, 3.9087e-06, 1.8463e-06,\n",
            "        2.5048e-05, 1.0250e-05, 1.2035e-06, 2.0514e-06, 2.1251e-03, 2.8756e-05,\n",
            "        9.6176e-04, 9.9954e-01, 1.3460e-06, 4.3117e-06, 5.7738e-06, 1.8881e-05,\n",
            "        3.2399e-06, 4.2321e-05, 6.3047e-05, 8.3863e-01, 4.0120e-06, 9.2467e-01,\n",
            "        1.9744e-05, 1.8931e-03, 3.1473e-01, 1.5450e-05])\n",
            "Sum of predicted tensor(5.2589)\n",
            "Loss tensor(0.3510)\n",
            "Acc tensor(0.9219)\n",
            "tensor([1.0250e-05, 9.4148e-06, 5.0757e-06, 1.3347e-05, 4.4948e-06, 6.7949e-06,\n",
            "        6.9007e-06, 6.3820e-01, 1.2769e-05, 2.2653e-05, 1.4594e-06, 6.9817e-06,\n",
            "        7.9114e-06, 5.8881e-03, 6.4888e-06, 5.0296e-06, 1.0211e-05, 1.7601e-05,\n",
            "        6.9737e-05, 9.9954e-01, 1.0910e-04, 1.4077e-06, 1.9065e-06, 1.1633e-05,\n",
            "        1.9065e-06, 8.7635e-06, 1.0392e-05, 6.2338e-06, 4.0393e-06, 7.8193e-06,\n",
            "        3.7550e-06, 4.5116e-06, 2.7047e-05, 1.1359e-06, 1.0663e-06, 8.8729e-04,\n",
            "        3.9639e-06, 2.6574e-05, 9.9917e-01, 9.9981e-01, 6.0269e-04, 1.1300e-04,\n",
            "        4.8176e-06, 4.2079e-06, 1.6271e-05, 5.1607e-06, 3.3255e-04, 7.4323e-06,\n",
            "        1.6842e-05, 1.2155e-05, 1.0598e-05, 9.9370e-01, 9.9912e-01, 1.2030e-06,\n",
            "        9.2647e-05, 3.2008e-03, 4.7687e-05, 4.1350e-01, 9.8743e-01, 8.9501e-01,\n",
            "        1.1937e-05, 2.0483e-02, 1.6130e-01, 1.7447e-06])\n",
            "Sum of predicted tensor(8.1190)\n",
            "Loss tensor(0.3950)\n",
            "Acc tensor(0.9062)\n",
            "tensor([6.1465e-05, 1.4583e-05, 5.6171e-06, 9.9797e-01, 2.4179e-03, 1.3205e-05,\n",
            "        2.0492e-05, 6.6127e-06, 8.2023e-06, 1.4400e-05, 1.9065e-06, 6.0103e-06,\n",
            "        1.2135e-05, 3.5483e-06, 1.1339e-04, 3.6550e-06, 3.5491e-05, 5.4275e-06,\n",
            "        9.9947e-01, 4.5038e-04, 1.6019e-06, 2.8325e-06, 1.3118e-05, 1.1545e-05,\n",
            "        7.1734e-05, 6.6523e-06, 1.2193e-04, 3.2853e-05, 5.1818e-04, 3.7364e-05,\n",
            "        5.8221e-06, 7.0549e-06, 1.4441e-05, 7.2406e-06, 5.8796e-06, 1.1815e-04,\n",
            "        4.2055e-05, 4.4520e-06, 3.5197e-06, 8.2862e-06, 3.3823e-06, 2.0687e-05,\n",
            "        9.9088e-05, 2.1935e-06, 4.9975e-06, 2.4393e-06, 4.2490e-06, 7.0950e-06,\n",
            "        5.7031e-06, 1.2582e-06, 1.0306e-05, 1.2298e-05, 2.3048e-06, 1.0969e-06,\n",
            "        2.0065e-06, 7.6858e-06, 3.9639e-06, 1.7155e-02, 4.1674e-03, 5.5168e-05,\n",
            "        2.4257e-06, 9.9954e-01, 5.0783e-06, 2.1066e-06])\n",
            "Sum of predicted tensor(3.0228)\n",
            "Loss tensor(0.0973)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.9923e-01, 5.3905e-06, 1.0834e-05, 3.3045e-05, 6.2859e-02, 6.7414e-05,\n",
            "        3.0020e-06, 7.9430e-05, 9.2387e-06, 3.7480e-06, 3.7489e-06, 2.2857e-02,\n",
            "        3.7854e-05, 6.7680e-05, 9.9954e-01, 9.9771e-01, 2.4986e-02, 1.5231e-06,\n",
            "        6.1494e-06, 3.8454e-06, 1.7475e-06, 1.2674e-06, 9.9266e-01, 1.3224e-05,\n",
            "        1.9722e-06, 9.9947e-01, 1.3420e-05, 6.1981e-04, 1.9825e-05, 9.9903e-01,\n",
            "        9.9954e-01, 6.3594e-06, 5.9430e-06, 9.9184e-01, 3.1505e-06, 2.4938e-05,\n",
            "        1.6143e-05, 7.9030e-06, 1.6061e-05, 3.5686e-04, 2.0506e-05, 9.9865e-01,\n",
            "        1.1461e-06, 2.1727e-06, 1.8556e-05, 2.3227e-04, 2.0861e-06, 1.5000e-05,\n",
            "        5.6463e-03, 1.2321e-06, 4.1125e-06, 7.1172e-06, 1.7739e-06, 6.4008e-06,\n",
            "        1.3350e-04, 3.2191e-02, 3.9639e-06, 1.5141e-05, 2.0177e-04, 8.7061e-06,\n",
            "        7.4753e-06, 4.6044e-05, 1.5045e-06, 8.8043e-06])\n",
            "Sum of predicted tensor(9.1284)\n",
            "Loss tensor(0.4085)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.2000e-04, 2.1323e-05, 6.0547e-06, 2.4398e-06, 4.8604e-06, 5.7347e-05,\n",
            "        9.9991e-01, 4.9913e-06, 1.6671e-06, 5.3094e-06, 5.5112e-02, 9.9988e-01,\n",
            "        7.4833e-06, 6.9404e-06, 4.5359e-06, 4.1061e-06, 9.9614e-01, 2.8372e-06,\n",
            "        4.2008e-05, 7.1842e-06, 9.9956e-01, 3.7979e-06, 1.1688e-06, 7.6953e-06,\n",
            "        1.0583e-05, 5.1548e-06, 6.9809e-06, 2.7358e-04, 1.2489e-04, 2.0264e-03,\n",
            "        3.3730e-06, 6.5853e-06, 5.4271e-06, 1.9498e-04, 5.4615e-03, 7.3505e-06,\n",
            "        9.9008e-05, 7.0457e-01, 6.8483e-05, 9.9663e-01, 6.7315e-05, 3.3941e-06,\n",
            "        1.8016e-06, 3.9172e-06, 6.7499e-05, 2.3081e-06, 4.9758e-06, 1.7230e-05,\n",
            "        4.4853e-06, 2.2523e-06, 3.3762e-06, 2.7702e-06, 9.2073e-06, 1.6158e-05,\n",
            "        6.8604e-06, 3.2013e-05, 2.3880e-06, 5.4175e-04, 3.4328e-06, 1.8782e-06,\n",
            "        4.0040e-05, 3.2670e-05, 1.1693e-03, 2.0994e-01])\n",
            "Sum of predicted tensor(5.9724)\n",
            "Loss tensor(0.1858)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.5161e-06, 2.8521e-05, 5.6312e-04, 4.6149e-05, 1.4918e-06, 2.3863e-06,\n",
            "        9.9959e-01, 9.9991e-01, 1.3555e-06, 1.0626e-05, 5.8158e-06, 5.1978e-06,\n",
            "        2.5680e-02, 1.5853e-05, 4.7473e-05, 3.0511e-05, 3.0545e-06, 2.8911e-05,\n",
            "        2.3784e-04, 1.5311e-05, 1.2012e-05, 7.6577e-01, 1.4509e-05, 4.6827e-06,\n",
            "        2.3880e-06, 3.7091e-06, 8.7981e-06, 3.7049e-06, 3.2534e-06, 9.3548e-02,\n",
            "        5.6036e-05, 3.2736e-06, 3.9254e-06, 4.2914e-06, 9.9959e-01, 1.3837e-06,\n",
            "        9.9407e-01, 1.0455e-06, 1.1898e-05, 5.6615e-06, 3.4328e-06, 1.2716e-05,\n",
            "        4.3464e-06, 9.9662e-01, 2.5838e-03, 8.5131e-06, 1.7505e-05, 2.0659e-06,\n",
            "        3.2545e-06, 9.9470e-01, 9.5577e-06, 9.9900e-01, 5.5058e-06, 2.1177e-06,\n",
            "        1.0440e-05, 1.7291e-02, 1.8250e-04, 6.8806e-06, 3.0036e-05, 2.0674e-05,\n",
            "        2.0001e-05, 6.8830e-04, 9.3994e-06, 3.9630e-05])\n",
            "Sum of predicted tensor(7.8906)\n",
            "Loss tensor(0.2798)\n",
            "Acc tensor(0.9531)\n",
            "tensor([5.6362e-06, 2.8239e-06, 9.0081e-06, 1.1137e-05, 3.8525e-06, 4.1255e-05,\n",
            "        2.2490e-06, 3.8763e-01, 1.1411e-05, 9.7035e-06, 2.1081e-06, 9.9921e-01,\n",
            "        3.0497e-04, 2.3880e-06, 1.0953e-04, 1.5319e-05, 1.0496e-05, 9.9888e-01,\n",
            "        1.0546e-05, 2.4372e-05, 2.8346e-06, 4.3308e-06, 1.8342e-02, 6.7123e-06,\n",
            "        9.8445e-06, 7.9751e-06, 6.3084e-06, 2.1128e-06, 1.4486e-06, 9.8308e-07,\n",
            "        9.9827e-01, 5.4588e-06, 9.8926e-01, 9.2523e-06, 4.4504e-01, 1.9700e-05,\n",
            "        6.6053e-06, 3.8916e-06, 6.8176e-02, 9.6821e-06, 9.9965e-01, 4.9662e-06,\n",
            "        1.1220e-04, 7.4286e-05, 2.2251e-06, 5.7925e-04, 1.1834e-05, 6.9242e-06,\n",
            "        4.9932e-06, 3.6274e-06, 1.4039e-05, 9.1580e-06, 1.1137e-05, 1.9683e-04,\n",
            "        1.6110e-06, 4.9554e-06, 4.6219e-05, 3.8938e-05, 1.6987e-05, 1.8831e-06,\n",
            "        9.1047e-06, 9.9995e-01, 1.1560e-05, 9.0782e-03])\n",
            "Sum of predicted tensor(6.9153)\n",
            "Loss tensor(0.2711)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9982e-01, 4.6008e-06, 2.6967e-06, 7.2737e-05, 1.3989e-05, 4.6798e-06,\n",
            "        9.9928e-01, 4.8830e-05, 3.3413e-06, 2.0933e-06, 8.1161e-06, 4.1251e-06,\n",
            "        1.2903e-05, 9.9742e-01, 1.7283e-06, 1.3690e-05, 1.8063e-05, 6.9885e-06,\n",
            "        8.2727e-04, 1.4564e-06, 1.1325e-06, 2.2710e-05, 1.2129e-06, 7.5156e-05,\n",
            "        2.7702e-06, 4.9655e-06, 1.5277e-04, 1.0035e-04, 1.2849e-04, 7.6284e-03,\n",
            "        7.6454e-04, 5.9251e-06, 1.7477e-06, 1.1329e-05, 9.9949e-01, 3.5184e-06,\n",
            "        9.7654e-06, 2.3880e-06, 1.5236e-06, 4.0339e-06, 9.9912e-01, 9.1007e-06,\n",
            "        9.9888e-01, 1.2484e-05, 8.8567e-05, 2.4169e-02, 1.1387e-05, 3.8818e-06,\n",
            "        1.7362e-06, 1.3650e-05, 6.6179e-02, 1.8153e-06, 2.3880e-06, 4.8645e-04,\n",
            "        1.1574e-06, 3.0148e-06, 7.4771e-06, 3.2515e-06, 1.1496e-02, 5.2907e-04,\n",
            "        1.4931e-04, 2.7666e-06, 5.6658e-06, 2.3880e-06])\n",
            "Sum of predicted tensor(6.1072)\n",
            "Loss tensor(0.6544)\n",
            "Acc tensor(0.9062)\n",
            "tensor([6.1877e-06, 3.7019e-03, 8.5603e-06, 7.6585e-06, 1.5642e-06, 4.6804e-05,\n",
            "        2.1910e-06, 6.0909e-04, 2.6077e-06, 8.7018e-01, 4.5359e-06, 1.9907e-06,\n",
            "        1.5895e-03, 1.1659e-06, 6.5842e-06, 6.0444e-04, 1.6688e-05, 1.1094e-06,\n",
            "        2.4789e-05, 2.2335e-05, 1.0597e-04, 1.0055e-05, 6.3404e-06, 7.8100e-05,\n",
            "        5.9862e-06, 5.7687e-06, 4.4769e-01, 5.5203e-06, 1.8854e-06, 3.5161e-06,\n",
            "        6.2445e-06, 3.1191e-06, 4.6489e-06, 3.6692e-02, 6.1997e-03, 2.1330e-05,\n",
            "        1.2571e-05, 5.0113e-06, 1.3664e-05, 3.5244e-06, 3.7517e-06, 1.2397e-05,\n",
            "        1.0122e-01, 7.9032e-05, 9.6314e-07, 1.0309e-05, 5.2319e-06, 7.0129e-04,\n",
            "        8.9447e-07, 8.1801e-04, 9.8639e-01, 1.9837e-05, 1.9348e-04, 3.7965e-01,\n",
            "        5.6918e-02, 2.0818e-05, 5.9864e-06, 4.3175e-06, 5.0010e-05, 1.2199e-05,\n",
            "        9.0287e-06, 7.7476e-04, 8.0814e-06, 1.6775e-05])\n",
            "Sum of predicted tensor(2.8946)\n",
            "Loss tensor(0.0971)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.1678e-06, 6.5777e-06, 1.4693e-05, 3.6328e-05, 8.5955e-05, 8.2661e-06,\n",
            "        2.8357e-06, 2.6275e-06, 5.3094e-06, 2.2772e-06, 6.0506e-06, 5.7071e-06,\n",
            "        9.3653e-06, 2.7241e-05, 6.2557e-04, 2.4580e-06, 1.9267e-03, 7.3156e-06,\n",
            "        1.3536e-06, 8.1798e-06, 2.7546e-06, 3.0229e-05, 6.1183e-06, 1.0372e-05,\n",
            "        8.8709e-06, 3.6901e-05, 3.8211e-05, 8.0024e-06, 4.9267e-05, 5.2935e-04,\n",
            "        2.9872e-05, 8.8747e-06, 1.9377e-06, 1.8854e-06, 2.2165e-05, 3.5796e-06,\n",
            "        4.7556e-06, 2.9201e-03, 1.0158e-05, 4.9455e-06, 1.4463e-02, 3.2903e-06,\n",
            "        5.2449e-06, 4.2062e-06, 2.7016e-05, 2.6672e-02, 6.0660e-06, 7.3377e-06,\n",
            "        2.8472e-06, 1.6266e-04, 1.6157e-06, 1.1005e-04, 1.5075e-05, 3.5387e-06,\n",
            "        3.1767e-06, 5.6454e-04, 5.8601e-06, 7.3332e-06, 3.4927e-05, 6.5013e-06,\n",
            "        3.4932e-01, 8.8421e-06, 7.6926e-03, 2.3880e-06])\n",
            "Sum of predicted tensor(0.4057)\n",
            "Loss tensor(0.0736)\n",
            "Acc tensor(0.9844)\n",
            "tensor([5.4618e-06, 9.9571e-06, 2.7034e-05, 1.2779e-05, 6.4534e-06, 2.7445e-03,\n",
            "        1.5741e-03, 9.9604e-01, 4.7133e-06, 2.3880e-06, 1.0227e-01, 9.9677e-01,\n",
            "        1.9977e-06, 8.0540e-06, 2.6856e-06, 1.7033e-03, 8.0505e-06, 4.3310e-06,\n",
            "        1.1137e-05, 5.3585e-05, 1.2423e-05, 5.6998e-05, 9.4905e-05, 8.9272e-03,\n",
            "        6.1677e-06, 5.8556e-05, 2.6579e-06, 9.9966e-01, 4.7182e-04, 3.0810e-05,\n",
            "        7.9667e-06, 4.2417e-06, 1.2849e-04, 2.8611e-06, 1.5489e-04, 2.0659e-06,\n",
            "        3.6423e-05, 2.1777e-06, 2.2001e-06, 1.1633e-04, 4.1362e-06, 2.1228e-06,\n",
            "        6.2514e-06, 5.7268e-01, 1.4603e-05, 3.4032e-06, 5.5918e-06, 1.6115e-04,\n",
            "        1.0179e-03, 9.9669e-01, 1.3049e-05, 7.9524e-06, 4.9435e-05, 6.7749e-06,\n",
            "        1.2971e-05, 6.2993e-06, 6.5994e-05, 1.1256e-05, 1.0383e-01, 5.4515e-06,\n",
            "        1.6551e-05, 7.8175e-05, 1.3241e-05, 5.2821e-06])\n",
            "Sum of predicted tensor(4.7857)\n",
            "Loss tensor(0.1984)\n",
            "Acc tensor(0.9531)\n",
            "tensor([7.7986e-06, 5.4865e-02, 2.6809e-05, 1.0635e-04, 1.9537e-05, 1.3806e-05,\n",
            "        9.2660e-01, 6.1762e-06, 5.6697e-06, 1.0090e-06, 8.9055e-01, 2.1516e-06,\n",
            "        3.3388e-05, 9.8555e-01, 1.2118e-06, 9.3699e-01, 2.7973e-02, 1.0987e-04,\n",
            "        1.4627e-05, 4.6520e-06, 1.1342e-06, 5.3683e-06, 1.0460e-05, 2.7786e-05,\n",
            "        1.3189e-05, 2.6567e-05, 2.6136e-06, 3.1094e-06, 1.1763e-03, 1.0626e-05,\n",
            "        6.0262e-06, 2.1093e-05, 6.4713e-05, 1.9084e-06, 9.9888e-01, 7.5148e-06,\n",
            "        9.9948e-01, 3.3658e-05, 5.9860e-06, 4.9076e-06, 2.5923e-06, 5.9455e-06,\n",
            "        3.9597e-04, 5.2448e-06, 1.7230e-05, 1.3371e-05, 9.9778e-01, 9.9888e-01,\n",
            "        1.2042e-05, 3.0764e-06, 1.0845e-05, 2.3275e-04, 2.0659e-06, 1.8404e-04,\n",
            "        6.7098e-06, 2.0672e-05, 1.2621e-06, 1.3405e-06, 2.4690e-06, 5.3713e-06,\n",
            "        1.1989e-06, 1.0865e-06, 3.4285e-06, 2.3880e-06])\n",
            "Sum of predicted tensor(7.8203)\n",
            "Loss tensor(0.1535)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.0924e-06, 4.8689e-02, 9.9985e-01, 3.2139e-06, 3.1922e-06, 1.5210e-02,\n",
            "        4.9549e-06, 9.2661e-06, 8.4771e-01, 1.0778e-03, 1.7277e-06, 2.1019e-04,\n",
            "        3.9562e-06, 7.4981e-06, 1.4987e-06, 4.5823e-06, 7.1306e-06, 3.5726e-01,\n",
            "        4.1761e-06, 1.5511e-06, 3.3858e-06, 3.8921e-06, 1.9200e-06, 1.4589e-04,\n",
            "        1.5355e-05, 8.4478e-05, 1.4536e-05, 3.6103e-05, 6.9490e-03, 1.1134e-06,\n",
            "        4.7681e-06, 2.4727e-05, 6.6535e-05, 2.3226e-06, 3.0806e-02, 1.3003e-06,\n",
            "        4.1143e-06, 3.3252e-06, 5.5366e-06, 3.4173e-06, 9.9972e-01, 2.5687e-05,\n",
            "        1.6064e-05, 3.1781e-04, 3.4049e-05, 9.9090e-06, 6.2869e-01, 1.1614e-06,\n",
            "        7.7981e-06, 8.4788e-06, 2.8507e-06, 8.3326e-06, 3.7137e-06, 2.2903e-05,\n",
            "        1.8562e-04, 5.4869e-06, 1.9330e-05, 2.0499e-05, 9.9989e-01, 2.1683e-05,\n",
            "        2.5978e-06, 9.7255e-07, 3.1509e-06, 2.7175e-06])\n",
            "Sum of predicted tensor(4.9373)\n",
            "Loss tensor(0.0999)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.2679e-05, 8.7330e-06, 3.6504e-02, 8.2534e-05, 1.5380e-06, 1.0419e-02,\n",
            "        1.5942e-04, 8.5386e-06, 1.8992e-06, 7.7879e-04, 2.5868e-06, 5.1142e-06,\n",
            "        1.9573e-03, 5.5652e-06, 6.3534e-06, 6.5422e-06, 8.6461e-07, 3.6510e-04,\n",
            "        4.3811e-06, 1.3881e-05, 3.0691e-05, 2.8260e-06, 2.6732e-06, 3.0826e-06,\n",
            "        9.0300e-06, 1.1450e-03, 4.0461e-06, 1.1416e-05, 3.3814e-06, 1.4435e-06,\n",
            "        2.0494e-06, 2.9533e-06, 4.1228e-06, 1.0635e-04, 1.4251e-04, 8.5573e-06,\n",
            "        7.5132e-06, 4.4444e-05, 4.5261e-06, 9.0197e-06, 2.5971e-06, 8.3444e-06,\n",
            "        1.5360e-06, 1.0513e-06, 8.2434e-06, 1.4906e-01, 2.4831e-06, 1.9905e-06,\n",
            "        1.4498e-02, 3.2048e-06, 2.3444e-06, 1.3361e-04, 2.3655e-05, 2.2973e-01,\n",
            "        1.9875e-06, 3.8548e-06, 5.2404e-06, 5.5717e-06, 5.2364e-06, 8.2843e-06,\n",
            "        1.0208e-05, 3.4680e-05, 9.9918e-01, 9.9818e-01])\n",
            "Sum of predicted tensor(2.4428)\n",
            "Loss tensor(0.2172)\n",
            "Acc tensor(0.9375)\n",
            "tensor([3.8703e-06, 3.6643e-06, 3.3252e-06, 4.3806e-06, 1.0719e-03, 3.7899e-06,\n",
            "        4.6345e-06, 4.5450e-05, 9.9951e-01, 4.3057e-04, 8.9692e-06, 3.8353e-06,\n",
            "        4.7998e-06, 1.9285e-06, 5.4718e-06, 1.1870e-06, 5.3203e-05, 9.1976e-06,\n",
            "        9.1001e-06, 5.1939e-06, 5.1684e-02, 1.8886e-06, 2.7535e-06, 4.7678e-05,\n",
            "        7.7704e-06, 3.8292e-06, 1.2604e-05, 2.6125e-06, 4.7931e-05, 1.7713e-05,\n",
            "        9.8526e-01, 3.7126e-05, 9.8910e-06, 1.1197e-05, 1.0934e-05, 2.7727e-06,\n",
            "        9.0607e-06, 2.6362e-06, 2.5413e-02, 9.1360e-03, 1.5534e-05, 5.2306e-03,\n",
            "        5.4287e-06, 3.0196e-03, 2.4362e-05, 4.3650e-06, 3.2896e-06, 1.4292e-02,\n",
            "        8.3606e-07, 1.6372e-04, 8.2884e-06, 1.6401e-03, 3.3833e-06, 1.1977e-06,\n",
            "        6.0988e-06, 7.3388e-01, 2.4178e-06, 5.8463e-06, 1.7958e-05, 4.5180e-02,\n",
            "        2.3405e-06, 7.1456e-01, 9.9828e-01, 2.1536e-05])\n",
            "Sum of predicted tensor(4.5893)\n",
            "Loss tensor(0.3921)\n",
            "Acc tensor(0.9219)\n",
            "tensor([9.9227e-01, 4.9754e-04, 1.4299e-06, 5.5963e-06, 7.9993e-06, 3.1713e-06,\n",
            "        9.9647e-05, 1.0475e-05, 9.9917e-01, 1.1730e-05, 1.8238e-05, 1.0154e-03,\n",
            "        4.1092e-06, 6.2347e-06, 1.4958e-06, 5.8875e-05, 9.5410e-01, 4.9184e-06,\n",
            "        1.3586e-05, 3.7573e-06, 6.3199e-03, 1.7684e-03, 5.3619e-06, 8.7134e-02,\n",
            "        3.4311e-05, 4.7223e-05, 1.1068e-05, 1.4984e-05, 3.2799e-05, 2.7904e-05,\n",
            "        3.7625e-06, 9.8358e-06, 5.6127e-06, 8.4518e-06, 2.9373e-04, 9.9992e-01,\n",
            "        1.7780e-06, 1.9804e-06, 2.6437e-04, 3.4321e-02, 9.7512e-01, 9.9990e-06,\n",
            "        5.2339e-06, 1.5653e-06, 5.5624e-04, 9.7095e-01, 7.0226e-06, 5.5910e-06,\n",
            "        5.8866e-06, 7.6324e-06, 2.0093e-02, 1.2024e-02, 2.5831e-02, 5.5682e-06,\n",
            "        1.5059e-06, 6.6750e-04, 4.5256e-04, 1.4583e-05, 3.8751e-06, 7.0658e-05,\n",
            "        5.0070e-06, 3.9022e-05, 2.7754e-06, 5.2574e-06])\n",
            "Sum of predicted tensor(6.0834)\n",
            "Loss tensor(0.1747)\n",
            "Acc tensor(0.9531)\n",
            "tensor([8.4070e-03, 2.9505e-06, 7.4462e-06, 1.8928e-05, 1.2415e-05, 3.0477e-06,\n",
            "        1.1017e-06, 4.5423e-04, 2.2370e-06, 1.2733e-05, 3.9964e-06, 1.5014e-04,\n",
            "        2.9733e-04, 8.6826e-05, 7.6205e-06, 5.4423e-06, 4.8484e-06, 9.9966e-01,\n",
            "        1.1762e-06, 9.9945e-01, 2.9428e-06, 2.1496e-06, 2.1634e-04, 2.0606e-05,\n",
            "        1.4394e-05, 1.2099e-05, 6.3870e-06, 8.4369e-01, 1.9847e-04, 4.7477e-06,\n",
            "        1.3642e-05, 4.3719e-06, 1.8360e-06, 4.6376e-05, 1.1284e-06, 6.0177e-06,\n",
            "        9.9974e-01, 1.6457e-06, 6.8604e-06, 5.4677e-05, 9.4965e-06, 3.9270e-06,\n",
            "        7.2981e-06, 1.5367e-05, 3.7899e-06, 9.9966e-01, 2.2165e-06, 3.4857e-06,\n",
            "        9.8663e-01, 5.8628e-01, 1.0243e-05, 2.7336e-06, 5.4869e-06, 5.4814e-05,\n",
            "        2.2510e-06, 3.6442e-06, 2.7453e-06, 1.5545e-04, 1.1895e-06, 3.8895e-04,\n",
            "        1.3959e-04, 7.7858e-03, 1.0745e-05, 1.6645e-03])\n",
            "Sum of predicted tensor(6.4355)\n",
            "Loss tensor(0.2291)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.5352e-05, 1.4984e-05, 9.9984e-01, 2.0845e-02, 1.4026e-05, 4.6228e-06,\n",
            "        9.9947e-01, 1.6801e-05, 1.5125e-06, 3.2685e-06, 3.2384e-06, 1.2163e-06,\n",
            "        3.9145e-06, 9.3567e-07, 1.7081e-06, 1.7110e-05, 6.4402e-06, 1.2958e-05,\n",
            "        1.5451e-06, 3.2021e-06, 7.9988e-06, 1.5393e-06, 3.0773e-06, 4.3354e-03,\n",
            "        3.3594e-05, 2.6075e-06, 7.1041e-06, 3.3938e-06, 7.2898e-05, 1.2498e-06,\n",
            "        2.1062e-06, 9.9966e-01, 1.1111e-05, 2.7238e-06, 1.4160e-06, 9.1849e-06,\n",
            "        1.7010e-05, 9.7896e-01, 9.6334e-05, 2.0933e-05, 7.2567e-01, 1.1189e-04,\n",
            "        2.3195e-06, 9.9977e-01, 2.7265e-05, 2.3655e-05, 2.7293e-06, 2.5414e-05,\n",
            "        2.0056e-05, 5.0449e-06, 5.8978e-06, 9.9896e-01, 5.3647e-05, 6.8662e-06,\n",
            "        4.7374e-06, 1.8238e-05, 1.4796e-04, 5.0639e-06, 7.0844e-05, 9.9959e-01,\n",
            "        9.8280e-06, 2.3006e-06, 9.9915e-01, 9.9918e-01])\n",
            "Sum of predicted tensor(9.7264)\n",
            "Loss tensor(0.1283)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.9567e-01, 4.8744e-06, 1.7925e-06, 1.4987e-05, 1.1703e-05, 1.3475e-04,\n",
            "        3.5299e-06, 1.5560e-06, 9.7010e-06, 3.0748e-06, 1.8238e-05, 9.9902e-01,\n",
            "        6.1754e-06, 1.0453e-05, 5.6303e-06, 4.7324e-06, 2.4629e-05, 4.0335e-05,\n",
            "        4.7380e-06, 2.7228e-06, 4.2094e-06, 9.9981e-01, 5.6937e-06, 9.9706e-01,\n",
            "        2.7430e-06, 4.1159e-06, 1.6215e-02, 1.3525e-06, 2.6039e-06, 1.7966e-02,\n",
            "        1.0780e-05, 9.9437e-01, 4.4011e-06, 2.0483e-06, 9.6424e-06, 1.3943e-05,\n",
            "        6.5742e-05, 1.2690e-06, 1.4984e-05, 2.6174e-06, 9.9760e-01, 2.0249e-06,\n",
            "        4.6911e-05, 2.2655e-05, 1.5932e-06, 9.9437e-01, 4.9658e-06, 7.1787e-06,\n",
            "        7.3012e-06, 2.2475e-05, 1.4746e-06, 2.0223e-05, 4.4077e-06, 8.1429e-01,\n",
            "        1.5179e-05, 3.2277e-02, 4.5344e-06, 4.8202e-05, 1.9896e-05, 3.4564e-05,\n",
            "        5.5910e-06, 7.9870e-01, 9.6506e-07, 3.2437e-05])\n",
            "Sum of predicted tensor(8.6581)\n",
            "Loss tensor(0.2104)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1978e-05, 3.3283e-06, 1.9715e-05, 5.6149e-06, 5.1540e-06, 4.4508e-06,\n",
            "        2.8383e-05, 2.7174e-04, 5.7268e-06, 1.0134e-06, 1.8539e-04, 5.4332e-06,\n",
            "        3.3640e-05, 8.0446e-07, 1.8316e-06, 3.6137e-06, 8.5711e-06, 8.9486e-06,\n",
            "        1.3175e-05, 1.7102e-06, 2.5138e-06, 2.4207e-05, 6.6478e-06, 1.8617e-05,\n",
            "        2.4114e-06, 1.6507e-06, 9.9978e-01, 4.8673e-03, 1.0347e-05, 3.6267e-06,\n",
            "        2.2390e-06, 6.5391e-06, 5.4004e-06, 1.4306e-06, 6.5230e-06, 3.9265e-03,\n",
            "        8.4490e-06, 2.8055e-04, 8.3891e-07, 9.9649e-06, 1.7696e-06, 4.7393e-06,\n",
            "        3.6442e-06, 6.9598e-05, 5.3468e-05, 2.2387e-06, 1.3418e-05, 6.4513e-05,\n",
            "        1.3568e-05, 9.2664e-06, 1.0403e-03, 2.4307e-06, 1.6444e-06, 1.7932e-05,\n",
            "        8.9327e-06, 1.5211e-03, 9.7368e-05, 5.4346e-06, 2.2985e-06, 2.3655e-05,\n",
            "        4.4683e-01, 2.7189e-06, 9.8446e-06, 2.5965e-06])\n",
            "Sum of predicted tensor(1.4594)\n",
            "Loss tensor(0.0926)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.7571e-06, 1.0772e-06, 9.4683e-06, 3.3459e-05, 4.2000e-06, 1.2434e-06,\n",
            "        4.0243e-06, 1.5288e-06, 7.7281e-06, 6.6041e-06, 8.7155e-01, 8.5916e-06,\n",
            "        3.2031e-06, 4.8513e-06, 6.6605e-06, 1.3777e-06, 1.2758e-05, 2.5111e-06,\n",
            "        1.7115e-06, 7.7117e-01, 9.9917e-01, 5.2517e-05, 9.6191e-06, 8.3238e-06,\n",
            "        1.1986e-06, 4.5113e-06, 3.0568e-03, 5.3153e-06, 3.2879e-06, 1.2261e-05,\n",
            "        2.2431e-04, 2.6926e-06, 5.2915e-03, 4.6206e-06, 2.3743e-06, 5.2014e-06,\n",
            "        2.0438e-06, 1.3445e-06, 9.9997e-01, 4.1446e-06, 6.2315e-06, 3.4418e-06,\n",
            "        9.1730e-05, 2.9230e-05, 5.8553e-06, 9.8000e-04, 1.4569e-06, 1.2051e-06,\n",
            "        3.1664e-06, 4.3363e-05, 1.2838e-05, 2.3794e-06, 3.6060e-06, 9.9933e-01,\n",
            "        4.0586e-05, 1.4653e-05, 3.9136e-06, 2.6537e-06, 5.9662e-06, 1.3332e-05,\n",
            "        1.5517e-05, 1.5641e-06, 4.5326e-03, 7.0270e-01])\n",
            "Sum of predicted tensor(5.3585)\n",
            "Loss tensor(0.1348)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.8778e-04, 2.6418e-06, 3.1369e-06, 1.5808e-04, 1.5494e-05, 9.9922e-01,\n",
            "        7.0397e-06, 3.2204e-06, 9.3045e-04, 6.8535e-06, 3.0384e-05, 5.5645e-06,\n",
            "        9.8649e-07, 3.7147e-06, 9.9298e-06, 4.3177e-06, 4.8252e-06, 6.8190e-02,\n",
            "        8.8006e-06, 4.0499e-06, 5.0311e-05, 2.7803e-06, 1.0599e-05, 2.6610e-04,\n",
            "        1.2586e-06, 1.6441e-06, 8.1070e-06, 1.8057e-05, 7.7427e-07, 3.5656e-06,\n",
            "        1.5316e-05, 4.3840e-04, 3.3788e-06, 6.1094e-04, 2.5361e-06, 1.5981e-05,\n",
            "        3.6926e-06, 2.1597e-06, 1.5839e-06, 2.4630e-06, 6.7789e-06, 1.6913e-06,\n",
            "        1.3223e-05, 1.7611e-06, 3.0669e-06, 2.8644e-02, 3.3995e-05, 2.7046e-04,\n",
            "        9.9254e-06, 1.8520e-05, 7.0750e-06, 8.9629e-06, 2.1836e-05, 2.0210e-05,\n",
            "        1.1180e-05, 3.5236e-06, 8.6080e-06, 9.3879e-06, 6.7170e-06, 1.2665e-06,\n",
            "        1.5052e-06, 2.2783e-06, 7.6649e-05, 1.8463e-05])\n",
            "Sum of predicted tensor(1.0996)\n",
            "Loss tensor(0.1816)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.9444e-06, 2.4498e-06, 3.2644e-06, 5.8758e-06, 1.8646e-06, 2.1497e-06,\n",
            "        4.9443e-03, 2.9023e-03, 2.3919e-06, 5.2468e-06, 4.0879e-05, 3.4996e-06,\n",
            "        3.4095e-05, 1.9390e-06, 1.9489e-01, 2.7620e-04, 8.9117e-06, 1.1381e-05,\n",
            "        1.9831e-06, 3.4199e-04, 2.0615e-06, 3.8120e-06, 4.8155e-06, 9.9934e-01,\n",
            "        9.9953e-01, 1.8697e-05, 5.5883e-06, 5.1522e-06, 8.7127e-04, 6.7170e-06,\n",
            "        1.6388e-05, 9.5959e-06, 4.6992e-06, 6.4675e-06, 1.0437e-06, 1.9159e-05,\n",
            "        1.3663e-05, 8.5919e-06, 9.7084e-06, 7.6119e-06, 4.9809e-06, 5.6576e-02,\n",
            "        1.4084e-06, 9.9922e-01, 1.0612e-04, 5.1483e-06, 7.1572e-05, 4.9239e-06,\n",
            "        9.2139e-06, 2.3025e-06, 6.0904e-06, 1.0160e-06, 1.2379e-04, 6.4321e-04,\n",
            "        5.8992e-05, 3.4146e-06, 1.1457e-04, 1.9975e-06, 6.0287e-06, 9.6996e-05,\n",
            "        9.7225e-01, 4.1202e-05, 1.8280e-05, 1.3922e-04])\n",
            "Sum of predicted tensor(4.2329)\n",
            "Loss tensor(0.1045)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.2103e-06, 1.0341e-05, 4.9806e-06, 3.4959e-06, 3.1909e-05, 2.9656e-06,\n",
            "        9.9943e-01, 5.6619e-06, 5.2031e-06, 1.9765e-06, 4.5877e-05, 4.7284e-06,\n",
            "        2.3308e-06, 9.6815e-06, 6.3480e-01, 1.3557e-06, 1.4189e-06, 1.5469e-06,\n",
            "        4.4985e-03, 1.0685e-04, 7.2472e-02, 1.1252e-06, 4.6206e-06, 4.9501e-06,\n",
            "        4.0466e-05, 3.7771e-04, 8.4485e-04, 1.2276e-06, 1.9747e-05, 3.8270e-06,\n",
            "        6.6840e-06, 1.4346e-06, 4.4224e-06, 1.9214e-05, 1.8289e-06, 2.2266e-05,\n",
            "        8.7403e-06, 7.9456e-06, 2.3657e-06, 3.2274e-05, 3.5490e-06, 3.5414e-06,\n",
            "        6.7763e-06, 1.1199e-06, 9.2688e-01, 1.9872e-06, 9.7464e-04, 2.2990e-04,\n",
            "        3.2996e-06, 4.1724e-05, 1.4218e-06, 3.4887e-06, 3.8784e-06, 6.8238e-06,\n",
            "        1.6408e-05, 3.6117e-06, 1.3352e-05, 8.5352e-02, 1.2778e-04, 6.1966e-06,\n",
            "        1.9292e-05, 4.5175e-06, 3.3470e-06, 9.5454e-06])\n",
            "Sum of predicted tensor(2.7266)\n",
            "Loss tensor(0.1802)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.2780e-04, 3.3647e-06, 2.3058e-06, 5.0623e-06, 8.8704e-06, 1.0914e-06,\n",
            "        2.8138e-06, 1.1583e-05, 1.7615e-04, 1.4933e-05, 4.6556e-06, 1.1466e-06,\n",
            "        8.5332e-06, 9.3054e-06, 2.6806e-06, 2.9629e-06, 5.6733e-05, 2.3451e-05,\n",
            "        1.2611e-04, 9.8690e-01, 9.1203e-05, 9.9917e-01, 9.9954e-01, 2.2276e-06,\n",
            "        7.7283e-03, 9.7557e-06, 1.8035e-06, 1.3086e-05, 5.7952e-02, 4.5004e-06,\n",
            "        9.9962e-01, 3.2420e-06, 5.9577e-06, 3.1465e-04, 4.0042e-06, 1.1345e-05,\n",
            "        2.9623e-06, 5.0121e-06, 7.1329e-04, 1.3094e-05, 2.5251e-04, 9.9735e-01,\n",
            "        3.9502e-06, 2.8609e-06, 1.3721e-05, 2.1074e-05, 2.0166e-05, 1.4569e-06,\n",
            "        6.7393e-06, 9.9917e-01, 1.2966e-03, 4.4701e-06, 2.7518e-06, 9.1395e-06,\n",
            "        3.6788e-05, 1.0095e-01, 4.3872e-03, 7.5324e-03, 9.6279e-06, 1.9804e-05,\n",
            "        3.3817e-06, 6.9591e-06, 2.0889e-04, 9.9924e-01])\n",
            "Sum of predicted tensor(7.1632)\n",
            "Loss tensor(0.5160)\n",
            "Acc tensor(0.9062)\n",
            "tensor([3.2791e-06, 4.8577e-06, 1.1691e-04, 2.9623e-06, 3.0855e-06, 5.2634e-05,\n",
            "        2.7526e-06, 5.4796e-06, 4.4642e-06, 7.0395e-06, 3.3036e-06, 1.5678e-06,\n",
            "        1.1632e-04, 5.8231e-06, 6.7415e-01, 4.3871e-06, 7.1750e-04, 3.8270e-06,\n",
            "        1.3126e-04, 1.2338e-05, 5.3108e-06, 8.5947e-06, 7.6277e-06, 3.5178e-06,\n",
            "        2.4805e-06, 2.1111e-04, 9.7638e-06, 1.4546e-04, 1.6088e-04, 9.5701e-05,\n",
            "        9.3092e-01, 8.3238e-06, 7.9736e-03, 2.0816e-06, 7.4655e-06, 2.3727e-06,\n",
            "        5.7220e-06, 2.2055e-06, 7.6008e-02, 9.5311e-06, 6.6008e-06, 4.6992e-06,\n",
            "        2.3540e-06, 3.3813e-06, 2.7795e-05, 2.5997e-06, 1.4476e-05, 9.8956e-01,\n",
            "        2.1375e-06, 2.9456e-05, 3.7819e-06, 2.3637e-05, 2.3763e-06, 7.3093e-04,\n",
            "        9.3883e-06, 2.8335e-06, 2.7457e-06, 1.8557e-05, 2.0382e-06, 2.4053e-06,\n",
            "        9.2235e-06, 1.9086e-06, 6.0657e-05, 4.4752e-06])\n",
            "Sum of predicted tensor(2.6815)\n",
            "Loss tensor(0.4202)\n",
            "Acc tensor(0.9219)\n",
            "tensor([7.1517e-04, 5.8248e-06, 1.4159e-06, 3.6313e-06, 6.4198e-06, 2.6390e-05,\n",
            "        1.6601e-05, 2.1616e-01, 1.6971e-02, 8.1842e-01, 2.2672e-06, 3.9218e-06,\n",
            "        6.3322e-06, 1.9586e-06, 4.1607e-06, 2.8450e-05, 5.7194e-06, 1.3708e-05,\n",
            "        2.1570e-05, 1.2079e-04, 8.9629e-06, 1.2796e-05, 5.6110e-06, 1.1929e-04,\n",
            "        2.3514e-06, 9.8018e-02, 2.5496e-06, 1.1077e-06, 8.2807e-06, 1.1786e-06,\n",
            "        2.5496e-06, 3.0611e-06, 2.8575e-02, 5.5114e-06, 2.3339e-06, 2.6521e-05,\n",
            "        7.7358e-06, 1.3449e-05, 3.1093e-06, 1.8974e-05, 2.2424e-06, 1.2136e-05,\n",
            "        9.8532e-01, 1.1644e-04, 1.4274e-06, 2.8538e-06, 1.4282e-04, 1.2236e-06,\n",
            "        1.4569e-06, 1.2002e-06, 9.0547e-06, 1.2051e-06, 8.7729e-06, 1.4119e-05,\n",
            "        1.4504e-06, 3.8649e-06, 4.0921e-06, 4.2303e-06, 4.2738e-06, 4.5737e-04,\n",
            "        2.0277e-05, 6.8670e-03, 9.9894e-01, 2.3317e-06])\n",
            "Sum of predicted tensor(3.1713)\n",
            "Loss tensor(0.3001)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.7905e-05, 9.4482e-06, 1.0950e-06, 2.8838e-06, 2.8918e-06, 7.2291e-06,\n",
            "        5.0113e-06, 4.0639e-06, 1.2501e-06, 1.1563e-05, 3.9606e-05, 7.8984e-06,\n",
            "        1.1381e-05, 2.3439e-06, 5.1344e-05, 2.2223e-06, 7.5230e-06, 5.6931e-06,\n",
            "        3.5853e-02, 2.7571e-03, 1.0795e-06, 4.5072e-05, 2.3649e-06, 1.2401e-05,\n",
            "        4.6129e-05, 9.8944e-06, 2.5127e-06, 2.0717e-06, 2.2862e-06, 2.4900e-06,\n",
            "        3.3047e-06, 3.4366e-06, 1.5998e-05, 6.5898e-05, 1.1541e-03, 1.2741e-06,\n",
            "        5.7339e-06, 1.0029e-05, 1.8504e-05, 7.0703e-06, 9.8743e-01, 4.2300e-06,\n",
            "        1.7990e-06, 1.4380e-05, 2.4350e-06, 1.5968e-05, 1.9251e-06, 5.8612e-05,\n",
            "        2.0274e-06, 1.4628e-05, 3.9225e-06, 1.4633e-03, 1.5842e-04, 2.5384e-06,\n",
            "        2.8223e-05, 4.3217e-06, 1.8845e-05, 2.7685e-06, 9.3192e-06, 1.6129e-05,\n",
            "        3.6689e-06, 5.6726e-06, 3.2501e-02, 1.4569e-06])\n",
            "Sum of predicted tensor(1.0620)\n",
            "Loss tensor(0.1598)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.2149e-01, 2.0565e-06, 4.1179e-06, 1.1761e-03, 2.2854e-05, 3.9030e-04,\n",
            "        8.7614e-06, 3.4350e-05, 1.3886e-06, 8.9402e-06, 1.5062e-05, 2.4465e-05,\n",
            "        9.4315e-05, 1.4366e-02, 1.0012e-05, 2.9473e-06, 8.4710e-06, 9.4314e-06,\n",
            "        1.0698e-04, 6.2436e-06, 1.6698e-06, 5.5402e-06, 1.7921e-06, 7.1292e-06,\n",
            "        2.1549e-06, 1.6400e-06, 5.7717e-06, 5.9094e-06, 2.6086e-06, 9.0146e-05,\n",
            "        1.9678e-05, 4.8633e-06, 1.6305e-05, 1.5152e-04, 1.3908e-05, 9.9330e-01,\n",
            "        3.0738e-06, 2.3347e-01, 5.4743e-05, 3.1892e-05, 2.8010e-06, 4.4600e-06,\n",
            "        3.7759e-04, 2.2861e-05, 3.0886e-04, 1.8265e-05, 1.8176e-05, 3.6993e-06,\n",
            "        8.5722e-05, 3.1360e-05, 1.1408e-05, 9.7233e-06, 2.0819e-06, 2.0822e-06,\n",
            "        9.9902e-01, 8.3423e-06, 2.2836e-05, 5.6079e-05, 1.6813e-02, 3.8669e-05,\n",
            "        7.1283e-05, 4.4750e-06, 2.0531e-06, 6.2575e-06])\n",
            "Sum of predicted tensor(3.1819)\n",
            "Loss tensor(0.4066)\n",
            "Acc tensor(0.9375)\n",
            "tensor([2.3200e-05, 3.5945e-06, 1.0965e-05, 1.5114e-05, 3.8165e-06, 4.5228e-06,\n",
            "        5.0145e-06, 3.9691e-06, 7.7330e-06, 1.6446e-06, 9.0610e-07, 1.7376e-06,\n",
            "        1.3184e-06, 6.0778e-05, 4.8268e-05, 1.1037e-05, 2.9545e-06, 8.5210e-06,\n",
            "        7.3280e-06, 7.1832e-06, 1.4817e-04, 1.2758e-03, 6.1411e-05, 1.1157e-05,\n",
            "        1.5114e-05, 2.8022e-06, 1.7944e-04, 5.4277e-06, 5.3345e-06, 1.1440e-05,\n",
            "        4.4600e-06, 1.3832e-05, 3.5513e-05, 3.6913e-06, 4.2511e-06, 1.3557e-06,\n",
            "        1.5540e-05, 6.0783e-05, 6.7485e-06, 1.9155e-06, 1.7398e-04, 2.4389e-06,\n",
            "        3.7181e-04, 2.0846e-05, 4.0035e-06, 1.5185e-05, 1.0562e-04, 1.4242e-05,\n",
            "        4.4600e-06, 3.2858e-06, 6.9114e-05, 1.4390e-06, 5.9970e-06, 1.0973e-04,\n",
            "        6.6105e-06, 3.7127e-06, 2.0427e-05, 5.8434e-06, 1.6477e-06, 1.5612e-05,\n",
            "        2.3797e-06, 6.8730e-05, 3.6660e-06, 1.1777e-05])\n",
            "Sum of predicted tensor(0.0031)\n",
            "Loss tensor(4.9026e-05)\n",
            "Acc tensor(1.)\n",
            "tensor([5.9085e-01, 6.0052e-06, 4.4410e-06, 1.5011e-05, 3.8165e-06, 3.3476e-06,\n",
            "        6.9195e-06, 3.6779e-06, 9.1598e-06, 3.4620e-06, 1.1076e-02, 3.8165e-06,\n",
            "        6.1495e-06, 4.4847e-06, 9.9981e-01, 1.2623e-06, 6.9286e-06, 2.2886e-06,\n",
            "        5.4761e-04, 3.8790e-06, 7.3206e-05, 2.2294e-06, 1.8194e-06, 1.4075e-06,\n",
            "        3.0496e-06, 1.0004e-05, 8.2536e-05, 5.8548e-06, 6.4546e-06, 1.0397e-05,\n",
            "        1.6698e-06, 8.6368e-01, 1.3489e-05, 5.6914e-06, 9.8444e-01, 9.8427e-06,\n",
            "        4.9083e-01, 1.0194e-04, 9.9784e-01, 8.8985e-05, 6.7485e-06, 1.6962e-06,\n",
            "        2.4768e-06, 5.2614e-04, 1.0937e-06, 4.5791e-06, 9.8726e-01, 3.4442e-06,\n",
            "        2.1803e-06, 1.0399e-02, 4.0752e-06, 6.9363e-06, 2.8858e-06, 9.7578e-06,\n",
            "        3.9642e-03, 4.4575e-05, 3.1666e-06, 1.3249e-01, 3.3116e-06, 2.2866e-06,\n",
            "        6.9073e-04, 1.3083e-06, 1.0301e-05, 3.8165e-06])\n",
            "Sum of predicted tensor(6.0750)\n",
            "Loss tensor(0.4050)\n",
            "Acc tensor(0.9062)\n",
            "tensor([2.3501e-06, 6.8143e-05, 1.4386e-05, 9.9922e-01, 5.8520e-06, 9.9969e-01,\n",
            "        3.9949e-05, 8.3862e-06, 2.4233e-06, 9.9967e-01, 9.9361e-01, 3.1894e-06,\n",
            "        1.5563e-06, 9.4289e-06, 5.1788e-06, 4.9250e-06, 4.8661e-05, 6.1690e-06,\n",
            "        1.1058e-06, 1.9478e-06, 5.7461e-06, 1.9817e-06, 1.4739e-06, 1.6582e-06,\n",
            "        7.0941e-05, 1.5283e-06, 1.1054e-04, 1.7830e-01, 9.9967e-01, 1.6335e-06,\n",
            "        7.8337e-04, 6.5758e-05, 7.3081e-06, 3.0136e-05, 1.7603e-06, 1.5062e-05,\n",
            "        9.9954e-01, 9.9926e-01, 1.8526e-05, 2.8874e-03, 2.3529e-06, 2.7047e-01,\n",
            "        4.3459e-06, 1.8145e-05, 9.9950e-01, 4.0289e-05, 7.6255e-06, 5.7102e-06,\n",
            "        4.7195e-05, 2.3817e-06, 1.5353e-02, 1.3894e-03, 7.4827e-06, 2.7800e-04,\n",
            "        3.7194e-03, 1.1637e-05, 9.0393e-05, 9.0377e-06, 1.6403e-05, 1.2541e-06,\n",
            "        3.0635e-05, 1.5255e-04, 2.7836e-06, 1.5917e-06])\n",
            "Sum of predicted tensor(8.4643)\n",
            "Loss tensor(0.5111)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.4869e-06, 2.1247e-06, 1.9888e-06, 8.3552e-03, 1.8398e-06, 1.8444e-06,\n",
            "        8.2836e-06, 1.4951e-05, 1.3792e-05, 2.4048e-06, 2.6576e-02, 2.1607e-03,\n",
            "        2.0252e-06, 6.6533e-05, 3.4526e-05, 2.6197e-05, 3.9295e-06, 9.9849e-01,\n",
            "        1.8231e-06, 6.5876e-06, 2.7738e-06, 7.5600e-06, 4.9214e-06, 4.5534e-06,\n",
            "        7.2470e-06, 4.4577e-05, 3.5491e-06, 3.3886e-06, 2.5021e-03, 1.3247e-05,\n",
            "        4.4624e-06, 1.7819e-06, 8.2593e-06, 3.9274e-06, 9.9879e-01, 1.2602e-06,\n",
            "        1.4484e-05, 1.1658e-05, 3.8456e-02, 9.9809e-01, 1.9265e-06, 1.6921e-05,\n",
            "        8.4284e-06, 2.3817e-06, 9.9992e-04, 4.6572e-01, 1.9218e-06, 3.5924e-05,\n",
            "        4.1719e-04, 6.1896e-05, 1.6620e-06, 1.3646e-06, 9.9949e-01, 6.9233e-06,\n",
            "        2.5717e-05, 1.5573e-05, 1.4801e-06, 2.0206e-06, 4.5014e-06, 3.1295e-06,\n",
            "        3.8165e-06, 3.5107e-06, 3.3234e-06, 5.4419e-06])\n",
            "Sum of predicted tensor(4.5406)\n",
            "Loss tensor(0.1161)\n",
            "Acc tensor(0.9844)\n",
            "tensor([4.0601e-06, 3.6262e-06, 3.3428e-06, 3.2449e-06, 1.0404e-01, 1.2577e-06,\n",
            "        5.3899e-05, 5.4043e-06, 1.5468e-05, 5.8700e-06, 9.9984e-01, 3.8165e-06,\n",
            "        3.7986e-06, 9.9808e-01, 9.1318e-03, 2.8215e-06, 2.6194e-06, 5.0760e-04,\n",
            "        3.8796e-06, 2.1002e-06, 9.9620e-01, 1.1550e-06, 7.2605e-05, 5.5336e-06,\n",
            "        6.1862e-06, 9.1071e-06, 2.3713e-06, 2.9984e-06, 1.5086e-05, 6.7707e-05,\n",
            "        1.9187e-05, 9.7656e-07, 3.6406e-05, 1.0028e-05, 3.6038e-05, 2.0856e-06,\n",
            "        3.8006e-06, 1.9324e-06, 1.4231e-06, 2.8563e-06, 1.6104e-01, 1.1690e-06,\n",
            "        4.4600e-06, 8.0180e-01, 1.3102e-05, 7.5665e-04, 3.0438e-05, 8.5032e-06,\n",
            "        3.5105e-04, 6.1433e-06, 1.9665e-05, 9.3676e-06, 1.8631e-05, 9.5045e-06,\n",
            "        3.0520e-05, 1.3363e-05, 3.4748e-06, 3.3829e-04, 1.1896e-05, 1.0843e-05,\n",
            "        4.0350e-06, 5.9893e-06, 2.3103e-05, 1.6925e-05])\n",
            "Sum of predicted tensor(4.0727)\n",
            "Loss tensor(0.0558)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.2528e-05, 9.9430e-01, 9.9455e-05, 8.8153e-06, 3.0852e-05, 1.9666e-05,\n",
            "        2.4893e-06, 7.6635e-06, 1.4967e-06, 5.9825e-06, 3.4570e-06, 2.1714e-06,\n",
            "        4.5956e-06, 1.0034e-04, 4.1613e-06, 6.3541e-06, 9.9931e-01, 3.4038e-02,\n",
            "        1.3526e-06, 3.3389e-06, 3.6068e-06, 5.7679e-06, 1.6862e-05, 3.9530e-03,\n",
            "        5.3730e-05, 2.9521e-06, 9.9984e-01, 3.6333e-06, 3.7905e-06, 9.9771e-01,\n",
            "        2.3756e-06, 1.1421e-03, 3.8165e-06, 1.4951e-05, 1.7491e-06, 1.3721e-06,\n",
            "        7.6756e-06, 9.9916e-01, 9.9900e-01, 8.7614e-05, 4.2003e-02, 5.2271e-02,\n",
            "        3.2924e-06, 2.4568e-04, 8.4954e-02, 1.9506e-05, 4.2984e-04, 3.6943e-05,\n",
            "        1.1801e-06, 1.7459e-06, 4.2726e-06, 1.6322e-05, 1.1727e-05, 1.5254e-03,\n",
            "        4.0460e-06, 6.2361e-05, 3.1994e-06, 9.8188e-01, 8.7752e-06, 9.9938e-01,\n",
            "        5.2032e-06, 6.2287e-04, 9.9361e-01, 1.9263e-06])\n",
            "Sum of predicted tensor(9.1861)\n",
            "Loss tensor(0.2406)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.3411e-06, 1.6292e-05, 7.6532e-05, 6.6386e-03, 1.2179e-06, 8.2899e-06,\n",
            "        7.8500e-01, 6.4443e-06, 1.7513e-06, 1.4143e-05, 4.7323e-06, 1.7840e-06,\n",
            "        5.2280e-06, 1.1008e-04, 3.3626e-06, 1.8932e-06, 3.7644e-06, 5.9801e-06,\n",
            "        9.3525e-05, 6.1685e-06, 4.1466e-04, 3.1695e-06, 1.9406e-05, 4.0058e-06,\n",
            "        3.8851e-05, 5.5684e-06, 3.0893e-04, 3.8183e-04, 4.8576e-06, 2.0319e-05,\n",
            "        3.2752e-06, 4.1573e-06, 1.1289e-05, 1.9006e-06, 1.3662e-05, 3.4586e-06,\n",
            "        4.8542e-05, 6.3723e-06, 9.3800e-04, 4.6329e-06, 3.9084e-06, 2.5779e-06,\n",
            "        1.1882e-05, 6.1792e-05, 1.0950e-06, 6.7299e-05, 1.4318e-05, 1.2560e-06,\n",
            "        4.6467e-06, 2.4774e-06, 5.4851e-05, 1.4129e-06, 9.8289e-01, 8.1044e-05,\n",
            "        3.3419e-05, 3.2770e-05, 6.3297e-05, 1.0131e-05, 2.2909e-06, 7.5664e-06,\n",
            "        2.4675e-05, 2.3515e-06, 4.8148e-04, 6.7697e-04])\n",
            "Sum of predicted tensor(1.7788)\n",
            "Loss tensor(0.2939)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.6879e-04, 1.3598e-05, 9.1786e-05, 4.2488e-03, 6.4306e-06, 1.4950e-06,\n",
            "        2.1587e-06, 2.3268e-06, 2.7949e-04, 9.9132e-01, 4.2853e-06, 9.6909e-06,\n",
            "        3.5761e-04, 2.0091e-04, 2.5257e-06, 3.0949e-06, 7.0159e-06, 5.9688e-06,\n",
            "        1.7050e-06, 1.0192e-02, 1.4925e-06, 8.1815e-05, 5.9398e-06, 1.7116e-06,\n",
            "        6.2218e-06, 4.7097e-01, 1.7413e-06, 4.4951e-06, 1.7970e-04, 3.9692e-06,\n",
            "        2.2436e-04, 1.7426e-05, 1.6037e-04, 1.3930e-06, 1.6975e-06, 3.7827e-06,\n",
            "        2.8218e-05, 2.9792e-06, 6.3822e-04, 6.1685e-06, 5.3496e-06, 4.7002e-06,\n",
            "        3.0606e-06, 2.6074e-05, 7.2643e-06, 9.3490e-06, 9.9840e-01, 4.7587e-06,\n",
            "        2.3088e-06, 6.7800e-06, 2.1496e-05, 1.8267e-05, 8.2667e-05, 1.0922e-05,\n",
            "        1.0792e-05, 1.1035e-05, 3.7928e-05, 1.5976e-05, 1.5590e-05, 5.6846e-03,\n",
            "        4.7137e-03, 5.9192e-05, 2.6568e-05, 8.3466e-05])\n",
            "Sum of predicted tensor(2.4885)\n",
            "Loss tensor(0.3025)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.0558e-05, 7.4940e-05, 5.4576e-06, 1.3340e-02, 3.3678e-06, 2.1331e-06,\n",
            "        8.7505e-06, 2.0760e-06, 9.9923e-01, 4.2318e-05, 7.9450e-06, 2.1704e-06,\n",
            "        1.8639e-04, 5.0794e-04, 4.9452e-06, 3.2257e-06, 1.3099e-05, 8.7670e-04,\n",
            "        1.6762e-05, 1.6104e-01, 4.2597e-06, 6.7325e-06, 2.2206e-05, 1.8116e-05,\n",
            "        4.3882e-06, 7.5612e-06, 1.7769e-06, 9.9979e-01, 2.1077e-03, 1.9824e-06,\n",
            "        2.0929e-04, 7.3509e-06, 3.2310e-05, 2.5236e-06, 7.6283e-06, 2.2891e-04,\n",
            "        9.9981e-01, 3.4229e-06, 1.6527e-06, 1.3885e-05, 2.2168e-05, 1.7643e-05,\n",
            "        1.7863e-04, 7.2439e-06, 9.9916e-01, 9.9956e-01, 2.4143e-06, 1.0246e-05,\n",
            "        7.2277e-06, 9.9988e-01, 3.2560e-06, 1.3125e-02, 6.4738e-06, 8.3405e-06,\n",
            "        7.8667e-06, 6.1760e-06, 1.7533e-04, 3.0618e-05, 1.5474e-01, 1.3812e-05,\n",
            "        4.0476e-06, 4.0576e-06, 2.4447e-01, 3.5592e-06])\n",
            "Sum of predicted tensor(6.5891)\n",
            "Loss tensor(0.1546)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.0865e-06, 7.8885e-06, 9.1934e-06, 1.7068e-05, 8.5319e-06, 6.1685e-06,\n",
            "        8.2034e-06, 9.9990e-01, 4.1895e-06, 2.2069e-06, 3.2747e-06, 6.2976e-06,\n",
            "        3.5857e-06, 6.1685e-06, 5.5290e-06, 1.4779e-05, 1.7338e-04, 3.1213e-04,\n",
            "        1.0885e-06, 2.5662e-06, 3.8202e-03, 2.6443e-06, 2.3894e-06, 7.8841e-04,\n",
            "        4.7025e-02, 4.9843e-06, 6.3571e-06, 1.0841e-06, 6.5478e-06, 2.5263e-06,\n",
            "        5.2836e-06, 1.5221e-06, 4.7155e-06, 7.4518e-06, 9.1802e-06, 2.1359e-06,\n",
            "        3.5706e-03, 1.5692e-05, 1.1068e-04, 9.5529e-06, 4.0670e-06, 6.1685e-06,\n",
            "        7.4152e-05, 1.1933e-05, 4.6650e-06, 5.2627e-06, 2.0869e-06, 2.4285e-04,\n",
            "        1.3382e-01, 2.0643e-04, 4.8362e-06, 1.7236e-06, 3.1711e-05, 9.9891e-01,\n",
            "        3.8620e-04, 2.1865e-06, 3.5715e-05, 7.1857e-05, 9.9993e-01, 6.3203e-06,\n",
            "        4.6473e-06, 1.6990e-04, 6.9238e-06, 3.1480e-06])\n",
            "Sum of predicted tensor(3.1898)\n",
            "Loss tensor(0.1193)\n",
            "Acc tensor(0.9688)\n",
            "tensor([8.0047e-02, 4.6411e-03, 1.0633e-04, 2.7822e-06, 1.7050e-06, 6.2333e-06,\n",
            "        1.6835e-05, 1.6681e-03, 9.9964e-01, 2.1988e-06, 3.3365e-06, 4.7440e-06,\n",
            "        6.3015e-06, 1.2794e-04, 3.4436e-06, 2.1435e-05, 1.0776e-05, 9.6345e-06,\n",
            "        1.1059e-05, 3.5910e-05, 9.9984e-01, 6.9067e-05, 1.2376e-06, 3.6551e-06,\n",
            "        2.5257e-05, 1.7914e-05, 1.0790e-04, 9.7664e-06, 7.9072e-06, 1.1390e-06,\n",
            "        1.5871e-06, 3.1599e-06, 1.4175e-06, 6.1685e-06, 1.1408e-05, 1.3117e-05,\n",
            "        1.9439e-06, 4.5032e-06, 8.7086e-06, 2.9982e-05, 6.1685e-06, 1.0021e-05,\n",
            "        9.1802e-06, 3.5906e-06, 5.7178e-06, 7.0621e-06, 1.9374e-04, 4.8887e-05,\n",
            "        2.8886e-02, 6.2227e-06, 4.5731e-06, 3.5186e-06, 3.6646e-06, 3.2752e-06,\n",
            "        1.6339e-05, 6.6999e-06, 1.0295e-05, 4.3014e-06, 9.2890e-05, 1.9374e-04,\n",
            "        1.3003e-06, 6.5317e-03, 1.4849e-05, 6.1685e-06])\n",
            "Sum of predicted tensor(2.1226)\n",
            "Loss tensor(0.2900)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.9435e-06, 8.2065e-06, 6.4374e-04, 8.2901e-01, 9.9705e-01, 6.3428e-06,\n",
            "        2.4285e-04, 2.8705e-06, 1.8127e-05, 5.4002e-06, 4.5472e-05, 6.7742e-01,\n",
            "        1.5045e-06, 3.2991e-06, 1.6284e-05, 4.4850e-02, 3.9315e-06, 5.5651e-05,\n",
            "        3.2016e-06, 2.0219e-05, 2.3057e-05, 5.5398e-05, 1.3431e-03, 1.4584e-06,\n",
            "        1.4034e-06, 1.1298e-06, 1.7836e-05, 3.0099e-06, 1.3113e-05, 1.2125e-06,\n",
            "        1.7857e-06, 5.5387e-04, 3.5740e-06, 2.7854e-03, 1.2816e-05, 1.1598e-05,\n",
            "        1.5145e-06, 3.8851e-05, 6.9473e-06, 2.9939e-05, 2.0109e-06, 2.7914e-05,\n",
            "        1.6609e-05, 2.3597e-05, 2.3508e-05, 9.4904e-01, 2.9855e-06, 1.4612e-06,\n",
            "        7.9124e-06, 1.4305e-05, 2.6003e-03, 1.4624e-06, 2.8886e-02, 6.1685e-06,\n",
            "        6.2604e-03, 6.2145e-06, 3.5557e-02, 2.9853e-06, 1.4110e-06, 1.1512e-04,\n",
            "        3.3708e-06, 9.6663e-05, 9.9937e-01, 5.7078e-05])\n",
            "Sum of predicted tensor(4.5764)\n",
            "Loss tensor(0.2222)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.3452e-05, 1.3629e-06, 6.2263e-01, 3.2937e-06, 3.6015e-04, 2.3977e-06,\n",
            "        4.1801e-06, 8.6855e-06, 4.8330e-06, 2.1130e-04, 4.9478e-06, 1.1089e-05,\n",
            "        1.6302e-05, 5.2201e-06, 3.2355e-04, 3.2874e-06, 7.7128e-04, 1.1963e-06,\n",
            "        3.3424e-06, 1.6754e-06, 1.8225e-06, 9.9247e-01, 9.8243e-01, 1.1768e-06,\n",
            "        1.4346e-06, 2.0450e-06, 3.7725e-06, 2.2651e-05, 1.5985e-06, 1.5457e-05,\n",
            "        3.0802e-06, 4.5444e-06, 9.9781e-01, 2.6441e-01, 1.7278e-05, 2.4843e-06,\n",
            "        1.1619e-05, 2.0085e-06, 1.4760e-05, 4.4807e-06, 2.8552e-06, 5.2334e-04,\n",
            "        1.3866e-04, 3.1741e-01, 4.5203e-06, 4.4839e-05, 4.1097e-06, 1.1855e-06,\n",
            "        2.4080e-06, 5.2292e-06, 9.6957e-05, 7.3850e-05, 9.1116e-06, 9.9333e-01,\n",
            "        3.6476e-06, 1.0838e-06, 4.1097e-06, 1.3979e-06, 1.8494e-04, 1.1044e-05,\n",
            "        5.1750e-06, 1.0539e-02, 3.4475e-06, 9.8211e-01])\n",
            "Sum of predicted tensor(6.1661)\n",
            "Loss tensor(0.2651)\n",
            "Acc tensor(0.9062)\n",
            "tensor([1.3562e-06, 1.4232e-05, 1.4578e-06, 2.2398e-05, 6.0468e-06, 3.6920e-06,\n",
            "        1.2986e-05, 6.3213e-06, 1.8718e-05, 9.8304e-06, 1.7904e-05, 2.6220e-05,\n",
            "        2.6137e-01, 1.1920e-06, 9.8378e-01, 9.8919e-01, 8.2046e-06, 9.9612e-01,\n",
            "        6.9648e-02, 2.5221e-03, 6.0939e-06, 1.0662e-06, 4.0523e-06, 8.8482e-07,\n",
            "        6.9245e-04, 6.6797e-06, 3.9075e-06, 1.9387e-05, 2.6331e-06, 1.9783e-05,\n",
            "        4.5392e-05, 5.5246e-05, 2.8891e-06, 7.8574e-06, 9.9753e-01, 1.3985e-05,\n",
            "        2.1211e-05, 5.5329e-06, 3.9951e-06, 2.8175e-06, 3.4493e-06, 4.7438e-05,\n",
            "        4.5203e-06, 3.7435e-06, 2.7340e-06, 3.0755e-03, 2.7899e-06, 6.1108e-06,\n",
            "        1.3101e-02, 9.1670e-04, 1.2367e-05, 1.4547e-05, 6.6797e-06, 4.5203e-06,\n",
            "        1.2773e-05, 1.1061e-02, 2.9777e-05, 1.5097e-05, 2.6502e-04, 3.6479e-04,\n",
            "        1.2589e-02, 1.4794e-03, 1.5369e-05, 1.1955e-06])\n",
            "Sum of predicted tensor(4.3443)\n",
            "Loss tensor(0.3626)\n",
            "Acc tensor(0.9062)\n",
            "tensor([6.2689e-06, 9.0907e-06, 1.8892e-06, 1.4467e-05, 3.9932e-06, 9.6570e-01,\n",
            "        9.9993e-01, 1.1193e-05, 2.4561e-06, 5.4148e-06, 3.4042e-06, 4.3201e-05,\n",
            "        3.1069e-06, 1.8413e-05, 1.3687e-03, 1.2142e-01, 4.2803e-06, 1.5538e-05,\n",
            "        1.3257e-05, 2.9753e-03, 3.0179e-05, 6.0307e-05, 1.0622e-06, 4.4651e-01,\n",
            "        1.1159e-06, 1.3927e-01, 1.6656e-05, 1.9538e-05, 3.6038e-06, 3.5933e-06,\n",
            "        5.4676e-05, 1.9529e-05, 3.2968e-03, 1.9217e-06, 7.7845e-05, 1.6964e-06,\n",
            "        6.7568e-05, 4.8685e-06, 1.6542e-04, 1.2268e-06, 5.3182e-06, 3.2871e-06,\n",
            "        9.8587e-01, 1.0792e-05, 6.9339e-06, 9.9983e-01, 4.6367e-05, 9.9560e-01,\n",
            "        8.2688e-04, 1.0903e-06, 1.1551e-05, 6.6797e-06, 1.0186e-02, 2.8435e-05,\n",
            "        9.4167e-06, 1.4888e-06, 5.7361e-05, 9.9318e-01, 1.9717e-06, 2.5243e-04,\n",
            "        1.4805e-06, 3.6483e-06, 6.2860e-06, 1.8990e-06])\n",
            "Sum of predicted tensor(6.6671)\n",
            "Loss tensor(0.4078)\n",
            "Acc tensor(0.9375)\n",
            "tensor([4.5203e-06, 3.6923e-05, 6.4135e-06, 2.7884e-06, 4.2472e-06, 1.4977e-06,\n",
            "        1.4185e-05, 2.0623e-06, 2.0737e-06, 5.2994e-06, 5.6970e-06, 5.0216e-05,\n",
            "        4.3573e-06, 9.5471e-01, 5.4979e-06, 1.2184e-01, 1.7944e-05, 3.8305e-06,\n",
            "        4.1698e-05, 3.8237e-06, 2.8575e-06, 1.2470e-05, 2.3949e-06, 3.6210e-05,\n",
            "        3.1414e-06, 8.5559e-05, 2.2142e-05, 7.4194e-06, 1.7723e-06, 3.3103e-05,\n",
            "        3.4207e-06, 2.7445e-02, 9.9526e-06, 1.6172e-06, 2.1202e-06, 9.8823e-01,\n",
            "        2.9696e-06, 6.2728e-06, 5.3731e-06, 3.3326e-06, 1.0775e-05, 5.3443e-01,\n",
            "        2.3502e-05, 1.4398e-05, 9.9933e-01, 9.9728e-01, 3.0141e-06, 5.5408e-06,\n",
            "        3.8936e-05, 4.5940e-06, 1.3863e-02, 4.5203e-06, 2.3504e-01, 1.4775e-06,\n",
            "        2.0584e-06, 3.6595e-06, 3.5616e-06, 2.3760e-03, 2.2746e-06, 9.9942e-01,\n",
            "        6.2989e-06, 4.1097e-06, 1.2792e-06, 8.9407e-06])\n",
            "Sum of predicted tensor(5.8746)\n",
            "Loss tensor(0.3415)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9990e-01, 4.8732e-06, 4.6205e-06, 4.1069e-04, 1.1979e-05, 7.0490e-06,\n",
            "        1.0156e-05, 3.4932e-05, 2.0770e-05, 4.2092e-02, 1.6612e-06, 1.2095e-02,\n",
            "        3.3801e-05, 3.7410e-06, 3.8837e-05, 1.0608e-03, 4.8616e-06, 1.1232e-02,\n",
            "        6.8975e-06, 1.1718e-05, 9.7596e-01, 3.8155e-05, 9.9204e-01, 5.5891e-06,\n",
            "        2.0963e-05, 2.0601e-05, 3.3929e-03, 1.8708e-05, 5.0650e-05, 1.3846e-05,\n",
            "        2.3881e-06, 8.1280e-05, 2.8664e-05, 7.0359e-05, 4.8086e-06, 3.6907e-05,\n",
            "        9.0935e-05, 9.9201e-01, 1.5100e-06, 4.0495e-06, 1.5114e-06, 2.4228e-04,\n",
            "        5.6738e-05, 9.4549e-05, 5.0281e-03, 1.7884e-06, 6.8933e-06, 2.9573e-06,\n",
            "        4.8243e-04, 1.6567e-05, 2.3396e-06, 5.8326e-06, 2.8161e-06, 7.8731e-06,\n",
            "        4.5203e-06, 7.4895e-02, 1.0681e-05, 4.7138e-06, 9.9993e-01, 1.4346e-06,\n",
            "        1.1075e-05, 1.9291e-06, 1.5853e-06, 4.2235e-06])\n",
            "Sum of predicted tensor(5.1117)\n",
            "Loss tensor(0.3522)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.8550e-06, 4.2034e-05, 1.4130e-06, 6.3587e-06, 5.9961e-06, 5.3117e-06,\n",
            "        3.0626e-06, 2.0452e-06, 7.3934e-01, 7.7715e-06, 6.0368e-05, 5.4932e-01,\n",
            "        2.1085e-05, 2.0584e-05, 2.3669e-06, 2.3416e-06, 2.5043e-06, 3.7750e-06,\n",
            "        5.4615e-05, 2.6338e-04, 2.2374e-05, 3.0416e-03, 4.5155e-05, 1.1469e-05,\n",
            "        7.7879e-06, 3.8294e-05, 7.4393e-06, 4.6774e-04, 2.3316e-06, 5.6673e-03,\n",
            "        2.1384e-01, 5.6634e-06, 4.4079e-06, 2.5095e-06, 1.0819e-06, 4.3342e-06,\n",
            "        2.6813e-05, 7.3565e-06, 3.5991e-06, 1.9240e-04, 1.0413e-05, 1.4810e-06,\n",
            "        4.0054e-06, 8.0504e-05, 4.8557e-05, 7.2716e-04, 6.3571e-06, 1.7408e-04,\n",
            "        6.6674e-04, 2.1573e-06, 2.4920e-02, 5.4296e-05, 4.2351e-06, 8.6693e-06,\n",
            "        4.6793e-06, 2.3375e-06, 7.8451e-05, 3.8396e-06, 4.8258e-05, 9.7569e-01,\n",
            "        3.4251e-06, 1.1223e-05, 1.2713e-05, 7.7150e-06])\n",
            "Sum of predicted tensor(2.5151)\n",
            "Loss tensor(0.2107)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.5185e-06, 6.1197e-06, 6.8481e-05, 3.2568e-05, 2.5652e-02, 5.4775e-05,\n",
            "        1.0539e-05, 3.1642e-06, 1.9379e-06, 4.3881e-06, 3.9947e-03, 2.6768e-06,\n",
            "        1.8126e-05, 1.5409e-05, 5.4648e-06, 4.4523e-06, 9.9967e-01, 3.8313e-06,\n",
            "        1.1920e-05, 1.8261e-06, 1.0945e-06, 3.4874e-05, 7.2503e-05, 2.8656e-06,\n",
            "        1.6349e-06, 4.2202e-03, 5.2405e-04, 1.7454e-06, 9.9949e-01, 3.9045e-02,\n",
            "        3.2202e-06, 1.5211e-06, 9.1471e-05, 8.6920e-06, 4.1128e-06, 9.8268e-06,\n",
            "        1.8782e-06, 9.6950e-06, 3.1279e-05, 5.9710e-06, 3.2890e-04, 7.5131e-01,\n",
            "        8.2203e-06, 3.9594e-06, 2.0925e-05, 7.0337e-06, 9.4880e-06, 9.2978e-06,\n",
            "        5.7652e-06, 5.6249e-06, 1.1818e-05, 5.7287e-06, 1.2659e-05, 1.2659e-05,\n",
            "        1.2073e-05, 6.0967e-06, 4.8313e-02, 2.4063e-06, 2.0952e-06, 1.1470e-06,\n",
            "        1.4022e-06, 7.6472e-03, 3.7930e-06, 1.4610e-05])\n",
            "Sum of predicted tensor(2.8809)\n",
            "Loss tensor(0.0238)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.3322e-05, 1.2296e-04, 1.5302e-05, 3.2986e-06, 1.1674e-06, 4.6573e-05,\n",
            "        4.8409e-06, 1.0160e-05, 6.1282e-05, 8.0962e-03, 5.8813e-06, 6.1208e-06,\n",
            "        1.4001e-05, 3.0403e-06, 1.8183e-06, 3.8041e-05, 2.6802e-06, 2.2631e-05,\n",
            "        3.2192e-05, 2.4019e-06, 2.3299e-06, 6.1258e-05, 2.5909e-06, 5.7561e-05,\n",
            "        2.0046e-05, 3.9395e-06, 7.8845e-06, 9.9949e-01, 6.0930e-06, 2.8226e-06,\n",
            "        9.0985e-05, 1.6130e-04, 3.1503e-05, 2.0552e-05, 3.9679e-06, 7.6475e-06,\n",
            "        9.5883e-06, 4.6905e-06, 1.8790e-05, 9.2539e-06, 1.7695e-06, 1.1772e-05,\n",
            "        2.9810e-06, 2.4503e-06, 1.9088e-05, 3.9881e-05, 2.1271e-06, 9.4366e-01,\n",
            "        3.6218e-06, 5.5704e-06, 9.5587e-06, 4.6224e-05, 4.4310e-06, 1.4164e-05,\n",
            "        1.0558e-03, 5.3660e-06, 1.4361e-05, 5.9997e-01, 1.3131e-05, 6.6833e-06,\n",
            "        3.4143e-06, 9.9879e-01, 1.7842e-05, 3.8595e-05])\n",
            "Sum of predicted tensor(3.5523)\n",
            "Loss tensor(0.2110)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.9845e-06, 8.9820e-06, 2.2567e-06, 2.3624e-06, 1.9482e-04, 1.2987e-05,\n",
            "        3.7044e-01, 8.7456e-06, 3.0927e-06, 1.8666e-06, 1.1508e-04, 6.4592e-03,\n",
            "        2.2399e-05, 1.2788e-03, 6.5067e-04, 1.5405e-05, 8.8684e-07, 3.5893e-06,\n",
            "        1.5691e-06, 1.2106e-05, 8.9694e-06, 2.0219e-04, 4.2536e-06, 2.8586e-06,\n",
            "        1.2878e-05, 1.9738e-04, 1.3401e-05, 4.9127e-06, 2.4070e-05, 8.0962e-03,\n",
            "        9.9218e-06, 2.3125e-06, 2.3123e-06, 2.1464e-04, 6.9526e-05, 4.5249e-05,\n",
            "        3.0875e-06, 2.6868e-06, 8.7062e-06, 7.0900e-01, 1.4274e-05, 2.6461e-06,\n",
            "        2.0781e-05, 5.6491e-05, 1.0095e-05, 3.9972e-06, 8.4517e-01, 6.4430e-06,\n",
            "        6.0363e-05, 8.5524e-06, 1.7150e-04, 5.3916e-05, 2.0748e-06, 9.9828e-01,\n",
            "        3.6728e-06, 5.5407e-06, 2.2069e-05, 9.5114e-06, 6.5146e-02, 2.3769e-06,\n",
            "        3.8088e-06, 1.6652e-05, 2.4608e-05, 6.4460e-06])\n",
            "Sum of predicted tensor(3.0063)\n",
            "Loss tensor(0.0570)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.0821e-06, 6.2917e-05, 1.9892e-05, 4.1348e-06, 9.3224e-06, 2.1970e-06,\n",
            "        3.7791e-05, 9.3940e-06, 5.5084e-06, 2.8873e-06, 4.4696e-06, 3.2498e-02,\n",
            "        7.8295e-05, 8.9603e-06, 1.6692e-05, 6.4768e-05, 5.9830e-06, 2.8786e-05,\n",
            "        1.3028e-04, 2.6190e-04, 3.1245e-05, 3.0851e-03, 6.8943e-05, 4.4661e-06,\n",
            "        2.2094e-06, 3.9632e-06, 7.1557e-02, 1.2395e-01, 2.5079e-06, 9.0963e-06,\n",
            "        4.4700e-06, 2.9180e-05, 9.8848e-01, 5.0554e-06, 1.9020e-06, 9.0183e-06,\n",
            "        4.7521e-06, 1.2494e-05, 3.9395e-05, 8.5758e-06, 2.9293e-06, 5.4317e-06,\n",
            "        4.1024e-05, 4.8233e-05, 3.4383e-05, 1.7242e-06, 2.1569e-05, 5.6749e-05,\n",
            "        2.8134e-06, 1.0810e-05, 2.7970e-06, 4.9258e-06, 2.0875e-06, 4.8934e-06,\n",
            "        3.6491e-06, 6.5073e-06, 6.0793e-05, 2.4470e-05, 2.6976e-05, 3.1716e-05,\n",
            "        2.1032e-05, 6.4495e-04, 4.1067e-06, 9.7502e-05])\n",
            "Sum of predicted tensor(1.2217)\n",
            "Loss tensor(0.2632)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.1418e-05, 8.7452e-06, 7.5413e-06, 5.5671e-06, 1.0482e-04, 1.1390e-02,\n",
            "        1.7110e-06, 9.9938e-01, 1.6358e-04, 6.8556e-06, 4.5847e-06, 2.6400e-06,\n",
            "        4.6535e-06, 1.3936e-05, 3.5806e-06, 2.4301e-06, 5.0321e-06, 9.0010e-01,\n",
            "        1.0581e-04, 1.9560e-01, 5.2137e-06, 1.9754e-06, 8.4116e-06, 3.6856e-05,\n",
            "        3.2784e-03, 1.2354e-04, 9.9977e-01, 4.9149e-05, 6.4094e-01, 2.6840e-03,\n",
            "        8.1043e-05, 1.0731e-02, 3.0752e-05, 9.9976e-01, 6.4686e-06, 6.4543e-01,\n",
            "        3.8478e-06, 5.1660e-05, 3.3627e-03, 4.5183e-04, 3.4374e-06, 1.6319e-06,\n",
            "        2.0257e-05, 2.8079e-06, 1.4149e-05, 5.8686e-06, 1.4110e-04, 3.3386e-06,\n",
            "        5.8457e-01, 7.2384e-06, 1.8796e-06, 5.0147e-06, 3.0903e-06, 3.1754e-05])\n",
            "Sum of predicted tensor(5.9986)\n",
            "Loss tensor(0.3400)\n",
            "Acc tensor(0.8889)\n",
            "\tTrain Loss: 0.053 | Train Acc: 98.43%\n",
            "\t Val. Loss: 0.246 |  Val. Acc: 94.96%\n",
            "Sum of predicted tensor(3.7345, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8167, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9978, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0806, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3959, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0364, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2761, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7737, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1250, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7218, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2673, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9730, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1913, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8039, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1020, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7233, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5938, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9929, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7858, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8349, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7046, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0493, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8759, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0466, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9077, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0295, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0002, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0556, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8420, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0851, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8623, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2235, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9716, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8665, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8663, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2402, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0588, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0621, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9928, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7204, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1621, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.5104, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3219, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9860, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.6285, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1324, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9875, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.5401, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.5900, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9832, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9431, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5541, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2645, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3199, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2095, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0481, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1240, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.6223, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.1605, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7571, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6645, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9397, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9434, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2563, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8633, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9934, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4387, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0262, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9434, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0910, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.4359, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8681, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.6511, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3946, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2969, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.7243, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9602, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0238, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0397, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0089, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2326, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.6116, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0787, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0182, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2785, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.1201, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0160, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9989, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3147, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0507, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.5533, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.4406, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9127, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.0611, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1937, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.7873, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.1154, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.7441, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1718, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1869, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0817, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3530, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7026, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.8328, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.7057, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8472, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1900, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.0050, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7908, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8250, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3865, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0841, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1700, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9625, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0406, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9303, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4289, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1202, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5395, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.6121, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3768, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1574, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0110, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5086, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4246, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4222, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.5891, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1229, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2983, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5619, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9939, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5724, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0397, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0275, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0003, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0020, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0503, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.0264, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0389, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5143, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8214, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6983, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.2183, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1449, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2069, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8280, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9726, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1766, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1605, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2618, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0558, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0322, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.4058, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.4508, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0541, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4631, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6033, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2447, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1997, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8812, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8384, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.5044, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9780, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2637, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9853, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9798, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6064, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.4338, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2383, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8587, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0947, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0148, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7152, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9820, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9592, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1794, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.2409, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.2633, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.4711, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9247, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1993, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.3514, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4664, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4177, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4415, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.8477, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8607, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2816, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8410, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0208, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.1753, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0461, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2715, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.5697, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0094, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5829, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1890, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.2078, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1034, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9653, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2960, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2472, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1290, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4833, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0918, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1654, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2611, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.0945, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2051, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.8317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7198, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1315, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5823, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7762, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.4557, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9410, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2451, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.5550, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9548, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0642, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0381, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0525, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7499, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1557, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6464, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8943, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1153, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8072, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9325, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5405, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2009, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7547, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1938, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.5336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0610, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4289, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0597, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9248, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5496, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2044, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9981, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1070, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2673, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2955, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9700, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0515, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6667, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8396, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0392, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3703, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0063, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4477, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4609, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7824, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1855, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7311, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4522, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9919, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.7594, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9315, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0563, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0087, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.4697, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3939, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0193, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3773, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0022, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9786, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1909, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6643, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4769, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0252, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8161, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0156, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1420, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.9220, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8071, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9331, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3987, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2301, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1475, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0118, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4959, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.5391, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1320, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6605, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3154, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1251, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6751, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8863, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0752, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.1355, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.3456, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8083, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0395, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0820, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0684, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9911, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1891, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0048, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2398, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2381, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0027, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5932, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0368, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8340, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9043, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0737, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1148, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9952, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8304, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8450, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.3409, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9905, grad_fn=<SumBackward0>)\n",
            "tensor([2.8567e-05, 1.6221e-04, 7.2973e-06, 1.6221e-04, 2.5611e-04, 8.2724e-03,\n",
            "        2.1311e-04, 1.2677e-04, 1.4366e-05, 7.0842e-05, 1.1910e-04, 3.1042e-06,\n",
            "        1.1424e-04, 1.4167e-05, 3.4655e-03, 1.7071e-05, 1.6906e-04, 1.4287e-02,\n",
            "        3.4655e-03, 7.7859e-05, 3.4589e-05, 2.3197e-04, 2.8075e-05, 3.9693e-05,\n",
            "        1.4287e-02, 4.4742e-05, 1.5016e-04, 5.3215e-04, 3.4655e-03, 6.0674e-04,\n",
            "        9.6622e-05, 1.7771e-05, 3.2223e-05, 3.4655e-03, 5.8638e-04, 4.6508e-05,\n",
            "        2.1486e-04, 2.6005e-04, 3.4655e-03, 8.7576e-06, 3.4655e-03, 3.8159e-05,\n",
            "        2.6352e-05, 2.3763e-05, 3.8159e-05, 4.5240e-05, 3.4655e-03, 3.4655e-03,\n",
            "        3.4655e-03, 4.2587e-05, 3.4655e-03, 3.4655e-03, 5.1484e-04, 2.8937e-04,\n",
            "        1.6014e-05, 3.8159e-05, 4.3224e-03, 2.5801e-04, 3.8159e-05, 1.8709e-05,\n",
            "        2.1197e-05, 8.6894e-06, 5.3361e-05, 4.4589e-05])\n",
            "Sum of predicted tensor(0.0853)\n",
            "Loss tensor(0.2218)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.0449e-05, 1.5837e-04, 7.1418e-06, 8.4172e-01, 1.0052e-05, 4.8814e-05,\n",
            "        1.0305e-05, 4.4583e-05, 5.2810e-01, 2.9005e-02, 2.4532e-05, 1.3311e-03,\n",
            "        8.6902e-05, 8.8191e-05, 1.5644e-03, 5.3006e-01, 8.0248e-06, 3.4367e-04,\n",
            "        2.0020e-05, 9.0356e-04, 9.0174e-06, 9.8104e-05, 3.1725e-06, 2.5611e-04,\n",
            "        2.2516e-03, 4.9658e-02, 9.7117e-06, 1.6221e-04, 7.0116e-06, 9.9015e-05,\n",
            "        9.9980e-01, 4.9007e-01, 4.4678e-06, 9.3703e-06, 3.8045e-05, 2.9179e-05,\n",
            "        6.6530e-06, 2.1311e-04, 7.6982e-06, 3.1725e-06, 6.9188e-04, 4.9004e-04,\n",
            "        1.0120e-04, 1.2110e-05, 1.1350e-05, 7.2598e-05, 8.6065e-05, 7.3922e-05,\n",
            "        1.7428e-05, 2.7540e-03, 9.7946e-06, 3.1529e-05, 1.2050e-04, 1.3014e-03,\n",
            "        5.0644e-05, 5.6382e-06, 3.1725e-06, 3.1880e-04, 1.0363e-05, 8.4396e-05,\n",
            "        1.7952e-04, 3.4120e-05, 4.1592e-06, 6.9188e-04])\n",
            "Sum of predicted tensor(3.4834)\n",
            "Loss tensor(0.0644)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.3423e-06, 6.9188e-04, 1.9463e-06, 2.4116e-05, 9.0980e-06, 1.4426e-02,\n",
            "        9.9955e-01, 3.0755e-03, 7.7273e-03, 3.6958e-05, 1.6555e-05, 2.7243e-06,\n",
            "        6.9188e-04, 9.0510e-05, 2.3527e-05, 1.6221e-04, 4.3360e-05, 3.1725e-06,\n",
            "        6.9188e-04, 6.3639e-06, 1.1679e-05, 2.8448e-05, 2.4787e-02, 1.0292e-02,\n",
            "        1.6759e-04, 5.7330e-05, 1.5974e-05, 2.4682e-04, 8.4590e-06, 6.9641e-06,\n",
            "        1.1509e-05, 7.3341e-05, 3.1725e-06, 6.9188e-04, 1.7813e-05, 1.6221e-04,\n",
            "        6.9188e-04, 1.1516e-05, 6.9188e-04, 3.1458e-04, 3.5712e-05, 8.6902e-05,\n",
            "        7.2843e-06, 2.7621e-05, 7.0884e-01, 1.6262e-05, 6.2168e-05, 6.2487e-04,\n",
            "        6.9188e-04, 1.0251e-05, 2.7360e-04, 1.4815e-03, 1.7764e-03, 5.4626e-03,\n",
            "        2.9361e-05, 1.4029e-04, 1.3468e-04, 1.7813e-05, 3.4706e-05, 1.3434e-05,\n",
            "        9.0162e-01, 4.2513e-03, 1.1282e-03, 4.2475e-05])\n",
            "Sum of predicted tensor(2.6924)\n",
            "Loss tensor(0.5775)\n",
            "Acc tensor(0.8906)\n",
            "tensor([2.8465e-06, 1.0713e-04, 1.3498e-06, 2.7710e-05, 7.6029e-06, 3.9458e-05,\n",
            "        3.4329e-05, 3.3100e-05, 1.0408e-05, 2.3016e-03, 9.1537e-05, 8.6100e-06,\n",
            "        2.3983e-06, 1.1662e-04, 1.9808e-04, 2.0100e-05, 1.0242e-03, 6.2919e-05,\n",
            "        6.2493e-01, 1.6177e-05, 3.6746e-06, 9.5269e-01, 8.6855e-06, 9.9117e-01,\n",
            "        2.3954e-06, 1.5643e-05, 3.8605e-06, 3.6862e-03, 4.0906e-06, 3.7667e-06,\n",
            "        6.3339e-02, 1.2358e-03, 2.7961e-05, 2.5155e-06, 2.4169e-05, 2.6260e-06,\n",
            "        3.3251e-06, 4.3369e-06, 2.0630e-02, 9.0949e-06, 1.6735e-06, 5.2143e-06,\n",
            "        3.5366e-06, 2.4169e-05, 1.5610e-01, 5.4031e-05, 3.1338e-03, 9.5636e-01,\n",
            "        6.5196e-05, 3.1629e-06, 5.4539e-06, 3.8735e-06, 7.5073e-06, 4.8352e-03,\n",
            "        1.0466e-05, 4.4103e-06, 3.3988e-05, 2.9990e-06, 2.5180e-04, 2.4169e-05,\n",
            "        2.9094e-01, 2.6751e-05, 5.8116e-04, 3.4653e-06])\n",
            "Sum of predicted tensor(4.0744)\n",
            "Loss tensor(0.3552)\n",
            "Acc tensor(0.9219)\n",
            "tensor([5.0896e-04, 7.6393e-05, 2.6416e-06, 2.2418e-04, 1.1266e-02, 1.0977e-03,\n",
            "        1.1521e-03, 3.6751e-02, 5.9401e-01, 6.7427e-06, 6.7777e-05, 3.6209e-03,\n",
            "        7.4068e-06, 6.0025e-06, 5.2975e-06, 1.7603e-05, 1.6145e-03, 1.3421e-05,\n",
            "        7.4346e-04, 7.2841e-06, 2.1453e-02, 1.8521e-06, 6.7755e-06, 4.2315e-05,\n",
            "        5.8986e-04, 1.6735e-06, 3.8094e-05, 5.0403e-05, 3.8754e-03, 1.2952e-06,\n",
            "        4.1001e-06, 2.8282e-06, 1.8411e-03, 9.4599e-07, 7.8109e-04, 1.9239e-05,\n",
            "        2.4485e-06, 2.7849e-02, 5.8476e-05, 5.5553e-06, 9.3625e-05, 8.1569e-06,\n",
            "        5.0022e-06, 7.2168e-06, 5.5706e-06, 6.0451e-02, 1.4496e-05, 2.2868e-06,\n",
            "        6.5643e-04, 9.8571e-06, 1.0914e-05, 2.4483e-06, 1.0693e-05, 5.8116e-04,\n",
            "        6.5197e-03, 2.7510e-06, 8.9639e-05, 1.9781e-05, 4.2311e-05, 6.1993e-06,\n",
            "        2.3018e-06, 3.5392e-05, 4.4655e-06, 5.1990e-03])\n",
            "Sum of predicted tensor(0.7816)\n",
            "Loss tensor(0.0111)\n",
            "Acc tensor(1.)\n",
            "tensor([1.7531e-04, 1.1791e-06, 1.7168e-05, 5.3114e-06, 4.5266e-06, 1.2952e-06,\n",
            "        2.5556e-06, 5.0374e-06, 1.5314e-02, 5.5939e-05, 1.2952e-06, 2.9894e-01,\n",
            "        1.2166e-03, 2.0833e-05, 1.4263e-05, 1.4021e-04, 2.2872e-06, 3.8424e-05,\n",
            "        4.8137e-06, 5.9273e-05, 5.8286e-04, 7.8146e-05, 8.8186e-06, 7.3489e-06,\n",
            "        9.8178e-01, 9.8195e-06, 2.3762e-05, 1.4554e-05, 3.0985e-05, 1.2952e-06,\n",
            "        1.4263e-05, 6.3204e-05, 6.2944e-04, 6.2427e-06, 5.2142e-06, 8.0064e-06,\n",
            "        1.1626e-04, 2.9475e-06, 1.1144e-05, 2.3268e-05, 1.2952e-06, 3.3907e-06,\n",
            "        4.9363e-04, 1.9570e-05, 4.1550e-05, 2.1729e-05, 1.2316e-04, 2.9773e-04,\n",
            "        1.7973e-05, 2.8940e-05, 2.1711e-04, 8.6958e-02, 1.2952e-06, 2.9342e-04,\n",
            "        7.4300e-04, 2.1092e-05, 2.3980e-05, 2.7233e-05, 1.2952e-06, 4.4110e-05,\n",
            "        5.8116e-04, 4.3087e-06, 4.6430e-06, 8.7927e-06])\n",
            "Sum of predicted tensor(1.3894)\n",
            "Loss tensor(0.2229)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.6414e-06, 2.9586e-06, 3.7291e-06, 5.2432e-05, 1.0918e-06, 3.2072e-06,\n",
            "        1.0753e-06, 2.6641e-06, 1.1950e-06, 5.0137e-03, 1.2743e-06, 4.2272e-04,\n",
            "        3.5935e-06, 6.8112e-01, 5.6259e-04, 5.8116e-04, 2.4203e-06, 5.5428e-05,\n",
            "        6.8788e-05, 4.8261e-06, 1.2952e-06, 2.9990e-06, 2.8953e-05, 6.2480e-05,\n",
            "        1.3521e-05, 1.4080e-05, 1.1941e-03, 1.2952e-06, 1.4097e-04, 9.8195e-06,\n",
            "        3.6836e-05, 5.7275e-01, 1.1510e-05, 5.9551e-06, 3.8994e-05, 2.0431e-05,\n",
            "        7.3913e-06, 1.9102e-05, 2.0587e-06, 5.8528e-06, 2.1838e-04, 2.8644e-05,\n",
            "        4.9307e-03, 1.6735e-06, 6.5677e-05, 1.1363e-04, 5.2949e-04, 2.4297e-06,\n",
            "        5.2077e-06, 2.7601e-05, 1.6154e-03, 8.7806e-06, 1.1573e-04, 2.7544e-05,\n",
            "        3.1605e-02, 5.8116e-04, 2.4970e-06, 3.1344e-05, 8.8539e-06, 3.3535e-04,\n",
            "        1.2952e-06, 3.4702e-04, 3.0087e-05, 2.5146e-03])\n",
            "Sum of predicted tensor(1.3054)\n",
            "Loss tensor(0.1484)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.0781e-06, 3.1866e-06, 4.6728e-05, 6.2975e-05, 1.5235e-05, 3.8727e-05,\n",
            "        5.2405e-06, 1.3391e-05, 1.3575e-06, 9.9017e-01, 8.6034e-04, 7.8209e-02,\n",
            "        1.1984e-06, 1.7489e-05, 4.1163e-06, 1.3275e-05, 2.3273e-05, 1.1534e-05,\n",
            "        2.7174e-06, 4.0799e-05, 1.3654e-05, 7.6052e-06, 8.8763e-04, 1.1631e-05,\n",
            "        3.5591e-06, 2.4749e-02, 3.3967e-06, 1.9416e-06, 1.9416e-06, 1.8388e-06,\n",
            "        7.2819e-06, 1.6384e-06, 6.6094e-05, 1.1458e-06, 9.9217e-01, 1.1674e-06,\n",
            "        1.0820e-03, 1.0222e-04, 5.7880e-04, 4.2162e-05, 2.1210e-04, 6.1048e-04,\n",
            "        2.2117e-06, 8.6836e-06, 1.4258e-05, 6.2198e-06, 1.0913e-05, 2.1865e-06,\n",
            "        2.8338e-03, 2.1482e-05, 1.1950e-06, 8.8952e-06, 1.1187e-06, 5.9673e-05,\n",
            "        1.1950e-06, 1.5493e-01, 1.0905e-06, 1.1083e-06, 4.9538e-03, 1.0428e-06,\n",
            "        2.7174e-06, 1.0442e-06, 1.1085e-04, 1.2353e-06])\n",
            "Sum of predicted tensor(2.2531)\n",
            "Loss tensor(0.1790)\n",
            "Acc tensor(0.9531)\n",
            "tensor([7.6814e-06, 5.6767e-06, 3.1181e-04, 1.1950e-06, 2.8195e-06, 1.7794e-02,\n",
            "        1.8806e-05, 6.4724e-06, 4.1773e-06, 1.2314e-03, 1.1187e-06, 9.7013e-06,\n",
            "        1.1085e-04, 8.6034e-04, 2.0638e-06, 6.1687e-05, 7.2730e-04, 1.1095e-06,\n",
            "        5.3532e-01, 2.7026e-04, 1.4193e-02, 3.3406e-05, 1.8038e-05, 5.9474e-05,\n",
            "        1.9660e-06, 6.3179e-03, 2.2034e-06, 9.9970e-01, 1.1260e-06, 4.8457e-04,\n",
            "        1.9660e-06, 6.6057e-06, 8.6034e-04, 5.0211e-04, 1.0809e-06, 2.7174e-06,\n",
            "        1.1240e-05, 1.2353e-06, 2.1024e-04, 4.1359e-04, 1.3148e-04, 5.0737e-05,\n",
            "        2.2619e-06, 1.1950e-06, 2.9070e-06, 1.1636e-06, 1.2648e-05, 1.4258e-05,\n",
            "        1.0027e-01, 1.2529e-05, 1.1064e-05, 1.2472e-06, 1.0931e-06, 8.1594e-06,\n",
            "        6.7155e-06, 1.2353e-06, 7.9418e-01, 1.0216e-05, 9.4948e-05, 1.1950e-06,\n",
            "        8.1672e-06, 2.8057e-06, 1.3542e-05, 1.7547e-04])\n",
            "Sum of predicted tensor(2.4746)\n",
            "Loss tensor(0.2777)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1003e-06, 7.7477e-06, 3.2302e-06, 1.9325e-05, 4.2486e-04, 1.6361e-06,\n",
            "        2.1736e-05, 1.2069e-06, 8.3008e-06, 2.0257e-04, 1.1152e-06, 1.4799e-05,\n",
            "        3.1530e-05, 2.5520e-05, 2.8436e-06, 1.6332e-02, 1.4834e-01, 1.1059e-06,\n",
            "        3.2604e-06, 2.0840e-06, 9.6296e-06, 6.3769e-06, 2.8260e-05, 3.8929e-05,\n",
            "        1.5314e-05, 1.4091e-05, 1.9567e-05, 9.4358e-05, 1.0174e-05, 1.2518e-06,\n",
            "        4.3511e-04, 2.7269e-04, 1.1626e-06, 1.5018e-04, 7.2693e-05, 1.2353e-06,\n",
            "        1.1326e-05, 1.7618e-04, 3.0976e-06, 3.8353e-05, 6.8199e-06, 8.5875e-06,\n",
            "        1.1950e-06, 4.7649e-01, 1.7748e-03, 9.9782e-01, 1.0641e-06, 2.6730e-06,\n",
            "        1.4874e-05, 1.0754e-06, 1.1989e-06, 4.3613e-06, 1.5532e-05, 1.3207e-05,\n",
            "        1.5568e-05, 2.5521e-03, 1.9563e-04, 3.6921e-05, 8.3918e-06, 6.3350e-06,\n",
            "        1.4468e-06, 8.6375e-04, 1.6093e-06, 7.5848e-06])\n",
            "Sum of predicted tensor(1.6467)\n",
            "Loss tensor(0.1732)\n",
            "Acc tensor(0.9688)\n",
            "tensor([4.5961e-05, 9.2316e-06, 1.1613e-06, 5.2952e-04, 9.2224e-06, 2.6675e-05,\n",
            "        1.1287e-05, 1.2995e-06, 6.2096e-05, 2.3140e-05, 1.8615e-04, 4.8250e-06,\n",
            "        2.7685e-03, 6.7320e-06, 8.6034e-04, 9.9964e-01, 8.4212e-05, 1.0959e-06,\n",
            "        5.8313e-01, 1.4702e-03, 2.3656e-05, 6.6267e-06, 1.5193e-05, 7.5671e-06,\n",
            "        3.1654e-01, 3.4372e-05, 1.8494e-05, 1.1950e-06, 7.8344e-05, 4.1242e-05,\n",
            "        3.7208e-05, 1.0947e-06, 1.1950e-06, 1.1680e-06, 1.0383e-06, 4.1657e-06,\n",
            "        1.5870e-02, 4.0799e-05, 1.1198e-06, 3.3823e-06, 1.2659e-05, 9.9159e-06,\n",
            "        3.9255e-05, 1.6401e-05, 1.2420e-06, 9.9018e-01, 3.5173e-04, 3.3544e-03,\n",
            "        2.0745e-05, 2.7174e-06, 1.9660e-06, 1.2533e-03, 1.7756e-06, 3.5060e-04,\n",
            "        2.1493e-05, 3.9270e-01, 3.4810e-05, 1.8990e-04, 2.0291e-05, 1.3207e-05,\n",
            "        1.4435e-04, 5.8775e-05, 9.0721e-06, 4.9943e-05])\n",
            "Sum of predicted tensor(3.3104)\n",
            "Loss tensor(0.2781)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1950e-06, 1.1658e-06, 8.7323e-01, 9.6811e-02, 5.3795e-06, 1.1950e-06,\n",
            "        3.4828e-05, 1.1956e-06, 3.8135e-06, 2.0203e-06, 1.1240e-05, 8.1007e-05,\n",
            "        7.4660e-03, 1.1519e-06, 1.1536e-06, 6.4595e-06, 5.9325e-02, 2.2819e-06,\n",
            "        1.4003e-01, 2.6541e-04, 9.9752e-01, 3.2831e-01, 1.8307e-06, 9.4268e-01,\n",
            "        1.2709e-06, 1.1875e-04, 7.5266e-05, 2.7174e-06, 1.5656e-03, 9.9439e-01,\n",
            "        9.1285e-06, 5.6741e-04, 1.6601e-02, 7.7427e-01, 1.1198e-06, 6.8190e-06,\n",
            "        8.8300e-06, 3.6215e-06, 1.0508e-06, 6.9674e-05, 1.1240e-05, 3.3104e-06,\n",
            "        7.2791e-02, 1.5194e-06, 3.7645e-06, 2.7174e-06, 2.2333e-04, 1.3121e-05,\n",
            "        1.4140e-01, 4.0799e-05, 1.6169e-04, 4.0046e-05, 1.2272e-06, 4.5572e-05,\n",
            "        8.3036e-01, 1.0831e-06, 1.0943e-06, 1.1950e-06, 2.4645e-05, 1.2171e-02,\n",
            "        1.1411e-06, 1.8759e-04, 1.8041e-06, 1.8390e-06])\n",
            "Sum of predicted tensor(6.2910)\n",
            "Loss tensor(0.2662)\n",
            "Acc tensor(0.9219)\n",
            "tensor([8.9695e-01, 4.3589e-06, 1.1372e-03, 6.6092e-04, 1.0657e-06, 2.4645e-05,\n",
            "        2.5736e-05, 2.7174e-06, 5.9011e-05, 1.6876e-04, 5.2154e-06, 9.7795e-01,\n",
            "        5.9682e-05, 2.8081e-04, 9.9689e-01, 5.2798e-02, 3.1843e-05, 2.5815e-05,\n",
            "        1.3639e-05, 1.1008e-06, 2.9367e-06, 2.7174e-06, 1.3923e-05, 5.6450e-06,\n",
            "        1.7814e-05, 1.0688e-06, 5.5377e-06, 2.3008e-05, 3.0042e-04, 1.0264e-05,\n",
            "        4.3207e-06, 1.8278e-06, 5.0317e-05, 1.0473e-04, 1.0798e-04, 1.1950e-06,\n",
            "        1.2103e-06, 1.8498e-04, 4.3041e-06, 1.1365e-06, 7.3307e-06, 9.6530e-06,\n",
            "        2.0413e-01, 5.5150e-04, 1.7477e-05, 3.3652e-05, 7.3556e-06, 4.9832e-05,\n",
            "        1.1950e-06, 4.0687e-05, 9.2761e-06, 1.1950e-06, 2.3266e-05, 7.8560e-06,\n",
            "        1.0794e-06, 1.1669e-05, 9.9570e-01, 6.3425e-06, 1.1142e-06, 3.5654e-06,\n",
            "        1.1950e-06, 6.8352e-06, 2.6488e-05, 1.1348e-06])\n",
            "Sum of predicted tensor(4.1286)\n",
            "Loss tensor(0.2369)\n",
            "Acc tensor(0.9375)\n",
            "tensor([2.9852e-06, 5.7306e-06, 7.0232e-06, 4.4854e-06, 8.1796e-05, 4.8251e-06,\n",
            "        5.6584e-04, 9.7370e-01, 8.6481e-02, 3.2473e-05, 9.6112e-01, 1.1435e-04,\n",
            "        1.1412e-06, 5.5752e-05, 3.6586e-02, 5.0175e-05, 6.3542e-05, 1.4311e-05,\n",
            "        5.4912e-06, 1.4156e-05, 4.3295e-06, 4.4854e-06, 1.5954e-05, 2.8813e-05,\n",
            "        5.9923e-06, 2.1502e-06, 3.8713e-06, 4.6811e-06, 2.1997e-05, 9.5370e-04,\n",
            "        5.1663e-06, 2.7595e-05, 3.2616e-06, 1.2624e-05, 9.9975e-01, 1.3887e-05,\n",
            "        2.4587e-05, 3.9887e-01, 4.4854e-06, 3.5338e-02, 9.8359e-01, 1.0459e-06,\n",
            "        1.5208e-03, 4.4854e-06, 9.9795e-01, 9.1612e-06, 5.6682e-05, 4.2647e-05,\n",
            "        4.4854e-06, 5.1147e-06, 4.1958e-06, 1.5374e-05, 1.7038e-04, 2.0681e-06,\n",
            "        2.0568e-03, 1.2203e-06, 1.0917e-04, 3.5544e-03, 4.4854e-06, 4.4854e-06,\n",
            "        1.3145e-04, 2.3081e-05, 4.9478e-04, 4.4854e-06])\n",
            "Sum of predicted tensor(5.4837)\n",
            "Loss tensor(0.4972)\n",
            "Acc tensor(0.9062)\n",
            "tensor([5.0697e-03, 1.5753e-03, 2.1869e-04, 1.9146e-05, 5.0175e-05, 4.4854e-06,\n",
            "        2.2183e-05, 2.3431e-06, 4.4854e-06, 1.4869e-04, 1.0970e-04, 1.0423e-06,\n",
            "        1.6875e-05, 8.3553e-06, 1.2321e-06, 1.6409e-05, 1.9756e-05, 2.7377e-04,\n",
            "        3.5760e-04, 3.3039e-06, 6.9282e-06, 1.4943e-05, 2.8856e-06, 4.9046e-06,\n",
            "        1.3118e-05, 4.4854e-06, 1.1595e-06, 3.4391e-03, 2.2389e-04, 2.9334e-05,\n",
            "        2.2492e-04, 3.4372e-06, 6.4262e-05, 5.3184e-04, 1.1697e-06, 9.8842e-03,\n",
            "        3.2120e-05, 4.4854e-06, 3.0488e-06, 1.8717e-05, 2.2102e-06, 2.5537e-06,\n",
            "        3.7189e-06, 2.1544e-03, 5.1182e-03, 2.4671e-05, 3.5533e-06, 1.7533e-05,\n",
            "        2.6230e-02, 5.4816e-05, 2.3431e-06, 3.1178e-05, 2.9311e-06, 1.0276e-05,\n",
            "        9.9777e-01, 2.6378e-06, 9.0994e-01, 5.2777e-06, 1.6128e-05, 2.3867e-02,\n",
            "        6.3899e-05, 1.9673e-06, 5.9685e-06, 2.5538e-06])\n",
            "Sum of predicted tensor(1.9878)\n",
            "Loss tensor(0.2575)\n",
            "Acc tensor(0.9531)\n",
            "tensor([8.8661e-06, 1.1374e-06, 4.4854e-06, 3.3946e-05, 1.9243e-05, 5.2809e-05,\n",
            "        3.1947e-05, 2.6212e-05, 3.8778e-05, 4.4854e-06, 2.4397e-06, 4.9708e-06,\n",
            "        8.1821e-06, 1.0959e-04, 9.2627e-02, 1.1838e-01, 3.2151e-05, 2.9701e-06,\n",
            "        5.3957e-06, 9.9790e-05, 1.2751e-06, 5.7836e-05, 6.3240e-06, 4.0189e-06,\n",
            "        2.2225e-05, 9.9981e-01, 4.4854e-06, 3.7940e-02, 9.1122e-05, 1.0423e-06,\n",
            "        1.6843e-05, 2.3452e-05, 5.1550e-06, 9.5603e-05, 4.4854e-06, 4.4854e-06,\n",
            "        2.5406e-06, 9.7865e-01, 1.2845e-05, 1.1630e-05, 2.5146e-06, 1.1394e-04,\n",
            "        5.4774e-01, 4.4854e-06, 8.0044e-04, 2.3899e-05, 2.6689e-06, 1.0797e-05,\n",
            "        4.4854e-06, 2.1662e-04, 8.0937e-06, 4.4854e-06, 6.6748e-04, 3.6247e-05,\n",
            "        2.9626e-02, 9.8994e-01, 1.2644e-05, 9.4970e-01, 1.9503e-06, 9.9985e-01,\n",
            "        4.7184e-06, 1.1614e-05, 1.4784e-05, 2.8434e-03])\n",
            "Sum of predicted tensor(5.7499)\n",
            "Loss tensor(0.3038)\n",
            "Acc tensor(0.9375)\n",
            "tensor([4.8011e-06, 4.4854e-06, 2.7072e-06, 9.1905e-06, 6.1287e-04, 2.0318e-06,\n",
            "        3.0480e-05, 2.2404e-05, 5.3290e-06, 5.5561e-05, 1.4992e-04, 1.4927e-06,\n",
            "        1.7117e-05, 4.0477e-05, 6.4231e-06, 4.4854e-06, 4.4854e-06, 2.9355e-06,\n",
            "        2.5770e-06, 4.9748e-06, 1.0842e-06, 2.0887e-05, 3.8347e-04, 1.2627e-06,\n",
            "        2.8308e-06, 4.9316e-06, 6.4814e-06, 5.2919e-06, 3.3929e-05, 2.5752e-06,\n",
            "        2.6395e-06, 1.0090e-05, 4.3492e-06, 4.9404e-06, 7.1629e-06, 1.8123e-05,\n",
            "        1.5598e-04, 4.2807e-06, 4.1290e-05, 1.6516e-06, 1.4014e-05, 4.8630e-05,\n",
            "        1.6151e-06, 1.6015e-04, 6.8949e-06, 3.0044e-05, 7.3493e-05, 4.4854e-06,\n",
            "        4.4854e-06, 7.3944e-06, 5.1682e-06, 9.0765e-01, 4.6692e-05, 9.9055e-01,\n",
            "        5.8836e-03, 1.3767e-05, 4.1229e-06, 1.2300e-05, 6.8971e-06, 5.5711e-03,\n",
            "        1.9694e-06, 1.0086e-04, 1.0423e-06, 1.0423e-06])\n",
            "Sum of predicted tensor(1.9119)\n",
            "Loss tensor(0.2776)\n",
            "Acc tensor(0.9531)\n",
            "tensor([6.8804e-06, 5.9565e-06, 5.8801e-06, 4.5717e-02, 4.4854e-06, 3.8347e-04,\n",
            "        1.1762e-06, 1.7776e-05, 3.3035e-05, 1.0812e-05, 2.7065e-05, 4.4854e-06,\n",
            "        4.8213e-06, 7.3224e-05, 4.1064e-06, 5.4644e-05, 1.0459e-06, 6.2092e-06,\n",
            "        2.9958e-06, 4.4854e-06, 3.2970e-06, 4.4854e-06, 2.9899e-06, 6.3048e-06,\n",
            "        2.7985e-03, 3.3662e-05, 8.8955e-07, 1.2997e-03, 3.8382e-03, 4.4854e-06,\n",
            "        1.0500e-05, 5.0300e-06, 4.4854e-06, 5.4138e-02, 3.1310e-05, 3.5439e-05,\n",
            "        1.1086e-06, 4.4854e-06, 9.9971e-01, 2.2736e-06, 1.7706e-05, 1.0383e-01,\n",
            "        1.7776e-05, 2.1940e-05, 1.6979e-06, 3.2776e-06, 4.5381e-06, 4.4854e-06,\n",
            "        7.8410e-05, 1.9270e-06, 3.3690e-03, 5.8225e-05, 1.2295e-06, 5.5798e-02,\n",
            "        4.4854e-06, 8.1375e-06, 1.2879e-03, 4.4967e-03, 4.2797e-03, 4.0198e-04,\n",
            "        1.0711e-01, 3.7582e-04, 5.5768e-06, 9.1239e-06])\n",
            "Sum of predicted tensor(1.3895)\n",
            "Loss tensor(0.3947)\n",
            "Acc tensor(0.9219)\n",
            "tensor([6.3478e-06, 1.0651e-03, 3.4557e-06, 4.4854e-06, 4.4854e-06, 3.1826e-05,\n",
            "        4.4854e-06, 2.1475e-06, 1.0548e-01, 1.3635e-03, 2.2864e-02, 1.7591e-05,\n",
            "        3.9518e-05, 1.7241e-06, 1.3309e-04, 4.3132e-03, 1.0020e-05, 3.2203e-05,\n",
            "        9.9976e-01, 1.8965e-06, 1.1205e-01, 8.4253e-02, 4.4854e-06, 5.9848e-05,\n",
            "        1.2806e-04, 4.4143e-06, 1.3151e-04, 1.6233e-05, 6.8058e-04, 2.5465e-06,\n",
            "        1.2411e-05, 1.7077e-05, 2.4710e-05, 7.8524e-06, 8.0997e-06, 6.3333e-05,\n",
            "        1.3144e-04, 9.8886e-05, 1.0150e-04, 1.1969e-05, 2.0227e-05, 1.3653e-03,\n",
            "        1.2565e-06, 3.7280e-06, 1.3432e-05, 1.5431e-04, 1.6639e-05, 1.2627e-06,\n",
            "        1.1503e-06, 4.4854e-06, 4.4854e-06, 5.9981e-06, 4.5680e-05, 2.4839e-03,\n",
            "        4.4254e-06, 2.8856e-06, 4.4854e-06, 8.7223e-06, 6.4622e-06, 2.8464e-06,\n",
            "        3.0356e-05, 5.4769e-03, 5.6609e-05, 3.6525e-04])\n",
            "Sum of predicted tensor(1.3430)\n",
            "Loss tensor(0.2154)\n",
            "Acc tensor(0.9688)\n",
            "tensor([4.9572e-05, 1.2320e-04, 1.7779e-06, 4.5129e-06, 4.3920e-06, 4.8615e-05,\n",
            "        1.2977e-01, 9.7422e-01, 7.5195e-02, 9.0874e-01, 4.8527e-05, 1.5299e-06,\n",
            "        4.6092e-05, 6.8733e-06, 3.2029e-06, 4.5414e-06, 4.4524e-05, 1.8036e-05,\n",
            "        1.1654e-05, 5.2045e-05, 6.7431e-05, 1.3747e-06, 2.2878e-04, 2.4333e-04,\n",
            "        1.8149e-05, 5.6483e-06, 2.3910e-06, 9.3078e-05, 3.2701e-05, 1.3599e-06,\n",
            "        2.6679e-06, 6.1436e-03, 7.9672e-06, 8.1955e-05, 1.8645e-03, 1.0848e-06,\n",
            "        9.0181e-06, 9.6291e-05, 3.0991e-04, 6.5958e-06, 4.4854e-06, 7.4561e-05,\n",
            "        2.1544e-03, 2.7728e-05, 4.8138e-05, 3.8970e-06, 4.4854e-06, 9.9981e-01,\n",
            "        1.5883e-05, 9.9981e-01, 2.5959e-06, 9.8892e-06, 1.1348e-06, 4.4854e-06,\n",
            "        1.0184e-03, 2.6428e-06, 7.8650e-05, 9.9994e-01, 5.1432e-05, 9.9981e-01,\n",
            "        7.7055e-01, 3.1992e-06, 1.2879e-03, 8.0523e-01])\n",
            "Sum of predicted tensor(7.6775)\n",
            "Loss tensor(0.3192)\n",
            "Acc tensor(0.9219)\n",
            "tensor([2.1402e-06, 9.9664e-01, 5.7697e-06, 6.2634e-06, 9.9944e-01, 3.0571e-02,\n",
            "        1.3876e-05, 1.6918e-05, 6.6668e-06, 6.7406e-06, 4.0374e-06, 1.1698e-05,\n",
            "        1.0453e-05, 1.7208e-06, 5.8264e-06, 1.7523e-06, 3.3700e-05, 1.7338e-04,\n",
            "        1.0845e-06, 4.8669e-06, 7.7531e-06, 4.0412e-05, 4.0247e-04, 1.7450e-06,\n",
            "        1.3115e-04, 3.4904e-06, 4.1369e-06, 1.2861e-03, 9.9948e-01, 9.9392e-01,\n",
            "        6.3633e-04, 9.4325e-01, 1.3889e-06, 1.3747e-06, 6.9712e-06, 1.4371e-02,\n",
            "        1.1300e-05, 2.6263e-05, 4.4386e-06, 1.6666e-05, 9.2759e-07, 4.0656e-06,\n",
            "        9.9944e-01, 2.0523e-04, 4.1245e-05, 1.1360e-05, 2.5708e-05, 6.5049e-05,\n",
            "        1.8321e-05, 1.0453e-05, 1.6222e-05, 3.5204e-06, 3.8540e-04, 2.3517e-06,\n",
            "        4.2882e-06, 5.5242e-06, 4.1990e-04, 9.0192e-06, 4.4801e-06, 1.1094e-04,\n",
            "        1.8415e-04, 1.2386e-05, 1.3849e-05, 6.5309e-05])\n",
            "Sum of predicted tensor(5.9816)\n",
            "Loss tensor(0.1254)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.9520e-06, 2.1039e-06, 6.7706e-05, 4.6197e-02, 3.3581e-01, 8.0664e-04,\n",
            "        1.6271e-06, 2.2895e-06, 2.4798e-04, 2.4125e-06, 1.5078e-05, 9.2413e-06,\n",
            "        4.1174e-04, 9.7852e-04, 1.2620e-05, 4.7207e-06, 6.7643e-06, 2.7833e-06,\n",
            "        1.0105e-05, 5.0770e-06, 1.7673e-05, 2.4894e-05, 9.9944e-01, 1.3190e-03,\n",
            "        9.8737e-01, 3.0528e-05, 4.9615e-06, 2.4996e-06, 9.9948e-01, 2.8733e-06,\n",
            "        1.6709e-04, 3.6051e-06, 1.0475e-05, 9.9890e-01, 5.0616e-06, 1.7149e-06,\n",
            "        4.1994e-06, 4.7406e-05, 5.2885e-05, 3.3581e-01, 8.6233e-06, 1.5103e-04,\n",
            "        2.5825e-05, 2.3900e-06, 1.5569e-05, 8.1938e-01, 3.7072e-02, 7.4496e-05,\n",
            "        2.1780e-05, 1.2444e-02, 1.9298e-04, 1.5130e-03, 5.1753e-06, 1.9250e-06,\n",
            "        5.4536e-05, 8.7697e-03, 2.2891e-01, 1.6676e-06, 1.0357e-05, 1.1147e-05,\n",
            "        1.0905e-05, 3.0528e-05, 1.6273e-03, 4.5055e-06])\n",
            "Sum of predicted tensor(5.8176)\n",
            "Loss tensor(0.1734)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.5493e-05, 2.6743e-03, 9.9499e-01, 5.4927e-06, 9.2087e-06, 2.6977e-06,\n",
            "        9.0328e-06, 6.6817e-06, 1.0484e-05, 3.3102e-04, 2.9569e-05, 5.3178e-05,\n",
            "        4.5999e-04, 4.1910e-05, 1.0009e-05, 9.9451e-06, 7.5727e-06, 1.7560e-05,\n",
            "        6.1069e-05, 3.0451e-06, 1.3747e-06, 3.3581e-01, 3.1278e-05, 2.2600e-05,\n",
            "        5.6791e-05, 1.3735e-04, 4.0606e-03, 4.5079e-05, 3.7889e-06, 2.2895e-06,\n",
            "        4.3794e-01, 1.9014e-04, 1.0084e-05, 2.7929e-03, 5.8301e-06, 6.5999e-06,\n",
            "        3.4652e-06, 4.7684e-06, 1.2863e-04, 1.7587e-02, 2.3243e-05, 9.9948e-01,\n",
            "        1.1502e-05, 2.8230e-06, 1.3247e-05, 1.5804e-06, 3.4229e-05, 1.2751e-05,\n",
            "        4.4386e-06, 2.6445e-05, 3.6725e-06, 2.7616e-06, 1.0374e-05, 9.9948e-01,\n",
            "        3.0500e-01, 5.7558e-06, 2.2815e-04, 9.9982e-01, 2.1512e-06, 2.2895e-06,\n",
            "        2.1120e-06, 9.9671e-01, 3.8234e-06, 1.0978e-03])\n",
            "Sum of predicted tensor(6.0996)\n",
            "Loss tensor(0.1217)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.0777e-06, 1.6337e-05, 2.1569e-05, 2.1233e-05, 1.7523e-06, 1.5060e-02,\n",
            "        3.6373e-05, 9.4260e-06, 3.6814e-05, 2.1496e-06, 3.2911e-05, 2.9283e-06,\n",
            "        3.5618e-06, 9.8335e-06, 7.8549e-06, 5.9665e-02, 1.2704e-03, 9.6240e-03,\n",
            "        8.1405e-06, 7.0411e-06, 2.9482e-01, 4.3408e-05, 7.8762e-01, 1.6768e-05,\n",
            "        9.9896e-01, 4.0374e-06, 1.1494e-05, 2.8784e-05, 1.4320e-03, 6.2153e-06,\n",
            "        8.4102e-05, 2.5092e-03, 2.5423e-06, 3.9804e-05, 2.9753e-06, 8.2202e-06,\n",
            "        3.2088e-04, 3.7836e-06, 7.9414e-04, 7.4825e-06, 4.3728e-06, 2.0639e-06,\n",
            "        2.7839e-05, 1.1567e-05, 1.3454e-06, 2.2966e-06, 2.3754e-03, 3.2853e-05,\n",
            "        1.1148e-03, 9.9948e-01, 1.5002e-06, 4.8803e-06, 6.4454e-06, 2.2047e-05,\n",
            "        3.6634e-06, 4.7197e-05, 7.2579e-05, 8.3943e-01, 4.5259e-06, 9.2977e-01,\n",
            "        2.1929e-05, 2.1376e-03, 3.3581e-01, 1.7782e-05])\n",
            "Sum of predicted tensor(5.2829)\n",
            "Loss tensor(0.3456)\n",
            "Acc tensor(0.9219)\n",
            "tensor([1.1567e-05, 1.0603e-05, 5.7743e-06, 1.5063e-05, 5.0887e-06, 7.6672e-06,\n",
            "        7.8295e-06, 6.3998e-01, 1.4387e-05, 2.5656e-05, 1.6297e-06, 7.9102e-06,\n",
            "        9.0375e-06, 6.5355e-03, 7.4189e-06, 5.6360e-06, 1.1362e-05, 2.0123e-05,\n",
            "        7.8807e-05, 9.9948e-01, 1.3058e-04, 1.5681e-06, 2.1402e-06, 1.3146e-05,\n",
            "        2.1402e-06, 9.8726e-06, 1.1797e-05, 6.9857e-06, 4.4814e-06, 8.8523e-06,\n",
            "        4.1654e-06, 5.0631e-06, 3.0837e-05, 1.2680e-06, 1.1925e-06, 9.3724e-04,\n",
            "        4.4386e-06, 3.0912e-05, 9.9909e-01, 9.9978e-01, 7.1669e-04, 1.3319e-04,\n",
            "        5.3819e-06, 4.7684e-06, 1.8116e-05, 5.8301e-06, 3.8540e-04, 8.3996e-06,\n",
            "        1.9123e-05, 1.3816e-05, 1.2085e-05, 9.9303e-01, 9.9903e-01, 1.3447e-06,\n",
            "        1.0931e-04, 3.8295e-03, 5.6359e-05, 4.1340e-01, 9.8620e-01, 8.9046e-01,\n",
            "        1.3732e-05, 2.4229e-02, 1.6198e-01, 1.9520e-06])\n",
            "Sum of predicted tensor(8.1200)\n",
            "Loss tensor(0.3895)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.1165e-05, 1.6694e-05, 6.3455e-06, 9.9786e-01, 2.8666e-03, 1.4847e-05,\n",
            "        2.3643e-05, 7.5477e-06, 9.3033e-06, 1.6598e-05, 2.1402e-06, 6.7752e-06,\n",
            "        1.3619e-05, 3.9535e-06, 1.2863e-04, 4.0753e-06, 4.0861e-05, 6.1108e-06,\n",
            "        9.9944e-01, 5.5909e-04, 1.7784e-06, 3.1801e-06, 1.4999e-05, 1.3406e-05,\n",
            "        8.6695e-05, 7.5844e-06, 1.4483e-04, 3.7670e-05, 6.4599e-04, 4.3094e-05,\n",
            "        6.5104e-06, 7.8816e-06, 1.6461e-05, 8.1021e-06, 6.5578e-06, 1.4012e-04,\n",
            "        4.9522e-05, 4.9990e-06, 3.9445e-06, 9.6506e-06, 3.7942e-06, 2.2889e-05,\n",
            "        1.1407e-04, 2.4629e-06, 5.6611e-06, 2.7474e-06, 4.7167e-06, 8.0700e-06,\n",
            "        6.4113e-06, 1.4172e-06, 1.1654e-05, 1.4230e-05, 2.5599e-06, 1.2217e-06,\n",
            "        2.2458e-06, 9.0004e-06, 4.4386e-06, 1.9120e-02, 4.9554e-03, 6.1296e-05,\n",
            "        2.6988e-06, 9.9948e-01, 5.7073e-06, 2.3273e-06])\n",
            "Sum of predicted tensor(3.0262)\n",
            "Loss tensor(0.0965)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.9915e-01, 5.9168e-06, 1.2351e-05, 3.8814e-05, 5.8116e-02, 8.0181e-05,\n",
            "        3.3955e-06, 9.3034e-05, 1.0312e-05, 4.2568e-06, 4.1676e-06, 2.5906e-02,\n",
            "        4.4368e-05, 7.8542e-05, 9.9948e-01, 9.9753e-01, 2.7458e-02, 1.7090e-06,\n",
            "        6.7976e-06, 4.2858e-06, 1.9545e-06, 1.4153e-06, 9.9206e-01, 1.5096e-05,\n",
            "        2.1909e-06, 9.9944e-01, 1.5185e-05, 7.3108e-04, 2.2808e-05, 9.9896e-01,\n",
            "        9.9948e-01, 7.1215e-06, 6.7234e-06, 9.9090e-01, 3.5176e-06, 2.9274e-05,\n",
            "        1.8621e-05, 9.1813e-06, 1.8097e-05, 4.4001e-04, 2.3528e-05, 9.9848e-01,\n",
            "        1.2888e-06, 2.4144e-06, 2.0915e-05, 2.7705e-04, 2.3402e-06, 1.7033e-05,\n",
            "        6.6723e-03, 1.3747e-06, 4.6019e-06, 8.0567e-06, 1.9811e-06, 7.2247e-06,\n",
            "        1.5103e-04, 3.2440e-02, 4.4386e-06, 1.7208e-05, 2.3400e-04, 9.6550e-06,\n",
            "        8.3580e-06, 5.2860e-05, 1.6844e-06, 1.0095e-05])\n",
            "Sum of predicted tensor(9.1286)\n",
            "Loss tensor(0.4019)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.4088e-04, 2.4640e-05, 6.8387e-06, 2.7176e-06, 5.3910e-06, 6.6649e-05,\n",
            "        9.9990e-01, 5.5424e-06, 1.8553e-06, 6.0686e-06, 5.2948e-02, 9.9987e-01,\n",
            "        8.3383e-06, 7.8433e-06, 5.0881e-06, 4.5476e-06, 9.9568e-01, 3.2044e-06,\n",
            "        4.8225e-05, 8.0979e-06, 9.9950e-01, 4.2359e-06, 1.3079e-06, 8.6534e-06,\n",
            "        1.1788e-05, 5.8759e-06, 7.8785e-06, 3.2142e-04, 1.4288e-04, 2.2564e-03,\n",
            "        3.8546e-06, 7.2497e-06, 6.0948e-06, 2.2679e-04, 6.0518e-03, 8.3269e-06,\n",
            "        1.1682e-04, 7.0794e-01, 8.2083e-05, 9.9592e-01, 7.7961e-05, 3.7566e-06,\n",
            "        2.0047e-06, 4.3848e-06, 8.1199e-05, 2.5872e-06, 5.6028e-06, 1.9044e-05,\n",
            "        5.0389e-06, 2.5007e-06, 3.7311e-06, 3.1097e-06, 1.0276e-05, 1.8387e-05,\n",
            "        7.6236e-06, 3.6716e-05, 2.6700e-06, 6.6934e-04, 3.8993e-06, 2.1191e-06,\n",
            "        4.6636e-05, 3.7532e-05, 1.3673e-03, 2.0568e-01])\n",
            "Sum of predicted tensor(5.9695)\n",
            "Loss tensor(0.1833)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.9559e-06, 3.2007e-05, 6.5882e-04, 5.3488e-05, 1.6643e-06, 2.6629e-06,\n",
            "        9.9955e-01, 9.9990e-01, 1.5158e-06, 1.1763e-05, 6.5735e-06, 5.8117e-06,\n",
            "        3.0506e-02, 1.8403e-05, 5.1805e-05, 3.5753e-05, 3.4164e-06, 3.2972e-05,\n",
            "        2.7629e-04, 1.7692e-05, 1.3653e-05, 7.6106e-01, 1.6578e-05, 5.3237e-06,\n",
            "        2.6700e-06, 4.1555e-06, 9.7437e-06, 4.1511e-06, 3.6488e-06, 1.0762e-01,\n",
            "        6.2099e-05, 3.6213e-06, 4.4029e-06, 4.8695e-06, 9.9955e-01, 1.5379e-06,\n",
            "        9.9241e-01, 1.1654e-06, 1.3646e-05, 6.3997e-06, 3.8993e-06, 1.4290e-05,\n",
            "        4.8475e-06, 9.9612e-01, 2.7328e-03, 9.7221e-06, 1.9941e-05, 2.3099e-06,\n",
            "        3.6722e-06, 9.9395e-01, 1.0926e-05, 9.9891e-01, 6.2213e-06, 2.3712e-06,\n",
            "        1.1886e-05, 1.9118e-02, 2.1097e-04, 7.6956e-06, 3.4815e-05, 2.2714e-05,\n",
            "        2.2884e-05, 7.8963e-04, 1.0628e-05, 4.4181e-05])\n",
            "Sum of predicted tensor(7.9040)\n",
            "Loss tensor(0.2732)\n",
            "Acc tensor(0.9531)\n",
            "tensor([6.2530e-06, 3.1759e-06, 9.9784e-06, 1.2326e-05, 4.2608e-06, 4.8156e-05,\n",
            "        2.5371e-06, 4.1592e-01, 1.3121e-05, 1.0951e-05, 2.3674e-06, 9.9915e-01,\n",
            "        3.5490e-04, 2.6700e-06, 1.3004e-04, 1.7593e-05, 1.2136e-05, 9.9873e-01,\n",
            "        1.1755e-05, 2.8564e-05, 3.1913e-06, 4.8963e-06, 1.8957e-02, 7.6304e-06,\n",
            "        1.1225e-05, 9.0314e-06, 7.0752e-06, 2.3827e-06, 1.6222e-06, 1.0950e-06,\n",
            "        9.9804e-01, 6.2356e-06, 9.8832e-01, 1.0253e-05, 4.2767e-01, 2.2637e-05,\n",
            "        7.4523e-06, 4.3362e-06, 6.7302e-02, 1.1006e-05, 9.9961e-01, 5.5294e-06,\n",
            "        1.2880e-04, 8.9510e-05, 2.5039e-06, 6.8381e-04, 1.3528e-05, 7.7981e-06,\n",
            "        5.5843e-06, 4.1122e-06, 1.6069e-05, 1.0303e-05, 1.2326e-05, 2.2817e-04,\n",
            "        1.8053e-06, 5.5715e-06, 5.3566e-05, 4.4582e-05, 1.9556e-05, 2.1264e-06,\n",
            "        1.0310e-05, 9.9995e-01, 1.3051e-05, 9.2237e-03])\n",
            "Sum of predicted tensor(6.9250)\n",
            "Loss tensor(0.2660)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9979e-01, 5.1506e-06, 3.0268e-06, 8.3673e-05, 1.6092e-05, 5.2544e-06,\n",
            "        9.9920e-01, 5.7651e-05, 3.7350e-06, 2.3489e-06, 9.4718e-06, 4.5980e-06,\n",
            "        1.4857e-05, 9.9700e-01, 1.9197e-06, 1.5160e-05, 2.0548e-05, 7.9087e-06,\n",
            "        9.3545e-04, 1.6344e-06, 1.2608e-06, 2.6091e-05, 1.3563e-06, 8.9003e-05,\n",
            "        3.1097e-06, 5.5879e-06, 1.8539e-04, 1.1774e-04, 1.4342e-04, 8.4844e-03,\n",
            "        8.8878e-04, 6.7475e-06, 1.9640e-06, 1.2655e-05, 9.9944e-01, 3.9044e-06,\n",
            "        1.1143e-05, 2.6700e-06, 1.7093e-06, 4.5424e-06, 9.9888e-01, 1.0414e-05,\n",
            "        9.9873e-01, 1.3830e-05, 9.6978e-05, 2.8873e-02, 1.3038e-05, 4.3494e-06,\n",
            "        1.9538e-06, 1.5748e-05, 7.1489e-02, 2.0446e-06, 2.6700e-06, 5.9116e-04,\n",
            "        1.2890e-06, 3.3371e-06, 8.4385e-06, 3.7095e-06, 1.3958e-02, 5.7912e-04,\n",
            "        1.6931e-04, 3.1045e-06, 6.3606e-06, 2.6700e-06])\n",
            "Sum of predicted tensor(6.1201)\n",
            "Loss tensor(0.6418)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.0361e-06, 4.2459e-03, 9.9248e-06, 8.6111e-06, 1.7516e-06, 5.3428e-05,\n",
            "        2.4671e-06, 7.5789e-04, 2.9404e-06, 8.4504e-01, 5.0881e-06, 2.2424e-06,\n",
            "        1.9729e-03, 1.3025e-06, 7.4989e-06, 6.4584e-04, 1.9366e-05, 1.2383e-06,\n",
            "        2.8939e-05, 2.5927e-05, 1.2758e-04, 1.1289e-05, 7.1358e-06, 9.2253e-05,\n",
            "        6.7854e-06, 6.4625e-06, 4.4083e-01, 6.2188e-06, 2.1266e-06, 3.9559e-06,\n",
            "        6.9508e-06, 3.5304e-06, 5.2692e-06, 4.4845e-02, 6.9948e-03, 2.4480e-05,\n",
            "        1.4503e-05, 5.8080e-06, 1.5678e-05, 3.9316e-06, 4.1869e-06, 1.4209e-05,\n",
            "        9.5999e-02, 9.0817e-05, 1.0766e-06, 1.1766e-05, 5.8515e-06, 8.1848e-04,\n",
            "        9.9754e-07, 9.7658e-04, 9.8530e-01, 2.2481e-05, 2.2812e-04, 3.7489e-01,\n",
            "        5.7271e-02, 2.4058e-05, 6.7038e-06, 4.8496e-06, 5.3385e-05, 1.3495e-05,\n",
            "        1.0272e-05, 8.7274e-04, 9.1110e-06, 1.9148e-05])\n",
            "Sum of predicted tensor(2.8625)\n",
            "Loss tensor(0.0965)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.7312e-06, 7.3385e-06, 1.6982e-05, 4.1539e-05, 1.0156e-04, 9.3795e-06,\n",
            "        3.1413e-06, 2.9636e-06, 6.0686e-06, 2.5283e-06, 6.8160e-06, 6.3971e-06,\n",
            "        1.0769e-05, 3.1168e-05, 7.4328e-04, 2.7302e-06, 2.1201e-03, 8.3316e-06,\n",
            "        1.5146e-06, 9.4283e-06, 3.0650e-06, 3.5573e-05, 6.9470e-06, 1.1654e-05,\n",
            "        9.8287e-06, 4.2302e-05, 4.5259e-05, 9.1750e-06, 5.7723e-05, 6.2103e-04,\n",
            "        3.4475e-05, 1.0149e-05, 2.1791e-06, 2.1266e-06, 2.5278e-05, 3.9961e-06,\n",
            "        5.3040e-06, 3.3981e-03, 1.1483e-05, 5.5739e-06, 1.6997e-02, 3.7715e-06,\n",
            "        5.8912e-06, 4.7856e-06, 3.1790e-05, 2.5420e-02, 6.8707e-06, 8.2254e-06,\n",
            "        3.1969e-06, 1.9790e-04, 1.8114e-06, 1.2712e-04, 1.7381e-05, 3.9597e-06,\n",
            "        3.5395e-06, 6.6786e-04, 6.5845e-06, 8.2035e-06, 4.0297e-05, 7.3487e-06,\n",
            "        3.6028e-01, 1.0011e-05, 9.6945e-03, 2.6700e-06])\n",
            "Sum of predicted tensor(0.4210)\n",
            "Loss tensor(0.0713)\n",
            "Acc tensor(0.9844)\n",
            "tensor([6.1468e-06, 1.1350e-05, 3.1051e-05, 1.4641e-05, 7.2421e-06, 3.3402e-03,\n",
            "        1.7057e-03, 9.9545e-01, 5.2817e-06, 2.6700e-06, 1.0113e-01, 9.9638e-01,\n",
            "        2.2516e-06, 9.1588e-06, 3.0116e-06, 2.0401e-03, 9.1044e-06, 4.8627e-06,\n",
            "        1.2326e-05, 6.1980e-05, 1.4232e-05, 6.5725e-05, 1.0324e-04, 1.0905e-02,\n",
            "        6.9658e-06, 6.4751e-05, 2.9322e-06, 9.9961e-01, 5.2420e-04, 3.5833e-05,\n",
            "        9.0101e-06, 4.7044e-06, 1.4342e-04, 3.2366e-06, 1.9403e-04, 2.3099e-06,\n",
            "        4.3341e-05, 2.4515e-06, 2.4582e-06, 1.3733e-04, 4.6953e-06, 2.3569e-06,\n",
            "        7.0252e-06, 5.8349e-01, 1.6783e-05, 3.8480e-06, 6.4017e-06, 1.8846e-04,\n",
            "        1.1036e-03, 9.9626e-01, 1.4541e-05, 9.0353e-06, 5.4882e-05, 7.6640e-06,\n",
            "        1.4870e-05, 7.0574e-06, 7.6138e-05, 1.2667e-05, 1.1886e-01, 6.1423e-06,\n",
            "        1.8704e-05, 9.2312e-05, 1.5236e-05, 5.9655e-06])\n",
            "Sum of predicted tensor(4.8124)\n",
            "Loss tensor(0.1942)\n",
            "Acc tensor(0.9531)\n",
            "tensor([8.9376e-06, 6.2348e-02, 3.0839e-05, 1.1554e-04, 2.2442e-05, 1.5455e-05,\n",
            "        9.2247e-01, 6.9465e-06, 6.4123e-06, 1.1256e-06, 8.8523e-01, 2.3989e-06,\n",
            "        3.8254e-05, 9.8382e-01, 1.3525e-06, 9.3125e-01, 3.2063e-02, 1.2817e-04,\n",
            "        1.6977e-05, 5.2907e-06, 1.2716e-06, 6.0436e-06, 1.1635e-05, 3.1227e-05,\n",
            "        1.5087e-05, 3.0563e-05, 2.9815e-06, 3.5077e-06, 1.4253e-03, 1.1763e-05,\n",
            "        6.9934e-06, 2.3905e-05, 7.3407e-05, 2.1712e-06, 9.9873e-01, 8.5887e-06,\n",
            "        9.9943e-01, 3.8534e-05, 6.7558e-06, 5.5365e-06, 2.9116e-06, 6.6988e-06,\n",
            "        4.8234e-04, 5.8650e-06, 1.9872e-05, 1.4707e-05, 9.9754e-01, 9.9873e-01,\n",
            "        1.3644e-05, 3.4241e-06, 1.2281e-05, 2.6412e-04, 2.3099e-06, 2.1995e-04,\n",
            "        7.7787e-06, 2.3749e-05, 1.4050e-06, 1.4994e-06, 2.7547e-06, 6.0508e-06,\n",
            "        1.3394e-06, 1.2129e-06, 3.8644e-06, 2.6700e-06])\n",
            "Sum of predicted tensor(7.8148)\n",
            "Loss tensor(0.1498)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.6107e-06, 5.2315e-02, 9.9982e-01, 3.6046e-06, 3.5491e-06, 1.8288e-02,\n",
            "        5.5872e-06, 1.0401e-05, 8.4134e-01, 1.3356e-03, 1.9373e-06, 2.4802e-04,\n",
            "        4.4754e-06, 8.5237e-06, 1.6742e-06, 5.1912e-06, 8.0830e-06, 3.5208e-01,\n",
            "        4.6961e-06, 1.7347e-06, 3.7990e-06, 4.3655e-06, 2.1451e-06, 1.6345e-04,\n",
            "        1.7589e-05, 9.8038e-05, 1.6501e-05, 4.2851e-05, 8.4475e-03, 1.2424e-06,\n",
            "        5.3697e-06, 2.7957e-05, 7.6598e-05, 2.6022e-06, 3.5810e-02, 1.4512e-06,\n",
            "        4.5637e-06, 3.6948e-06, 6.3811e-06, 3.8474e-06, 9.9968e-01, 2.9725e-05,\n",
            "        1.7790e-05, 3.6909e-04, 3.9609e-05, 1.1145e-05, 6.2087e-01, 1.2959e-06,\n",
            "        8.9001e-06, 9.6933e-06, 3.2404e-06, 9.4390e-06, 4.1542e-06, 2.6448e-05,\n",
            "        2.2125e-04, 6.0889e-06, 2.2116e-05, 2.3483e-05, 9.9988e-01, 2.4734e-05,\n",
            "        2.9009e-06, 1.0865e-06, 3.5367e-06, 3.0227e-06])\n",
            "Sum of predicted tensor(4.9315)\n",
            "Loss tensor(0.0978)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.6156e-05, 9.7902e-06, 3.7971e-02, 9.8353e-05, 1.7205e-06, 1.1608e-02,\n",
            "        1.8156e-04, 9.6877e-06, 2.1218e-06, 9.2248e-04, 2.8854e-06, 5.8861e-06,\n",
            "        2.2375e-03, 6.2152e-06, 7.1751e-06, 7.3743e-06, 9.6426e-07, 4.2166e-04,\n",
            "        4.8835e-06, 1.5543e-05, 3.5318e-05, 3.1772e-06, 3.0056e-06, 3.4863e-06,\n",
            "        1.0131e-05, 1.3206e-03, 4.5142e-06, 1.3354e-05, 3.8337e-06, 1.6250e-06,\n",
            "        2.2806e-06, 3.3558e-06, 4.6440e-06, 1.1554e-04, 1.7217e-04, 9.8040e-06,\n",
            "        8.5031e-06, 4.8861e-05, 5.1503e-06, 1.0358e-05, 2.9354e-06, 9.5287e-06,\n",
            "        1.7151e-06, 1.1762e-06, 9.3107e-06, 1.6978e-01, 2.7733e-06, 2.2251e-06,\n",
            "        1.5660e-02, 3.5894e-06, 2.6152e-06, 1.4804e-04, 2.6572e-05, 1.9801e-01,\n",
            "        2.2357e-06, 4.3014e-06, 5.9260e-06, 6.2560e-06, 5.8932e-06, 9.4165e-06,\n",
            "        1.1623e-05, 4.0045e-05, 9.9912e-01, 9.9802e-01])\n",
            "Sum of predicted tensor(2.4362)\n",
            "Loss tensor(0.2142)\n",
            "Acc tensor(0.9375)\n",
            "tensor([4.3583e-06, 4.1001e-06, 3.6948e-06, 4.9168e-06, 1.3689e-03, 4.2471e-06,\n",
            "        5.1862e-06, 5.1458e-05, 9.9945e-01, 4.8759e-04, 1.0175e-05, 4.2682e-06,\n",
            "        5.4662e-06, 2.1728e-06, 6.1606e-06, 1.3238e-06, 6.0102e-05, 1.0244e-05,\n",
            "        1.0296e-05, 5.8282e-06, 5.1815e-02, 2.1101e-06, 3.0576e-06, 5.4892e-05,\n",
            "        8.7724e-06, 4.2860e-06, 1.4441e-05, 2.9053e-06, 5.6390e-05, 1.9199e-05,\n",
            "        9.8456e-01, 4.2464e-05, 1.1524e-05, 1.2679e-05, 1.2325e-05, 3.0874e-06,\n",
            "        1.0269e-05, 2.9766e-06, 2.5962e-02, 1.0421e-02, 1.7884e-05, 5.4914e-03,\n",
            "        6.1301e-06, 3.4730e-03, 2.7223e-05, 4.8799e-06, 3.6848e-06, 1.2827e-02,\n",
            "        9.3204e-07, 1.7930e-04, 9.3332e-06, 1.9735e-03, 3.7666e-06, 1.3333e-06,\n",
            "        6.9368e-06, 7.2458e-01, 2.7265e-06, 6.7043e-06, 2.0401e-05, 4.4800e-02,\n",
            "        2.6225e-06, 7.0975e-01, 9.9812e-01, 2.4912e-05])\n",
            "Sum of predicted tensor(4.5759)\n",
            "Loss tensor(0.3878)\n",
            "Acc tensor(0.9219)\n",
            "tensor([9.9155e-01, 5.8324e-04, 1.6048e-06, 6.3200e-06, 9.4722e-06, 3.5705e-06,\n",
            "        1.1186e-04, 1.2099e-05, 9.9907e-01, 1.3272e-05, 2.0207e-05, 1.1869e-03,\n",
            "        4.6374e-06, 7.0915e-06, 1.6697e-06, 6.9042e-05, 9.5102e-01, 5.6109e-06,\n",
            "        1.5346e-05, 4.2525e-06, 7.9974e-03, 2.1190e-03, 6.0663e-06, 8.6897e-02,\n",
            "        4.0046e-05, 5.5425e-05, 1.2756e-05, 1.6502e-05, 3.7156e-05, 3.2850e-05,\n",
            "        4.2020e-06, 1.1290e-05, 6.2981e-06, 9.5800e-06, 3.5065e-04, 9.9991e-01,\n",
            "        2.0132e-06, 2.1955e-06, 3.0927e-04, 3.3163e-02, 9.6675e-01, 1.1424e-05,\n",
            "        5.8877e-06, 1.7502e-06, 6.2196e-04, 9.6916e-01, 8.0021e-06, 6.1987e-06,\n",
            "        6.6208e-06, 8.6429e-06, 2.2772e-02, 1.4695e-02, 2.8615e-02, 6.2584e-06,\n",
            "        1.6826e-06, 7.2906e-04, 5.2215e-04, 1.6746e-05, 4.3412e-06, 8.3944e-05,\n",
            "        5.5945e-06, 4.6648e-05, 3.1228e-06, 5.9578e-06])\n",
            "Sum of predicted tensor(6.0788)\n",
            "Loss tensor(0.1690)\n",
            "Acc tensor(0.9531)\n",
            "tensor([9.5799e-03, 3.2969e-06, 8.3983e-06, 2.1050e-05, 1.4147e-05, 3.3752e-06,\n",
            "        1.2304e-06, 4.2707e-04, 2.4929e-06, 1.4585e-05, 4.5292e-06, 1.8198e-04,\n",
            "        3.3055e-04, 9.6375e-05, 8.6098e-06, 6.1195e-06, 5.4427e-06, 9.9962e-01,\n",
            "        1.3131e-06, 9.9938e-01, 3.2796e-06, 2.3969e-06, 2.5479e-04, 2.3314e-05,\n",
            "        1.6593e-05, 1.3773e-05, 7.2160e-06, 8.2729e-01, 2.3589e-04, 5.3727e-06,\n",
            "        1.5743e-05, 4.9670e-06, 2.0771e-06, 5.2974e-05, 1.2594e-06, 6.7941e-06,\n",
            "        9.9971e-01, 1.8491e-06, 7.7102e-06, 6.5155e-05, 1.0783e-05, 4.3946e-06,\n",
            "        8.2896e-06, 1.7506e-05, 4.2471e-06, 9.9962e-01, 2.4908e-06, 3.8783e-06,\n",
            "        9.8486e-01, 5.6407e-01, 1.1576e-05, 3.0134e-06, 6.0889e-06, 6.2393e-05,\n",
            "        2.5391e-06, 4.0953e-06, 3.1101e-06, 1.8894e-04, 1.3195e-06, 4.2754e-04,\n",
            "        1.6556e-04, 9.0028e-03, 1.2350e-05, 1.8837e-03])\n",
            "Sum of predicted tensor(6.3978)\n",
            "Loss tensor(0.2236)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.7524e-05, 1.6502e-05, 9.9981e-01, 2.3338e-02, 1.6130e-05, 5.1480e-06,\n",
            "        9.9941e-01, 1.9322e-05, 1.6997e-06, 3.6497e-06, 3.6379e-06, 1.3540e-06,\n",
            "        4.4210e-06, 1.0429e-06, 1.9098e-06, 1.9781e-05, 7.4342e-06, 1.4758e-05,\n",
            "        1.7261e-06, 3.6107e-06, 8.9553e-06, 1.7303e-06, 3.4709e-06, 4.9455e-03,\n",
            "        4.0094e-05, 2.8972e-06, 8.0479e-06, 3.7940e-06, 8.4372e-05, 1.3925e-06,\n",
            "        2.3451e-06, 9.9962e-01, 1.2374e-05, 3.0283e-06, 1.5728e-06, 1.0422e-05,\n",
            "        1.9247e-05, 9.7705e-01, 1.1485e-04, 2.4622e-05, 7.1937e-01, 1.3342e-04,\n",
            "        2.6164e-06, 9.9973e-01, 3.1365e-05, 2.6572e-05, 3.0583e-06, 2.9192e-05,\n",
            "        2.2937e-05, 5.7036e-06, 6.6900e-06, 9.9881e-01, 6.4039e-05, 7.7543e-06,\n",
            "        5.3110e-06, 2.0207e-05, 1.7011e-04, 5.7206e-06, 8.1305e-05, 9.9952e-01,\n",
            "        1.1092e-05, 2.6008e-06, 9.9905e-01, 9.9909e-01])\n",
            "Sum of predicted tensor(9.7209)\n",
            "Loss tensor(0.1259)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.9499e-01, 5.4812e-06, 2.0084e-06, 1.7173e-05, 1.3488e-05, 1.5852e-04,\n",
            "        3.9592e-06, 1.7399e-06, 1.0885e-05, 3.4743e-06, 2.0207e-05, 9.9887e-01,\n",
            "        6.9734e-06, 1.1979e-05, 6.2207e-06, 5.3725e-06, 2.8388e-05, 4.7062e-05,\n",
            "        5.3064e-06, 3.0527e-06, 4.7031e-06, 9.9979e-01, 6.3802e-06, 9.9665e-01,\n",
            "        3.0507e-06, 4.5839e-06, 1.9907e-02, 1.5064e-06, 2.8828e-06, 2.0969e-02,\n",
            "        1.2275e-05, 9.9385e-01, 4.9746e-06, 2.2902e-06, 1.0934e-05, 1.5903e-05,\n",
            "        7.4870e-05, 1.4351e-06, 1.6502e-05, 2.9452e-06, 9.9722e-01, 2.2771e-06,\n",
            "        5.6923e-05, 2.6103e-05, 1.7744e-06, 9.9385e-01, 5.7311e-06, 8.0465e-06,\n",
            "        8.1164e-06, 2.6015e-05, 1.6423e-06, 2.3062e-05, 4.9809e-06, 8.1690e-01,\n",
            "        1.7067e-05, 3.2738e-02, 5.0808e-06, 5.5098e-05, 2.2871e-05, 3.9900e-05,\n",
            "        6.1987e-06, 7.8219e-01, 1.0764e-06, 3.7734e-05])\n",
            "Sum of predicted tensor(8.6488)\n",
            "Loss tensor(0.2050)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.3069e-05, 3.7259e-06, 2.3988e-05, 6.3584e-06, 5.8704e-06, 5.0516e-06,\n",
            "        3.2288e-05, 3.1682e-04, 6.4393e-06, 1.1298e-06, 2.1748e-04, 6.1751e-06,\n",
            "        3.6810e-05, 8.9786e-07, 2.0362e-06, 4.0701e-06, 9.8281e-06, 1.0138e-05,\n",
            "        1.4810e-05, 1.9153e-06, 2.8324e-06, 2.7850e-05, 7.4488e-06, 2.1246e-05,\n",
            "        2.7145e-06, 1.8459e-06, 9.9975e-01, 5.7024e-03, 1.1478e-05, 4.0404e-06,\n",
            "        2.5084e-06, 7.4267e-06, 6.0938e-06, 1.6061e-06, 7.3507e-06, 4.4280e-03,\n",
            "        9.7296e-06, 3.2176e-04, 9.3503e-07, 1.1468e-05, 1.9868e-06, 5.3282e-06,\n",
            "        4.0953e-06, 8.1981e-05, 5.8677e-05, 2.5013e-06, 1.5438e-05, 7.4303e-05,\n",
            "        1.5448e-05, 1.0630e-05, 1.2863e-03, 2.7515e-06, 1.8401e-06, 2.0734e-05,\n",
            "        1.0084e-05, 1.7525e-03, 1.1047e-04, 6.1795e-06, 2.5673e-06, 2.6572e-05,\n",
            "        4.1924e-01, 3.0482e-06, 1.1232e-05, 2.9359e-06])\n",
            "Sum of predicted tensor(1.4338)\n",
            "Loss tensor(0.0894)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.9730e-06, 1.2079e-06, 1.0878e-05, 3.9538e-05, 4.7020e-06, 1.3976e-06,\n",
            "        4.5213e-06, 1.7106e-06, 8.8797e-06, 7.5034e-06, 8.6089e-01, 9.8006e-06,\n",
            "        3.5802e-06, 5.4879e-06, 7.5172e-06, 1.5418e-06, 1.4043e-05, 2.8158e-06,\n",
            "        1.9140e-06, 7.6473e-01, 9.9909e-01, 6.3309e-05, 1.0897e-05, 9.3959e-06,\n",
            "        1.3457e-06, 5.1461e-06, 3.6263e-03, 5.8914e-06, 3.6647e-06, 1.3770e-05,\n",
            "        2.7404e-04, 3.0320e-06, 6.2158e-03, 5.2253e-06, 2.6434e-06, 5.8843e-06,\n",
            "        2.2992e-06, 1.5139e-06, 9.9997e-01, 4.5960e-06, 7.0331e-06, 3.9412e-06,\n",
            "        1.0828e-04, 3.3345e-05, 6.5252e-06, 1.2017e-03, 1.6183e-06, 1.3438e-06,\n",
            "        3.5935e-06, 5.0336e-05, 1.4821e-05, 2.7014e-06, 4.0299e-06, 9.9920e-01,\n",
            "        4.6390e-05, 1.6709e-05, 4.4284e-06, 2.9833e-06, 6.7653e-06, 1.4951e-05,\n",
            "        1.7959e-05, 1.7529e-06, 4.8130e-03, 7.0738e-01])\n",
            "Sum of predicted tensor(5.3480)\n",
            "Loss tensor(0.1322)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.0026e-04, 2.9992e-06, 3.4930e-06, 1.7008e-04, 1.7872e-05, 9.9913e-01,\n",
            "        7.9508e-06, 3.5994e-06, 1.0678e-03, 7.7894e-06, 3.3645e-05, 6.3044e-06,\n",
            "        1.1028e-06, 4.1244e-06, 1.1402e-05, 4.8849e-06, 5.4360e-06, 7.7003e-02,\n",
            "        1.0046e-05, 4.5445e-06, 6.0343e-05, 3.1385e-06, 1.2032e-05, 3.1877e-04,\n",
            "        1.3990e-06, 1.8408e-06, 9.2524e-06, 2.0104e-05, 8.6346e-07, 4.0040e-06,\n",
            "        1.7555e-05, 5.3405e-04, 3.7797e-06, 7.1055e-04, 2.8650e-06, 1.8268e-05,\n",
            "        4.1180e-06, 2.4036e-06, 1.7723e-06, 2.7568e-06, 7.6189e-06, 1.8987e-06,\n",
            "        1.4977e-05, 1.9555e-06, 3.4344e-06, 3.2141e-02, 3.9135e-05, 3.0669e-04,\n",
            "        1.1312e-05, 2.1188e-05, 8.0275e-06, 9.8408e-06, 2.5388e-05, 2.2775e-05,\n",
            "        1.2743e-05, 3.9020e-06, 9.6104e-06, 1.0695e-05, 7.4733e-06, 1.4204e-06,\n",
            "        1.6845e-06, 2.5443e-06, 8.6946e-05, 2.1650e-05])\n",
            "Sum of predicted tensor(1.1123)\n",
            "Loss tensor(0.1797)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.2043e-06, 2.7493e-06, 3.6453e-06, 6.6263e-06, 2.0782e-06, 2.3964e-06,\n",
            "        5.5300e-03, 3.5522e-03, 2.6817e-06, 5.8497e-06, 4.6360e-05, 3.8993e-06,\n",
            "        3.9900e-05, 2.1703e-06, 2.0759e-01, 3.1571e-04, 1.0083e-05, 1.2775e-05,\n",
            "        2.1984e-06, 3.6570e-04, 2.3033e-06, 4.2601e-06, 5.3980e-06, 9.9927e-01,\n",
            "        9.9945e-01, 2.1173e-05, 6.3409e-06, 5.7488e-06, 1.0736e-03, 7.4733e-06,\n",
            "        1.9027e-05, 1.0650e-05, 5.3573e-06, 7.3342e-06, 1.1638e-06, 2.1694e-05,\n",
            "        1.5233e-05, 9.8142e-06, 1.0956e-05, 8.4099e-06, 5.5994e-06, 4.6535e-02,\n",
            "        1.5730e-06, 9.9917e-01, 1.2517e-04, 5.8443e-06, 7.7475e-05, 5.5469e-06,\n",
            "        1.0513e-05, 2.5705e-06, 6.9694e-06, 1.1297e-06, 1.3869e-04, 7.2854e-04,\n",
            "        6.9545e-05, 3.8602e-06, 1.2814e-04, 2.2582e-06, 6.7259e-06, 1.1884e-04,\n",
            "        9.6965e-01, 4.5686e-05, 2.1096e-05, 1.6440e-04])\n",
            "Sum of predicted tensor(4.2345)\n",
            "Loss tensor(0.1064)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.3444e-06, 1.1316e-05, 5.6212e-06, 3.9573e-06, 3.7232e-05, 3.2803e-06,\n",
            "        9.9932e-01, 6.4054e-06, 5.9086e-06, 2.1928e-06, 5.4403e-05, 5.3303e-06,\n",
            "        2.6464e-06, 1.1452e-05, 6.5671e-01, 1.5158e-06, 1.6080e-06, 1.7371e-06,\n",
            "        5.1457e-03, 1.2189e-04, 7.2595e-02, 1.2544e-06, 5.2544e-06, 5.5955e-06,\n",
            "        4.5998e-05, 4.5131e-04, 9.5822e-04, 1.3664e-06, 2.2783e-05, 4.2450e-06,\n",
            "        7.5231e-06, 1.5993e-06, 4.9608e-06, 2.2060e-05, 2.0456e-06, 2.4868e-05,\n",
            "        1.0119e-05, 8.9938e-06, 2.6494e-06, 4.0413e-05, 3.9453e-06, 4.0480e-06,\n",
            "        7.6800e-06, 1.2586e-06, 9.2220e-01, 2.2200e-06, 1.1758e-03, 2.7376e-04,\n",
            "        3.7750e-06, 4.7552e-05, 1.5845e-06, 3.9317e-06, 4.3792e-06, 7.7581e-06,\n",
            "        1.8415e-05, 4.0067e-06, 1.5073e-05, 9.2954e-02, 1.5101e-04, 6.9862e-06,\n",
            "        2.1970e-05, 5.1206e-06, 3.7434e-06, 1.0879e-05])\n",
            "Sum of predicted tensor(2.7526)\n",
            "Loss tensor(0.1777)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.5064e-04, 3.7966e-06, 2.5905e-06, 5.7096e-06, 9.8541e-06, 1.2173e-06,\n",
            "        3.1515e-06, 1.3259e-05, 2.1108e-04, 1.7292e-05, 5.2104e-06, 1.2734e-06,\n",
            "        9.6958e-06, 1.0603e-05, 2.9704e-06, 3.3093e-06, 6.7754e-05, 2.7101e-05,\n",
            "        1.5024e-04, 9.8552e-01, 1.0848e-04, 9.9905e-01, 9.9948e-01, 2.5042e-06,\n",
            "        8.8854e-03, 1.0991e-05, 2.0026e-06, 1.4448e-05, 5.3631e-02, 5.0240e-06,\n",
            "        9.9956e-01, 3.6907e-06, 6.8211e-06, 3.4498e-04, 4.4745e-06, 1.2970e-05,\n",
            "        3.3406e-06, 5.6003e-06, 8.2437e-04, 1.5087e-05, 2.8967e-04, 9.9694e-01,\n",
            "        4.4285e-06, 3.2750e-06, 1.5322e-05, 2.4304e-05, 2.3330e-05, 1.6183e-06,\n",
            "        7.6506e-06, 9.9907e-01, 1.3171e-03, 5.0109e-06, 3.1436e-06, 1.0228e-05,\n",
            "        4.4138e-05, 1.0605e-01, 5.1572e-03, 8.1623e-03, 1.1014e-05, 2.2718e-05,\n",
            "        3.7899e-06, 7.8960e-06, 2.3556e-04, 9.9916e-01])\n",
            "Sum of predicted tensor(7.1648)\n",
            "Loss tensor(0.5083)\n",
            "Acc tensor(0.9062)\n",
            "tensor([3.6622e-06, 5.4549e-06, 1.3896e-04, 3.3406e-06, 3.4587e-06, 5.7967e-05,\n",
            "        3.0967e-06, 6.2313e-06, 5.0419e-06, 7.9501e-06, 3.7640e-06, 1.7409e-06,\n",
            "        1.4276e-04, 6.6321e-06, 6.7738e-01, 4.9919e-06, 8.4915e-04, 4.2450e-06,\n",
            "        1.5795e-04, 1.4668e-05, 6.0053e-06, 9.8329e-06, 8.6547e-06, 3.9340e-06,\n",
            "        2.7592e-06, 2.4637e-04, 1.1235e-05, 1.7175e-04, 1.8057e-04, 1.1515e-04,\n",
            "        9.2337e-01, 9.3807e-06, 9.8987e-03, 2.3421e-06, 8.6260e-06, 2.6430e-06,\n",
            "        6.5877e-06, 2.4719e-06, 7.5438e-02, 1.1087e-05, 7.3931e-06, 5.3573e-06,\n",
            "        2.6554e-06, 3.7777e-06, 3.2628e-05, 2.9270e-06, 1.6501e-05, 9.8724e-01,\n",
            "        2.4135e-06, 3.4651e-05, 4.2379e-06, 2.6855e-05, 2.6664e-06, 8.4533e-04,\n",
            "        1.0624e-05, 3.2472e-06, 3.0796e-06, 2.1191e-05, 2.2712e-06, 2.6707e-06,\n",
            "        1.0482e-05, 2.1354e-06, 6.8548e-05, 4.9640e-06])\n",
            "Sum of predicted tensor(2.6767)\n",
            "Loss tensor(0.4119)\n",
            "Acc tensor(0.9219)\n",
            "tensor([8.1025e-04, 6.5631e-06, 1.5760e-06, 4.0552e-06, 7.3664e-06, 3.0834e-05,\n",
            "        1.9258e-05, 2.3121e-01, 1.9109e-02, 8.1195e-01, 2.5352e-06, 4.3825e-06,\n",
            "        7.0746e-06, 2.2008e-06, 4.7017e-06, 3.3345e-05, 6.4274e-06, 1.5771e-05,\n",
            "        2.4803e-05, 1.4164e-04, 9.8408e-06, 1.4721e-05, 6.3027e-06, 1.3508e-04,\n",
            "        2.6675e-06, 1.0775e-01, 2.8445e-06, 1.2298e-06, 9.4351e-06, 1.3139e-06,\n",
            "        2.8445e-06, 3.4546e-06, 3.1031e-02, 6.2999e-06, 2.6124e-06, 3.0603e-05,\n",
            "        8.8085e-06, 1.5434e-05, 3.4793e-06, 2.0893e-05, 2.5172e-06, 1.4826e-05,\n",
            "        9.8261e-01, 1.3221e-04, 1.5989e-06, 3.1844e-06, 1.6286e-04, 1.3735e-06,\n",
            "        1.6183e-06, 1.3339e-06, 1.0435e-05, 1.3438e-06, 9.9489e-06, 1.6106e-05,\n",
            "        1.6172e-06, 4.3191e-06, 4.5688e-06, 4.7642e-06, 4.7871e-06, 5.6071e-04,\n",
            "        2.3146e-05, 8.5784e-03, 9.9885e-01, 2.6093e-06])\n",
            "Sum of predicted tensor(3.1935)\n",
            "Loss tensor(0.2946)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.0466e-05, 1.0746e-05, 1.2153e-06, 3.2015e-06, 3.2470e-06, 8.2729e-06,\n",
            "        5.6619e-06, 4.5421e-06, 1.3995e-06, 1.3204e-05, 4.8286e-05, 8.9179e-06,\n",
            "        1.2775e-05, 2.5841e-06, 5.7936e-05, 2.4977e-06, 8.5362e-06, 6.5017e-06,\n",
            "        3.5951e-02, 3.0694e-03, 1.2012e-06, 5.0370e-05, 2.6231e-06, 1.4147e-05,\n",
            "        5.2536e-05, 1.0972e-05, 2.8082e-06, 2.3207e-06, 2.5594e-06, 2.8089e-06,\n",
            "        3.7413e-06, 3.9336e-06, 1.8564e-05, 7.4709e-05, 1.3938e-03, 1.4293e-06,\n",
            "        6.4776e-06, 1.1419e-05, 2.0982e-05, 8.0814e-06, 9.8424e-01, 4.7719e-06,\n",
            "        2.0084e-06, 1.6687e-05, 2.7184e-06, 1.8278e-05, 2.1462e-06, 6.9254e-05,\n",
            "        2.2602e-06, 1.6578e-05, 4.3919e-06, 1.5843e-03, 1.8747e-04, 2.8379e-06,\n",
            "        3.1551e-05, 4.8300e-06, 2.1435e-05, 3.0897e-06, 1.0786e-05, 1.8471e-05,\n",
            "        4.0943e-06, 6.4533e-06, 3.5544e-02, 1.6183e-06])\n",
            "Sum of predicted tensor(1.0627)\n",
            "Loss tensor(0.1568)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.0334e-01, 2.3094e-06, 4.6254e-06, 1.3165e-03, 2.6486e-05, 4.8104e-04,\n",
            "        1.0138e-05, 3.9924e-05, 1.5623e-06, 1.0182e-05, 1.6550e-05, 2.9027e-05,\n",
            "        1.1048e-04, 1.8963e-02, 1.1073e-05, 3.3338e-06, 9.7469e-06, 1.0732e-05,\n",
            "        1.2521e-04, 7.2796e-06, 1.8711e-06, 6.2861e-06, 2.0042e-06, 8.1646e-06,\n",
            "        2.4188e-06, 1.8215e-06, 6.5309e-06, 6.6834e-06, 2.9584e-06, 1.0802e-04,\n",
            "        2.2519e-05, 5.6102e-06, 1.8253e-05, 1.8179e-04, 1.5590e-05, 9.9256e-01,\n",
            "        3.4201e-06, 2.4664e-01, 6.4181e-05, 3.7252e-05, 3.1425e-06, 5.0134e-06,\n",
            "        4.3913e-04, 2.7292e-05, 4.1189e-04, 2.0375e-05, 2.0569e-05, 4.1034e-06,\n",
            "        9.8198e-05, 3.5808e-05, 1.3011e-05, 1.0684e-05, 2.3226e-06, 2.3575e-06,\n",
            "        9.9889e-01, 9.4025e-06, 2.6685e-05, 6.3488e-05, 1.8774e-02, 4.4812e-05,\n",
            "        8.2779e-05, 5.0305e-06, 2.2989e-06, 6.8923e-06])\n",
            "Sum of predicted tensor(3.1832)\n",
            "Loss tensor(0.3973)\n",
            "Acc tensor(0.9375)\n",
            "tensor([2.6829e-05, 3.9609e-06, 1.2420e-05, 1.6399e-05, 4.2942e-06, 5.1222e-06,\n",
            "        5.7094e-06, 4.4008e-06, 8.7094e-06, 1.8441e-06, 1.0096e-06, 1.9375e-06,\n",
            "        1.4740e-06, 7.1408e-05, 5.5851e-05, 1.2712e-05, 3.3321e-06, 9.8070e-06,\n",
            "        8.3097e-06, 7.9706e-06, 1.7231e-04, 1.5646e-03, 6.8599e-05, 1.2782e-05,\n",
            "        1.6399e-05, 3.1153e-06, 2.0918e-04, 6.1215e-06, 5.9963e-06, 1.3273e-05,\n",
            "        5.0134e-06, 1.5933e-05, 4.3390e-05, 4.1202e-06, 4.7654e-06, 1.5125e-06,\n",
            "        1.7334e-05, 6.9998e-05, 7.4492e-06, 2.1339e-06, 1.9858e-04, 2.7076e-06,\n",
            "        4.5162e-04, 2.3517e-05, 4.4655e-06, 1.6690e-05, 1.2201e-04, 1.6784e-05,\n",
            "        5.0134e-06, 3.7075e-06, 8.0989e-05, 1.6029e-06, 6.9505e-06, 1.2937e-04,\n",
            "        7.3273e-06, 4.1392e-06, 2.3624e-05, 6.5750e-06, 1.8507e-06, 1.8093e-05,\n",
            "        2.6482e-06, 8.1406e-05, 4.1643e-06, 1.3430e-05])\n",
            "Sum of predicted tensor(0.0037)\n",
            "Loss tensor(5.8378e-05)\n",
            "Acc tensor(1.)\n",
            "tensor([5.5046e-01, 6.7282e-06, 5.0847e-06, 1.6170e-05, 4.2942e-06, 3.7854e-06,\n",
            "        7.8655e-06, 4.0765e-06, 1.0466e-05, 3.9017e-06, 1.2173e-02, 4.2942e-06,\n",
            "        6.9687e-06, 4.9949e-06, 9.9979e-01, 1.4024e-06, 7.8098e-06, 2.5810e-06,\n",
            "        6.3409e-04, 4.3950e-06, 8.6918e-05, 2.5010e-06, 2.0252e-06, 1.5844e-06,\n",
            "        3.3948e-06, 1.1287e-05, 9.7104e-05, 6.5721e-06, 7.3564e-06, 1.1665e-05,\n",
            "        1.8636e-06, 8.5704e-01, 1.5311e-05, 6.3368e-06, 9.8164e-01, 1.1437e-05,\n",
            "        4.8197e-01, 1.1586e-04, 9.9743e-01, 1.0533e-04, 7.4492e-06, 1.8882e-06,\n",
            "        2.7967e-06, 5.7023e-04, 1.2145e-06, 5.1978e-06, 9.7743e-01, 3.8500e-06,\n",
            "        2.4455e-06, 1.1246e-02, 4.5681e-06, 7.7544e-06, 3.2903e-06, 1.0954e-05,\n",
            "        4.1941e-03, 5.1636e-05, 3.5548e-06, 1.3241e-01, 3.6735e-06, 2.5902e-06,\n",
            "        7.9774e-04, 1.4623e-06, 1.1861e-05, 4.2942e-06])\n",
            "Sum of predicted tensor(6.0085)\n",
            "Loss tensor(0.3913)\n",
            "Acc tensor(0.9062)\n",
            "tensor([2.6288e-06, 7.6919e-05, 1.6474e-05, 9.9913e-01, 6.6007e-06, 9.9965e-01,\n",
            "        4.6844e-05, 9.3581e-06, 2.7788e-06, 9.9961e-01, 9.9289e-01, 3.5673e-06,\n",
            "        1.7205e-06, 1.0672e-05, 5.7865e-06, 5.5292e-06, 5.5981e-05, 6.8780e-06,\n",
            "        1.2277e-06, 2.1796e-06, 6.4544e-06, 2.2268e-06, 1.6473e-06, 1.8492e-06,\n",
            "        8.0031e-05, 1.7172e-06, 1.2368e-04, 2.0364e-01, 9.9964e-01, 1.8306e-06,\n",
            "        8.6972e-04, 7.5144e-05, 8.0660e-06, 3.4315e-05, 1.9588e-06, 1.6550e-05,\n",
            "        9.9949e-01, 9.9918e-01, 2.1273e-05, 3.3040e-03, 2.6400e-06, 2.7586e-01,\n",
            "        4.9103e-06, 2.1155e-05, 9.9944e-01, 4.5218e-05, 8.7747e-06, 6.4693e-06,\n",
            "        5.5406e-05, 2.6338e-06, 1.6719e-02, 1.6058e-03, 8.4725e-06, 3.4089e-04,\n",
            "        4.4394e-03, 1.3291e-05, 1.0506e-04, 1.0262e-05, 1.8651e-05, 1.4020e-06,\n",
            "        3.5779e-05, 1.8622e-04, 3.1257e-06, 1.7772e-06])\n",
            "Sum of predicted tensor(8.4970)\n",
            "Loss tensor(0.5043)\n",
            "Acc tensor(0.9375)\n",
            "tensor([6.1647e-06, 2.3539e-06, 2.2326e-06, 9.7322e-03, 2.0853e-06, 2.0557e-06,\n",
            "        9.4202e-06, 1.6221e-05, 1.5145e-05, 2.6825e-06, 2.7102e-02, 2.4562e-03,\n",
            "        2.2849e-06, 7.7978e-05, 3.9796e-05, 3.0309e-05, 4.3902e-06, 9.9829e-01,\n",
            "        2.0400e-06, 7.7730e-06, 3.1521e-06, 8.4573e-06, 5.4922e-06, 5.1874e-06,\n",
            "        8.2539e-06, 5.2290e-05, 3.9540e-06, 3.7751e-06, 2.8049e-03, 1.5121e-05,\n",
            "        4.9972e-06, 1.9987e-06, 9.5748e-06, 4.4704e-06, 9.9861e-01, 1.4038e-06,\n",
            "        1.5917e-05, 1.2956e-05, 4.1186e-02, 9.9775e-01, 2.1543e-06, 1.9482e-05,\n",
            "        9.5506e-06, 2.6338e-06, 1.1588e-03, 4.6364e-01, 2.1566e-06, 4.2047e-05,\n",
            "        4.8085e-04, 7.1284e-05, 1.8657e-06, 1.5141e-06, 9.9942e-01, 7.7777e-06,\n",
            "        2.9242e-05, 1.7711e-05, 1.6452e-06, 2.2667e-06, 5.0562e-06, 3.5265e-06,\n",
            "        4.2942e-06, 3.9088e-06, 3.7028e-06, 6.1448e-06])\n",
            "Sum of predicted tensor(4.5433)\n",
            "Loss tensor(0.1139)\n",
            "Acc tensor(0.9844)\n",
            "tensor([4.5479e-06, 4.1345e-06, 3.7242e-06, 3.6193e-06, 1.1476e-01, 1.4085e-06,\n",
            "        6.4216e-05, 6.1007e-06, 1.8056e-05, 6.6223e-06, 9.9982e-01, 4.2942e-06,\n",
            "        4.2495e-06, 9.9755e-01, 9.4552e-03, 3.1592e-06, 2.9515e-06, 5.6488e-04,\n",
            "        4.4311e-06, 2.3432e-06, 9.9567e-01, 1.2835e-06, 8.5284e-05, 6.1921e-06,\n",
            "        7.0212e-06, 9.9867e-06, 2.6542e-06, 3.3354e-06, 1.7886e-05, 7.6828e-05,\n",
            "        2.2119e-05, 1.0878e-06, 4.2250e-05, 1.1598e-05, 4.1724e-05, 2.3145e-06,\n",
            "        4.2763e-06, 2.1571e-06, 1.5873e-06, 3.1904e-06, 1.6691e-01, 1.3036e-06,\n",
            "        5.0134e-06, 7.9541e-01, 1.4830e-05, 8.7798e-04, 3.4539e-05, 9.5994e-06,\n",
            "        3.8178e-04, 7.0674e-06, 2.2808e-05, 1.0680e-05, 2.1322e-05, 1.0893e-05,\n",
            "        3.5363e-05, 1.5092e-05, 3.9162e-06, 4.0421e-04, 1.3342e-05, 1.2266e-05,\n",
            "        4.5349e-06, 6.8122e-06, 2.5750e-05, 1.9491e-05])\n",
            "Sum of predicted tensor(4.0826)\n",
            "Loss tensor(0.0550)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.4283e-05, 9.9330e-01, 1.1971e-04, 1.0171e-05, 3.5801e-05, 2.2455e-05,\n",
            "        2.7767e-06, 8.6727e-06, 1.6702e-06, 6.7287e-06, 3.8213e-06, 2.4430e-06,\n",
            "        5.0877e-06, 1.2030e-04, 4.6604e-06, 7.1764e-06, 9.9924e-01, 3.7319e-02,\n",
            "        1.5095e-06, 3.7022e-06, 3.9976e-06, 6.7160e-06, 1.9350e-05, 4.6880e-03,\n",
            "        6.3297e-05, 3.3374e-06, 9.9982e-01, 4.0306e-06, 4.2352e-06, 9.9717e-01,\n",
            "        2.6403e-06, 1.2750e-03, 4.2942e-06, 1.7218e-05, 1.9573e-06, 1.5347e-06,\n",
            "        8.7846e-06, 9.9905e-01, 9.9888e-01, 1.0095e-04, 4.4067e-02, 6.0938e-02,\n",
            "        3.6697e-06, 2.7338e-04, 8.6676e-02, 2.2140e-05, 4.8897e-04, 4.3219e-05,\n",
            "        1.3132e-06, 1.9455e-06, 4.8384e-06, 1.8820e-05, 1.3017e-05, 1.7853e-03,\n",
            "        4.4851e-06, 6.9199e-05, 3.6573e-06, 9.7798e-01, 9.9798e-06, 9.9927e-01,\n",
            "        5.8712e-06, 6.5524e-04, 9.9289e-01, 2.1742e-06])\n",
            "Sum of predicted tensor(9.1966)\n",
            "Loss tensor(0.2382)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.6213e-06, 1.8472e-05, 8.9320e-05, 7.6759e-03, 1.3702e-06, 9.4404e-06,\n",
            "        7.8615e-01, 7.2927e-06, 1.9633e-06, 1.6067e-05, 5.3655e-06, 2.0128e-06,\n",
            "        5.9927e-06, 1.3169e-04, 3.7528e-06, 2.1053e-06, 4.2056e-06, 6.6574e-06,\n",
            "        1.1316e-04, 6.8565e-06, 4.5034e-04, 3.5017e-06, 2.3197e-05, 4.5677e-06,\n",
            "        4.2179e-05, 6.2373e-06, 3.4667e-04, 4.2359e-04, 5.4506e-06, 2.2925e-05,\n",
            "        3.6932e-06, 4.6475e-06, 1.3007e-05, 2.1358e-06, 1.5641e-05, 3.9607e-06,\n",
            "        5.7851e-05, 7.2488e-06, 1.1528e-03, 5.2253e-06, 4.4411e-06, 2.8770e-06,\n",
            "        1.3831e-05, 7.1499e-05, 1.2197e-06, 7.8177e-05, 1.6253e-05, 1.3943e-06,\n",
            "        5.2238e-06, 2.7454e-06, 6.3545e-05, 1.5753e-06, 9.8123e-01, 9.3561e-05,\n",
            "        3.8388e-05, 3.6604e-05, 7.4579e-05, 1.1639e-05, 2.5688e-06, 8.5605e-06,\n",
            "        2.8141e-05, 2.6249e-06, 5.4431e-04, 7.8902e-04])\n",
            "Sum of predicted tensor(1.7800)\n",
            "Loss tensor(0.2898)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.1232e-04, 1.5706e-05, 1.1215e-04, 4.9555e-03, 7.4607e-06, 1.6822e-06,\n",
            "        2.3990e-06, 2.6092e-06, 3.3318e-04, 9.9027e-01, 4.8154e-06, 1.1163e-05,\n",
            "        3.7821e-04, 2.3586e-04, 2.8080e-06, 3.4518e-06, 8.1064e-06, 6.7335e-06,\n",
            "        1.8884e-06, 9.4817e-03, 1.6706e-06, 1.0076e-04, 6.6742e-06, 1.8958e-06,\n",
            "        7.0243e-06, 4.6079e-01, 1.9438e-06, 5.0693e-06, 2.0491e-04, 4.4699e-06,\n",
            "        2.5820e-04, 1.9840e-05, 1.9004e-04, 1.5494e-06, 1.8966e-06, 4.2502e-06,\n",
            "        3.1824e-05, 3.3310e-06, 7.0880e-04, 6.8565e-06, 6.0880e-06, 5.2766e-06,\n",
            "        3.4739e-06, 3.0172e-05, 8.1702e-06, 1.0855e-05, 9.9816e-01, 5.3692e-06,\n",
            "        2.5680e-06, 7.4406e-06, 2.4222e-05, 2.0820e-05, 9.7237e-05, 1.2633e-05,\n",
            "        1.2235e-05, 1.2565e-05, 4.2068e-05, 1.8372e-05, 1.7152e-05, 6.5611e-03,\n",
            "        5.3502e-03, 6.5375e-05, 3.0092e-05, 9.2992e-05])\n",
            "Sum of predicted tensor(2.4790)\n",
            "Loss tensor(0.2995)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.2186e-05, 8.8691e-05, 6.1872e-06, 1.3477e-02, 3.8163e-06, 2.3778e-06,\n",
            "        1.0079e-05, 2.3486e-06, 9.9916e-01, 4.8998e-05, 9.0937e-06, 2.4224e-06,\n",
            "        2.1137e-04, 5.4761e-04, 5.5656e-06, 3.6724e-06, 1.5141e-05, 1.0953e-03,\n",
            "        1.9071e-05, 1.6864e-01, 4.7880e-06, 7.6549e-06, 2.5173e-05, 2.0868e-05,\n",
            "        4.9525e-06, 8.7381e-06, 1.9786e-06, 9.9976e-01, 2.4383e-03, 2.2111e-06,\n",
            "        2.5320e-04, 8.4335e-06, 3.7688e-05, 2.8674e-06, 8.6387e-06, 2.1478e-04,\n",
            "        9.9978e-01, 3.8240e-06, 1.8682e-06, 1.6036e-05, 2.5755e-05, 2.0340e-05,\n",
            "        2.1062e-04, 8.1871e-06, 9.9908e-01, 9.9951e-01, 2.7330e-06, 1.1585e-05,\n",
            "        8.3305e-06, 9.9986e-01, 3.6256e-06, 1.3279e-02, 7.3101e-06, 9.5589e-06,\n",
            "        9.0819e-06, 6.9970e-06, 2.0198e-04, 3.5538e-05, 1.5861e-01, 1.6604e-05,\n",
            "        4.6007e-06, 4.6003e-06, 2.6215e-01, 3.9765e-06])\n",
            "Sum of predicted tensor(6.6191)\n",
            "Loss tensor(0.1532)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.7366e-06, 9.1052e-06, 1.0188e-05, 1.9819e-05, 9.5053e-06, 6.8565e-06,\n",
            "        9.3969e-06, 9.9988e-01, 4.6802e-06, 2.4931e-06, 3.6299e-06, 6.9975e-06,\n",
            "        3.9725e-06, 6.8565e-06, 6.2579e-06, 1.7062e-05, 2.0325e-04, 3.5849e-04,\n",
            "        1.2128e-06, 2.8881e-06, 4.1976e-03, 2.9566e-06, 2.6712e-06, 9.6468e-04,\n",
            "        5.3905e-02, 5.6658e-06, 7.3571e-06, 1.2095e-06, 7.4697e-06, 2.8405e-06,\n",
            "        5.8905e-06, 1.6919e-06, 5.3323e-06, 8.6610e-06, 1.0437e-05, 2.3735e-06,\n",
            "        4.2868e-03, 1.8122e-05, 1.3306e-04, 1.0999e-05, 4.5682e-06, 6.8565e-06,\n",
            "        9.0547e-05, 1.3707e-05, 5.2355e-06, 6.0304e-06, 2.3361e-06, 2.9675e-04,\n",
            "        1.2407e-01, 2.4678e-04, 5.3653e-06, 1.9433e-06, 3.6548e-05, 9.9874e-01,\n",
            "        4.3202e-04, 2.4511e-06, 4.1127e-05, 8.4710e-05, 9.9991e-01, 7.1570e-06,\n",
            "        5.2400e-06, 2.0390e-04, 7.8195e-06, 3.5690e-06])\n",
            "Sum of predicted tensor(3.1884)\n",
            "Loss tensor(0.1191)\n",
            "Acc tensor(0.9688)\n",
            "tensor([8.5825e-02, 5.2190e-03, 1.2673e-04, 3.0811e-06, 1.8884e-06, 7.0572e-06,\n",
            "        1.9276e-05, 1.8980e-03, 9.9956e-01, 2.4435e-06, 3.7310e-06, 5.3604e-06,\n",
            "        7.1403e-06, 1.5335e-04, 3.8645e-06, 2.4052e-05, 1.2328e-05, 1.0776e-05,\n",
            "        1.2675e-05, 4.1848e-05, 9.9982e-01, 7.9869e-05, 1.3740e-06, 4.1144e-06,\n",
            "        2.8255e-05, 1.9855e-05, 1.2924e-04, 1.1029e-05, 8.9544e-06, 1.2653e-06,\n",
            "        1.7766e-06, 3.5220e-06, 1.5793e-06, 6.8565e-06, 1.3115e-05, 1.5085e-05,\n",
            "        2.1952e-06, 5.0682e-06, 9.9290e-06, 3.4568e-05, 6.8565e-06, 1.1324e-05,\n",
            "        1.0437e-05, 4.0609e-06, 6.4498e-06, 7.9679e-06, 2.3328e-04, 5.6607e-05,\n",
            "        3.1419e-02, 6.9723e-06, 5.1482e-06, 3.9245e-06, 4.1499e-06, 3.6932e-06,\n",
            "        1.8202e-05, 7.5269e-06, 1.1752e-05, 4.9151e-06, 1.1158e-04, 2.3328e-04,\n",
            "        1.4496e-06, 7.7668e-03, 1.6568e-05, 6.8565e-06])\n",
            "Sum of predicted tensor(2.1331)\n",
            "Loss tensor(0.2846)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.1800e-06, 9.4819e-06, 7.4570e-04, 8.2024e-01, 9.9678e-01, 7.1757e-06,\n",
            "        2.9675e-04, 3.2024e-06, 2.0836e-05, 6.0919e-06, 5.2340e-05, 6.7339e-01,\n",
            "        1.6700e-06, 3.7002e-06, 1.8753e-05, 4.6803e-02, 4.4362e-06, 6.3861e-05,\n",
            "        3.5726e-06, 2.2808e-05, 2.7095e-05, 6.3172e-05, 1.6625e-03, 1.6247e-06,\n",
            "        1.5671e-06, 1.2543e-06, 2.0370e-05, 3.3379e-06, 1.5038e-05, 1.3510e-06,\n",
            "        2.0005e-06, 6.7627e-04, 4.0011e-06, 3.2087e-03, 1.4536e-05, 1.3097e-05,\n",
            "        1.6851e-06, 4.2179e-05, 7.9429e-06, 3.4606e-05, 2.2481e-06, 3.2493e-05,\n",
            "        1.9151e-05, 2.7308e-05, 2.6374e-05, 9.4639e-01, 3.3562e-06, 1.6311e-06,\n",
            "        8.8812e-06, 1.6092e-05, 2.7849e-03, 1.6499e-06, 3.1419e-02, 6.8565e-06,\n",
            "        7.4120e-03, 7.0443e-06, 3.6622e-02, 3.3644e-06, 1.5738e-06, 1.4024e-04,\n",
            "        3.7995e-06, 1.1386e-04, 9.9927e-01, 6.6687e-05])\n",
            "Sum of predicted tensor(4.5687)\n",
            "Loss tensor(0.2194)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.4849e-05, 1.5262e-06, 5.9585e-01, 3.7039e-06, 4.0004e-04, 2.6838e-06,\n",
            "        4.6761e-06, 9.8530e-06, 5.3766e-06, 2.5315e-04, 5.6048e-06, 1.2625e-05,\n",
            "        1.8796e-05, 5.8499e-06, 3.6157e-04, 3.6608e-06, 9.2048e-04, 1.3286e-06,\n",
            "        3.7317e-06, 1.8725e-06, 2.0419e-06, 9.9136e-01, 9.8033e-01, 1.3072e-06,\n",
            "        1.6112e-06, 2.3067e-06, 4.2030e-06, 2.6371e-05, 1.7754e-06, 1.7754e-05,\n",
            "        3.4260e-06, 5.0707e-06, 9.9725e-01, 2.6598e-01, 1.9923e-05, 2.7579e-06,\n",
            "        1.2984e-05, 2.2385e-06, 1.7072e-05, 5.0864e-06, 3.1493e-06, 6.0446e-04,\n",
            "        1.6438e-04, 3.1899e-01, 5.1408e-06, 5.1191e-05, 4.6495e-06, 1.3176e-06,\n",
            "        2.6673e-06, 5.8490e-06, 1.0858e-04, 8.5976e-05, 1.0372e-05, 9.9270e-01,\n",
            "        4.1837e-06, 1.2027e-06, 4.6495e-06, 1.5602e-06, 2.2480e-04, 1.2713e-05,\n",
            "        5.7336e-06, 1.2124e-02, 3.8564e-06, 9.7853e-01])\n",
            "Sum of predicted tensor(6.1366)\n",
            "Loss tensor(0.2578)\n",
            "Acc tensor(0.9062)\n",
            "tensor([1.5266e-06, 1.6150e-05, 1.6274e-06, 2.6084e-05, 6.7922e-06, 4.1866e-06,\n",
            "        1.4719e-05, 7.1833e-06, 2.1934e-05, 1.1054e-05, 2.0453e-05, 2.9721e-05,\n",
            "        2.8269e-01, 1.3233e-06, 9.7866e-01, 9.8832e-01, 9.2753e-06, 9.9506e-01,\n",
            "        7.7103e-02, 3.0689e-03, 6.8750e-06, 1.1832e-06, 4.5689e-06, 9.8859e-07,\n",
            "        8.0478e-04, 7.4810e-06, 4.4826e-06, 2.1876e-05, 2.9431e-06, 2.3440e-05,\n",
            "        5.2826e-05, 6.4837e-05, 3.2062e-06, 8.8780e-06, 9.9716e-01, 1.6194e-05,\n",
            "        2.4047e-05, 6.2487e-06, 4.5776e-06, 3.1615e-06, 3.8778e-06, 5.5517e-05,\n",
            "        5.1408e-06, 4.1921e-06, 3.0713e-06, 3.5637e-03, 3.1410e-06, 6.7851e-06,\n",
            "        1.4391e-02, 9.9962e-04, 1.4429e-05, 1.6717e-05, 7.4810e-06, 5.1408e-06,\n",
            "        1.4700e-05, 1.1319e-02, 3.3277e-05, 1.7287e-05, 3.1112e-04, 4.2113e-04,\n",
            "        1.3821e-02, 1.7218e-03, 1.7649e-05, 1.3385e-06])\n",
            "Sum of predicted tensor(4.3700)\n",
            "Loss tensor(0.3518)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.1272e-06, 1.0228e-05, 2.1204e-06, 1.6253e-05, 4.4436e-06, 9.6452e-01,\n",
            "        9.9992e-01, 1.2550e-05, 2.7410e-06, 6.2463e-06, 3.8711e-06, 5.1313e-05,\n",
            "        3.4614e-06, 2.1107e-05, 1.4221e-03, 1.2495e-01, 4.8700e-06, 1.7740e-05,\n",
            "        1.4895e-05, 3.2944e-03, 3.5149e-05, 6.9934e-05, 1.1788e-06, 4.4715e-01,\n",
            "        1.2389e-06, 1.4241e-01, 1.9246e-05, 2.1961e-05, 3.9971e-06, 3.9771e-06,\n",
            "        6.3373e-05, 2.2264e-05, 4.2244e-03, 2.1665e-06, 9.0672e-05, 1.9052e-06,\n",
            "        7.8367e-05, 5.4480e-06, 1.8816e-04, 1.3624e-06, 6.0364e-06, 3.7003e-06,\n",
            "        9.8405e-01, 1.2387e-05, 7.8698e-06, 9.9979e-01, 5.3616e-05, 9.9489e-01,\n",
            "        1.0474e-03, 1.2104e-06, 1.3345e-05, 7.4810e-06, 1.1216e-02, 3.2517e-05,\n",
            "        1.0665e-05, 1.6697e-06, 6.5515e-05, 9.9229e-01, 2.2245e-06, 3.0017e-04,\n",
            "        1.6575e-06, 4.1572e-06, 7.1846e-06, 2.1422e-06])\n",
            "Sum of predicted tensor(6.6725)\n",
            "Loss tensor(0.4013)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.1408e-06, 4.2708e-05, 7.2375e-06, 3.1300e-06, 4.7896e-06, 1.6746e-06,\n",
            "        1.6036e-05, 2.3119e-06, 2.3003e-06, 5.9892e-06, 6.4855e-06, 5.8674e-05,\n",
            "        4.9032e-06, 9.5066e-01, 6.2386e-06, 1.2202e-01, 2.0598e-05, 4.3039e-06,\n",
            "        4.9380e-05, 4.3215e-06, 3.2319e-06, 1.4194e-05, 2.6775e-06, 4.2307e-05,\n",
            "        3.5038e-06, 1.0262e-04, 2.5177e-05, 8.3932e-06, 1.9818e-06, 3.8530e-05,\n",
            "        3.7855e-06, 3.2487e-02, 1.1239e-05, 1.8152e-06, 2.3974e-06, 9.8659e-01,\n",
            "        3.3051e-06, 7.0954e-06, 5.9910e-06, 3.7266e-06, 1.2355e-05, 5.2802e-01,\n",
            "        2.6209e-05, 1.6636e-05, 9.9925e-01, 9.9661e-01, 3.3556e-06, 6.2542e-06,\n",
            "        4.5616e-05, 5.0685e-06, 1.4875e-02, 5.1408e-06, 2.6640e-01, 1.6435e-06,\n",
            "        2.3252e-06, 4.0976e-06, 3.9834e-06, 2.5942e-03, 2.5538e-06, 9.9936e-01,\n",
            "        7.0110e-06, 4.6495e-06, 1.4289e-06, 1.0265e-05])\n",
            "Sum of predicted tensor(5.8995)\n",
            "Loss tensor(0.3365)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9988e-01, 5.4449e-06, 5.2611e-06, 5.1006e-04, 1.3627e-05, 7.8815e-06,\n",
            "        1.1714e-05, 3.9562e-05, 2.4332e-05, 4.1683e-02, 1.8524e-06, 1.0503e-02,\n",
            "        4.0817e-05, 4.3026e-06, 4.5014e-05, 1.2417e-03, 5.4529e-06, 1.0834e-02,\n",
            "        7.6813e-06, 1.3435e-05, 9.7126e-01, 4.5179e-05, 9.9127e-01, 6.2937e-06,\n",
            "        2.4279e-05, 2.3719e-05, 3.5125e-03, 2.1935e-05, 5.7855e-05, 1.5792e-05,\n",
            "        2.6955e-06, 9.6310e-05, 3.3226e-05, 8.2994e-05, 5.4096e-06, 4.2492e-05,\n",
            "        1.0235e-04, 9.9188e-01, 1.6861e-06, 4.5279e-06, 1.6742e-06, 2.8179e-04,\n",
            "        6.5793e-05, 1.1333e-04, 6.0005e-03, 1.9886e-06, 7.8873e-06, 3.3149e-06,\n",
            "        5.8132e-04, 1.9449e-05, 2.6161e-06, 6.5210e-06, 3.1515e-06, 8.9248e-06,\n",
            "        5.1408e-06, 8.7823e-02, 1.2094e-05, 5.3620e-06, 9.9992e-01, 1.6112e-06,\n",
            "        1.2324e-05, 2.1692e-06, 1.7633e-06, 4.6957e-06])\n",
            "Sum of predicted tensor(5.1183)\n",
            "Loss tensor(0.3490)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.0962e-05, 4.7942e-05, 1.5766e-06, 7.2079e-06, 6.8095e-06, 5.9803e-06,\n",
            "        3.4069e-06, 2.2928e-06, 7.1900e-01, 9.3200e-06, 6.9466e-05, 5.4715e-01,\n",
            "        2.4466e-05, 2.3775e-05, 2.6636e-06, 2.6725e-06, 2.8484e-06, 4.2364e-06,\n",
            "        6.1987e-05, 2.9977e-04, 2.6293e-05, 3.0437e-03, 5.1075e-05, 1.3222e-05,\n",
            "        8.8900e-06, 4.4232e-05, 8.5732e-06, 5.4385e-04, 2.6393e-06, 6.6353e-03,\n",
            "        1.5816e-01, 6.2933e-06, 5.0526e-06, 2.8245e-06, 1.2007e-06, 4.8977e-06,\n",
            "        3.1358e-05, 8.3916e-06, 4.0963e-06, 2.2831e-04, 1.2017e-05, 1.6423e-06,\n",
            "        4.5576e-06, 9.2154e-05, 5.6205e-05, 8.3556e-04, 7.2984e-06, 1.9985e-04,\n",
            "        7.3837e-04, 2.4057e-06, 2.9896e-02, 6.2487e-05, 4.7050e-06, 1.0155e-05,\n",
            "        5.2672e-06, 2.6114e-06, 9.1756e-05, 4.2966e-06, 5.5710e-05, 9.7145e-01,\n",
            "        3.8347e-06, 1.3493e-05, 1.4629e-05, 8.8429e-06])\n",
            "Sum of predicted tensor(2.4391)\n",
            "Loss tensor(0.2123)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.9187e-06, 6.8268e-06, 7.9863e-05, 3.7814e-05, 2.6749e-02, 5.9288e-05,\n",
            "        1.1928e-05, 3.5856e-06, 2.1659e-06, 4.9375e-06, 4.5949e-03, 3.0081e-06,\n",
            "        2.0302e-05, 1.7656e-05, 6.1744e-06, 5.1200e-06, 9.9960e-01, 4.3523e-06,\n",
            "        1.3521e-05, 2.0615e-06, 1.2148e-06, 3.8919e-05, 8.4798e-05, 3.2258e-06,\n",
            "        1.8158e-06, 4.7441e-03, 6.6748e-04, 1.9611e-06, 9.9944e-01, 4.1814e-02,\n",
            "        3.5838e-06, 1.7081e-06, 1.0723e-04, 9.8711e-06, 4.6107e-06, 1.0949e-05,\n",
            "        2.0932e-06, 1.1331e-05, 3.3005e-05, 6.7351e-06, 3.4113e-04, 7.1542e-01,\n",
            "        9.2588e-06, 4.4937e-06, 2.4155e-05, 7.9985e-06, 1.0819e-05, 1.0795e-05,\n",
            "        6.5129e-06, 6.3006e-06, 1.3572e-05, 6.5275e-06, 1.4599e-05, 1.4599e-05,\n",
            "        1.3716e-05, 6.9100e-06, 5.0451e-02, 2.7174e-06, 2.3319e-06, 1.2735e-06,\n",
            "        1.5667e-06, 8.4576e-03, 4.1767e-06, 1.6045e-05])\n",
            "Sum of predicted tensor(2.8531)\n",
            "Loss tensor(0.0219)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.6751e-05, 1.4443e-04, 1.7623e-05, 3.6739e-06, 1.2962e-06, 5.6698e-05,\n",
            "        5.5761e-06, 1.1577e-05, 6.8892e-05, 9.6497e-03, 6.7038e-06, 6.8574e-06,\n",
            "        1.6198e-05, 3.4444e-06, 2.0328e-06, 4.5485e-05, 3.0231e-06, 2.6327e-05,\n",
            "        3.7591e-05, 2.6709e-06, 2.6152e-06, 7.4505e-05, 2.9148e-06, 6.7902e-05,\n",
            "        2.3100e-05, 4.5195e-06, 9.1376e-06, 9.9943e-01, 6.8965e-06, 3.1677e-06,\n",
            "        1.0988e-04, 1.9035e-04, 3.7008e-05, 2.4106e-05, 4.4472e-06, 8.7901e-06,\n",
            "        1.0980e-05, 5.2859e-06, 2.1608e-05, 1.0534e-05, 1.9898e-06, 1.3252e-05,\n",
            "        3.3874e-06, 2.7241e-06, 2.1907e-05, 4.5520e-05, 2.3719e-06, 9.3828e-01,\n",
            "        4.1032e-06, 6.3033e-06, 1.0929e-05, 5.3503e-05, 4.9644e-06, 1.6402e-05,\n",
            "        1.2626e-03, 6.1261e-06, 1.5910e-05, 5.7866e-01, 1.4756e-05, 7.6078e-06,\n",
            "        3.7968e-06, 9.9859e-01, 2.0791e-05, 4.4381e-05])\n",
            "Sum of predicted tensor(3.5273)\n",
            "Loss tensor(0.2058)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.3136e-06, 1.0202e-05, 2.5527e-06, 2.6781e-06, 2.2566e-04, 1.5158e-05,\n",
            "        3.4986e-01, 9.8612e-06, 3.4821e-06, 2.0909e-06, 1.3605e-04, 7.5605e-03,\n",
            "        2.5560e-05, 1.4863e-03, 7.3354e-04, 1.7794e-05, 9.9072e-07, 4.0095e-06,\n",
            "        1.7489e-06, 1.3645e-05, 1.0088e-05, 2.2041e-04, 4.8485e-06, 3.1983e-06,\n",
            "        1.4998e-05, 2.4078e-04, 1.5517e-05, 5.5370e-06, 2.7889e-05, 9.6497e-03,\n",
            "        1.1455e-05, 2.6004e-06, 2.5920e-06, 2.4462e-04, 7.9350e-05, 5.0249e-05,\n",
            "        3.4593e-06, 2.9850e-06, 9.7837e-06, 7.0276e-01, 1.6373e-05, 2.9470e-06,\n",
            "        2.3735e-05, 6.6365e-05, 1.1658e-05, 4.4836e-06, 8.4077e-01, 7.3295e-06,\n",
            "        7.1300e-05, 9.7523e-06, 1.8122e-04, 6.4644e-05, 2.3158e-06, 9.9791e-01,\n",
            "        4.1113e-06, 6.2158e-06, 2.5326e-05, 1.0880e-05, 6.8979e-02, 2.7176e-06,\n",
            "        4.2573e-06, 1.9178e-05, 2.8804e-05, 7.3124e-06])\n",
            "Sum of predicted tensor(2.9817)\n",
            "Loss tensor(0.0559)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.5352e-06, 7.3998e-05, 2.3288e-05, 4.6057e-06, 1.0569e-05, 2.4459e-06,\n",
            "        4.4309e-05, 1.0722e-05, 6.2012e-06, 3.2913e-06, 5.0083e-06, 3.5419e-02,\n",
            "        9.0403e-05, 1.0015e-05, 1.9036e-05, 7.5090e-05, 6.7958e-06, 3.3955e-05,\n",
            "        1.5269e-04, 3.0234e-04, 3.6280e-05, 3.3914e-03, 8.0530e-05, 5.1056e-06,\n",
            "        2.4574e-06, 4.4932e-06, 7.2852e-02, 1.2845e-01, 2.8216e-06, 1.0475e-05,\n",
            "        5.0083e-06, 3.3493e-05, 9.8676e-01, 5.6645e-06, 2.1410e-06, 1.0269e-05,\n",
            "        5.5317e-06, 1.4187e-05, 4.4770e-05, 9.9129e-06, 3.3146e-06, 6.2201e-06,\n",
            "        4.9434e-05, 5.6268e-05, 3.5575e-05, 1.9153e-06, 2.5827e-05, 6.7071e-05,\n",
            "        3.1451e-06, 1.2411e-05, 3.1775e-06, 5.6192e-06, 2.3269e-06, 5.5032e-06,\n",
            "        4.1294e-06, 7.2672e-06, 7.3117e-05, 2.8636e-05, 3.1403e-05, 3.7369e-05,\n",
            "        2.3420e-05, 6.4160e-04, 4.7084e-06, 1.1074e-04])\n",
            "Sum of predicted tensor(1.2293)\n",
            "Loss tensor(0.2594)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.5310e-05, 9.9519e-06, 8.4948e-06, 6.3591e-06, 1.1857e-04, 1.3108e-02,\n",
            "        1.9187e-06, 9.9924e-01, 1.9972e-04, 7.8466e-06, 5.1386e-06, 2.9419e-06,\n",
            "        5.4051e-06, 1.6118e-05, 4.0651e-06, 2.7352e-06, 5.6862e-06, 8.9725e-01,\n",
            "        1.3044e-04, 1.5308e-01, 5.9394e-06, 2.2335e-06, 9.6892e-06, 4.2363e-05,\n",
            "        3.2761e-03, 1.3875e-04, 9.9973e-01, 5.5037e-05, 6.2820e-01, 3.0170e-03,\n",
            "        8.4295e-05, 1.0602e-02, 3.6297e-05, 9.9972e-01, 7.4322e-06, 6.2961e-01,\n",
            "        4.3805e-06, 6.5807e-05, 3.6709e-03, 5.1437e-04, 3.8875e-06, 1.8267e-06,\n",
            "        2.3225e-05, 3.1083e-06, 1.5996e-05, 6.6794e-06, 1.7431e-04, 3.7228e-06,\n",
            "        5.7960e-01, 8.2834e-06, 2.1063e-06, 5.5971e-06, 3.4342e-06, 3.7644e-05])\n",
            "Sum of predicted tensor(5.9219)\n",
            "Loss tensor(0.3344)\n",
            "Acc tensor(0.8889)\n",
            "\tTrain Loss: 0.051 | Train Acc: 98.46%\n",
            "\t Val. Loss: 0.242 |  Val. Acc: 94.98%\n",
            "Sum of predicted tensor(3.2486, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7896, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9788, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0666, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0197, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1605, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1364, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2940, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3016, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0758, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9953, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1882, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7548, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1332, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5733, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8227, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7829, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4079, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8733, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.6344, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0362, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3325, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0194, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0244, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7972, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0014, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8912, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9000, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4881, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2452, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0867, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.3729, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.2657, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0908, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4187, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1554, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3072, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0424, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0004, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1157, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3464, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2655, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1857, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2597, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.1875, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6092, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0015, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9032, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(11.9710, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2622, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.7850, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.4281, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0567, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8294, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5873, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4028, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.3887, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7175, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.6704, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0865, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.4350, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7668, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0161, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.2409, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9564, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3156, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1513, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1386, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5686, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5870, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4296, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8314, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3925, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4702, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0280, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(11.9168, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7849, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9911, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5499, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1497, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.7142, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.3674, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3589, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8071, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0298, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9821, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1747, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0093, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3625, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.5738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0814, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1541, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1378, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4441, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5790, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1252, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.4215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5517, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0636, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7654, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9862, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.2205, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.3066, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1099, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3525, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7464, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0882, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0662, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0199, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8273, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9966, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0141, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.2576, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9652, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0871, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0778, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9973, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6859, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3959, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.5404, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9550, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7684, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3576, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0132, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0616, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9166, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1899, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3202, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.3556, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9155, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.9523, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9620, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.8034, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4527, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3396, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9991, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0088, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4827, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.3440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9796, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1390, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2197, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.2045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9103, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9969, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0174, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3113, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.7568, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.3838, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9889, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0062, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7091, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8268, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.6402, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9495, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8702, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1844, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3585, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9895, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8683, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9519, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0158, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9982, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3205, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2021, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.2860, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9881, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0771, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8850, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8706, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2195, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3582, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.2271, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.3088, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5538, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3876, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9961, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9878, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0237, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4270, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0281, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1630, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9787, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9312, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.2325, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9773, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0140, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0167, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8055, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7159, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.2173, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0345, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.5043, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.7599, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2128, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7733, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2346, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8365, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.5337, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.5647, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3169, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.5119, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1247, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4132, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0557, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8455, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0578, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.4981, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0305, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5558, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0281, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7164, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.4961, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3354, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.0499, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0407, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0338, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1289, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0857, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9763, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7919, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7863, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1648, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.1626, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1018, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4661, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0560, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0729, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7518, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3414, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.1413, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7588, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0015, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5398, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0753, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7962, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.4488, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4803, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9923, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.9478, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8861, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0086, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1111, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9851, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3100, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8088, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.5268, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9039, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8634, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1457, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0064, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0400, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5449, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9438, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1739, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6134, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0948, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.6330, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9540, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9008, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0853, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0296, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5110, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2765, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8897, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0157, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9835, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5316, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0793, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8865, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0371, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6356, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7920, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0163, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2907, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1092, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5107, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5894, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0923, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4060, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0476, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0194, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1077, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9562, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3524, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9964, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1373, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7761, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.1384, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.3587, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.2713, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7789, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5309, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1119, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9002, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1796, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8728, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8610, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4233, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2884, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9428, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0196, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0283, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0269, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0812, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4218, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4071, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0791, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0405, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6931, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3879, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1706, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3828, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0022, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.7318, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4013, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3882, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9453, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.9203, grad_fn=<SumBackward0>)\n",
            "tensor([3.1872e-05, 1.8463e-04, 8.1012e-06, 1.8463e-04, 2.9012e-04, 8.8711e-03,\n",
            "        2.3882e-04, 1.4205e-04, 1.5754e-05, 7.8328e-05, 1.3145e-04, 3.4391e-06,\n",
            "        1.2694e-04, 1.5565e-05, 3.8210e-03, 1.8824e-05, 1.8165e-04, 1.4942e-02,\n",
            "        3.8210e-03, 8.5532e-05, 3.8269e-05, 2.4937e-04, 3.0796e-05, 4.3592e-05,\n",
            "        1.4942e-02, 4.8965e-05, 1.6383e-04, 5.9569e-04, 3.8210e-03, 6.5049e-04,\n",
            "        1.0538e-04, 1.9771e-05, 3.5416e-05, 3.8210e-03, 6.2472e-04, 5.0938e-05,\n",
            "        2.3570e-04, 2.8188e-04, 3.8210e-03, 9.5743e-06, 3.8210e-03, 4.1301e-05,\n",
            "        2.8862e-05, 2.6086e-05, 4.1301e-05, 5.0124e-05, 3.8210e-03, 3.8210e-03,\n",
            "        3.8210e-03, 4.6812e-05, 3.8210e-03, 3.8210e-03, 5.6167e-04, 3.2066e-04,\n",
            "        1.7588e-05, 4.1301e-05, 4.6488e-03, 2.8014e-04, 4.1301e-05, 2.0451e-05,\n",
            "        2.3381e-05, 9.5630e-06, 5.9152e-05, 4.8262e-05])\n",
            "Sum of predicted tensor(0.0920)\n",
            "Loss tensor(0.2189)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.1575e-05, 1.7652e-04, 7.9571e-06, 8.3747e-01, 1.1014e-05, 5.4218e-05,\n",
            "        1.1471e-05, 4.8046e-05, 5.2023e-01, 3.0357e-02, 2.7671e-05, 1.4300e-03,\n",
            "        9.7683e-05, 9.8299e-05, 1.6514e-03, 5.3709e-01, 8.8746e-06, 3.9173e-04,\n",
            "        2.2106e-05, 9.8879e-04, 9.9859e-06, 1.0774e-04, 3.5231e-06, 2.9012e-04,\n",
            "        2.4742e-03, 5.4481e-02, 1.0654e-05, 1.8463e-04, 7.7059e-06, 1.1090e-04,\n",
            "        9.9978e-01, 4.7141e-01, 4.8728e-06, 1.0404e-05, 4.2139e-05, 3.2126e-05,\n",
            "        7.3740e-06, 2.3882e-04, 8.5938e-06, 3.5231e-06, 7.8159e-04, 5.4847e-04,\n",
            "        1.1318e-04, 1.3432e-05, 1.2575e-05, 8.1887e-05, 9.5778e-05, 8.1719e-05,\n",
            "        1.9255e-05, 2.9782e-03, 1.0928e-05, 3.4459e-05, 1.3561e-04, 1.4717e-03,\n",
            "        5.6445e-05, 6.2773e-06, 3.5231e-06, 3.5743e-04, 1.1500e-05, 9.5446e-05,\n",
            "        2.0269e-04, 3.7410e-05, 4.5950e-06, 7.8159e-04])\n",
            "Sum of predicted tensor(3.4673)\n",
            "Loss tensor(0.0635)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.7010e-06, 7.8159e-04, 2.1558e-06, 2.6748e-05, 9.9758e-06, 1.5341e-02,\n",
            "        9.9949e-01, 3.2193e-03, 8.4686e-03, 4.1285e-05, 1.8453e-05, 3.0016e-06,\n",
            "        7.8159e-04, 9.9454e-05, 2.6078e-05, 1.8463e-04, 4.8587e-05, 3.5231e-06,\n",
            "        7.8159e-04, 7.0683e-06, 1.3146e-05, 3.1667e-05, 2.6499e-02, 1.1519e-02,\n",
            "        1.8838e-04, 6.3928e-05, 1.7634e-05, 2.7949e-04, 9.3781e-06, 7.6492e-06,\n",
            "        1.2832e-05, 8.1969e-05, 3.5231e-06, 7.8159e-04, 1.9723e-05, 1.8463e-04,\n",
            "        7.8159e-04, 1.2974e-05, 7.8159e-04, 3.5640e-04, 3.9177e-05, 9.7683e-05,\n",
            "        8.0549e-06, 3.1188e-05, 6.9802e-01, 1.8162e-05, 6.9206e-05, 6.9915e-04,\n",
            "        7.8159e-04, 1.1321e-05, 3.0892e-04, 1.5276e-03, 1.9577e-03, 5.5458e-03,\n",
            "        3.1940e-05, 1.5562e-04, 1.4560e-04, 1.9723e-05, 3.8668e-05, 1.4593e-05,\n",
            "        8.9848e-01, 4.5927e-03, 1.2638e-03, 4.8036e-05])\n",
            "Sum of predicted tensor(2.6849)\n",
            "Loss tensor(0.5710)\n",
            "Acc tensor(0.8906)\n",
            "tensor([3.1484e-06, 1.2236e-04, 1.4976e-06, 3.1603e-05, 8.4013e-06, 4.3358e-05,\n",
            "        3.7547e-05, 3.7369e-05, 1.1580e-05, 2.4081e-03, 1.0348e-04, 9.4989e-06,\n",
            "        2.6597e-06, 1.2835e-04, 2.1571e-04, 2.1903e-05, 1.0969e-03, 7.0633e-05,\n",
            "        6.3259e-01, 1.8000e-05, 4.0701e-06, 9.4958e-01, 9.6228e-06, 9.9007e-01,\n",
            "        2.6633e-06, 1.7392e-05, 4.2927e-06, 4.0099e-03, 4.5613e-06, 4.1696e-06,\n",
            "        6.9637e-02, 1.4036e-03, 3.1022e-05, 2.7853e-06, 2.7167e-05, 2.8992e-06,\n",
            "        3.6876e-06, 4.8041e-06, 2.1825e-02, 9.9981e-06, 1.8634e-06, 5.7384e-06,\n",
            "        3.9303e-06, 2.7167e-05, 1.5561e-01, 6.1313e-05, 3.1648e-03, 9.5220e-01,\n",
            "        7.0900e-05, 3.4984e-06, 6.1282e-06, 4.2594e-06, 8.3811e-06, 5.5627e-03,\n",
            "        1.1642e-05, 4.8947e-06, 3.7710e-05, 3.3260e-06, 2.7468e-04, 2.7167e-05,\n",
            "        2.8569e-01, 3.0219e-05, 6.6089e-04, 3.8442e-06])\n",
            "Sum of predicted tensor(4.0771)\n",
            "Loss tensor(0.3505)\n",
            "Acc tensor(0.9219)\n",
            "tensor([5.4945e-04, 8.6745e-05, 2.9284e-06, 2.4221e-04, 1.1991e-02, 1.2458e-03,\n",
            "        1.2947e-03, 3.6984e-02, 6.0043e-01, 7.3938e-06, 7.7166e-05, 4.1593e-03,\n",
            "        8.2178e-06, 6.6296e-06, 5.8730e-06, 1.9598e-05, 1.7917e-03, 1.4938e-05,\n",
            "        8.0584e-04, 8.1175e-06, 2.2796e-02, 2.0534e-06, 7.4556e-06, 4.6750e-05,\n",
            "        6.4323e-04, 1.8634e-06, 4.1806e-05, 5.5649e-05, 4.0847e-03, 1.4355e-06,\n",
            "        4.5488e-06, 3.1299e-06, 2.0311e-03, 1.0476e-06, 8.6942e-04, 2.1680e-05,\n",
            "        2.6989e-06, 2.8657e-02, 6.6201e-05, 6.2066e-06, 1.0623e-04, 9.1030e-06,\n",
            "        5.5085e-06, 7.9246e-06, 6.1472e-06, 6.1100e-02, 1.6195e-05, 2.5356e-06,\n",
            "        7.1150e-04, 1.1106e-05, 1.2137e-05, 2.7252e-06, 1.1827e-05, 6.6089e-04,\n",
            "        6.8157e-03, 3.0904e-06, 1.0073e-04, 2.2471e-05, 4.7173e-05, 6.9658e-06,\n",
            "        2.5409e-06, 3.9472e-05, 4.8974e-06, 6.0455e-03])\n",
            "Sum of predicted tensor(0.7948)\n",
            "Loss tensor(0.0111)\n",
            "Acc tensor(1.)\n",
            "tensor([1.8800e-04, 1.3069e-06, 1.8927e-05, 5.8584e-06, 4.9930e-06, 1.4355e-06,\n",
            "        2.8390e-06, 5.5951e-06, 1.7692e-02, 6.3226e-05, 1.4355e-06, 3.1230e-01,\n",
            "        1.3919e-03, 2.3073e-05, 1.6250e-05, 1.4958e-04, 2.5284e-06, 4.3165e-05,\n",
            "        5.3713e-06, 6.4367e-05, 6.7703e-04, 8.6078e-05, 9.8505e-06, 8.0763e-06,\n",
            "        9.8034e-01, 1.1013e-05, 2.6355e-05, 1.6396e-05, 3.4867e-05, 1.4355e-06,\n",
            "        1.5646e-05, 7.1478e-05, 7.0621e-04, 6.8453e-06, 5.7600e-06, 8.8728e-06,\n",
            "        1.2768e-04, 3.2673e-06, 1.2461e-05, 2.6158e-05, 1.4355e-06, 3.7248e-06,\n",
            "        5.7692e-04, 2.2003e-05, 4.5272e-05, 2.4388e-05, 1.4069e-04, 3.4466e-04,\n",
            "        2.0159e-05, 3.1921e-05, 2.3843e-04, 8.7485e-02, 1.4355e-06, 3.2057e-04,\n",
            "        8.0071e-04, 2.3875e-05, 2.6905e-05, 3.0713e-05, 1.4355e-06, 4.9921e-05,\n",
            "        6.6089e-04, 4.7833e-06, 5.1640e-06, 9.7976e-06])\n",
            "Sum of predicted tensor(1.4051)\n",
            "Loss tensor(0.2201)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.9439e-06, 3.2868e-06, 4.0971e-06, 5.8358e-05, 1.2097e-06, 3.5470e-06,\n",
            "        1.1910e-06, 2.9586e-06, 1.3257e-06, 5.6651e-03, 1.4150e-06, 4.8086e-04,\n",
            "        3.9786e-06, 6.6746e-01, 6.3187e-04, 6.6089e-04, 2.6678e-06, 6.0788e-05,\n",
            "        7.5006e-05, 5.3652e-06, 1.4355e-06, 3.3260e-06, 3.1898e-05, 6.9222e-05,\n",
            "        1.5074e-05, 1.5798e-05, 1.3732e-03, 1.4355e-06, 1.6037e-04, 1.1013e-05,\n",
            "        4.1628e-05, 5.6432e-01, 1.2902e-05, 6.5838e-06, 4.3875e-05, 2.2327e-05,\n",
            "        8.1683e-06, 2.1098e-05, 2.2739e-06, 6.5055e-06, 2.4518e-04, 3.2773e-05,\n",
            "        5.3007e-03, 1.8634e-06, 7.2907e-05, 1.2688e-04, 6.0523e-04, 2.6808e-06,\n",
            "        5.8127e-06, 3.0169e-05, 1.7543e-03, 9.8299e-06, 1.2623e-04, 3.0324e-05,\n",
            "        3.1421e-02, 6.6089e-04, 2.7722e-06, 3.4917e-05, 9.7424e-06, 3.8918e-04,\n",
            "        1.4355e-06, 3.9198e-04, 3.4053e-05, 2.7970e-03])\n",
            "Sum of predicted tensor(1.2854)\n",
            "Loss tensor(0.1454)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1942e-06, 3.5251e-06, 5.2340e-05, 7.2375e-05, 1.7089e-05, 4.3622e-05,\n",
            "        5.8416e-06, 1.5102e-05, 1.4999e-06, 9.8950e-01, 9.7832e-04, 8.5009e-02,\n",
            "        1.3292e-06, 1.9658e-05, 4.5518e-06, 1.5101e-05, 2.6334e-05, 1.2918e-05,\n",
            "        3.0039e-06, 4.6372e-05, 1.5484e-05, 8.4290e-06, 9.3480e-04, 1.2943e-05,\n",
            "        3.9964e-06, 2.5747e-02, 3.7927e-06, 2.1593e-06, 2.1593e-06, 2.0433e-06,\n",
            "        8.0199e-06, 1.8209e-06, 7.5181e-05, 1.2699e-06, 9.9128e-01, 1.2950e-06,\n",
            "        1.1709e-03, 1.1747e-04, 6.1530e-04, 4.7138e-05, 2.0970e-04, 6.6575e-04,\n",
            "        2.4537e-06, 9.8642e-06, 1.6133e-05, 6.9469e-06, 1.2443e-05, 2.4205e-06,\n",
            "        3.0382e-03, 2.4329e-05, 1.3257e-06, 9.8503e-06, 1.2394e-06, 6.7745e-05,\n",
            "        1.3257e-06, 1.6619e-01, 1.2090e-06, 1.2279e-06, 5.2494e-03, 1.1552e-06,\n",
            "        3.0039e-06, 1.1564e-06, 1.2220e-04, 1.3713e-06])\n",
            "Sum of predicted tensor(2.2715)\n",
            "Loss tensor(0.1753)\n",
            "Acc tensor(0.9531)\n",
            "tensor([8.5226e-06, 6.3308e-06, 3.6149e-04, 1.3257e-06, 3.1291e-06, 2.0188e-02,\n",
            "        2.1275e-05, 7.2695e-06, 4.6376e-06, 1.4115e-03, 1.2394e-06, 1.0828e-05,\n",
            "        1.2220e-04, 9.7832e-04, 2.2864e-06, 6.8965e-05, 8.4515e-04, 1.2295e-06,\n",
            "        5.4044e-01, 2.8973e-04, 1.4464e-02, 3.7322e-05, 1.9834e-05, 6.6882e-05,\n",
            "        2.1824e-06, 6.7250e-03, 2.4454e-06, 9.9966e-01, 1.2484e-06, 5.4754e-04,\n",
            "        2.1824e-06, 7.2829e-06, 9.7832e-04, 5.7570e-04, 1.1971e-06, 3.0039e-06,\n",
            "        1.2272e-05, 1.3713e-06, 2.3727e-04, 4.4858e-04, 1.5114e-04, 5.6942e-05,\n",
            "        2.5192e-06, 1.3257e-06, 3.2347e-06, 1.2903e-06, 1.4000e-05, 1.6133e-05,\n",
            "        1.0441e-01, 1.3994e-05, 1.2365e-05, 1.3847e-06, 1.2112e-06, 9.1368e-06,\n",
            "        7.3843e-06, 1.3713e-06, 7.8517e-01, 1.1285e-05, 1.0742e-04, 1.3257e-06,\n",
            "        9.0522e-06, 3.1045e-06, 1.5353e-05, 2.0178e-04])\n",
            "Sum of predicted tensor(2.4788)\n",
            "Loss tensor(0.2763)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.2192e-06, 8.6353e-06, 3.5685e-06, 2.1626e-05, 4.8449e-04, 1.8119e-06,\n",
            "        2.3747e-05, 1.3408e-06, 9.2540e-06, 2.3281e-04, 1.2357e-06, 1.6807e-05,\n",
            "        3.5241e-05, 2.8777e-05, 3.1361e-06, 1.8001e-02, 1.4894e-01, 1.2255e-06,\n",
            "        3.6469e-06, 2.3088e-06, 1.0879e-05, 7.0637e-06, 3.1560e-05, 4.3618e-05,\n",
            "        1.7375e-05, 1.5410e-05, 2.1885e-05, 1.0566e-04, 1.1399e-05, 1.3898e-06,\n",
            "        4.8060e-04, 3.0147e-04, 1.2887e-06, 1.7230e-04, 8.3469e-05, 1.3713e-06,\n",
            "        1.2622e-05, 1.9722e-04, 3.4396e-06, 4.3700e-05, 7.6274e-06, 9.5887e-06,\n",
            "        1.3257e-06, 4.6768e-01, 1.9478e-03, 9.9748e-01, 1.1789e-06, 2.9822e-06,\n",
            "        1.6562e-05, 1.1913e-06, 1.3308e-06, 4.8562e-06, 1.6854e-05, 1.4534e-05,\n",
            "        1.7050e-05, 2.8630e-03, 2.1692e-04, 4.1263e-05, 9.1904e-06, 6.9667e-06,\n",
            "        1.5975e-06, 9.7335e-04, 1.7982e-06, 8.3862e-06])\n",
            "Sum of predicted tensor(1.6407)\n",
            "Loss tensor(0.1717)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.1576e-05, 1.0255e-05, 1.2880e-06, 6.2037e-04, 1.0257e-05, 3.0428e-05,\n",
            "        1.2465e-05, 1.4433e-06, 7.0233e-05, 2.6139e-05, 2.1166e-04, 5.3550e-06,\n",
            "        2.9141e-03, 7.4158e-06, 9.7832e-04, 9.9960e-01, 9.6265e-05, 1.2140e-06,\n",
            "        5.6724e-01, 1.6364e-03, 2.6221e-05, 7.3401e-06, 1.6703e-05, 8.3817e-06,\n",
            "        3.1943e-01, 3.8042e-05, 2.0881e-05, 1.3257e-06, 9.0863e-05, 4.6633e-05,\n",
            "        4.1292e-05, 1.2128e-06, 1.3257e-06, 1.2951e-06, 1.1497e-06, 4.6093e-06,\n",
            "        1.8700e-02, 4.6372e-05, 1.2407e-06, 3.7447e-06, 1.4032e-05, 1.1124e-05,\n",
            "        4.3355e-05, 1.8468e-05, 1.3796e-06, 9.8900e-01, 3.9439e-04, 3.7071e-03,\n",
            "        2.2962e-05, 3.0039e-06, 2.1824e-06, 1.4655e-03, 1.9742e-06, 3.9320e-04,\n",
            "        2.4331e-05, 3.9313e-01, 3.9974e-05, 2.1803e-04, 2.2709e-05, 1.4534e-05,\n",
            "        1.6229e-04, 6.6391e-05, 1.0268e-05, 5.5333e-05])\n",
            "Sum of predicted tensor(3.3008)\n",
            "Loss tensor(0.2743)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.3257e-06, 1.2928e-06, 8.5553e-01, 9.7501e-02, 6.0129e-06, 1.3257e-06,\n",
            "        3.9730e-05, 1.3274e-06, 4.2819e-06, 2.2556e-06, 1.2272e-05, 9.4416e-05,\n",
            "        7.9076e-03, 1.2774e-06, 1.2789e-06, 7.2111e-06, 6.5285e-02, 2.5473e-06,\n",
            "        1.3762e-01, 2.9109e-04, 9.9721e-01, 3.3469e-01, 2.0372e-06, 9.3869e-01,\n",
            "        1.4103e-06, 1.3996e-04, 8.6025e-05, 3.0039e-06, 1.7260e-03, 9.9375e-01,\n",
            "        1.0115e-05, 6.4546e-04, 1.8404e-02, 7.7756e-01, 1.2407e-06, 7.6052e-06,\n",
            "        9.8306e-06, 4.0442e-06, 1.1639e-06, 7.7897e-05, 1.2272e-05, 3.6545e-06,\n",
            "        7.6173e-02, 1.6769e-06, 4.2032e-06, 3.0039e-06, 2.4761e-04, 1.4653e-05,\n",
            "        1.5643e-01, 4.6372e-05, 1.8534e-04, 4.3246e-05, 1.3616e-06, 5.1048e-05,\n",
            "        8.1749e-01, 1.2001e-06, 1.2125e-06, 1.3257e-06, 2.8139e-05, 1.3626e-02,\n",
            "        1.2651e-06, 2.0730e-04, 2.0133e-06, 2.0476e-06])\n",
            "Sum of predicted tensor(6.2919)\n",
            "Loss tensor(0.2612)\n",
            "Acc tensor(0.9219)\n",
            "tensor([8.8865e-01, 4.8588e-06, 1.3298e-03, 7.0167e-04, 1.1805e-06, 2.8139e-05,\n",
            "        2.8192e-05, 3.0039e-06, 6.3861e-05, 1.9005e-04, 5.7707e-06, 9.7556e-01,\n",
            "        6.5634e-05, 3.0951e-04, 9.9650e-01, 5.5403e-02, 3.5716e-05, 2.9038e-05,\n",
            "        1.5154e-05, 1.2197e-06, 3.2770e-06, 3.0039e-06, 1.5564e-05, 6.1920e-06,\n",
            "        2.0365e-05, 1.1837e-06, 6.2222e-06, 2.5564e-05, 3.4990e-04, 1.1437e-05,\n",
            "        4.8266e-06, 2.0234e-06, 5.6898e-05, 1.1773e-04, 1.2427e-04, 1.3257e-06,\n",
            "        1.3431e-06, 2.0931e-04, 4.7665e-06, 1.2608e-06, 8.1615e-06, 1.0782e-05,\n",
            "        2.1001e-01, 5.6841e-04, 1.9393e-05, 3.7496e-05, 8.2628e-06, 5.5572e-05,\n",
            "        1.3257e-06, 4.6604e-05, 1.0464e-05, 1.3257e-06, 2.5790e-05, 8.7438e-06,\n",
            "        1.1957e-06, 1.3168e-05, 9.9528e-01, 7.0608e-06, 1.2351e-06, 3.9650e-06,\n",
            "        1.3257e-06, 7.5446e-06, 2.9541e-05, 1.2585e-06])\n",
            "Sum of predicted tensor(4.1260)\n",
            "Loss tensor(0.2321)\n",
            "Acc tensor(0.9375)\n",
            "tensor([3.2907e-06, 6.4054e-06, 7.9101e-06, 5.0006e-06, 9.2417e-05, 5.4175e-06,\n",
            "        6.4884e-04, 9.7105e-01, 9.5102e-02, 3.6225e-05, 9.5862e-01, 1.2875e-04,\n",
            "        1.2653e-06, 6.3545e-05, 3.4782e-02, 5.5294e-05, 7.1601e-05, 1.6134e-05,\n",
            "        6.0169e-06, 1.5825e-05, 4.8175e-06, 5.0006e-06, 1.7989e-05, 3.2466e-05,\n",
            "        6.6722e-06, 2.3950e-06, 4.2831e-06, 5.1785e-06, 2.4688e-05, 1.0523e-03,\n",
            "        5.7421e-06, 3.1118e-05, 3.6278e-06, 1.4291e-05, 9.9971e-01, 1.5332e-05,\n",
            "        2.7713e-05, 4.1297e-01, 5.0006e-06, 4.0294e-02, 9.8200e-01, 1.1583e-06,\n",
            "        1.6967e-03, 5.0006e-06, 9.9768e-01, 1.0314e-05, 6.3447e-05, 4.8425e-05,\n",
            "        5.0006e-06, 5.6444e-06, 4.6281e-06, 1.7031e-05, 1.9054e-04, 2.2847e-06,\n",
            "        2.3699e-03, 1.3543e-06, 1.2316e-04, 4.0485e-03, 5.0006e-06, 5.0006e-06,\n",
            "        1.4828e-04, 2.5266e-05, 5.6211e-04, 5.0006e-06])\n",
            "Sum of predicted tensor(5.5040)\n",
            "Loss tensor(0.4873)\n",
            "Acc tensor(0.9062)\n",
            "tensor([5.8652e-03, 1.7983e-03, 2.5126e-04, 2.1427e-05, 5.5294e-05, 5.0006e-06,\n",
            "        2.5015e-05, 2.6197e-06, 5.0006e-06, 1.6735e-04, 1.2294e-04, 1.1541e-06,\n",
            "        1.9288e-05, 9.2510e-06, 1.3672e-06, 1.8415e-05, 2.1929e-05, 3.0222e-04,\n",
            "        4.1577e-04, 3.6926e-06, 7.7326e-06, 1.6594e-05, 3.2463e-06, 5.4290e-06,\n",
            "        1.4580e-05, 5.0006e-06, 1.2851e-06, 3.6391e-03, 2.5666e-04, 3.2511e-05,\n",
            "        2.5465e-04, 3.8413e-06, 7.3044e-05, 5.9581e-04, 1.2975e-06, 1.0476e-02,\n",
            "        3.6184e-05, 5.0006e-06, 3.3808e-06, 2.1215e-05, 2.4582e-06, 2.8262e-06,\n",
            "        4.0672e-06, 2.5237e-03, 5.4901e-03, 2.7194e-05, 3.9285e-06, 1.9863e-05,\n",
            "        2.7036e-02, 5.9876e-05, 2.6197e-06, 3.5578e-05, 3.2627e-06, 1.1499e-05,\n",
            "        9.9753e-01, 2.9328e-06, 9.1201e-01, 5.8781e-06, 1.8200e-05, 2.6359e-02,\n",
            "        7.3062e-05, 2.1909e-06, 6.5900e-06, 2.8277e-06])\n",
            "Sum of predicted tensor(1.9958)\n",
            "Loss tensor(0.2514)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.0007e-05, 1.2609e-06, 5.0006e-06, 3.6955e-05, 2.1707e-05, 5.9328e-05,\n",
            "        3.5728e-05, 2.9421e-05, 4.4706e-05, 5.0006e-06, 2.7092e-06, 5.5754e-06,\n",
            "        9.2064e-06, 1.2803e-04, 9.7542e-02, 1.2152e-01, 3.6011e-05, 3.3037e-06,\n",
            "        6.0520e-06, 1.1138e-04, 1.4179e-06, 6.4635e-05, 7.0777e-06, 4.4445e-06,\n",
            "        2.4477e-05, 9.9979e-01, 5.0006e-06, 4.1113e-02, 1.0384e-04, 1.1541e-06,\n",
            "        1.9177e-05, 2.6508e-05, 5.7164e-06, 1.0613e-04, 5.0006e-06, 5.0006e-06,\n",
            "        2.8169e-06, 9.7494e-01, 1.4565e-05, 1.2693e-05, 2.8235e-06, 1.3167e-04,\n",
            "        5.5532e-01, 5.0006e-06, 9.5239e-04, 2.6809e-05, 2.9810e-06, 1.2078e-05,\n",
            "        5.0006e-06, 2.4402e-04, 8.9986e-06, 5.0006e-06, 7.7297e-04, 4.0825e-05,\n",
            "        3.3352e-02, 9.8813e-01, 1.4056e-05, 9.4513e-01, 2.1590e-06, 9.9983e-01,\n",
            "        5.3116e-06, 1.2990e-05, 1.6440e-05, 2.9720e-03])\n",
            "Sum of predicted tensor(5.7629)\n",
            "Loss tensor(0.2969)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.3009e-06, 5.0006e-06, 2.9837e-06, 1.0364e-05, 6.7315e-04, 2.2535e-06,\n",
            "        3.3856e-05, 2.4681e-05, 5.8618e-06, 6.0226e-05, 1.6071e-04, 1.6515e-06,\n",
            "        1.8905e-05, 4.5365e-05, 7.1342e-06, 5.0006e-06, 5.0006e-06, 3.2918e-06,\n",
            "        2.8653e-06, 5.5060e-06, 1.2010e-06, 2.2989e-05, 4.4174e-04, 1.4024e-06,\n",
            "        3.1502e-06, 5.5254e-06, 7.1947e-06, 5.9166e-06, 3.7301e-05, 2.8799e-06,\n",
            "        2.9286e-06, 1.1319e-05, 4.8665e-06, 5.4823e-06, 8.0401e-06, 2.0302e-05,\n",
            "        1.7384e-04, 4.8283e-06, 4.5704e-05, 1.8255e-06, 1.5731e-05, 5.6181e-05,\n",
            "        1.7928e-06, 1.7673e-04, 7.7702e-06, 3.3601e-05, 8.4239e-05, 5.0006e-06,\n",
            "        5.0006e-06, 8.2243e-06, 5.6985e-06, 9.0636e-01, 5.2926e-05, 9.8916e-01,\n",
            "        6.4095e-03, 1.5359e-05, 4.5574e-06, 1.3985e-05, 7.6128e-06, 5.5649e-03,\n",
            "        2.2049e-06, 1.1275e-04, 1.1541e-06, 1.1541e-06])\n",
            "Sum of predicted tensor(1.9100)\n",
            "Loss tensor(0.2720)\n",
            "Acc tensor(0.9531)\n",
            "tensor([7.6506e-06, 6.5926e-06, 6.5381e-06, 4.7626e-02, 5.0006e-06, 4.4174e-04,\n",
            "        1.3051e-06, 2.0082e-05, 3.7259e-05, 1.2307e-05, 3.0196e-05, 5.0006e-06,\n",
            "        5.3454e-06, 8.3701e-05, 4.6102e-06, 6.1914e-05, 1.1583e-06, 6.8893e-06,\n",
            "        3.3511e-06, 5.0006e-06, 3.6445e-06, 5.0006e-06, 3.2927e-06, 6.9876e-06,\n",
            "        3.2370e-03, 3.8081e-05, 9.8575e-07, 1.3737e-03, 4.2520e-03, 5.0006e-06,\n",
            "        1.1760e-05, 5.5127e-06, 5.0006e-06, 5.6342e-02, 3.5363e-05, 4.0035e-05,\n",
            "        1.2286e-06, 5.0006e-06, 9.9966e-01, 2.5597e-06, 1.9941e-05, 1.1185e-01,\n",
            "        2.0082e-05, 2.4687e-05, 1.8804e-06, 3.6481e-06, 5.0197e-06, 5.0006e-06,\n",
            "        8.9207e-05, 2.1339e-06, 3.9032e-03, 6.4663e-05, 1.3648e-06, 5.7320e-02,\n",
            "        5.0006e-06, 9.0071e-06, 1.4540e-03, 4.7344e-03, 4.8464e-03, 4.6364e-04,\n",
            "        1.2051e-01, 4.3785e-04, 6.1668e-06, 1.0227e-05])\n",
            "Sum of predicted tensor(1.4192)\n",
            "Loss tensor(0.3886)\n",
            "Acc tensor(0.9219)\n",
            "tensor([7.0276e-06, 1.1896e-03, 3.8232e-06, 5.0006e-06, 5.0006e-06, 3.6129e-05,\n",
            "        5.0006e-06, 2.3876e-06, 1.0608e-01, 1.5776e-03, 3.3341e-02, 1.9882e-05,\n",
            "        4.4164e-05, 1.9256e-06, 1.4485e-04, 4.4399e-03, 1.1181e-05, 3.5587e-05,\n",
            "        9.9972e-01, 2.1168e-06, 1.1635e-01, 9.0712e-02, 5.0006e-06, 6.8490e-05,\n",
            "        1.4695e-04, 4.8654e-06, 1.5102e-04, 1.7964e-05, 7.9000e-04, 2.8358e-06,\n",
            "        1.3813e-05, 1.9044e-05, 2.7096e-05, 8.6945e-06, 9.0618e-06, 7.0787e-05,\n",
            "        1.4909e-04, 1.1497e-04, 1.1074e-04, 1.3110e-05, 2.2491e-05, 1.5701e-03,\n",
            "        1.3965e-06, 4.1594e-06, 1.5080e-05, 1.7513e-04, 1.8471e-05, 1.4024e-06,\n",
            "        1.2763e-06, 5.0006e-06, 5.0006e-06, 6.7916e-06, 4.9660e-05, 2.7658e-03,\n",
            "        4.9360e-06, 3.2463e-06, 5.0006e-06, 9.7450e-06, 7.1137e-06, 3.1371e-06,\n",
            "        3.3807e-05, 6.2847e-03, 6.4486e-05, 4.0525e-04])\n",
            "Sum of predicted tensor(1.3669)\n",
            "Loss tensor(0.2079)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.5602e-05, 1.3838e-04, 1.9706e-06, 5.0833e-06, 4.8653e-06, 5.4397e-05,\n",
            "        1.2884e-01, 9.6945e-01, 8.2693e-02, 9.0645e-01, 5.5108e-05, 1.6989e-06,\n",
            "        5.2334e-05, 7.6493e-06, 3.5492e-06, 5.0225e-06, 5.0008e-05, 2.0253e-05,\n",
            "        1.2966e-05, 6.0070e-05, 7.2943e-05, 1.5234e-06, 2.6405e-04, 2.8114e-04,\n",
            "        2.0288e-05, 6.2679e-06, 2.6739e-06, 1.0829e-04, 3.6270e-05, 1.5056e-06,\n",
            "        2.9740e-06, 7.0714e-03, 8.8718e-06, 8.8381e-05, 2.1777e-03, 1.2022e-06,\n",
            "        1.0024e-05, 1.0649e-04, 3.7494e-04, 7.3519e-06, 5.0006e-06, 8.3715e-05,\n",
            "        2.5237e-03, 3.1162e-05, 5.2058e-05, 4.3501e-06, 5.0006e-06, 9.9979e-01,\n",
            "        1.7767e-05, 9.9979e-01, 2.8748e-06, 1.1176e-05, 1.2586e-06, 5.0006e-06,\n",
            "        1.2160e-03, 2.9354e-06, 9.0152e-05, 9.9993e-01, 5.7906e-05, 9.9979e-01,\n",
            "        7.6825e-01, 3.5580e-06, 1.4540e-03, 7.8083e-01])\n",
            "Sum of predicted tensor(7.6525)\n",
            "Loss tensor(0.3111)\n",
            "Acc tensor(0.9219)\n",
            "tensor([2.3833e-06, 9.9606e-01, 6.3927e-06, 6.9612e-06, 9.9940e-01, 3.5207e-02,\n",
            "        1.5553e-05, 1.8617e-05, 7.4753e-06, 7.5274e-06, 4.4553e-06, 1.3136e-05,\n",
            "        1.1545e-05, 1.8992e-06, 6.4932e-06, 1.9465e-06, 3.7651e-05, 1.9866e-04,\n",
            "        1.2001e-06, 5.4531e-06, 8.6707e-06, 4.4776e-05, 4.4455e-04, 1.9428e-06,\n",
            "        1.5011e-04, 3.9055e-06, 4.5689e-06, 1.5141e-03, 9.9942e-01, 9.9218e-01,\n",
            "        6.7060e-04, 9.4126e-01, 1.5414e-06, 1.5234e-06, 7.9009e-06, 1.6193e-02,\n",
            "        1.2353e-05, 2.9595e-05, 4.9201e-06, 1.9191e-05, 1.0279e-06, 4.4991e-06,\n",
            "        9.9940e-01, 2.3875e-04, 4.6004e-05, 1.2739e-05, 2.8900e-05, 7.4405e-05,\n",
            "        2.1081e-05, 1.1545e-05, 1.7835e-05, 3.8922e-06, 4.3301e-04, 2.6168e-06,\n",
            "        4.7883e-06, 6.2337e-06, 4.9984e-04, 1.0131e-05, 5.0312e-06, 1.2909e-04,\n",
            "        2.0914e-04, 1.3876e-05, 1.6016e-05, 7.3438e-05])\n",
            "Sum of predicted tensor(5.9843)\n",
            "Loss tensor(0.1211)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.1676e-06, 2.3274e-06, 7.6753e-05, 4.7770e-02, 3.4989e-01, 8.6088e-04,\n",
            "        1.8026e-06, 2.5473e-06, 2.9039e-04, 2.6821e-06, 1.6876e-05, 1.0320e-05,\n",
            "        4.4129e-04, 1.0501e-03, 1.3890e-05, 5.2897e-06, 7.6430e-06, 3.0952e-06,\n",
            "        1.1350e-05, 5.6457e-06, 1.9861e-05, 2.7400e-05, 9.9940e-01, 1.5018e-03,\n",
            "        9.8599e-01, 3.4329e-05, 5.5366e-06, 2.7701e-06, 9.9942e-01, 3.1738e-06,\n",
            "        1.7818e-04, 4.0222e-06, 1.1765e-05, 9.9879e-01, 5.6478e-06, 1.9027e-06,\n",
            "        4.6513e-06, 5.1970e-05, 6.0083e-05, 3.4989e-01, 9.7538e-06, 1.6707e-04,\n",
            "        2.9329e-05, 2.6589e-06, 1.7554e-05, 8.1743e-01, 3.9680e-02, 8.2813e-05,\n",
            "        2.4642e-05, 1.3896e-02, 2.1612e-04, 1.6398e-03, 5.7356e-06, 2.1354e-06,\n",
            "        6.2528e-05, 1.0168e-02, 2.0116e-01, 1.8444e-06, 1.1421e-05, 1.2369e-05,\n",
            "        1.2076e-05, 3.4329e-05, 1.8046e-03, 4.9948e-06])\n",
            "Sum of predicted tensor(5.8223)\n",
            "Loss tensor(0.1700)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.7351e-05, 3.0093e-03, 9.9445e-01, 6.0936e-06, 1.0163e-05, 2.9763e-06,\n",
            "        1.0095e-05, 7.5465e-06, 1.1514e-05, 3.7271e-04, 3.3493e-05, 6.0683e-05,\n",
            "        5.3008e-04, 4.7240e-05, 1.1189e-05, 1.1147e-05, 8.4051e-06, 1.9379e-05,\n",
            "        6.5783e-05, 3.3562e-06, 1.5234e-06, 3.4989e-01, 3.5214e-05, 2.5543e-05,\n",
            "        6.4400e-05, 1.5487e-04, 4.2249e-03, 5.0860e-05, 4.2984e-06, 2.5473e-06,\n",
            "        4.4106e-01, 2.1255e-04, 1.1323e-05, 2.7877e-03, 6.4780e-06, 7.3403e-06,\n",
            "        3.8301e-06, 5.2782e-06, 1.4036e-04, 1.9124e-02, 2.6229e-05, 9.9942e-01,\n",
            "        1.2756e-05, 3.1672e-06, 1.5132e-05, 1.7500e-06, 3.8768e-05, 1.4475e-05,\n",
            "        4.9201e-06, 3.0106e-05, 4.0851e-06, 3.0654e-06, 1.1776e-05, 9.9942e-01,\n",
            "        3.1011e-01, 6.3919e-06, 2.6339e-04, 9.9979e-01, 2.3775e-06, 2.5473e-06,\n",
            "        2.3611e-06, 9.9613e-01, 4.2586e-06, 1.1307e-03])\n",
            "Sum of predicted tensor(6.1230)\n",
            "Loss tensor(0.1187)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.1951e-06, 1.8380e-05, 2.4241e-05, 2.3944e-05, 1.9465e-06, 1.6363e-02,\n",
            "        4.0230e-05, 1.0657e-05, 4.1552e-05, 2.3743e-06, 3.5992e-05, 3.2708e-06,\n",
            "        3.9285e-06, 1.1031e-05, 8.6938e-06, 6.3006e-02, 1.3089e-03, 1.0285e-02,\n",
            "        9.0223e-06, 7.7267e-06, 3.0138e-01, 4.7191e-05, 7.5627e-01, 1.8559e-05,\n",
            "        9.9888e-01, 4.4553e-06, 1.2909e-05, 3.1199e-05, 1.6973e-03, 6.9021e-06,\n",
            "        9.2448e-05, 2.6398e-03, 2.8013e-06, 4.4774e-05, 3.3262e-06, 9.1307e-06,\n",
            "        3.7404e-04, 4.1762e-06, 9.0234e-04, 8.4888e-06, 4.8465e-06, 2.2871e-06,\n",
            "        3.0587e-05, 1.2837e-05, 1.4943e-06, 2.5452e-06, 2.5619e-03, 3.6773e-05,\n",
            "        1.2288e-03, 9.9942e-01, 1.6603e-06, 5.4693e-06, 7.1021e-06, 2.5072e-05,\n",
            "        4.0991e-06, 5.1594e-05, 8.1700e-05, 8.3710e-01, 5.0315e-06, 9.2781e-01,\n",
            "        2.4041e-05, 2.2989e-03, 3.4989e-01, 2.0045e-05])\n",
            "Sum of predicted tensor(5.2743)\n",
            "Loss tensor(0.3404)\n",
            "Acc tensor(0.9219)\n",
            "tensor([1.2837e-05, 1.1800e-05, 6.4748e-06, 1.6818e-05, 5.7016e-06, 8.5402e-06,\n",
            "        8.7830e-06, 6.3707e-01, 1.6008e-05, 2.8758e-05, 1.8064e-06, 8.8455e-06,\n",
            "        1.0117e-05, 6.7471e-03, 8.3423e-06, 6.2544e-06, 1.2478e-05, 2.2679e-05,\n",
            "        8.6758e-05, 9.9942e-01, 1.5071e-04, 1.7347e-06, 2.3833e-06, 1.4597e-05,\n",
            "        2.3833e-06, 1.1003e-05, 1.3216e-05, 7.7160e-06, 4.9284e-06, 9.8868e-06,\n",
            "        4.5724e-06, 5.6312e-06, 3.4560e-05, 1.4054e-06, 1.3241e-06, 9.7663e-04,\n",
            "        4.9201e-06, 3.5183e-05, 9.9900e-01, 9.9976e-01, 8.1471e-04, 1.5279e-04,\n",
            "        5.9547e-06, 5.3297e-06, 1.9916e-05, 6.4780e-06, 4.3301e-04, 9.3638e-06,\n",
            "        2.1391e-05, 1.5436e-05, 1.3549e-05, 9.9230e-01, 9.9893e-01, 1.4913e-06,\n",
            "        1.2494e-04, 4.3255e-03, 6.4793e-05, 4.1503e-01, 9.8489e-01, 8.8435e-01,\n",
            "        1.5572e-05, 2.6657e-02, 1.5851e-01, 2.1676e-06])\n",
            "Sum of predicted tensor(8.1103)\n",
            "Loss tensor(0.3838)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.9602e-05, 1.8822e-05, 7.0950e-06, 9.9771e-01, 3.1198e-03, 1.6504e-05,\n",
            "        2.6782e-05, 8.5154e-06, 1.0423e-05, 1.8840e-05, 2.3833e-06, 7.5535e-06,\n",
            "        1.5049e-05, 4.3699e-06, 1.4036e-04, 4.5076e-06, 4.6191e-05, 6.8226e-06,\n",
            "        9.9940e-01, 6.5888e-04, 1.9580e-06, 3.5415e-06, 1.6859e-05, 1.5240e-05,\n",
            "        1.0130e-04, 8.5425e-06, 1.6578e-04, 4.2492e-05, 7.7314e-04, 4.8766e-05,\n",
            "        7.1933e-06, 8.7259e-06, 1.8288e-05, 8.9550e-06, 7.2415e-06, 1.6135e-04,\n",
            "        5.6826e-05, 5.5590e-06, 4.3843e-06, 1.1072e-05, 4.2158e-06, 2.5053e-05,\n",
            "        1.3063e-04, 2.7385e-06, 6.3548e-06, 3.0659e-06, 5.1925e-06, 9.0309e-06,\n",
            "        7.1253e-06, 1.5803e-06, 1.3014e-05, 1.6085e-05, 2.8236e-06, 1.3521e-06,\n",
            "        2.4905e-06, 1.0391e-05, 4.9201e-06, 2.0281e-02, 5.6739e-03, 6.6776e-05,\n",
            "        2.9815e-06, 9.9942e-01, 6.3519e-06, 2.5546e-06])\n",
            "Sum of predicted tensor(3.0285)\n",
            "Loss tensor(0.0955)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.9908e-01, 6.4416e-06, 1.3821e-05, 4.4583e-05, 5.3372e-02, 9.2162e-05,\n",
            "        3.8101e-06, 1.0454e-04, 1.1382e-05, 4.7868e-06, 4.5956e-06, 2.7587e-02,\n",
            "        5.0619e-05, 8.9041e-05, 9.9942e-01, 9.9733e-01, 2.9812e-02, 1.9019e-06,\n",
            "        7.4578e-06, 4.7418e-06, 2.1681e-06, 1.5692e-06, 9.9143e-01, 1.7014e-05,\n",
            "        2.4140e-06, 9.9940e-01, 1.6877e-05, 8.3347e-04, 2.5669e-05, 9.9888e-01,\n",
            "        9.9942e-01, 7.8729e-06, 7.5113e-06, 9.8992e-01, 3.8937e-06, 3.3713e-05,\n",
            "        2.1164e-05, 1.0446e-05, 2.0122e-05, 5.1677e-04, 2.6535e-05, 9.9827e-01,\n",
            "        1.4368e-06, 2.6642e-06, 2.3101e-05, 3.2117e-04, 2.6000e-06, 1.9053e-05,\n",
            "        7.4056e-03, 1.5234e-06, 5.1032e-06, 8.9682e-06, 2.1979e-06, 8.0510e-06,\n",
            "        1.6707e-04, 3.1636e-02, 4.9201e-06, 1.9265e-05, 2.6206e-04, 1.0595e-05,\n",
            "        9.2513e-06, 5.9793e-05, 1.8709e-06, 1.1428e-05])\n",
            "Sum of predicted tensor(9.1259)\n",
            "Loss tensor(0.3955)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.6103e-04, 2.7871e-05, 7.6373e-06, 3.0007e-06, 5.9242e-06, 7.5604e-05,\n",
            "        9.9989e-01, 6.0866e-06, 2.0507e-06, 6.8653e-06, 4.9950e-02, 9.9986e-01,\n",
            "        9.1748e-06, 8.7490e-06, 5.6394e-06, 4.9908e-06, 9.9520e-01, 3.5928e-06,\n",
            "        5.4158e-05, 9.0280e-06, 9.9944e-01, 4.6896e-06, 1.4533e-06, 9.6162e-06,\n",
            "        1.2985e-05, 6.5939e-06, 8.7786e-06, 3.6253e-04, 1.6014e-04, 2.4438e-03,\n",
            "        4.3582e-06, 7.9259e-06, 6.7820e-06, 2.4971e-04, 6.4793e-03, 9.3175e-06,\n",
            "        1.3380e-04, 7.0857e-01, 9.3469e-05, 9.9517e-01, 8.8704e-05, 4.1285e-06,\n",
            "        2.2094e-06, 4.8674e-06, 9.4943e-05, 2.8726e-06, 6.2442e-06, 2.0820e-05,\n",
            "        5.6098e-06, 2.7520e-06, 4.0917e-06, 3.4629e-06, 1.1352e-05, 2.0660e-05,\n",
            "        8.3711e-06, 4.1339e-05, 2.9578e-06, 7.9402e-04, 4.3936e-06, 2.3690e-06,\n",
            "        5.3069e-05, 4.2212e-05, 1.5358e-03, 1.9797e-01])\n",
            "Sum of predicted tensor(5.9592)\n",
            "Loss tensor(0.1814)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.4125e-06, 3.5309e-05, 7.5067e-04, 6.0152e-05, 1.8430e-06, 2.9448e-06,\n",
            "        9.9951e-01, 9.9989e-01, 1.6818e-06, 1.2922e-05, 7.3501e-06, 6.4144e-06,\n",
            "        3.5050e-02, 2.0915e-05, 5.4910e-05, 4.0887e-05, 3.7892e-06, 3.6781e-05,\n",
            "        3.0404e-04, 2.0046e-05, 1.5271e-05, 7.4644e-01, 1.8609e-05, 5.9867e-06,\n",
            "        2.9578e-06, 4.6154e-06, 1.0714e-05, 4.6115e-06, 4.0457e-06, 1.1707e-01,\n",
            "        6.7802e-05, 3.9764e-06, 4.8948e-06, 5.4171e-06, 9.9951e-01, 1.6969e-06,\n",
            "        9.9027e-01, 1.2907e-06, 1.5381e-05, 7.1415e-06, 4.3936e-06, 1.5845e-05,\n",
            "        5.3516e-06, 9.9559e-01, 2.8156e-03, 1.0948e-05, 2.2250e-05, 2.5647e-06,\n",
            "        4.1030e-06, 9.9298e-01, 1.2284e-05, 9.9881e-01, 6.9636e-06, 2.6320e-06,\n",
            "        1.3351e-05, 2.0088e-02, 2.3954e-04, 8.4933e-06, 3.9484e-05, 2.4724e-05,\n",
            "        2.5762e-05, 8.7962e-04, 1.1758e-05, 4.8659e-05])\n",
            "Sum of predicted tensor(7.9009)\n",
            "Loss tensor(0.2673)\n",
            "Acc tensor(0.9531)\n",
            "tensor([6.8752e-06, 3.5522e-06, 1.0974e-05, 1.3535e-05, 4.6738e-06, 5.4879e-05,\n",
            "        2.8302e-06, 4.3372e-01, 1.4857e-05, 1.2193e-05, 2.6402e-06, 9.9909e-01,\n",
            "        4.0102e-04, 2.9578e-06, 1.4993e-04, 1.9810e-05, 1.3864e-05, 9.9859e-01,\n",
            "        1.2911e-05, 3.2739e-05, 3.5626e-06, 5.4350e-06, 1.8792e-02, 8.5719e-06,\n",
            "        1.2588e-05, 1.0064e-05, 7.8421e-06, 2.6576e-06, 1.8018e-06, 1.2117e-06,\n",
            "        9.9778e-01, 7.0468e-06, 9.8728e-01, 1.1278e-05, 4.0846e-01, 2.5542e-05,\n",
            "        8.3151e-06, 4.7894e-06, 6.5047e-02, 1.2335e-05, 9.9957e-01, 6.0979e-06,\n",
            "        1.4646e-04, 1.0451e-04, 2.7922e-06, 7.4521e-04, 1.5172e-05, 8.6553e-06,\n",
            "        6.1909e-06, 4.6088e-06, 1.8064e-05, 1.1380e-05, 1.3535e-05, 2.5567e-04,\n",
            "        2.0083e-06, 6.2039e-06, 6.0535e-05, 4.9933e-05, 2.2121e-05, 2.3780e-06,\n",
            "        1.1519e-05, 9.9994e-01, 1.4521e-05, 9.4842e-03])\n",
            "Sum of predicted tensor(6.9201)\n",
            "Loss tensor(0.2618)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9976e-01, 5.7130e-06, 3.3478e-06, 9.3290e-05, 1.8192e-05, 5.8463e-06,\n",
            "        9.9911e-01, 6.4947e-05, 4.1432e-06, 2.6131e-06, 1.0821e-05, 5.0811e-06,\n",
            "        1.6804e-05, 9.9648e-01, 2.1174e-06, 1.6638e-05, 2.2872e-05, 8.8450e-06,\n",
            "        1.0287e-03, 1.8184e-06, 1.3946e-06, 2.9243e-05, 1.5057e-06, 1.0273e-04,\n",
            "        3.4629e-06, 6.2197e-06, 2.1480e-04, 1.3582e-04, 1.5544e-04, 8.6941e-03,\n",
            "        9.9698e-04, 7.5748e-06, 2.1890e-06, 1.3963e-05, 9.9938e-01, 4.2897e-06,\n",
            "        1.2507e-05, 2.9578e-06, 1.9028e-06, 5.0702e-06, 9.9861e-01, 1.1752e-05,\n",
            "        9.9859e-01, 1.5186e-05, 1.0366e-04, 3.2381e-02, 1.4701e-05, 4.8326e-06,\n",
            "        2.1764e-06, 1.7836e-05, 7.0506e-02, 2.2815e-06, 2.9578e-06, 6.8486e-04,\n",
            "        1.4263e-06, 3.6716e-06, 9.4209e-06, 4.1731e-06, 1.5567e-02, 6.2298e-04,\n",
            "        1.8859e-04, 3.4544e-06, 7.0716e-06, 2.9578e-06])\n",
            "Sum of predicted tensor(6.1238)\n",
            "Loss tensor(0.6314)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.8994e-06, 4.5643e-03, 1.1268e-05, 9.5814e-06, 1.9459e-06, 5.9864e-05,\n",
            "        2.7534e-06, 8.9390e-04, 3.2887e-06, 8.1963e-01, 5.6394e-06, 2.5035e-06,\n",
            "        2.3695e-03, 1.4448e-06, 8.3832e-06, 6.7283e-04, 2.2050e-05, 1.3734e-06,\n",
            "        3.2911e-05, 2.9390e-05, 1.4792e-04, 1.2522e-05, 7.9346e-06, 1.0576e-04,\n",
            "        7.5948e-06, 7.1632e-06, 4.1906e-01, 6.9160e-06, 2.3750e-06, 4.4125e-06,\n",
            "        7.6524e-06, 3.9508e-06, 5.9304e-06, 5.0724e-02, 7.5984e-03, 2.7792e-05,\n",
            "        1.6440e-05, 6.6029e-06, 1.7690e-05, 4.3485e-06, 4.6291e-06, 1.6005e-05,\n",
            "        9.1300e-02, 1.0085e-04, 1.1946e-06, 1.3168e-05, 6.4713e-06, 9.1320e-04,\n",
            "        1.1051e-06, 1.1228e-03, 9.8362e-01, 2.5148e-05, 2.6126e-04, 3.7215e-01,\n",
            "        5.4979e-02, 2.7136e-05, 7.4345e-06, 5.3970e-06, 5.6224e-05, 1.4793e-05,\n",
            "        1.1523e-05, 9.4256e-04, 1.0130e-05, 2.1541e-05])\n",
            "Sum of predicted tensor(2.8117)\n",
            "Loss tensor(0.0948)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.3070e-06, 8.0858e-06, 1.9307e-05, 4.6604e-05, 1.1753e-04, 1.0500e-05,\n",
            "        3.4578e-06, 3.3134e-06, 6.8653e-06, 2.7859e-06, 7.5916e-06, 7.0888e-06,\n",
            "        1.2256e-05, 3.4931e-05, 8.5396e-04, 3.0109e-06, 2.2232e-03, 9.3669e-06,\n",
            "        1.6810e-06, 1.0592e-05, 3.3872e-06, 4.0959e-05, 7.7875e-06, 1.2995e-05,\n",
            "        1.0811e-05, 4.7468e-05, 5.1903e-05, 1.0371e-05, 6.5874e-05, 6.8687e-04,\n",
            "        3.8731e-05, 1.1534e-05, 2.4273e-06, 2.3750e-06, 2.8339e-05, 4.4039e-06,\n",
            "        5.8688e-06, 3.7845e-03, 1.2783e-05, 6.2185e-06, 1.8974e-02, 4.2555e-06,\n",
            "        6.5266e-06, 5.4021e-06, 3.6470e-05, 2.3555e-02, 7.6982e-06, 9.1157e-06,\n",
            "        3.5580e-06, 2.3073e-04, 2.0141e-06, 1.4376e-04, 1.9694e-05, 4.3851e-06,\n",
            "        3.9151e-06, 7.6931e-04, 7.3265e-06, 9.0598e-06, 4.5405e-05, 8.1998e-06,\n",
            "        3.5903e-01, 1.1186e-05, 1.1396e-02, 2.9578e-06])\n",
            "Sum of predicted tensor(0.4225)\n",
            "Loss tensor(0.0696)\n",
            "Acc tensor(0.9844)\n",
            "tensor([6.8389e-06, 1.2751e-05, 3.4717e-05, 1.6450e-05, 8.0457e-06, 3.8761e-03,\n",
            "        1.7478e-03, 9.9484e-01, 5.8638e-06, 2.9578e-06, 9.9688e-02, 9.9597e-01,\n",
            "        2.5165e-06, 1.0251e-05, 3.3457e-06, 2.3469e-03, 1.0148e-05, 5.4027e-06,\n",
            "        1.3535e-05, 6.9364e-05, 1.6066e-05, 7.2879e-05, 1.1059e-04, 1.2445e-02,\n",
            "        7.7807e-06, 7.0366e-05, 3.2138e-06, 9.9956e-01, 5.7151e-04, 4.0859e-05,\n",
            "        1.0056e-05, 5.1689e-06, 1.5544e-04, 3.6201e-06, 2.2979e-04, 2.5647e-06,\n",
            "        4.9623e-05, 2.7351e-06, 2.7245e-06, 1.5593e-04, 5.2698e-06, 2.5986e-06,\n",
            "        7.8137e-06, 5.8629e-01, 1.9010e-05, 4.3063e-06, 7.1786e-06, 2.1195e-04,\n",
            "        1.1836e-03, 9.9580e-01, 1.6034e-05, 1.0020e-05, 6.0131e-05, 8.5643e-06,\n",
            "        1.6806e-05, 7.8199e-06, 8.5417e-05, 1.4103e-05, 1.2688e-01, 6.8434e-06,\n",
            "        2.0891e-05, 1.0569e-04, 1.7210e-05, 6.6657e-06])\n",
            "Sum of predicted tensor(4.8230)\n",
            "Loss tensor(0.1903)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.0049e-05, 6.4984e-02, 3.4720e-05, 1.2404e-04, 2.5236e-05, 1.7129e-05,\n",
            "        9.1744e-01, 7.7177e-06, 7.1712e-06, 1.2467e-06, 8.7921e-01, 2.6560e-06,\n",
            "        4.3327e-05, 9.8195e-01, 1.4993e-06, 9.2438e-01, 3.3047e-02, 1.4550e-04,\n",
            "        1.9321e-05, 5.9742e-06, 1.4144e-06, 6.7286e-06, 1.2801e-05, 3.4624e-05,\n",
            "        1.7023e-05, 3.4566e-05, 3.3595e-06, 3.9099e-06, 1.6158e-03, 1.2922e-05,\n",
            "        8.0140e-06, 2.6616e-05, 8.1768e-05, 2.4451e-06, 9.9859e-01, 9.6855e-06,\n",
            "        9.9936e-01, 4.3414e-05, 7.5495e-06, 6.1792e-06, 3.2417e-06, 7.4527e-06,\n",
            "        5.5803e-04, 6.4950e-06, 2.2547e-05, 1.6056e-05, 9.9729e-01, 9.9859e-01,\n",
            "        1.5248e-05, 3.7820e-06, 1.3669e-05, 2.8789e-04, 2.5647e-06, 2.5440e-04,\n",
            "        8.8492e-06, 2.6726e-05, 1.5532e-06, 1.6641e-06, 3.0460e-06, 6.7367e-06,\n",
            "        1.4857e-06, 1.3444e-06, 4.2985e-06, 2.9578e-06])\n",
            "Sum of predicted tensor(7.7985)\n",
            "Loss tensor(0.1458)\n",
            "Acc tensor(0.9531)\n",
            "tensor([5.1358e-06, 5.5098e-02, 9.9980e-01, 4.0029e-06, 3.9137e-06, 2.0645e-02,\n",
            "        6.2350e-06, 1.1504e-05, 8.3007e-01, 1.5202e-03, 2.1565e-06, 2.8354e-04,\n",
            "        5.0000e-06, 9.5567e-06, 1.8564e-06, 5.8160e-06, 9.0416e-06, 3.4275e-01,\n",
            "        5.2222e-06, 1.9254e-06, 4.2288e-06, 4.8485e-06, 2.3804e-06, 1.7683e-04,\n",
            "        1.9793e-05, 1.1081e-04, 1.8476e-05, 4.8873e-05, 9.7829e-03, 1.3767e-06,\n",
            "        5.9907e-06, 3.1122e-05, 8.6639e-05, 2.8908e-06, 3.8089e-02, 1.6085e-06,\n",
            "        5.0266e-06, 4.0712e-06, 7.2713e-06, 4.2960e-06, 9.9964e-01, 3.3706e-05,\n",
            "        1.9403e-05, 4.0655e-04, 4.4971e-05, 1.2401e-05, 6.1196e-01, 1.4356e-06,\n",
            "        1.0019e-05, 1.0951e-05, 3.6430e-06, 1.0527e-05, 4.6078e-06, 2.9967e-05,\n",
            "        2.5635e-04, 6.7072e-06, 2.4762e-05, 2.6449e-05, 9.9986e-01, 2.7768e-05,\n",
            "        3.2087e-06, 1.2047e-06, 3.9385e-06, 3.3398e-06])\n",
            "Sum of predicted tensor(4.9111)\n",
            "Loss tensor(0.0955)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.9474e-05, 1.0865e-05, 3.8152e-02, 1.1296e-04, 1.9092e-06, 1.2545e-02,\n",
            "        1.9797e-04, 1.0837e-05, 2.3518e-06, 1.0358e-03, 3.1867e-06, 6.6829e-06,\n",
            "        2.4988e-03, 6.8668e-06, 8.0132e-06, 8.1996e-06, 1.0683e-06, 4.7955e-04,\n",
            "        5.3944e-06, 1.7230e-05, 3.9768e-05, 3.5489e-06, 3.3462e-06, 3.8756e-06,\n",
            "        1.1155e-05, 1.4504e-03, 5.0007e-06, 1.5230e-05, 4.2923e-06, 1.8114e-06,\n",
            "        2.5164e-06, 3.7656e-06, 5.1498e-06, 1.2404e-04, 1.9994e-04, 1.0984e-05,\n",
            "        9.4986e-06, 5.2903e-05, 5.7886e-06, 1.1673e-05, 3.2829e-06, 1.0734e-05,\n",
            "        1.9017e-06, 1.3067e-06, 1.0391e-05, 1.7806e-01, 3.0707e-06, 2.4692e-06,\n",
            "        1.6473e-02, 3.9782e-06, 2.8947e-06, 1.6140e-04, 2.9484e-05, 1.6610e-01,\n",
            "        2.4878e-06, 4.7516e-06, 6.5801e-06, 6.9206e-06, 6.5655e-06, 1.0542e-05,\n",
            "        1.3010e-05, 4.5171e-05, 9.9906e-01, 9.9785e-01])\n",
            "Sum of predicted tensor(2.4150)\n",
            "Loss tensor(0.2137)\n",
            "Acc tensor(0.9375)\n",
            "tensor([4.8565e-06, 4.5503e-06, 4.0712e-06, 5.4706e-06, 1.6262e-03, 4.7148e-06,\n",
            "        5.7497e-06, 5.7144e-05, 9.9939e-01, 5.4672e-04, 1.1351e-05, 4.7108e-06,\n",
            "        6.1366e-06, 2.4229e-06, 6.8628e-06, 1.4665e-06, 6.5715e-05, 1.1270e-05,\n",
            "        1.1501e-05, 6.4376e-06, 5.0600e-02, 2.3366e-06, 3.3724e-06, 6.1702e-05,\n",
            "        9.7731e-06, 4.7597e-06, 1.6231e-05, 3.2098e-06, 6.4769e-05, 2.0611e-05,\n",
            "        9.8339e-01, 4.7347e-05, 1.3214e-05, 1.4174e-05, 1.3708e-05, 3.4125e-06,\n",
            "        1.1433e-05, 3.3257e-06, 2.6395e-02, 1.0913e-02, 2.0178e-05, 5.6596e-03,\n",
            "        6.8282e-06, 3.7383e-03, 3.0019e-05, 5.3981e-06, 4.0807e-06, 1.1573e-02,\n",
            "        1.0324e-06, 1.9132e-04, 1.0366e-05, 2.1565e-03, 4.1638e-06, 1.4739e-06,\n",
            "        7.7988e-06, 6.9800e-01, 3.0378e-06, 7.5710e-06, 2.2985e-05, 4.2708e-02,\n",
            "        2.9146e-06, 6.9561e-01, 9.9796e-01, 2.8237e-05])\n",
            "Sum of predicted tensor(4.5311)\n",
            "Loss tensor(0.3824)\n",
            "Acc tensor(0.9219)\n",
            "tensor([9.9073e-01, 6.5782e-04, 1.7879e-06, 7.0481e-06, 1.0835e-05, 3.9787e-06,\n",
            "        1.2196e-04, 1.3753e-05, 9.9895e-01, 1.4770e-05, 2.2195e-05, 1.3373e-03,\n",
            "        5.1781e-06, 7.9595e-06, 1.8487e-06, 7.8753e-05, 9.4683e-01, 6.3213e-06,\n",
            "        1.7069e-05, 4.7597e-06, 9.4177e-03, 2.4343e-03, 6.7814e-06, 8.2140e-02,\n",
            "        4.5685e-05, 6.3456e-05, 1.4420e-05, 1.7913e-05, 4.1447e-05, 3.7785e-05,\n",
            "        4.6522e-06, 1.2791e-05, 6.9918e-06, 1.0691e-05, 4.0856e-04, 9.9991e-01,\n",
            "        2.2597e-06, 2.4174e-06, 3.4512e-04, 3.2006e-02, 9.5584e-01, 1.2847e-05,\n",
            "        6.5605e-06, 1.9437e-06, 6.6115e-04, 9.6607e-01, 8.9951e-06, 6.8240e-06,\n",
            "        7.3579e-06, 9.6598e-06, 2.4125e-02, 1.6930e-02, 3.0554e-02, 6.9009e-06,\n",
            "        1.8649e-06, 7.9658e-04, 5.8522e-04, 1.8913e-05, 4.8201e-06, 9.6938e-05,\n",
            "        6.1799e-06, 5.4231e-05, 3.4787e-06, 6.6789e-06])\n",
            "Sum of predicted tensor(6.0616)\n",
            "Loss tensor(0.1643)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.0052e-02, 3.6425e-06, 9.3640e-06, 2.3000e-05, 1.5853e-05, 3.7096e-06,\n",
            "        1.3651e-06, 4.0063e-04, 2.7570e-06, 1.6440e-05, 5.0760e-06, 2.1320e-04,\n",
            "        3.5600e-04, 1.0635e-04, 9.5864e-06, 6.8049e-06, 6.0563e-06, 9.9958e-01,\n",
            "        1.4564e-06, 9.9931e-01, 3.6211e-06, 2.6524e-06, 2.8989e-04, 2.6007e-05,\n",
            "        1.8784e-05, 1.5428e-05, 8.0676e-06, 8.0897e-01, 2.6885e-04, 5.9944e-06,\n",
            "        1.7828e-05, 5.5821e-06, 2.3270e-06, 5.9515e-05, 1.3960e-06, 7.5913e-06,\n",
            "        9.9967e-01, 2.0559e-06, 8.5813e-06, 7.5478e-05, 1.2035e-05, 4.8697e-06,\n",
            "        9.3066e-06, 1.9616e-05, 4.7148e-06, 9.9958e-01, 2.7692e-06, 4.2826e-06,\n",
            "        9.8294e-01, 5.3360e-01, 1.2938e-05, 3.3000e-06, 6.7072e-06, 6.9059e-05,\n",
            "        2.8369e-06, 4.5634e-06, 3.4873e-06, 2.1560e-04, 1.4536e-06, 4.6206e-04,\n",
            "        1.8811e-04, 1.0199e-02, 1.3951e-05, 2.0475e-03])\n",
            "Sum of predicted tensor(6.3490)\n",
            "Loss tensor(0.2197)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.9548e-05, 1.7913e-05, 9.9978e-01, 2.5040e-02, 1.8164e-05, 5.6852e-06,\n",
            "        9.9935e-01, 2.1727e-05, 1.8948e-06, 4.0326e-06, 4.0544e-06, 1.4976e-06,\n",
            "        4.9418e-06, 1.1547e-06, 2.1169e-06, 2.2509e-05, 8.4244e-06, 1.6488e-05,\n",
            "        1.9148e-06, 4.0317e-06, 9.9111e-06, 1.9283e-06, 3.8829e-06, 5.3149e-03,\n",
            "        4.5564e-05, 3.1980e-06, 8.9736e-06, 4.1950e-06, 9.5056e-05, 1.5407e-06,\n",
            "        2.5912e-06, 9.9958e-01, 1.3635e-05, 3.3453e-06, 1.7355e-06, 1.1667e-05,\n",
            "        2.1457e-05, 9.7511e-01, 1.3238e-04, 2.8147e-05, 7.0397e-01, 1.5439e-04,\n",
            "        2.9186e-06, 9.9969e-01, 3.5361e-05, 2.9484e-05, 3.3931e-06, 3.2854e-05,\n",
            "        2.5581e-05, 6.3800e-06, 7.4268e-06, 9.9866e-01, 7.3807e-05, 8.6442e-06,\n",
            "        5.9000e-06, 2.2195e-05, 1.8898e-04, 6.3866e-06, 9.1025e-05, 9.9945e-01,\n",
            "        1.2293e-05, 2.9134e-06, 9.9894e-01, 9.9899e-01])\n",
            "Sum of predicted tensor(9.7051)\n",
            "Loss tensor(0.1233)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.9409e-01, 6.1077e-06, 2.2332e-06, 1.9368e-05, 1.5298e-05, 1.8098e-04,\n",
            "        4.3984e-06, 1.9311e-06, 1.2088e-05, 3.8931e-06, 2.2195e-05, 9.9871e-01,\n",
            "        7.7943e-06, 1.3484e-05, 6.8318e-06, 5.9832e-06, 3.2141e-05, 5.3695e-05,\n",
            "        5.8817e-06, 3.3914e-06, 5.2114e-06, 9.9977e-01, 7.0912e-06, 9.9623e-01,\n",
            "        3.3706e-06, 5.0650e-06, 2.3268e-02, 1.6673e-06, 3.1704e-06, 2.2763e-02,\n",
            "        1.3780e-05, 9.9325e-01, 5.5399e-06, 2.5385e-06, 1.2271e-05, 1.7867e-05,\n",
            "        8.4066e-05, 1.6077e-06, 1.7913e-05, 3.2874e-06, 9.9680e-01, 2.5345e-06,\n",
            "        6.6493e-05, 2.9701e-05, 1.9594e-06, 9.9325e-01, 6.4905e-06, 8.9192e-06,\n",
            "        8.9086e-06, 2.9419e-05, 1.8165e-06, 2.5782e-05, 5.5607e-06, 8.1575e-01,\n",
            "        1.9022e-05, 3.2514e-02, 5.6457e-06, 6.1298e-05, 2.5783e-05, 4.5032e-05,\n",
            "        6.8240e-06, 7.6991e-01, 1.1930e-06, 4.3259e-05])\n",
            "Sum of predicted tensor(8.6373)\n",
            "Loss tensor(0.2004)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.4104e-05, 4.1373e-06, 2.8017e-05, 7.1137e-06, 6.6047e-06, 5.6613e-06,\n",
            "        3.6167e-05, 3.5900e-04, 7.1504e-06, 1.2505e-06, 2.4777e-04, 6.9159e-06,\n",
            "        3.9850e-05, 9.9540e-07, 2.2489e-06, 4.5375e-06, 1.1075e-05, 1.1360e-05,\n",
            "        1.6370e-05, 2.1247e-06, 3.1636e-06, 3.1249e-05, 8.2451e-06, 2.3854e-05,\n",
            "        3.0299e-06, 2.0483e-06, 9.9971e-01, 6.4973e-03, 1.2586e-05, 4.4669e-06,\n",
            "        2.7883e-06, 8.3353e-06, 6.7993e-06, 1.7892e-06, 8.1841e-06, 4.7706e-03,\n",
            "        1.1031e-05, 3.4023e-04, 1.0354e-06, 1.2980e-05, 2.2091e-06, 5.9286e-06,\n",
            "        4.5634e-06, 9.3923e-05, 6.2306e-05, 2.7736e-06, 1.7456e-05, 8.1648e-05,\n",
            "        1.7307e-05, 1.2040e-05, 1.5162e-03, 3.0836e-06, 2.0428e-06, 2.3433e-05,\n",
            "        1.1245e-05, 1.9830e-03, 1.2074e-04, 6.9249e-06, 2.8459e-06, 2.9484e-05,\n",
            "        3.9157e-01, 3.3887e-06, 1.2643e-05, 3.2820e-06])\n",
            "Sum of predicted tensor(1.4079)\n",
            "Loss tensor(0.0866)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.1976e-06, 1.3436e-06, 1.2296e-05, 4.5535e-05, 5.2223e-06, 1.5572e-06,\n",
            "        5.0190e-06, 1.8989e-06, 1.0079e-05, 8.4059e-06, 8.4674e-01, 1.0985e-05,\n",
            "        3.9694e-06, 6.1449e-06, 8.3668e-06, 1.7117e-06, 1.5296e-05, 3.1341e-06,\n",
            "        2.1264e-06, 7.4956e-01, 9.9900e-01, 7.4388e-05, 1.2197e-05, 1.0414e-05,\n",
            "        1.4988e-06, 5.7916e-06, 4.1386e-03, 6.4635e-06, 4.0485e-06, 1.5278e-05,\n",
            "        3.1795e-04, 3.3814e-06, 6.9021e-03, 5.8438e-06, 2.9208e-06, 6.5752e-06,\n",
            "        2.5607e-06, 1.6882e-06, 9.9996e-01, 5.0533e-06, 7.8430e-06, 4.4508e-06,\n",
            "        1.2280e-04, 3.7417e-05, 7.1945e-06, 1.3642e-03, 1.7864e-06, 1.4883e-06,\n",
            "        4.0299e-06, 5.7424e-05, 1.6801e-05, 3.0336e-06, 4.4678e-06, 9.9906e-01,\n",
            "        5.2418e-05, 1.8776e-05, 4.9602e-06, 3.3220e-06, 7.5786e-06, 1.6524e-05,\n",
            "        2.0347e-05, 1.9481e-06, 4.9170e-03, 7.0620e-01])\n",
            "Sum of predicted tensor(5.3188)\n",
            "Loss tensor(0.1294)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.0345e-04, 3.3691e-06, 3.8600e-06, 1.7647e-04, 2.0186e-05, 9.9903e-01,\n",
            "        8.8718e-06, 3.9831e-06, 1.1633e-03, 8.7537e-06, 3.6767e-05, 7.0336e-06,\n",
            "        1.2242e-06, 4.5399e-06, 1.2843e-05, 5.4561e-06, 6.0571e-06, 8.0412e-02,\n",
            "        1.1272e-05, 5.0463e-06, 7.0104e-05, 3.5109e-06, 1.3442e-05, 3.6722e-04,\n",
            "        1.5426e-06, 2.0441e-06, 1.0373e-05, 2.1884e-05, 9.5681e-07, 4.4531e-06,\n",
            "        1.9814e-05, 6.2181e-04, 4.1936e-06, 7.9039e-04, 3.2032e-06, 2.0579e-05,\n",
            "        4.5554e-06, 2.6505e-06, 1.9696e-06, 3.0614e-06, 8.4823e-06, 2.1134e-06,\n",
            "        1.6748e-05, 2.1585e-06, 3.8146e-06, 3.4844e-02, 4.4028e-05, 3.3823e-04,\n",
            "        1.2663e-05, 2.3885e-05, 9.0016e-06, 1.0723e-05, 2.8830e-05, 2.5443e-05,\n",
            "        1.4299e-05, 4.2854e-06, 1.0579e-05, 1.1981e-05, 8.2503e-06, 1.5810e-06,\n",
            "        1.8708e-06, 2.8195e-06, 9.5259e-05, 2.4898e-05])\n",
            "Sum of predicted tensor(1.1187)\n",
            "Loss tensor(0.1779)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.4719e-06, 3.0564e-06, 4.0355e-06, 7.3907e-06, 2.3006e-06, 2.6421e-06,\n",
            "        6.0327e-03, 4.0921e-03, 2.9783e-06, 6.4639e-06, 5.1384e-05, 4.3150e-06,\n",
            "        4.5740e-05, 2.4040e-06, 2.1615e-01, 3.5109e-04, 1.1213e-05, 1.4218e-05,\n",
            "        2.4201e-06, 3.8503e-04, 2.5506e-06, 4.7171e-06, 5.9691e-06, 9.9919e-01,\n",
            "        9.9936e-01, 2.3510e-05, 7.0871e-06, 6.3574e-06, 1.2306e-03, 8.2503e-06,\n",
            "        2.1601e-05, 1.1676e-05, 6.0306e-06, 8.1897e-06, 1.2894e-06, 2.3993e-05,\n",
            "        1.6650e-05, 1.0999e-05, 1.2207e-05, 9.2037e-06, 6.2379e-06, 4.0029e-02,\n",
            "        1.7447e-06, 9.9911e-01, 1.4539e-04, 6.5117e-06, 8.2657e-05, 6.1760e-06,\n",
            "        1.1818e-05, 2.8446e-06, 7.8757e-06, 1.2477e-06, 1.5088e-04, 7.9985e-04,\n",
            "        7.8802e-05, 4.3239e-06, 1.4009e-04, 2.5315e-06, 7.4159e-06, 1.3876e-04,\n",
            "        9.6513e-01, 4.9831e-05, 2.3880e-05, 1.8622e-04])\n",
            "Sum of predicted tensor(4.2333)\n",
            "Loss tensor(0.1068)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.4818e-06, 1.2289e-05, 6.2611e-06, 4.4272e-06, 4.2186e-05, 3.6061e-06,\n",
            "        9.9917e-01, 7.1606e-06, 6.6356e-06, 2.4163e-06, 6.2746e-05, 5.9495e-06,\n",
            "        2.9716e-06, 1.3139e-05, 6.6293e-01, 1.6830e-06, 1.8008e-06, 1.9352e-06,\n",
            "        5.6611e-03, 1.3399e-04, 7.1656e-02, 1.3894e-06, 5.9042e-06, 6.2716e-06,\n",
            "        5.1607e-05, 4.8624e-04, 1.0592e-03, 1.5117e-06, 2.5831e-05, 4.6724e-06,\n",
            "        8.3786e-06, 1.7719e-06, 5.5172e-06, 2.4838e-05, 2.2708e-06, 2.7403e-05,\n",
            "        1.1473e-05, 1.0026e-05, 2.9410e-06, 4.8189e-05, 4.3357e-06, 4.5497e-06,\n",
            "        8.5832e-06, 1.4020e-06, 9.1481e-01, 2.4589e-06, 1.3448e-03, 3.0889e-04,\n",
            "        4.2649e-06, 5.3274e-05, 1.7506e-06, 4.3891e-06, 4.8633e-06, 8.6943e-06,\n",
            "        2.0347e-05, 4.4117e-06, 1.6715e-05, 9.4654e-02, 1.7462e-04, 7.7827e-06,\n",
            "        2.4502e-05, 5.7450e-06, 4.1459e-06, 1.2216e-05])\n",
            "Sum of predicted tensor(2.7530)\n",
            "Loss tensor(0.1758)\n",
            "Acc tensor(0.9844)\n",
            "tensor([1.7076e-04, 4.2332e-06, 2.8838e-06, 6.3639e-06, 1.0825e-05, 1.3487e-06,\n",
            "        3.4990e-06, 1.4942e-05, 2.3747e-04, 1.9672e-05, 5.7741e-06, 1.4040e-06,\n",
            "        1.0810e-05, 1.1882e-05, 3.2703e-06, 3.6659e-06, 7.8044e-05, 3.0597e-05,\n",
            "        1.6972e-04, 9.8410e-01, 1.2541e-04, 9.9891e-01, 9.9941e-01, 2.7901e-06,\n",
            "        1.0005e-02, 1.2122e-05, 2.2109e-06, 1.5771e-05, 4.8754e-02, 5.5556e-06,\n",
            "        9.9951e-01, 4.1474e-06, 7.7144e-06, 3.6906e-04, 4.9254e-06, 1.4496e-05,\n",
            "        3.7309e-06, 6.1887e-06, 9.2266e-04, 1.7088e-05, 3.2567e-04, 9.9650e-01,\n",
            "        4.9117e-06, 3.7066e-06, 1.6859e-05, 2.7639e-05, 2.6540e-05, 1.7864e-06,\n",
            "        8.5688e-06, 9.9897e-01, 1.3425e-03, 5.5462e-06, 3.5330e-06, 1.1321e-05,\n",
            "        5.0887e-05, 1.0415e-01, 5.7868e-03, 8.6918e-03, 1.2386e-05, 2.5858e-05,\n",
            "        4.1977e-06, 8.8422e-06, 2.5546e-04, 9.9908e-01])\n",
            "Sum of predicted tensor(7.1583)\n",
            "Loss tensor(0.5017)\n",
            "Acc tensor(0.9062)\n",
            "tensor([4.0457e-06, 6.0624e-06, 1.6147e-04, 3.7309e-06, 3.8428e-06, 6.0917e-05,\n",
            "        3.4569e-06, 6.9672e-06, 5.6319e-06, 8.8695e-06, 4.2301e-06, 1.9207e-06,\n",
            "        1.6858e-04, 7.4477e-06, 6.7429e-01, 5.5900e-06, 9.3408e-04, 4.6724e-06,\n",
            "        1.8400e-04, 1.7071e-05, 6.7100e-06, 1.1116e-05, 9.6722e-06, 4.3604e-06,\n",
            "        3.0479e-06, 2.7701e-04, 1.2668e-05, 1.9944e-04, 1.9654e-04, 1.3406e-04,\n",
            "        9.1312e-01, 1.0411e-05, 1.1668e-02, 2.6135e-06, 9.8109e-06, 2.9218e-06,\n",
            "        7.4475e-06, 2.7505e-06, 7.4130e-02, 1.2578e-05, 8.1349e-06, 6.0306e-06,\n",
            "        2.9660e-06, 4.1847e-06, 3.7233e-05, 3.2555e-06, 1.8530e-05, 9.8483e-01,\n",
            "        2.6953e-06, 3.9409e-05, 4.7015e-06, 3.0191e-05, 2.9715e-06, 9.4823e-04,\n",
            "        1.1814e-05, 3.6848e-06, 3.4251e-06, 2.3872e-05, 2.5108e-06, 2.9427e-06,\n",
            "        1.1740e-05, 2.3729e-06, 7.6126e-05, 5.4509e-06])\n",
            "Sum of predicted tensor(2.6618)\n",
            "Loss tensor(0.4041)\n",
            "Acc tensor(0.9219)\n",
            "tensor([8.8372e-04, 7.3119e-06, 1.7426e-06, 4.4827e-06, 8.3414e-06, 3.5096e-05,\n",
            "        2.1876e-05, 2.4152e-01, 2.1254e-02, 7.9598e-01, 2.8139e-06, 4.8472e-06,\n",
            "        7.8116e-06, 2.4532e-06, 5.2562e-06, 3.7964e-05, 7.1317e-06, 1.7824e-05,\n",
            "        2.7974e-05, 1.6001e-04, 1.0723e-05, 1.6691e-05, 7.0159e-06, 1.4774e-04,\n",
            "        2.9910e-06, 1.1150e-01, 3.1519e-06, 1.3558e-06, 1.0603e-05, 1.4548e-06,\n",
            "        3.1519e-06, 3.8540e-06, 3.2656e-02, 7.0964e-06, 2.9019e-06, 3.4494e-05,\n",
            "        9.9044e-06, 1.7368e-05, 3.8552e-06, 2.2792e-05, 2.8017e-06, 1.7497e-05,\n",
            "        9.7953e-01, 1.4738e-04, 1.7777e-06, 3.5225e-06, 1.8195e-04, 1.5296e-06,\n",
            "        1.7864e-06, 1.4712e-06, 1.1787e-05, 1.4883e-06, 1.1091e-05, 1.8072e-05,\n",
            "        1.7904e-06, 4.7826e-06, 5.0558e-06, 5.2990e-06, 5.3168e-06, 6.5674e-04,\n",
            "        2.6009e-05, 1.0099e-02, 9.9876e-01, 2.8977e-06])\n",
            "Sum of predicted tensor(3.1940)\n",
            "Loss tensor(0.2893)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.3070e-05, 1.2042e-05, 1.3397e-06, 3.5302e-06, 3.6142e-06, 9.2879e-06,\n",
            "        6.3251e-06, 5.0376e-06, 1.5548e-06, 1.4785e-05, 5.6238e-05, 9.9326e-06,\n",
            "        1.4218e-05, 2.8329e-06, 6.3976e-05, 2.7746e-06, 9.5579e-06, 7.3692e-06,\n",
            "        3.4681e-02, 3.2989e-03, 1.3277e-06, 5.4994e-05, 2.8902e-06, 1.5897e-05,\n",
            "        5.8525e-05, 1.2063e-05, 3.1124e-06, 2.5794e-06, 2.8391e-06, 3.1312e-06,\n",
            "        4.1989e-06, 4.4453e-06, 2.1098e-05, 8.2956e-05, 1.6001e-03, 1.5902e-06,\n",
            "        7.2244e-06, 1.2807e-05, 2.3499e-05, 9.0814e-06, 9.8050e-01, 5.3277e-06,\n",
            "        2.2242e-06, 1.8978e-05, 3.0068e-06, 2.0541e-05, 2.3763e-06, 7.8971e-05,\n",
            "        2.5031e-06, 1.8410e-05, 4.8777e-06, 1.6112e-03, 2.1385e-04, 3.1443e-06,\n",
            "        3.4777e-05, 5.3492e-06, 2.3801e-05, 3.4203e-06, 1.2314e-05, 2.0678e-05,\n",
            "        4.5266e-06, 7.1972e-06, 3.7855e-02, 1.7864e-06])\n",
            "Sum of predicted tensor(1.0606)\n",
            "Loss tensor(0.1545)\n",
            "Acc tensor(0.9844)\n",
            "tensor([8.8482e-01, 2.5729e-06, 5.1427e-06, 1.4327e-03, 3.0013e-05, 5.5226e-04,\n",
            "        1.1531e-05, 4.5518e-05, 1.7433e-06, 1.1407e-05, 1.8052e-05, 3.3572e-05,\n",
            "        1.2570e-04, 2.2979e-02, 1.2131e-05, 3.7419e-06, 1.1034e-05, 1.2115e-05,\n",
            "        1.4199e-04, 8.2897e-06, 2.0762e-06, 7.0477e-06, 2.2218e-06, 9.2114e-06,\n",
            "        2.6942e-06, 2.0106e-06, 7.2944e-06, 7.4665e-06, 3.3130e-06, 1.2367e-04,\n",
            "        2.5252e-05, 6.3963e-06, 2.0171e-05, 2.0986e-04, 1.7254e-05, 9.9167e-01,\n",
            "        3.7756e-06, 2.4796e-01, 7.3318e-05, 4.2381e-05, 3.4910e-06, 5.5675e-06,\n",
            "        4.7063e-04, 3.1678e-05, 4.9192e-04, 2.2346e-05, 2.2796e-05, 4.5209e-06,\n",
            "        1.0836e-04, 4.0364e-05, 1.4603e-05, 1.1631e-05, 2.5613e-06, 2.6379e-06,\n",
            "        9.9874e-01, 1.0465e-05, 3.0419e-05, 7.0333e-05, 2.0649e-02, 5.0685e-05,\n",
            "        9.3561e-05, 5.5836e-06, 2.5563e-06, 7.5350e-06])\n",
            "Sum of predicted tensor(3.1714)\n",
            "Loss tensor(0.3894)\n",
            "Acc tensor(0.9375)\n",
            "tensor([3.0282e-05, 4.3356e-06, 1.3908e-05, 1.7687e-05, 4.7715e-06, 5.7237e-06,\n",
            "        6.4344e-06, 4.8489e-06, 9.6688e-06, 2.0509e-06, 1.1177e-06, 2.1430e-06,\n",
            "        1.6329e-06, 8.1233e-05, 6.2832e-05, 1.4378e-05, 3.7256e-06, 1.1055e-05,\n",
            "        9.2884e-06, 8.7803e-06, 1.9255e-04, 1.7399e-03, 7.5132e-05, 1.4452e-05,\n",
            "        1.7687e-05, 3.4402e-06, 2.3567e-04, 6.8259e-06, 6.6674e-06, 1.5113e-05,\n",
            "        5.5675e-06, 1.8020e-05, 5.0571e-05, 4.5544e-06, 5.2917e-06, 1.6761e-06,\n",
            "        1.9076e-05, 7.8039e-05, 8.1632e-06, 2.3598e-06, 2.2107e-04, 2.9856e-06,\n",
            "        5.0682e-04, 2.6112e-05, 4.9341e-06, 1.8208e-05, 1.3767e-04, 1.9200e-05,\n",
            "        5.5675e-06, 4.1402e-06, 9.1994e-05, 1.7740e-06, 7.8101e-06, 1.4884e-04,\n",
            "        8.0511e-06, 4.5737e-06, 2.6700e-05, 7.3022e-06, 2.0600e-06, 2.0595e-05,\n",
            "        2.9256e-06, 9.2384e-05, 4.6749e-06, 1.5077e-05])\n",
            "Sum of predicted tensor(0.0042)\n",
            "Loss tensor(6.5309e-05)\n",
            "Acc tensor(1.)\n",
            "tensor([5.1035e-01, 7.4551e-06, 5.7420e-06, 1.7245e-05, 4.7715e-06, 4.2214e-06,\n",
            "        8.7920e-06, 4.4882e-06, 1.1767e-05, 4.3437e-06, 1.2484e-02, 4.7715e-06,\n",
            "        7.7983e-06, 5.5122e-06, 9.9977e-01, 1.5454e-06, 8.7091e-06, 2.8797e-06,\n",
            "        6.9893e-04, 4.9156e-06, 9.9287e-05, 2.7839e-06, 2.2392e-06, 1.7633e-06,\n",
            "        3.7525e-06, 1.2560e-05, 1.1110e-04, 7.3026e-06, 8.2661e-06, 1.2913e-05,\n",
            "        2.0661e-06, 8.4913e-01, 1.7055e-05, 6.9769e-06, 9.7834e-01, 1.3062e-05,\n",
            "        4.6596e-01, 1.2629e-04, 9.9692e-01, 1.2110e-04, 8.1632e-06, 2.0860e-06,\n",
            "        3.1311e-06, 6.0720e-04, 1.3391e-06, 5.8272e-06, 9.6083e-01, 4.2548e-06,\n",
            "        2.7189e-06, 1.1914e-02, 5.0684e-06, 8.5558e-06, 3.7122e-06, 1.2137e-05,\n",
            "        4.2748e-03, 5.8722e-05, 3.9497e-06, 1.2485e-01, 4.0408e-06, 2.9100e-06,\n",
            "        9.0720e-04, 1.6218e-06, 1.3418e-05, 4.7715e-06])\n",
            "Sum of predicted tensor(5.9178)\n",
            "Loss tensor(0.3799)\n",
            "Acc tensor(0.9062)\n",
            "tensor([2.9178e-06, 8.4477e-05, 1.8494e-05, 9.9904e-01, 7.3374e-06, 9.9960e-01,\n",
            "        5.4155e-05, 1.0331e-05, 3.1505e-06, 9.9956e-01, 9.9197e-01, 3.9533e-06,\n",
            "        1.8880e-06, 1.1902e-05, 6.3885e-06, 6.1416e-06, 6.2648e-05, 7.6014e-06,\n",
            "        1.3535e-06, 2.4227e-06, 7.1783e-06, 2.4827e-06, 1.8277e-06, 2.0485e-06,\n",
            "        8.8462e-05, 1.9139e-06, 1.3483e-04, 2.1345e-01, 9.9961e-01, 2.0343e-06,\n",
            "        8.8944e-04, 8.4408e-05, 8.8362e-06, 3.8223e-05, 2.1647e-06, 1.8052e-05,\n",
            "        9.9945e-01, 9.9910e-01, 2.3880e-05, 3.5568e-03, 2.9336e-06, 2.7472e-01,\n",
            "        5.4790e-06, 2.4128e-05, 9.9938e-01, 4.9226e-05, 9.8805e-06, 7.2348e-06,\n",
            "        6.3151e-05, 2.8933e-06, 1.7819e-02, 1.7233e-03, 9.4825e-06, 3.9374e-04,\n",
            "        4.8673e-03, 1.4982e-05, 1.1943e-04, 1.1467e-05, 2.0718e-05, 1.5569e-06,\n",
            "        4.0138e-05, 2.1816e-04, 3.4634e-06, 1.9696e-06])\n",
            "Sum of predicted tensor(8.5064)\n",
            "Loss tensor(0.4982)\n",
            "Acc tensor(0.9375)\n",
            "tensor([6.8560e-06, 2.5893e-06, 2.4841e-06, 1.0810e-02, 2.3328e-06, 2.2739e-06,\n",
            "        1.0584e-05, 1.7497e-05, 1.6515e-05, 2.9653e-06, 2.6883e-02, 2.7130e-03,\n",
            "        2.5564e-06, 8.9728e-05, 4.4755e-05, 3.4395e-05, 4.8622e-06, 9.9804e-01,\n",
            "        2.2639e-06, 8.9325e-06, 3.5438e-06, 9.3529e-06, 6.0664e-06, 5.8428e-06,\n",
            "        9.2641e-06, 5.9846e-05, 4.3679e-06, 4.1762e-06, 3.0848e-03, 1.6984e-05,\n",
            "        5.5360e-06, 2.2227e-06, 1.0894e-05, 5.0363e-06, 9.9841e-01, 1.5515e-06,\n",
            "        1.7368e-05, 1.4226e-05, 4.1504e-02, 9.9737e-01, 2.3869e-06, 2.1919e-05,\n",
            "        1.0680e-05, 2.8933e-06, 1.3028e-03, 4.5097e-01, 2.3998e-06, 4.8258e-05,\n",
            "        5.0200e-04, 7.9837e-05, 2.0772e-06, 1.6667e-06, 9.9935e-01, 8.6300e-06,\n",
            "        3.2795e-05, 1.9885e-05, 1.8137e-06, 2.5249e-06, 5.6151e-06, 3.9389e-06,\n",
            "        4.7715e-06, 4.3175e-06, 4.0949e-06, 6.8432e-06])\n",
            "Sum of predicted tensor(4.5316)\n",
            "Loss tensor(0.1115)\n",
            "Acc tensor(0.9844)\n",
            "tensor([5.0432e-06, 4.6632e-06, 4.1165e-06, 3.9996e-06, 1.1767e-01, 1.5651e-06,\n",
            "        7.4744e-05, 6.7934e-06, 2.0536e-05, 7.3909e-06, 9.9980e-01, 4.7715e-06,\n",
            "        4.7174e-06, 9.9697e-01, 9.4728e-03, 3.5068e-06, 3.2979e-06, 6.1248e-04,\n",
            "        4.9559e-06, 2.5935e-06, 9.9510e-01, 1.4158e-06, 9.5238e-05, 6.8448e-06,\n",
            "        7.8352e-06, 1.0853e-05, 2.9495e-06, 3.6705e-06, 2.0496e-05, 8.5185e-05,\n",
            "        2.4940e-05, 1.2036e-06, 4.7748e-05, 1.3074e-05, 4.7424e-05, 2.5489e-06,\n",
            "        4.7708e-06, 2.3911e-06, 1.7578e-06, 3.5335e-06, 1.6753e-01, 1.4431e-06,\n",
            "        5.5675e-06, 7.8691e-01, 1.6416e-05, 9.7647e-04, 3.8743e-05, 1.0710e-05,\n",
            "        4.1161e-04, 7.9708e-06, 2.5897e-05, 1.2010e-05, 2.3685e-05, 1.2275e-05,\n",
            "        4.0011e-05, 1.6869e-05, 4.3656e-06, 4.4861e-04, 1.4697e-05, 1.3636e-05,\n",
            "        5.0202e-06, 7.6464e-06, 2.8319e-05, 2.2011e-05])\n",
            "Sum of predicted tensor(4.0767)\n",
            "Loss tensor(0.0544)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.6020e-05, 9.9227e-01, 1.3741e-04, 1.1486e-05, 4.0899e-05, 2.5198e-05,\n",
            "        3.0733e-06, 9.6906e-06, 1.8496e-06, 7.4791e-06, 4.1925e-06, 2.7215e-06,\n",
            "        5.5776e-06, 1.3794e-04, 5.1746e-06, 8.0278e-06, 9.9917e-01, 3.8149e-02,\n",
            "        1.6718e-06, 4.0768e-06, 4.4013e-06, 7.6909e-06, 2.1747e-05, 5.3103e-03,\n",
            "        7.2477e-05, 3.7206e-06, 9.9980e-01, 4.4367e-06, 4.6915e-06, 9.9657e-01,\n",
            "        2.9110e-06, 1.3798e-03, 4.7715e-06, 1.9486e-05, 2.1736e-06, 1.7051e-06,\n",
            "        9.9005e-06, 9.9893e-01, 9.9876e-01, 1.1224e-04, 4.3842e-02, 6.5424e-02,\n",
            "        4.0569e-06, 2.9659e-04, 8.4151e-02, 2.4485e-05, 5.4841e-04, 4.8846e-05,\n",
            "        1.4511e-06, 2.1526e-06, 5.4147e-06, 2.1234e-05, 1.4298e-05, 1.9249e-03,\n",
            "        4.9407e-06, 7.5826e-05, 4.1232e-06, 9.7310e-01, 1.1211e-05, 9.9915e-01,\n",
            "        6.5361e-06, 6.6978e-04, 9.9197e-01, 2.4271e-06])\n",
            "Sum of predicted tensor(9.1923)\n",
            "Loss tensor(0.2362)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.9077e-06, 2.0586e-05, 1.0054e-04, 8.1076e-03, 1.5251e-06, 1.0603e-05,\n",
            "        7.7875e-01, 8.1440e-06, 2.1839e-06, 1.7970e-05, 6.0157e-06, 2.2488e-06,\n",
            "        6.7944e-06, 1.5156e-04, 4.1533e-06, 2.3272e-06, 4.6593e-06, 7.3419e-06,\n",
            "        1.3042e-04, 7.5271e-06, 4.7898e-04, 3.8453e-06, 2.7232e-05, 5.1487e-06,\n",
            "        4.5428e-05, 6.9012e-06, 3.8073e-04, 4.5593e-04, 6.0507e-06, 2.4902e-05,\n",
            "        4.1207e-06, 5.1380e-06, 1.4777e-05, 2.3816e-06, 1.7612e-05, 4.4595e-06,\n",
            "        6.6541e-05, 8.1591e-06, 1.3150e-03, 5.8287e-06, 4.9890e-06, 3.1745e-06,\n",
            "        1.5769e-05, 8.1215e-05, 1.3498e-06, 8.8260e-05, 1.8183e-05, 1.5361e-06,\n",
            "        5.8148e-06, 3.0218e-06, 7.1695e-05, 1.7445e-06, 9.7895e-01, 1.0132e-04,\n",
            "        4.3076e-05, 4.0366e-05, 8.5635e-05, 1.3146e-05, 2.8597e-06, 9.5006e-06,\n",
            "        3.1500e-05, 2.9088e-06, 6.0428e-04, 8.6575e-04])\n",
            "Sum of predicted tensor(1.7713)\n",
            "Loss tensor(0.2858)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.4932e-04, 1.7843e-05, 1.2675e-04, 5.6888e-03, 8.5069e-06, 1.8742e-06,\n",
            "        2.6476e-06, 2.9017e-06, 3.7918e-04, 9.8897e-01, 5.3551e-06, 1.2655e-05,\n",
            "        3.9762e-04, 2.6943e-04, 3.0997e-06, 3.8180e-06, 9.1824e-06, 7.5083e-06,\n",
            "        2.0797e-06, 8.8206e-03, 1.8566e-06, 1.2059e-04, 7.4233e-06, 2.0879e-06,\n",
            "        7.8247e-06, 4.4197e-01, 2.1541e-06, 5.6443e-06, 2.2739e-04, 4.9639e-06,\n",
            "        2.9427e-04, 2.2069e-05, 2.1929e-04, 1.7118e-06, 2.1007e-06, 4.7281e-06,\n",
            "        3.5462e-05, 3.6982e-06, 7.6918e-04, 7.5271e-06, 6.8089e-06, 5.8662e-06,\n",
            "        3.9034e-06, 3.4151e-05, 9.0553e-06, 1.2401e-05, 9.9791e-01, 5.9972e-06,\n",
            "        2.8350e-06, 8.1026e-06, 2.6938e-05, 2.3193e-05, 1.0988e-04, 1.4342e-05,\n",
            "        1.3654e-05, 1.4141e-05, 4.5383e-05, 2.0622e-05, 1.8663e-05, 7.2376e-03,\n",
            "        5.6845e-03, 7.0879e-05, 3.3535e-05, 1.0181e-04])\n",
            "Sum of predicted tensor(2.4601)\n",
            "Loss tensor(0.2966)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.3802e-05, 1.0018e-04, 6.8988e-06, 1.3099e-02, 4.2795e-06, 2.6300e-06,\n",
            "        1.1315e-05, 2.6272e-06, 9.9909e-01, 5.5128e-05, 1.0232e-05, 2.6829e-06,\n",
            "        2.2517e-04, 5.9136e-04, 6.2094e-06, 4.1156e-06, 1.7205e-05, 1.2825e-03,\n",
            "        2.1276e-05, 1.5793e-01, 5.3240e-06, 8.5879e-06, 2.8226e-05, 2.3511e-05,\n",
            "        5.4963e-06, 9.9588e-06, 2.1886e-06, 9.9974e-01, 2.6816e-03, 2.4498e-06,\n",
            "        2.9161e-04, 9.5412e-06, 4.1322e-05, 3.2028e-06, 9.6302e-06, 2.0284e-04,\n",
            "        9.9975e-01, 4.2365e-06, 2.0884e-06, 1.8175e-05, 2.9300e-05, 2.3040e-05,\n",
            "        2.3372e-04, 9.1373e-06, 9.9900e-01, 9.9946e-01, 3.0538e-06, 1.2926e-05,\n",
            "        9.4549e-06, 9.9985e-01, 4.0057e-06, 1.3127e-02, 8.1248e-06, 1.0794e-05,\n",
            "        1.0307e-05, 7.8279e-06, 2.1769e-04, 4.0279e-05, 1.5443e-01, 1.9390e-05,\n",
            "        5.1706e-06, 5.1636e-06, 2.6895e-01, 4.4068e-06])\n",
            "Sum of predicted tensor(6.6108)\n",
            "Loss tensor(0.1530)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.3879e-06, 1.0334e-05, 1.1088e-05, 2.2481e-05, 1.0465e-05, 7.5271e-06,\n",
            "        1.0632e-05, 9.9985e-01, 5.1778e-06, 2.7870e-06, 3.9945e-06, 7.7029e-06,\n",
            "        4.3632e-06, 7.5271e-06, 6.9958e-06, 1.9344e-05, 2.2946e-04, 3.9736e-04,\n",
            "        1.3430e-06, 3.2234e-06, 4.3281e-03, 3.2787e-06, 2.9628e-06, 1.1198e-03,\n",
            "        5.0646e-02, 6.3623e-06, 8.4102e-06, 1.3390e-06, 8.4108e-06, 3.1655e-06,\n",
            "        6.4765e-06, 1.8673e-06, 5.9455e-06, 9.8678e-06, 1.1721e-05, 2.6207e-06,\n",
            "        4.8087e-03, 2.0523e-05, 1.5317e-04, 1.2459e-05, 5.0862e-06, 7.5271e-06,\n",
            "        1.0612e-04, 1.5447e-05, 5.8063e-06, 6.8245e-06, 2.5907e-06, 3.4990e-04,\n",
            "        1.1063e-01, 2.7404e-04, 5.9020e-06, 2.1721e-06, 4.1414e-05, 9.9856e-01,\n",
            "        4.7270e-04, 2.7231e-06, 4.6368e-05, 9.7624e-05, 9.9990e-01, 7.9778e-06,\n",
            "        5.8477e-06, 2.2814e-04, 8.6825e-06, 4.0021e-06])\n",
            "Sum of predicted tensor(3.1726)\n",
            "Loss tensor(0.1204)\n",
            "Acc tensor(0.9688)\n",
            "tensor([8.8819e-02, 5.5591e-03, 1.4436e-04, 3.3884e-06, 2.0797e-06, 7.8867e-06,\n",
            "        2.1674e-05, 2.0503e-03, 9.9947e-01, 2.6963e-06, 4.1391e-06, 5.9836e-06,\n",
            "        8.0128e-06, 1.7718e-04, 4.2948e-06, 2.6736e-05, 1.3893e-05, 1.1927e-05,\n",
            "        1.4256e-05, 4.6866e-05, 9.9979e-01, 8.9216e-05, 1.5141e-06, 4.5814e-06,\n",
            "        3.0979e-05, 2.1785e-05, 1.5048e-04, 1.2260e-05, 9.9947e-06, 1.3951e-06,\n",
            "        1.9738e-06, 3.8964e-06, 1.7483e-06, 7.5271e-06, 1.4734e-05, 1.6921e-05,\n",
            "        2.4558e-06, 5.6267e-06, 1.1152e-05, 3.9137e-05, 7.5271e-06, 1.2618e-05,\n",
            "        1.1721e-05, 4.5399e-06, 7.1955e-06, 8.8587e-06, 2.6818e-04, 6.3471e-05,\n",
            "        3.2331e-02, 7.7275e-06, 5.7319e-06, 4.3423e-06, 4.6441e-06, 4.1207e-06,\n",
            "        2.0025e-05, 8.3326e-06, 1.3208e-05, 5.5414e-06, 1.2731e-04, 2.6818e-04,\n",
            "        1.6056e-06, 8.5573e-03, 1.8215e-05, 7.5271e-06])\n",
            "Sum of predicted tensor(2.1384)\n",
            "Loss tensor(0.2806)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.4223e-06, 1.0759e-05, 8.3541e-04, 8.1098e-01, 9.9647e-01, 8.0110e-06,\n",
            "        3.4990e-04, 3.5241e-06, 2.3474e-05, 6.8041e-06, 5.8871e-05, 6.6091e-01,\n",
            "        1.8419e-06, 4.1133e-06, 2.1188e-05, 4.8651e-02, 4.9569e-06, 7.1477e-05,\n",
            "        3.9532e-06, 2.5415e-05, 3.0772e-05, 6.9258e-05, 1.9306e-03, 1.7975e-06,\n",
            "        1.7380e-06, 1.3825e-06, 2.2721e-05, 3.6759e-06, 1.6931e-05, 1.4950e-06,\n",
            "        2.2233e-06, 7.9699e-04, 4.4386e-06, 3.6059e-03, 1.6279e-05, 1.4550e-05,\n",
            "        1.8621e-06, 4.5428e-05, 8.9604e-06, 3.8628e-05, 2.4949e-06, 3.6593e-05,\n",
            "        2.1555e-05, 3.0958e-05, 2.8845e-05, 9.4140e-01, 3.7286e-06, 1.8044e-06,\n",
            "        9.8568e-06, 1.7847e-05, 2.9276e-03, 1.8444e-06, 3.2331e-02, 7.5271e-06,\n",
            "        8.1940e-03, 7.8606e-06, 3.6695e-02, 3.7598e-06, 1.7429e-06, 1.6513e-04,\n",
            "        4.2404e-06, 1.2958e-04, 9.9917e-01, 7.4309e-05])\n",
            "Sum of predicted tensor(4.5463)\n",
            "Loss tensor(0.2159)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.6268e-05, 1.6942e-06, 5.7374e-01, 4.1322e-06, 4.3126e-04, 2.9825e-06,\n",
            "        5.1851e-06, 1.1029e-05, 5.9254e-06, 2.8439e-04, 6.2579e-06, 1.4122e-05,\n",
            "        2.1258e-05, 6.4779e-06, 3.9875e-04, 4.0445e-06, 1.0513e-03, 1.4639e-06,\n",
            "        4.1328e-06, 2.0775e-06, 2.2701e-06, 9.9017e-01, 9.7814e-01, 1.4405e-06,\n",
            "        1.7935e-06, 2.5792e-06, 4.6398e-06, 3.0071e-05, 1.9584e-06, 2.0065e-05,\n",
            "        3.7821e-06, 5.6149e-06, 9.9656e-01, 2.6458e-01, 2.2649e-05, 3.0424e-06,\n",
            "        1.4392e-05, 2.4770e-06, 1.9176e-05, 5.7032e-06, 3.4516e-06, 6.8357e-04,\n",
            "        1.8754e-04, 3.1858e-01, 5.7695e-06, 5.6714e-05, 5.1900e-06, 1.4534e-06,\n",
            "        2.9317e-06, 6.4801e-06, 1.1942e-04, 9.7800e-05, 1.1600e-05, 9.9175e-01,\n",
            "        4.7385e-06, 1.3253e-06, 5.1900e-06, 1.7286e-06, 2.5209e-04, 1.4358e-05,\n",
            "        6.2803e-06, 1.3622e-02, 4.2791e-06, 9.7376e-01])\n",
            "Sum of predicted tensor(6.1048)\n",
            "Loss tensor(0.2517)\n",
            "Acc tensor(0.9062)\n",
            "tensor([1.7013e-06, 1.8010e-05, 1.8045e-06, 2.9236e-05, 7.5186e-06, 4.6925e-06,\n",
            "        1.6471e-05, 8.0586e-06, 2.5112e-05, 1.2285e-05, 2.3002e-05, 3.3209e-05,\n",
            "        2.9681e-01, 1.4579e-06, 9.7285e-01, 9.8668e-01, 1.0334e-05, 9.9381e-01,\n",
            "        8.0431e-02, 3.5578e-03, 7.6554e-06, 1.3043e-06, 5.0909e-06, 1.0965e-06,\n",
            "        9.2446e-04, 8.3034e-06, 5.0534e-06, 2.4251e-05, 3.2563e-06, 2.7046e-05,\n",
            "        5.9831e-05, 7.2923e-05, 3.5281e-06, 9.8978e-06, 9.9661e-01, 1.8400e-05,\n",
            "        2.6829e-05, 6.9555e-06, 5.1810e-06, 3.5008e-06, 4.3107e-06, 6.2567e-05,\n",
            "        5.7695e-06, 4.6436e-06, 3.4154e-06, 3.7860e-03, 3.4985e-06, 7.4602e-06,\n",
            "        1.4681e-02, 1.0572e-03, 1.6521e-05, 1.8793e-05, 8.3034e-06, 5.7695e-06,\n",
            "        1.6504e-05, 1.1519e-02, 3.6725e-05, 1.9319e-05, 3.5821e-04, 4.6789e-04,\n",
            "        1.3503e-02, 1.9344e-03, 1.9986e-05, 1.4879e-06])\n",
            "Sum of predicted tensor(4.3797)\n",
            "Loss tensor(0.3415)\n",
            "Acc tensor(0.9062)\n",
            "tensor([7.9746e-06, 1.1345e-05, 2.3592e-06, 1.8070e-05, 4.9059e-06, 9.6223e-01,\n",
            "        9.9991e-01, 1.3577e-05, 3.0308e-06, 7.1266e-06, 4.3541e-06, 5.9470e-05,\n",
            "        3.8290e-06, 2.3653e-05, 1.4347e-03, 1.2505e-01, 5.4670e-06, 1.9847e-05,\n",
            "        1.6501e-05, 3.5938e-03, 4.0360e-05, 7.9456e-05, 1.2993e-06, 4.3907e-01,\n",
            "        1.3659e-06, 1.4536e-01, 2.1826e-05, 2.4231e-05, 4.3896e-06, 4.3652e-06,\n",
            "        7.1897e-05, 2.5055e-05, 5.0003e-03, 2.4181e-06, 1.0343e-04, 2.1219e-06,\n",
            "        8.9326e-05, 6.0380e-06, 2.0509e-04, 1.5011e-06, 6.7464e-06, 4.1201e-06,\n",
            "        9.8184e-01, 1.3948e-05, 8.7896e-06, 9.9976e-01, 5.9145e-05, 9.9406e-01,\n",
            "        1.1980e-03, 1.3343e-06, 1.4920e-05, 8.3034e-06, 1.1569e-02, 3.6519e-05,\n",
            "        1.1868e-05, 1.8583e-06, 7.2959e-05, 9.9137e-01, 2.4841e-06, 3.4414e-04,\n",
            "        1.8402e-06, 4.6762e-06, 8.1079e-06, 2.3912e-06])\n",
            "Sum of predicted tensor(6.6629)\n",
            "Loss tensor(0.3941)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.7695e-06, 4.8810e-05, 8.0682e-06, 3.4758e-06, 5.3461e-06, 1.8570e-06,\n",
            "        1.7960e-05, 2.5702e-06, 2.5324e-06, 6.6855e-06, 7.2955e-06, 6.6850e-05,\n",
            "        5.4582e-06, 9.4546e-01, 6.9948e-06, 1.1609e-01, 2.3192e-05, 4.7956e-06,\n",
            "        5.6672e-05, 4.8219e-06, 3.6073e-06, 1.5967e-05, 2.9634e-06, 4.8127e-05,\n",
            "        3.8781e-06, 1.1911e-04, 2.8239e-05, 9.3745e-06, 2.2005e-06, 4.4053e-05,\n",
            "        4.1564e-06, 3.6434e-02, 1.2528e-05, 2.0211e-06, 2.6840e-06, 9.8484e-01,\n",
            "        3.6532e-06, 7.9202e-06, 6.6204e-06, 4.1187e-06, 1.3937e-05, 5.1556e-01,\n",
            "        2.8633e-05, 1.8851e-05, 9.9916e-01, 9.9586e-01, 3.7102e-06, 6.9651e-06,\n",
            "        5.1873e-05, 5.5240e-06, 1.5474e-02, 5.7695e-06, 2.6827e-01, 1.8167e-06,\n",
            "        2.5973e-06, 4.5543e-06, 4.4177e-06, 2.7069e-03, 2.8385e-06, 9.9929e-01,\n",
            "        7.7160e-06, 5.1900e-06, 1.5853e-06, 1.1580e-05])\n",
            "Sum of predicted tensor(5.8799)\n",
            "Loss tensor(0.3308)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9987e-01, 6.0356e-06, 5.9083e-06, 6.0143e-04, 1.5255e-05, 8.6881e-06,\n",
            "        1.3247e-05, 4.3934e-05, 2.7767e-05, 3.9988e-02, 2.0472e-06, 9.6161e-03,\n",
            "        4.7983e-05, 4.8810e-06, 5.1054e-05, 1.3713e-03, 6.0442e-06, 1.0528e-02,\n",
            "        8.4238e-06, 1.5181e-05, 9.6588e-01, 5.1617e-05, 9.9036e-01, 7.0060e-06,\n",
            "        2.7598e-05, 2.5638e-05, 3.5495e-03, 2.5026e-05, 6.3993e-05, 1.7688e-05,\n",
            "        3.0098e-06, 1.1045e-04, 3.7727e-05, 9.4599e-05, 6.0211e-06, 4.8134e-05,\n",
            "        1.1280e-04, 9.9114e-01, 1.8691e-06, 5.0162e-06, 1.8398e-06, 3.2149e-04,\n",
            "        7.4893e-05, 1.3061e-04, 6.5745e-03, 2.1969e-06, 8.8862e-06, 3.6791e-06,\n",
            "        6.7787e-04, 2.2328e-05, 2.9025e-06, 7.2169e-06, 3.4911e-06, 9.9792e-06,\n",
            "        5.7695e-06, 9.2855e-02, 1.3540e-05, 6.0154e-06, 9.9991e-01, 1.7935e-06,\n",
            "        1.3609e-05, 2.4167e-06, 1.9483e-06, 5.1641e-06])\n",
            "Sum of predicted tensor(5.1145)\n",
            "Loss tensor(0.3443)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.2059e-05, 5.3966e-05, 1.7466e-06, 8.0666e-06, 7.6129e-06, 6.6616e-06,\n",
            "        3.7626e-06, 2.5522e-06, 7.0021e-01, 1.0670e-05, 7.8255e-05, 5.3645e-01,\n",
            "        2.7801e-05, 2.6792e-05, 2.9714e-06, 3.0167e-06, 3.2014e-06, 4.7078e-06,\n",
            "        6.8493e-05, 3.3050e-04, 2.9428e-05, 2.9194e-03, 5.6995e-05, 1.4919e-05,\n",
            "        9.8253e-06, 4.9858e-05, 9.7660e-06, 6.1961e-04, 2.9557e-06, 7.1939e-03,\n",
            "        1.2265e-01, 6.9301e-06, 5.7043e-06, 3.1459e-06, 1.3233e-06, 5.4761e-06,\n",
            "        3.5277e-05, 9.3838e-06, 4.6038e-06, 2.6336e-04, 1.3643e-05, 1.8059e-06,\n",
            "        5.1179e-06, 1.0254e-04, 6.4090e-05, 8.9736e-04, 8.2286e-06, 2.2447e-04,\n",
            "        7.8864e-04, 2.6651e-06, 3.3485e-02, 6.7668e-05, 5.1872e-06, 1.1670e-05,\n",
            "        5.8598e-06, 2.8908e-06, 1.0445e-04, 4.7369e-06, 6.2971e-05, 9.6549e-01,\n",
            "        4.2586e-06, 1.5817e-05, 1.6518e-05, 9.9089e-06])\n",
            "Sum of predicted tensor(2.3726)\n",
            "Loss tensor(0.2138)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.3275e-06, 7.5198e-06, 9.0772e-05, 4.3023e-05, 2.7237e-02, 6.3719e-05,\n",
            "        1.3295e-05, 4.0252e-06, 2.3987e-06, 5.5005e-06, 4.9888e-03, 3.3442e-06,\n",
            "        2.2170e-05, 1.9923e-05, 6.8872e-06, 5.8171e-06, 9.9952e-01, 4.8948e-06,\n",
            "        1.5099e-05, 2.3037e-06, 1.3391e-06, 4.2480e-05, 9.3820e-05, 3.5932e-06,\n",
            "        2.0044e-06, 5.1126e-03, 8.1009e-04, 2.1826e-06, 9.9939e-01, 4.0120e-02,\n",
            "        3.9572e-06, 1.9022e-06, 1.1638e-04, 1.1074e-05, 5.1167e-06, 1.2076e-05,\n",
            "        2.3126e-06, 1.2922e-05, 3.4344e-05, 7.4395e-06, 3.2105e-04, 6.5187e-01,\n",
            "        1.0282e-05, 5.0444e-06, 2.7461e-05, 8.9901e-06, 1.2098e-05, 1.2191e-05,\n",
            "        7.2670e-06, 6.9878e-06, 1.5311e-05, 7.3465e-06, 1.6550e-05, 1.6550e-05,\n",
            "        1.5354e-05, 7.6484e-06, 5.1705e-02, 3.0474e-06, 2.5720e-06, 1.4038e-06,\n",
            "        1.7372e-06, 9.1355e-03, 4.5695e-06, 1.7457e-05])\n",
            "Sum of predicted tensor(2.7911)\n",
            "Loss tensor(0.0187)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.9858e-05, 1.5841e-04, 1.9696e-05, 4.0626e-06, 1.4290e-06, 6.4979e-05,\n",
            "        6.2187e-06, 1.3024e-05, 7.6347e-05, 1.1076e-02, 7.5418e-06, 7.5892e-06,\n",
            "        1.8327e-05, 3.8614e-06, 2.2550e-06, 5.2204e-05, 3.3729e-06, 3.0105e-05,\n",
            "        4.2580e-05, 2.9473e-06, 2.9121e-06, 8.1898e-05, 3.2485e-06, 7.7300e-05,\n",
            "        2.6033e-05, 5.1062e-06, 1.0419e-05, 9.9937e-01, 7.7030e-06, 3.5297e-06,\n",
            "        1.2679e-04, 2.1769e-04, 4.2505e-05, 2.7664e-05, 4.9361e-06, 9.9289e-06,\n",
            "        1.2437e-05, 5.8912e-06, 2.4295e-05, 1.1781e-05, 2.2179e-06, 1.4766e-05,\n",
            "        3.8037e-06, 3.0090e-06, 2.4783e-05, 5.0263e-05, 2.6274e-06, 9.3118e-01,\n",
            "        4.6014e-06, 7.0421e-06, 1.2267e-05, 6.0144e-05, 5.4991e-06, 1.8603e-05,\n",
            "        1.4661e-03, 6.8261e-06, 1.7465e-05, 5.5296e-01, 1.6299e-05, 8.5480e-06,\n",
            "        4.1862e-06, 9.9838e-01, 2.3861e-05, 4.9181e-05])\n",
            "Sum of predicted tensor(3.4960)\n",
            "Loss tensor(0.2017)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.6522e-06, 1.1462e-05, 2.8621e-06, 3.0084e-06, 2.5361e-04, 1.7333e-05,\n",
            "        3.1145e-01, 1.0984e-05, 3.8723e-06, 2.3231e-06, 1.5874e-04, 8.6320e-03,\n",
            "        2.8628e-05, 1.6448e-03, 7.9791e-04, 2.0268e-05, 1.0990e-06, 4.4401e-06,\n",
            "        1.9366e-06, 1.5102e-05, 1.1120e-05, 2.3603e-04, 5.4501e-06, 3.5387e-06,\n",
            "        1.7139e-05, 2.8447e-04, 1.7532e-05, 6.1759e-06, 3.1598e-05, 1.1076e-02,\n",
            "        1.2997e-05, 2.8991e-06, 2.8805e-06, 2.7218e-04, 8.8864e-05, 5.5424e-05,\n",
            "        3.8327e-06, 3.2950e-06, 1.0842e-05, 6.9274e-01, 1.8434e-05, 3.2604e-06,\n",
            "        2.6729e-05, 7.5440e-05, 1.3162e-05, 4.9860e-06, 8.3275e-01, 8.2384e-06,\n",
            "        8.0451e-05, 1.0834e-05, 1.8770e-04, 7.3198e-05, 2.5650e-06, 9.9750e-01,\n",
            "        4.5625e-06, 6.8920e-06, 2.8456e-05, 1.2102e-05, 6.7360e-02, 3.0706e-06,\n",
            "        4.7096e-06, 2.1641e-05, 3.2838e-05, 8.1900e-06])\n",
            "Sum of predicted tensor(2.9262)\n",
            "Loss tensor(0.0537)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.0001e-06, 8.4614e-05, 2.6344e-05, 5.0921e-06, 1.1819e-05, 2.7057e-06,\n",
            "        5.0508e-05, 1.2010e-05, 6.8968e-06, 3.7111e-06, 5.5658e-06, 3.5787e-02,\n",
            "        1.0241e-04, 1.1050e-05, 2.1191e-05, 8.5640e-05, 7.6332e-06, 3.9098e-05,\n",
            "        1.7022e-04, 3.3779e-04, 4.0796e-05, 3.6264e-03, 9.2667e-05, 5.7518e-06,\n",
            "        2.7122e-06, 5.0484e-06, 6.9902e-02, 1.2581e-01, 3.1434e-06, 1.1959e-05,\n",
            "        5.5620e-06, 3.7943e-05, 9.8453e-01, 6.2645e-06, 2.3877e-06, 1.1512e-05,\n",
            "        6.3467e-06, 1.5853e-05, 5.0061e-05, 1.1266e-05, 3.7027e-06, 7.0250e-06,\n",
            "        5.6869e-05, 6.3876e-05, 3.6375e-05, 2.1116e-06, 2.9822e-05, 7.7041e-05,\n",
            "        3.4892e-06, 1.4011e-05, 3.5370e-06, 6.3297e-06, 2.5769e-06, 6.1252e-06,\n",
            "        4.6284e-06, 8.0115e-06, 8.4122e-05, 3.2683e-05, 3.5936e-05, 4.2602e-05,\n",
            "        2.5657e-05, 6.1257e-04, 5.2840e-06, 1.2374e-04])\n",
            "Sum of predicted tensor(1.2222)\n",
            "Loss tensor(0.2572)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.8742e-05, 1.1158e-05, 9.3771e-06, 7.1478e-06, 1.3071e-04, 1.4363e-02,\n",
            "        2.1337e-06, 9.9907e-01, 2.3065e-04, 8.8331e-06, 5.6965e-06, 3.2488e-06,\n",
            "        6.1391e-06, 1.8329e-05, 4.5591e-06, 3.0421e-06, 6.3402e-06, 8.8564e-01,\n",
            "        1.5171e-04, 1.2236e-01, 6.6478e-06, 2.4943e-06, 1.0952e-05, 4.7958e-05,\n",
            "        3.1933e-03, 1.5216e-04, 9.9968e-01, 6.0642e-05, 6.0902e-01, 3.2027e-03,\n",
            "        8.4705e-05, 1.0465e-02, 4.0905e-05, 9.9968e-01, 8.4162e-06, 6.0576e-01,\n",
            "        4.9151e-06, 7.1911e-05, 3.8012e-03, 5.6646e-04, 4.3536e-06, 2.0312e-06,\n",
            "        2.5993e-05, 3.4177e-06, 1.7737e-05, 7.4914e-06, 2.0017e-04, 4.1092e-06,\n",
            "        5.6955e-01, 9.3205e-06, 2.3412e-06, 6.2002e-06, 3.7894e-06, 4.3633e-05])\n",
            "Sum of predicted tensor(5.8278)\n",
            "Loss tensor(0.3269)\n",
            "Acc tensor(0.8889)\n",
            "\tTrain Loss: 0.049 | Train Acc: 98.49%\n",
            "\t Val. Loss: 0.238 |  Val. Acc: 94.98%\n",
            "Sum of predicted tensor(5.4695, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5747, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(12.7815, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2707, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4334, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4555, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8963, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.3019, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0077, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2505, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0745, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3750, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6594, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7866, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.3912, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3998, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4989, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6894, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0336, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.7902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8982, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0992, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0808, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.7145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0414, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2558, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0389, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1171, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.8231, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9306, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4517, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8238, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2310, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0948, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0266, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6045, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0174, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9452, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9965, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0590, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1179, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3936, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8654, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7390, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.1632, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0719, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.7009, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.8685, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.4739, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9662, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7490, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.7867, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0738, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5398, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6709, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5246, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7993, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1427, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4265, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0434, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6203, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4053, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9317, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0348, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0232, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0150, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0032, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7235, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(11.8730, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6640, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5666, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0896, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4342, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2994, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9810, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8038, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5021, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0022, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8778, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1442, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0041, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7611, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8253, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4595, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9640, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(12.0668, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0784, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1149, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9581, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6768, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0073, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.6474, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0749, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8228, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1642, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2649, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6860, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0261, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3606, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5439, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0373, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0782, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2855, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1363, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1705, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.5660, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0245, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0756, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2331, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2228, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6471, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0666, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.4752, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0196, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8371, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9514, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3848, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.7590, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0177, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7535, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0102, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.2599, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3259, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8459, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7135, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9455, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1512, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5707, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.7014, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8671, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1295, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.9772, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4851, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.2195, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9977, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.1311, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.2734, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0766, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9427, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0581, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0067, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.3521, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.2853, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0741, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6767, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0820, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0066, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5237, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.8254, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8789, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.5379, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1110, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0917, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.1413, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2452, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9746, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9899, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2772, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1293, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0327, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.3253, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1339, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6654, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4882, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7196, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.0359, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1041, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.6024, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0961, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4463, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7385, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4637, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6993, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0788, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8752, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0646, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4681, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.5526, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7848, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0019, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9363, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5426, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7631, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.5312, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9296, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0662, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9780, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7740, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0619, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8736, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6242, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1624, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6902, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3224, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.6209, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6976, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4006, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.2668, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.0268, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0599, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0133, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.6975, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6561, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.4316, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3290, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.8227, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.7321, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.3035, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.2656, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0145, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5252, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3513, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9322, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.5123, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.3017, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1440, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9254, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.3984, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3329, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0991, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0523, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9031, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.6490, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7739, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.8290, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9483, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2586, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.0222, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.6697, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.8223, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.8480, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9923, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2476, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.4930, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1029, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0186, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3393, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0573, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.4021, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9663, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3490, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2629, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7689, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.4781, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.9065, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3596, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3256, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1488, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.2044, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7309, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5460, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.7756, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.5951, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3935, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0677, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7501, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0491, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.7480, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5015, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9820, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9881, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3098, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1909, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9991, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0373, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9691, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.8673, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9983, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.9858, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.4068, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7413, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0038, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9163, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.4817, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0978, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.3533, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.6459, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.9751, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5570, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4741, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0105, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1493, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.5301, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5804, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.7128, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9453, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.3524, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.4913, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0184, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1727, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1163, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.2685, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.4858, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3529, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(9.0331, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0551, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0887, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9730, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1474, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2107, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9685, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.2382, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9824, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7482, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9829, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(10.3964, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.1108, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.7769, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1399, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1644, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.0516, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.8546, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9857, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0267, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1272, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.2934, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.9436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.7436, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.2800, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.1641, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.9692, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(1.0886, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9586, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.0799, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.9215, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0675, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1633, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.9497, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.1706, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.5682, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.6584, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.3934, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(2.0107, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.0400, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0790, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(8.4148, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(5.3277, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(0.1536, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(6.4857, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.1074, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(7.0163, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.0593, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(4.3107, grad_fn=<SumBackward0>)\n",
            "Sum of predicted tensor(3.9927, grad_fn=<SumBackward0>)\n",
            "tensor([3.5671e-05, 2.1157e-04, 8.9997e-06, 2.1157e-04, 3.3190e-04, 9.6418e-03,\n",
            "        2.7150e-04, 1.6024e-04, 1.7283e-05, 8.6803e-05, 1.4614e-04, 3.8082e-06,\n",
            "        1.4100e-04, 1.7066e-05, 4.2386e-03, 2.0738e-05, 1.9541e-04, 1.5693e-02,\n",
            "        4.2386e-03, 9.4015e-05, 4.2329e-05, 2.6939e-04, 3.3749e-05, 4.7862e-05,\n",
            "        1.5693e-02, 5.3566e-05, 1.7876e-04, 6.6697e-04, 4.2386e-03, 6.9970e-04,\n",
            "        1.1480e-04, 2.1958e-05, 3.8870e-05, 4.2386e-03, 6.6698e-04, 5.5782e-05,\n",
            "        2.5855e-04, 3.0527e-04, 4.2386e-03, 1.0460e-05, 4.2386e-03, 4.4681e-05,\n",
            "        3.1622e-05, 2.8604e-05, 4.4681e-05, 5.5450e-05, 4.2386e-03, 4.2386e-03,\n",
            "        4.2386e-03, 5.1479e-05, 4.2386e-03, 4.2386e-03, 6.1281e-04, 3.5602e-04,\n",
            "        1.9264e-05, 4.4681e-05, 5.0161e-03, 3.0420e-04, 4.4681e-05, 2.2339e-05,\n",
            "        2.5762e-05, 1.0513e-05, 6.5625e-05, 5.2234e-05])\n",
            "Sum of predicted tensor(0.0999)\n",
            "Loss tensor(0.2159)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.2823e-05, 1.9800e-04, 8.8600e-06, 8.3586e-01, 1.2062e-05, 6.0374e-05,\n",
            "        1.2815e-05, 5.1950e-05, 5.1386e-01, 3.1991e-02, 3.1276e-05, 1.5540e-03,\n",
            "        1.1041e-04, 1.0934e-04, 1.7650e-03, 5.5182e-01, 9.8034e-06, 4.5091e-04,\n",
            "        2.4453e-05, 1.0902e-03, 1.1064e-05, 1.1859e-04, 3.9101e-06, 3.3190e-04,\n",
            "        2.7456e-03, 6.0753e-02, 1.1678e-05, 2.1157e-04, 8.4757e-06, 1.2497e-04,\n",
            "        9.9975e-01, 4.5780e-01, 5.3089e-06, 1.1568e-05, 4.6826e-05, 3.5389e-05,\n",
            "        8.1783e-06, 2.7150e-04, 9.5941e-06, 3.9101e-06, 8.9367e-04, 6.1981e-04,\n",
            "        1.2735e-04, 1.4915e-05, 1.3950e-05, 9.2707e-05, 1.0660e-04, 9.0682e-05,\n",
            "        2.1293e-05, 3.2473e-03, 1.2191e-05, 3.7755e-05, 1.5350e-04, 1.6824e-03,\n",
            "        6.3249e-05, 6.9910e-06, 3.9101e-06, 4.0413e-04, 1.2767e-05, 1.0851e-04,\n",
            "        2.3056e-04, 4.1177e-05, 5.0740e-06, 8.9367e-04])\n",
            "Sum of predicted tensor(3.4702)\n",
            "Loss tensor(0.0634)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.0954e-06, 8.9367e-04, 2.3855e-06, 2.9726e-05, 1.0939e-05, 1.6548e-02,\n",
            "        9.9944e-01, 3.3926e-03, 9.3896e-03, 4.6264e-05, 2.0587e-05, 3.3039e-06,\n",
            "        8.9367e-04, 1.0952e-04, 2.8956e-05, 2.1157e-04, 5.4656e-05, 3.9101e-06,\n",
            "        8.9367e-04, 7.8514e-06, 1.4802e-05, 3.5384e-05, 2.8784e-02, 1.3095e-02,\n",
            "        2.1356e-04, 7.1403e-05, 1.9506e-05, 3.1865e-04, 1.0412e-05, 8.3953e-06,\n",
            "        1.4341e-05, 9.1807e-05, 3.9101e-06, 8.9367e-04, 2.1863e-05, 2.1157e-04,\n",
            "        8.9367e-04, 1.4586e-05, 8.9367e-04, 4.0571e-04, 4.3188e-05, 1.1041e-04,\n",
            "        8.9115e-06, 3.5118e-05, 6.9098e-01, 2.0315e-05, 7.7384e-05, 7.9200e-04,\n",
            "        8.9367e-04, 1.2501e-05, 3.5192e-04, 1.5861e-03, 2.1768e-03, 5.6610e-03,\n",
            "        3.4837e-05, 1.7349e-04, 1.5740e-04, 2.1863e-05, 4.3110e-05, 1.5857e-05,\n",
            "        8.9592e-01, 5.0124e-03, 1.4202e-03, 5.4513e-05])\n",
            "Sum of predicted tensor(2.6836)\n",
            "Loss tensor(0.5643)\n",
            "Acc tensor(0.8906)\n",
            "tensor([3.4790e-06, 1.4122e-04, 1.6593e-06, 3.6209e-05, 9.3047e-06, 4.8052e-05,\n",
            "        4.1095e-05, 4.2340e-05, 1.2918e-05, 2.5500e-03, 1.1815e-04, 1.0485e-05,\n",
            "        2.9496e-06, 1.4155e-04, 2.3710e-04, 2.3906e-05, 1.1940e-03, 7.9530e-05,\n",
            "        6.4324e-01, 2.0077e-05, 4.5132e-06, 9.4706e-01, 1.0674e-05, 9.8900e-01,\n",
            "        2.9606e-06, 1.9373e-05, 4.7772e-06, 4.4471e-03, 5.0936e-06, 4.6201e-06,\n",
            "        7.6913e-02, 1.6165e-03, 3.4475e-05, 3.0859e-06, 3.0571e-05, 3.2016e-06,\n",
            "        4.0883e-06, 5.3267e-06, 2.3472e-02, 1.1008e-05, 2.0747e-06, 6.3162e-06,\n",
            "        4.3689e-06, 3.0571e-05, 1.5939e-01, 7.0241e-05, 3.2662e-03, 9.4872e-01,\n",
            "        7.7269e-05, 3.8704e-06, 6.8836e-06, 4.6842e-06, 9.3756e-06, 6.5685e-03,\n",
            "        1.2977e-05, 5.4362e-06, 4.2066e-05, 3.6869e-06, 3.0064e-04, 3.0571e-05,\n",
            "        2.8349e-01, 3.4224e-05, 7.6643e-04, 4.2623e-06])\n",
            "Sum of predicted tensor(4.0935)\n",
            "Loss tensor(0.3453)\n",
            "Acc tensor(0.9219)\n",
            "tensor([5.9611e-04, 9.8869e-05, 3.2448e-06, 2.6573e-04, 1.3078e-02, 1.4410e-03,\n",
            "        1.4570e-03, 3.8625e-02, 6.0671e-01, 8.1096e-06, 8.8480e-05, 4.8810e-03,\n",
            "        9.1255e-06, 7.3206e-06, 6.5164e-06, 2.1891e-05, 2.0275e-03, 1.6682e-05,\n",
            "        8.8965e-04, 9.0682e-06, 2.4346e-02, 2.2743e-06, 8.2063e-06, 5.1704e-05,\n",
            "        7.1342e-04, 2.0747e-06, 4.6080e-05, 6.1401e-05, 4.3511e-03, 1.5894e-06,\n",
            "        5.0516e-06, 3.4624e-06, 2.2635e-03, 1.1581e-06, 9.8723e-04, 2.4530e-05,\n",
            "        2.9704e-06, 3.0005e-02, 7.5617e-05, 6.9467e-06, 1.2170e-04, 1.0168e-05,\n",
            "        6.0714e-06, 8.7085e-06, 6.7888e-06, 6.3084e-02, 1.8143e-05, 2.8097e-06,\n",
            "        7.7767e-04, 1.2527e-05, 1.3547e-05, 3.0321e-06, 1.3103e-05, 7.6643e-04,\n",
            "        7.2360e-03, 3.4700e-06, 1.1367e-04, 2.5686e-05, 5.2814e-05, 7.8312e-06,\n",
            "        2.8045e-06, 4.4244e-05, 5.3710e-06, 7.1256e-03])\n",
            "Sum of predicted tensor(0.8127)\n",
            "Loss tensor(0.0111)\n",
            "Acc tensor(1.)\n",
            "tensor([2.0373e-04, 1.4468e-06, 2.0905e-05, 6.4684e-06, 5.5089e-06, 1.5894e-06,\n",
            "        3.1556e-06, 6.2172e-06, 2.0986e-02, 7.2130e-05, 1.5894e-06, 3.3093e-01,\n",
            "        1.6254e-03, 2.5657e-05, 1.8584e-05, 1.6045e-04, 2.7951e-06, 4.8826e-05,\n",
            "        5.9937e-06, 7.0679e-05, 7.9881e-04, 9.5472e-05, 1.1041e-05, 8.8927e-06,\n",
            "        9.7948e-01, 1.2387e-05, 2.9332e-05, 1.8568e-05, 3.9439e-05, 1.5894e-06,\n",
            "        1.7197e-05, 8.1418e-05, 8.0498e-04, 7.5174e-06, 6.3779e-06, 9.8692e-06,\n",
            "        1.4247e-04, 3.6252e-06, 1.3955e-05, 2.9581e-05, 1.5894e-06, 4.0885e-06,\n",
            "        6.8266e-04, 2.4884e-05, 4.9571e-05, 2.7402e-05, 1.6238e-04, 4.0417e-04,\n",
            "        2.2657e-05, 3.5468e-05, 2.6245e-04, 9.0977e-02, 1.5894e-06, 3.5317e-04,\n",
            "        8.7328e-04, 2.7309e-05, 3.0344e-05, 3.4844e-05, 1.5894e-06, 5.6504e-05,\n",
            "        7.6643e-04, 5.3126e-06, 5.7491e-06, 1.0941e-05])\n",
            "Sum of predicted tensor(1.4306)\n",
            "Loss tensor(0.2180)\n",
            "Acc tensor(0.9688)\n",
            "tensor([3.2819e-06, 3.6505e-06, 4.5098e-06, 6.5329e-05, 1.3383e-06, 3.9255e-06,\n",
            "        1.3171e-06, 3.2825e-06, 1.4693e-06, 6.6688e-03, 1.5699e-06, 5.5477e-04,\n",
            "        4.4038e-06, 6.6095e-01, 7.2233e-04, 7.6643e-04, 2.9394e-06, 6.7024e-05,\n",
            "        8.2258e-05, 5.9743e-06, 1.5894e-06, 3.6869e-06, 3.5301e-05, 7.7191e-05,\n",
            "        1.6837e-05, 1.7765e-05, 1.6269e-03, 1.5894e-06, 1.8492e-04, 1.2387e-05,\n",
            "        4.7384e-05, 5.6316e-01, 1.4491e-05, 7.2840e-06, 4.9734e-05, 2.4484e-05,\n",
            "        9.0367e-06, 2.3336e-05, 2.5103e-06, 7.2372e-06, 2.7968e-04, 3.7563e-05,\n",
            "        5.8127e-03, 2.0747e-06, 8.1103e-05, 1.4385e-04, 7.0510e-04, 2.9607e-06,\n",
            "        6.4941e-06, 3.3130e-05, 1.9299e-03, 1.1027e-05, 1.3954e-04, 3.3458e-05,\n",
            "        3.2247e-02, 7.6643e-04, 3.0733e-06, 3.9040e-05, 1.0751e-05, 4.6059e-04,\n",
            "        1.5894e-06, 4.4794e-04, 3.8900e-05, 3.1553e-03])\n",
            "Sum of predicted tensor(1.2816)\n",
            "Loss tensor(0.1428)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.3207e-06, 3.8983e-06, 5.9240e-05, 8.4374e-05, 1.9290e-05, 4.9715e-05,\n",
            "        6.5312e-06, 1.7121e-05, 1.6555e-06, 9.8897e-01, 1.1463e-03, 9.4009e-02,\n",
            "        1.4723e-06, 2.2228e-05, 5.0348e-06, 1.7203e-05, 3.0068e-05, 1.4524e-05,\n",
            "        3.3192e-06, 5.2995e-05, 1.7705e-05, 9.3611e-06, 1.0152e-03, 1.4456e-05,\n",
            "        4.4898e-06, 2.7519e-02, 4.2418e-06, 2.4000e-06, 2.4000e-06, 2.2694e-06,\n",
            "        8.8270e-06, 2.0234e-06, 8.6863e-05, 1.4054e-06, 9.9042e-01, 1.4348e-06,\n",
            "        1.3057e-03, 1.3629e-04, 6.6419e-04, 5.2938e-05, 2.1397e-04, 7.3284e-04,\n",
            "        2.7215e-06, 1.1213e-05, 1.8376e-05, 7.7850e-06, 1.4226e-05, 2.6800e-06,\n",
            "        3.4451e-03, 2.7705e-05, 1.4693e-06, 1.0931e-05, 1.3710e-06, 7.7594e-05,\n",
            "        1.4693e-06, 1.8167e-01, 1.3384e-06, 1.3581e-06, 5.6690e-03, 1.2776e-06,\n",
            "        3.3192e-06, 1.2785e-06, 1.3655e-04, 1.5208e-06])\n",
            "Sum of predicted tensor(2.2978)\n",
            "Loss tensor(0.1719)\n",
            "Acc tensor(0.9531)\n",
            "tensor([9.4776e-06, 7.0733e-06, 4.3096e-04, 1.4693e-06, 3.4796e-06, 2.3482e-02,\n",
            "        2.4231e-05, 8.1778e-06, 5.1549e-06, 1.6649e-03, 1.3710e-06, 1.2154e-05,\n",
            "        1.3655e-04, 1.1463e-03, 2.5325e-06, 7.8085e-05, 1.0275e-03, 1.3605e-06,\n",
            "        5.5497e-01, 3.1409e-04, 1.5130e-02, 4.1983e-05, 2.1878e-05, 7.5975e-05,\n",
            "        2.4218e-06, 7.2821e-03, 2.7129e-06, 9.9962e-01, 1.3822e-06, 6.3231e-04,\n",
            "        2.4218e-06, 8.0348e-06, 1.1463e-03, 6.7788e-04, 1.3237e-06, 3.3192e-06,\n",
            "        1.3433e-05, 1.5208e-06, 2.7055e-04, 4.9689e-04, 1.7562e-04, 6.4181e-05,\n",
            "        2.8053e-06, 1.4693e-06, 3.6025e-06, 1.4290e-06, 1.5530e-05, 1.8376e-05,\n",
            "        1.0865e-01, 1.5714e-05, 1.3865e-05, 1.5359e-06, 1.3398e-06, 1.0238e-05,\n",
            "        8.1456e-06, 1.5208e-06, 7.7913e-01, 1.2547e-05, 1.2349e-04, 1.4693e-06,\n",
            "        1.0048e-05, 3.4365e-06, 1.7506e-05, 2.3766e-04])\n",
            "Sum of predicted tensor(2.4973)\n",
            "Loss tensor(0.2747)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.3490e-06, 9.6496e-06, 3.9457e-06, 2.4302e-05, 5.6522e-04, 2.0045e-06,\n",
            "        2.6132e-05, 1.4881e-06, 1.0329e-05, 2.6889e-04, 1.3672e-06, 1.9186e-05,\n",
            "        3.9520e-05, 3.2729e-05, 3.4566e-06, 2.0559e-02, 1.5674e-01, 1.3558e-06,\n",
            "        4.0815e-06, 2.5582e-06, 1.2348e-05, 7.8307e-06, 3.5335e-05, 4.9021e-05,\n",
            "        1.9958e-05, 1.6979e-05, 2.4573e-05, 1.1940e-04, 1.2854e-05, 1.5417e-06,\n",
            "        5.4690e-04, 3.3793e-04, 1.4265e-06, 2.0150e-04, 9.7098e-05, 1.5208e-06,\n",
            "        1.4115e-05, 2.2169e-04, 3.8247e-06, 5.0111e-05, 8.5667e-06, 1.0736e-05,\n",
            "        1.4693e-06, 4.6500e-01, 2.1758e-03, 9.9711e-01, 1.3041e-06, 3.3283e-06,\n",
            "        1.8524e-05, 1.3176e-06, 1.4759e-06, 5.4071e-06, 1.8307e-05, 1.6060e-05,\n",
            "        1.8767e-05, 3.2667e-03, 2.4460e-04, 4.6380e-05, 1.0073e-05, 7.6663e-06,\n",
            "        1.7629e-06, 1.1308e-03, 2.0102e-06, 9.2891e-06])\n",
            "Sum of predicted tensor(1.6492)\n",
            "Loss tensor(0.1702)\n",
            "Acc tensor(0.9688)\n",
            "tensor([5.8089e-05, 1.1427e-05, 1.4266e-06, 7.4635e-04, 1.1448e-05, 3.4860e-05,\n",
            "        1.3836e-05, 1.6019e-06, 8.0771e-05, 2.9755e-05, 2.4635e-04, 5.9464e-06,\n",
            "        3.1556e-03, 8.1765e-06, 1.1463e-03, 9.9955e-01, 1.1140e-04, 1.3427e-06,\n",
            "        5.5834e-01, 1.8719e-03, 2.9309e-05, 8.1268e-06, 1.8405e-05, 9.2962e-06,\n",
            "        3.3104e-01, 4.2425e-05, 2.3649e-05, 1.4693e-06, 1.0653e-04, 5.3404e-05,\n",
            "        4.5965e-05, 1.3414e-06, 1.4693e-06, 1.4338e-06, 1.2710e-06, 5.1021e-06,\n",
            "        2.2648e-02, 5.2995e-05, 1.3727e-06, 4.1564e-06, 1.5607e-05, 1.2527e-05,\n",
            "        4.8321e-05, 2.0824e-05, 1.5314e-06, 9.8792e-01, 4.4994e-04, 4.2793e-03,\n",
            "        2.5522e-05, 3.3192e-06, 2.4218e-06, 1.7529e-03, 2.1939e-06, 4.5212e-04,\n",
            "        2.7896e-05, 4.0332e-01, 4.6094e-05, 2.5466e-04, 2.5485e-05, 1.6060e-05,\n",
            "        1.8483e-04, 7.5594e-05, 1.1662e-05, 6.1619e-05])\n",
            "Sum of predicted tensor(3.3185)\n",
            "Loss tensor(0.2716)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.4693e-06, 1.4318e-06, 8.4146e-01, 1.0051e-01, 6.7274e-06, 1.4693e-06,\n",
            "        4.5610e-05, 1.4720e-06, 4.8124e-06, 2.5185e-06, 1.3433e-05, 1.1128e-04,\n",
            "        8.5013e-03, 1.4144e-06, 1.4158e-06, 8.0814e-06, 7.3763e-02, 2.8469e-06,\n",
            "        1.3865e-01, 3.2782e-04, 9.9688e-01, 3.4740e-01, 2.2683e-06, 9.3627e-01,\n",
            "        1.5634e-06, 1.6649e-04, 9.9719e-05, 3.3192e-06, 1.9632e-03, 9.9313e-01,\n",
            "        1.1253e-05, 7.6249e-04, 2.1438e-02, 7.9064e-01, 1.3727e-06, 8.4947e-06,\n",
            "        1.0990e-05, 4.5248e-06, 1.2872e-06, 8.7425e-05, 1.3433e-05, 4.0304e-06,\n",
            "        8.1893e-02, 1.8485e-06, 4.7034e-06, 3.3192e-06, 2.7740e-04, 1.6408e-05,\n",
            "        1.7729e-01, 5.2995e-05, 2.1730e-04, 4.6925e-05, 1.5088e-06, 5.7815e-05,\n",
            "        8.0767e-01, 1.3276e-06, 1.3417e-06, 1.4693e-06, 3.2532e-05, 1.5828e-02,\n",
            "        1.4007e-06, 2.3216e-04, 2.2439e-06, 2.2785e-06])\n",
            "Sum of predicted tensor(6.3360)\n",
            "Loss tensor(0.2569)\n",
            "Acc tensor(0.9219)\n",
            "tensor([8.8379e-01, 5.4211e-06, 1.5726e-03, 7.6710e-04, 1.3056e-06, 3.2532e-05,\n",
            "        3.1147e-05, 3.3192e-06, 6.9582e-05, 2.1662e-04, 6.3969e-06, 9.7350e-01,\n",
            "        7.3351e-05, 3.4916e-04, 9.9614e-01, 6.1354e-02, 4.0380e-05, 3.2737e-05,\n",
            "        1.6952e-05, 1.3497e-06, 3.6525e-06, 3.3192e-06, 1.7497e-05, 6.7979e-06,\n",
            "        2.3358e-05, 1.3088e-06, 7.0044e-06, 2.8694e-05, 4.2172e-04, 1.2768e-05,\n",
            "        5.3982e-06, 2.2389e-06, 6.4925e-05, 1.3391e-04, 1.4505e-04, 1.4693e-06,\n",
            "        1.4889e-06, 2.3982e-04, 5.2799e-06, 1.3971e-06, 9.1069e-06, 1.2099e-05,\n",
            "        2.2159e-01, 5.9252e-04, 2.1634e-05, 4.2179e-05, 9.3078e-06, 6.2597e-05,\n",
            "        1.4693e-06, 5.4081e-05, 1.1855e-05, 1.4693e-06, 2.8804e-05, 9.7544e-06,\n",
            "        1.3222e-06, 1.4879e-05, 9.9490e-01, 7.8701e-06, 1.3669e-06, 4.4066e-06,\n",
            "        1.4693e-06, 8.3355e-06, 3.3296e-05, 1.3939e-06])\n",
            "Sum of predicted tensor(4.1366)\n",
            "Loss tensor(0.2280)\n",
            "Acc tensor(0.9375)\n",
            "tensor([3.6252e-06, 7.1894e-06, 8.9330e-06, 5.5783e-06, 1.0733e-04, 6.0937e-06,\n",
            "        7.7346e-04, 9.6893e-01, 1.0849e-01, 4.0525e-05, 9.5697e-01, 1.4624e-04,\n",
            "        1.4012e-06, 7.3087e-05, 3.3432e-02, 6.1513e-05, 8.1896e-05, 1.8377e-05,\n",
            "        6.6143e-06, 1.7828e-05, 5.3692e-06, 5.5783e-06, 2.0466e-05, 3.7086e-05,\n",
            "        7.4423e-06, 2.6695e-06, 4.7513e-06, 5.7312e-06, 2.7919e-05, 1.1971e-03,\n",
            "        6.3976e-06, 3.5276e-05, 4.0440e-06, 1.6273e-05, 9.9968e-01, 1.6987e-05,\n",
            "        3.1677e-05, 4.3181e-01, 5.5783e-06, 4.7185e-02, 9.8051e-01, 1.2806e-06,\n",
            "        1.9505e-03, 5.5783e-06, 9.9743e-01, 1.1675e-05, 7.2686e-05, 5.5579e-05,\n",
            "        5.5783e-06, 6.2408e-06, 5.1033e-06, 1.8908e-05, 2.1718e-04, 2.5226e-06,\n",
            "        2.8515e-03, 1.5018e-06, 1.4113e-04, 4.6758e-03, 5.5783e-06, 5.5783e-06,\n",
            "        1.7012e-04, 2.7944e-05, 6.5460e-04, 5.5783e-06])\n",
            "Sum of predicted tensor(5.5381)\n",
            "Loss tensor(0.4768)\n",
            "Acc tensor(0.9062)\n",
            "tensor([6.8388e-03, 2.1132e-03, 2.9415e-04, 2.4298e-05, 6.1513e-05, 5.5783e-06,\n",
            "        2.8516e-05, 2.9336e-06, 5.5783e-06, 1.9080e-04, 1.3970e-04, 1.2758e-06,\n",
            "        2.2064e-05, 1.0275e-05, 1.5155e-06, 2.0810e-05, 2.4457e-05, 3.3619e-04,\n",
            "        4.9570e-04, 4.1353e-06, 8.6822e-06, 1.8601e-05, 3.6632e-06, 6.0239e-06,\n",
            "        1.6285e-05, 5.5783e-06, 1.4228e-06, 4.0015e-03, 2.9940e-04, 3.6430e-05,\n",
            "        2.9273e-04, 4.3044e-06, 8.4484e-05, 6.8098e-04, 1.4375e-06, 1.1530e-02,\n",
            "        4.1270e-05, 5.5783e-06, 3.7515e-06, 2.4393e-05, 2.7390e-06, 3.1259e-06,\n",
            "        4.4517e-06, 3.1062e-03, 6.1806e-03, 3.0133e-05, 4.3467e-06, 2.2710e-05,\n",
            "        2.8756e-02, 6.6037e-05, 2.9336e-06, 4.1209e-05, 3.6318e-06, 1.2942e-05,\n",
            "        9.9730e-01, 3.2619e-06, 9.1637e-01, 6.5608e-06, 2.0662e-05, 2.9871e-02,\n",
            "        8.5045e-05, 2.4401e-06, 7.3052e-06, 3.1308e-06])\n",
            "Sum of predicted tensor(2.0096)\n",
            "Loss tensor(0.2437)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1361e-05, 1.3957e-06, 5.5783e-06, 4.0334e-05, 2.4726e-05, 6.7885e-05,\n",
            "        4.0410e-05, 3.3276e-05, 5.1591e-05, 5.5783e-06, 3.0073e-06, 6.2861e-06,\n",
            "        1.0420e-05, 1.5195e-04, 1.0627e-01, 1.2817e-01, 4.0842e-05, 3.6876e-06,\n",
            "        6.7956e-06, 1.2724e-04, 1.5760e-06, 7.3462e-05, 7.9524e-06, 4.9138e-06,\n",
            "        2.7042e-05, 9.9976e-01, 5.5783e-06, 4.6751e-02, 1.2039e-04, 1.2758e-06,\n",
            "        2.1965e-05, 3.0230e-05, 6.3450e-06, 1.1985e-04, 5.5783e-06, 5.5783e-06,\n",
            "        3.1247e-06, 9.7133e-01, 1.6524e-05, 1.3861e-05, 3.1765e-06, 1.5453e-04,\n",
            "        5.7466e-01, 5.5783e-06, 1.1600e-03, 3.0385e-05, 3.3322e-06, 1.3591e-05,\n",
            "        5.5783e-06, 2.8062e-04, 1.0031e-05, 5.5783e-06, 9.4308e-04, 4.6536e-05,\n",
            "        3.9102e-02, 9.8635e-01, 1.5701e-05, 9.4229e-01, 2.3915e-06, 9.9981e-01,\n",
            "        6.0098e-06, 1.4626e-05, 1.8346e-05, 3.2123e-03])\n",
            "Sum of predicted tensor(5.8015)\n",
            "Loss tensor(0.2898)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.8741e-06, 5.5783e-06, 3.2911e-06, 1.1717e-05, 7.6720e-04, 2.5001e-06,\n",
            "        3.8007e-05, 2.7321e-05, 6.4693e-06, 6.6362e-05, 1.7496e-04, 1.8246e-06,\n",
            "        2.0990e-05, 5.1328e-05, 7.9862e-06, 5.5783e-06, 5.5783e-06, 3.7007e-06,\n",
            "        3.1849e-06, 6.1062e-06, 1.3283e-06, 2.5348e-05, 5.1961e-04, 1.5562e-06,\n",
            "        3.5079e-06, 6.2130e-06, 8.0030e-06, 6.6805e-06, 4.1657e-05, 3.2276e-06,\n",
            "        3.2514e-06, 1.2727e-05, 5.4543e-06, 6.0883e-06, 9.0820e-06, 2.2937e-05,\n",
            "        1.9835e-04, 5.4517e-06, 5.2021e-05, 2.0172e-06, 1.7767e-05, 6.5867e-05,\n",
            "        1.9874e-06, 1.9691e-04, 8.8060e-06, 3.7971e-05, 9.8811e-05, 5.5783e-06,\n",
            "        5.5783e-06, 9.1765e-06, 6.2904e-06, 9.0749e-01, 6.0633e-05, 9.8807e-01,\n",
            "        7.2766e-03, 1.7212e-05, 5.0398e-06, 1.5969e-05, 8.4322e-06, 5.7123e-03,\n",
            "        2.4717e-06, 1.2713e-04, 1.2758e-06, 1.2758e-06])\n",
            "Sum of predicted tensor(1.9114)\n",
            "Loss tensor(0.2659)\n",
            "Acc tensor(0.9531)\n",
            "tensor([8.5386e-06, 7.3275e-06, 7.2830e-06, 5.0894e-02, 5.5783e-06, 5.1961e-04,\n",
            "        1.4465e-06, 2.2899e-05, 4.2241e-05, 1.4106e-05, 3.3915e-05, 5.5783e-06,\n",
            "        5.9328e-06, 9.7710e-05, 5.1957e-06, 7.1302e-05, 1.2806e-06, 7.6490e-06,\n",
            "        3.7568e-06, 5.5783e-06, 4.0252e-06, 5.5783e-06, 3.6247e-06, 7.7537e-06,\n",
            "        3.8390e-03, 4.3601e-05, 1.0904e-06, 1.5049e-03, 4.8465e-03, 5.5783e-06,\n",
            "        1.3239e-05, 6.0502e-06, 5.5783e-06, 6.1591e-02, 4.0327e-05, 4.5655e-05,\n",
            "        1.3597e-06, 5.5783e-06, 9.9961e-01, 2.8921e-06, 2.2557e-05, 1.2710e-01,\n",
            "        2.2899e-05, 2.8052e-05, 2.0808e-06, 4.0660e-06, 5.5547e-06, 5.5783e-06,\n",
            "        1.0326e-04, 2.3613e-06, 4.6757e-03, 7.2502e-05, 1.5133e-06, 6.1761e-02,\n",
            "        5.5783e-06, 1.0011e-05, 1.7104e-03, 5.0927e-03, 5.6572e-03, 5.4737e-04,\n",
            "        1.4056e-01, 5.3030e-04, 6.8329e-06, 1.1544e-05])\n",
            "Sum of predicted tensor(1.4713)\n",
            "Loss tensor(0.3804)\n",
            "Acc tensor(0.9219)\n",
            "tensor([7.8001e-06, 1.3515e-03, 4.2338e-06, 5.5783e-06, 5.5783e-06, 4.1600e-05,\n",
            "        5.5783e-06, 2.6550e-06, 1.1238e-01, 1.8493e-03, 4.9885e-02, 2.2680e-05,\n",
            "        5.0226e-05, 2.1523e-06, 1.5906e-04, 4.7766e-03, 1.2539e-05, 3.9641e-05,\n",
            "        9.9967e-01, 2.3644e-06, 1.2721e-01, 1.0052e-01, 5.5783e-06, 8.0035e-05,\n",
            "        1.7368e-04, 5.3632e-06, 1.7697e-04, 2.0015e-05, 9.3559e-04, 3.1586e-06,\n",
            "        1.5447e-05, 2.1439e-05, 2.9980e-05, 9.6655e-06, 1.0213e-05, 7.9715e-05,\n",
            "        1.7197e-04, 1.3653e-04, 1.2221e-04, 1.4444e-05, 2.5136e-05, 1.8272e-03,\n",
            "        1.5514e-06, 4.6496e-06, 1.7050e-05, 2.0413e-04, 2.0627e-05, 1.5562e-06,\n",
            "        1.4146e-06, 5.5783e-06, 5.5783e-06, 7.7298e-06, 5.4377e-05, 3.1259e-03,\n",
            "        5.5247e-06, 3.6632e-06, 5.5783e-06, 1.0944e-05, 7.8462e-06, 3.4577e-06,\n",
            "        3.7946e-05, 7.4528e-03, 7.4452e-05, 4.6000e-04])\n",
            "Sum of predicted tensor(1.4134)\n",
            "Loss tensor(0.2003)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.3456e-05, 1.5705e-04, 2.1830e-06, 5.7697e-06, 5.3961e-06, 6.2054e-05,\n",
            "        1.3773e-01, 9.6487e-01, 9.9055e-02, 9.0806e-01, 6.3238e-05, 1.8873e-06,\n",
            "        6.0716e-05, 8.5635e-06, 3.9363e-06, 5.5603e-06, 5.7040e-05, 2.2864e-05,\n",
            "        1.4501e-05, 7.0482e-05, 8.0369e-05, 1.6865e-06, 3.1233e-04, 3.2965e-04,\n",
            "        2.2992e-05, 6.9922e-06, 3.0059e-06, 1.2983e-04, 4.1085e-05, 1.6649e-06,\n",
            "        3.3106e-06, 8.2990e-03, 9.9150e-06, 9.9484e-05, 2.6739e-03, 1.3303e-06,\n",
            "        1.1199e-05, 1.1958e-04, 4.6019e-04, 8.2392e-06, 5.5783e-06, 9.6889e-05,\n",
            "        3.1062e-03, 3.5232e-05, 5.6621e-05, 4.8716e-06, 5.5783e-06, 9.9976e-01,\n",
            "        2.0055e-05, 9.9976e-01, 3.1867e-06, 1.2719e-05, 1.3941e-06, 5.5783e-06,\n",
            "        1.4965e-03, 3.2632e-06, 1.0465e-04, 9.9993e-01, 6.5993e-05, 9.9976e-01,\n",
            "        7.6961e-01, 3.9581e-06, 1.7104e-03, 7.6092e-01])\n",
            "Sum of predicted tensor(7.6594)\n",
            "Loss tensor(0.3016)\n",
            "Acc tensor(0.9219)\n",
            "tensor([2.6551e-06, 9.9552e-01, 7.1037e-06, 7.7577e-06, 9.9937e-01, 4.2875e-02,\n",
            "        1.7629e-05, 2.0690e-05, 8.4064e-06, 8.4325e-06, 4.9228e-06, 1.4845e-05,\n",
            "        1.2780e-05, 2.0942e-06, 7.2683e-06, 2.1640e-06, 4.2381e-05, 2.3254e-04,\n",
            "        1.3258e-06, 6.1201e-06, 9.7941e-06, 5.0940e-05, 4.9876e-04, 2.1662e-06,\n",
            "        1.7731e-04, 4.3785e-06, 5.0551e-06, 1.8503e-03, 9.9936e-01, 9.9020e-01,\n",
            "        7.2473e-04, 9.4173e-01, 1.7102e-06, 1.6865e-06, 8.9906e-06, 1.8882e-02,\n",
            "        1.3560e-05, 3.3635e-05, 5.4690e-06, 2.2242e-05, 1.1373e-06, 4.9957e-06,\n",
            "        9.9937e-01, 2.9201e-04, 5.2416e-05, 1.4431e-05, 3.2949e-05, 8.6611e-05,\n",
            "        2.4506e-05, 1.2780e-05, 1.9748e-05, 4.3164e-06, 4.9692e-04, 2.9108e-06,\n",
            "        5.3601e-06, 7.0520e-06, 6.2203e-04, 1.1410e-05, 5.6845e-06, 1.5389e-04,\n",
            "        2.4344e-04, 1.5611e-05, 1.8710e-05, 8.3339e-05])\n",
            "Sum of predicted tensor(5.9933)\n",
            "Loss tensor(0.1179)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.4062e-06, 2.5781e-06, 8.7973e-05, 5.0429e-02, 3.7726e-01, 9.4051e-04,\n",
            "        1.9967e-06, 2.8384e-06, 3.4953e-04, 2.9843e-06, 1.9098e-05, 1.1575e-05,\n",
            "        4.8921e-04, 1.1526e-03, 1.5426e-05, 5.9445e-06, 8.6371e-06, 3.4504e-06,\n",
            "        1.2814e-05, 6.2864e-06, 2.2332e-05, 3.0336e-05, 9.9937e-01, 1.7655e-03,\n",
            "        9.8505e-01, 3.9146e-05, 6.1822e-06, 3.0742e-06, 9.9936e-01, 3.5050e-06,\n",
            "        1.9544e-04, 4.4937e-06, 1.3287e-05, 9.9868e-01, 6.3198e-06, 2.1095e-06,\n",
            "        5.1520e-06, 5.7517e-05, 6.9415e-05, 3.7726e-01, 1.1093e-05, 1.8863e-04,\n",
            "        3.3659e-05, 2.9592e-06, 1.9964e-05, 8.1869e-01, 4.4434e-02, 9.3764e-05,\n",
            "        2.7991e-05, 1.6499e-02, 2.4579e-04, 1.8905e-03, 6.3749e-06, 2.3669e-06,\n",
            "        7.3610e-05, 1.2487e-02, 1.8437e-01, 2.0381e-06, 1.2680e-05, 1.3839e-05,\n",
            "        1.3460e-05, 3.9146e-05, 2.0571e-03, 5.5434e-06])\n",
            "Sum of predicted tensor(5.8740)\n",
            "Loss tensor(0.1664)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.9589e-05, 3.4685e-03, 9.9393e-01, 6.7992e-06, 1.1241e-05, 3.2838e-06,\n",
            "        1.1343e-05, 8.5706e-06, 1.2700e-05, 4.3204e-04, 3.8443e-05, 7.0852e-05,\n",
            "        6.3206e-04, 5.3899e-05, 1.2598e-05, 1.2589e-05, 9.3668e-06, 2.1568e-05,\n",
            "        7.1800e-05, 3.7054e-06, 1.6865e-06, 3.7726e-01, 4.0361e-05, 2.9194e-05,\n",
            "        7.3453e-05, 1.8020e-04, 4.4242e-03, 5.7615e-05, 4.8886e-06, 2.8384e-06,\n",
            "        4.5059e-01, 2.4330e-04, 1.2919e-05, 2.8213e-03, 7.2575e-06, 8.1932e-06,\n",
            "        4.2394e-06, 5.8499e-06, 1.6024e-04, 2.1956e-02, 3.0090e-05, 9.9936e-01,\n",
            "        1.4237e-05, 3.5728e-06, 1.7456e-05, 1.9362e-06, 4.4448e-05, 1.6573e-05,\n",
            "        5.4690e-06, 3.4729e-05, 4.5481e-06, 3.4116e-06, 1.3460e-05, 9.9936e-01,\n",
            "        3.3675e-01, 7.1105e-06, 3.0868e-04, 9.9977e-01, 2.6269e-06, 2.8384e-06,\n",
            "        2.6439e-06, 9.9551e-01, 4.7670e-06, 1.2332e-03])\n",
            "Sum of predicted tensor(6.1892)\n",
            "Loss tensor(0.1162)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.3252e-06, 2.0944e-05, 2.7399e-05, 2.7132e-05, 2.1640e-06, 1.8812e-02,\n",
            "        4.4698e-05, 1.2251e-05, 4.7451e-05, 2.6194e-06, 3.9518e-05, 3.6568e-06,\n",
            "        4.3331e-06, 1.2448e-05, 9.6953e-06, 7.1080e-02, 1.4079e-03, 1.1579e-02,\n",
            "        1.0060e-05, 8.5053e-06, 3.1982e-01, 5.1832e-05, 7.3089e-01, 2.0628e-05,\n",
            "        9.9880e-01, 4.9228e-06, 1.4653e-05, 3.4195e-05, 2.0824e-03, 7.7112e-06,\n",
            "        1.0599e-04, 2.9251e-03, 3.0861e-06, 5.1016e-05, 3.7335e-06, 1.0219e-05,\n",
            "        4.4572e-04, 4.6130e-06, 1.0562e-03, 9.7031e-06, 5.3797e-06, 2.5368e-06,\n",
            "        3.3678e-05, 1.4358e-05, 1.6577e-06, 2.8266e-06, 2.8485e-03, 4.1705e-05,\n",
            "        1.4079e-03, 9.9936e-01, 1.8362e-06, 6.1395e-06, 7.8751e-06, 2.9050e-05,\n",
            "        4.6021e-06, 5.7061e-05, 9.3420e-05, 8.4073e-01, 5.6284e-06, 9.3169e-01,\n",
            "        2.6497e-05, 2.5746e-03, 3.7726e-01, 2.2784e-05])\n",
            "Sum of predicted tensor(5.3157)\n",
            "Loss tensor(0.3359)\n",
            "Acc tensor(0.9219)\n",
            "tensor([1.4358e-05, 1.3190e-05, 7.3070e-06, 1.8844e-05, 6.4098e-06, 9.5775e-06,\n",
            "        9.8697e-06, 6.4574e-01, 1.7886e-05, 3.2356e-05, 2.0013e-06, 9.9563e-06,\n",
            "        1.1456e-05, 7.3784e-03, 9.4430e-06, 6.9529e-06, 1.3784e-05, 2.5751e-05,\n",
            "        9.7629e-05, 9.9936e-01, 1.7786e-04, 1.9168e-06, 2.6551e-06, 1.6364e-05,\n",
            "        2.6551e-06, 1.2309e-05, 1.4896e-05, 8.5893e-06, 5.4241e-06, 1.1112e-05,\n",
            "        5.0418e-06, 6.2708e-06, 3.9150e-05, 1.5563e-06, 1.4689e-06, 1.0425e-03,\n",
            "        5.4690e-06, 4.0593e-05, 9.9892e-01, 9.9973e-01, 9.5092e-04, 1.7804e-04,\n",
            "        6.5974e-06, 5.9991e-06, 2.1963e-05, 7.2575e-06, 4.9692e-04, 1.0500e-05,\n",
            "        2.4090e-05, 1.7405e-05, 1.5355e-05, 9.9160e-01, 9.9882e-01, 1.6533e-06,\n",
            "        1.4666e-04, 5.0994e-03, 7.6159e-05, 4.1826e-01, 9.8365e-01, 8.8418e-01,\n",
            "        1.7792e-05, 3.0908e-02, 1.6231e-01, 2.4062e-06])\n",
            "Sum of predicted tensor(8.1296)\n",
            "Loss tensor(0.3797)\n",
            "Acc tensor(0.9062)\n",
            "tensor([9.1169e-05, 2.1413e-05, 7.9629e-06, 9.9759e-01, 3.6654e-03, 1.8441e-05,\n",
            "        3.0721e-05, 9.6237e-06, 1.1736e-05, 2.1574e-05, 2.6551e-06, 8.4574e-06,\n",
            "        1.6722e-05, 4.8303e-06, 1.6024e-04, 4.9888e-06, 5.2798e-05, 7.6229e-06,\n",
            "        9.9937e-01, 8.0814e-04, 2.1566e-06, 3.9456e-06, 1.9114e-05, 1.7519e-05,\n",
            "        1.2125e-04, 9.6708e-06, 1.9520e-04, 4.8298e-05, 9.5381e-04, 5.5732e-05,\n",
            "        7.9746e-06, 9.6852e-06, 2.0753e-05, 9.9548e-06, 8.0192e-06, 1.8918e-04,\n",
            "        6.6152e-05, 6.1873e-06, 4.8778e-06, 1.2818e-05, 4.6996e-06, 2.7513e-05,\n",
            "        1.4995e-04, 3.0534e-06, 7.1459e-06, 3.4245e-06, 5.7198e-06, 1.0188e-05,\n",
            "        7.9414e-06, 1.7659e-06, 1.4627e-05, 1.8432e-05, 3.1114e-06, 1.4939e-06,\n",
            "        2.7657e-06, 1.2070e-05, 5.4690e-06, 2.2512e-02, 6.7564e-03, 7.3537e-05,\n",
            "        3.2915e-06, 9.9936e-01, 7.0651e-06, 2.8017e-06])\n",
            "Sum of predicted tensor(3.0327)\n",
            "Loss tensor(0.0948)\n",
            "Acc tensor(0.9844)\n",
            "tensor([9.9900e-01, 7.0206e-06, 1.5638e-05, 5.2092e-05, 5.2302e-02, 1.0829e-04,\n",
            "        4.2764e-06, 1.2242e-04, 1.2591e-05, 5.3882e-06, 5.0738e-06, 3.1083e-02,\n",
            "        5.9095e-05, 1.0298e-04, 9.9936e-01, 9.9715e-01, 3.2910e-02, 2.1162e-06,\n",
            "        8.1889e-06, 5.2448e-06, 2.4045e-06, 1.7391e-06, 9.9089e-01, 1.9303e-05,\n",
            "        2.6643e-06, 9.9937e-01, 1.8951e-05, 9.8194e-04, 2.9270e-05, 9.9880e-01,\n",
            "        9.9936e-01, 8.7449e-06, 8.4385e-06, 9.8908e-01, 4.3117e-06, 3.9329e-05,\n",
            "        2.4227e-05, 1.2060e-05, 2.2509e-05, 6.3671e-04, 3.0275e-05, 9.9809e-01,\n",
            "        1.6023e-06, 2.9374e-06, 2.5798e-05, 3.8116e-04, 2.8935e-06, 2.1471e-05,\n",
            "        8.3977e-03, 1.6865e-06, 5.6676e-06, 1.0086e-05, 2.4351e-06, 9.0146e-06,\n",
            "        1.8863e-04, 3.2027e-02, 5.4690e-06, 2.1720e-05, 3.0232e-04, 1.1661e-05,\n",
            "        1.0252e-05, 6.8178e-05, 2.0771e-06, 1.3000e-05])\n",
            "Sum of predicted tensor(9.1313)\n",
            "Loss tensor(0.3894)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.8708e-04, 3.2043e-05, 8.5603e-06, 3.3147e-06, 6.5199e-06, 8.7330e-05,\n",
            "        9.9987e-01, 6.7130e-06, 2.2644e-06, 7.7927e-06, 4.8415e-02, 9.9984e-01,\n",
            "        1.0155e-05, 9.8037e-06, 6.2768e-06, 5.4853e-06, 9.9471e-01, 4.0340e-06,\n",
            "        6.1704e-05, 1.0099e-05, 9.9937e-01, 5.1947e-06, 1.6134e-06, 1.0741e-05,\n",
            "        1.4357e-05, 7.4613e-06, 9.8312e-06, 4.2650e-04, 1.8166e-04, 2.7885e-03,\n",
            "        4.9458e-06, 8.6637e-06, 7.5556e-06, 2.8970e-04, 7.1815e-03, 1.0474e-05,\n",
            "        1.5596e-04, 7.1423e-01, 1.1089e-04, 9.9437e-01, 1.0155e-04, 4.5380e-06,\n",
            "        2.4388e-06, 5.4066e-06, 1.1353e-04, 3.1920e-06, 6.9793e-06, 2.2817e-05,\n",
            "        6.2602e-06, 3.0320e-06, 4.4869e-06, 3.8572e-06, 1.2591e-05, 2.3351e-05,\n",
            "        9.2294e-06, 4.7200e-05, 3.2790e-06, 9.7384e-04, 4.9558e-06, 2.6530e-06,\n",
            "        6.1351e-05, 4.8240e-05, 1.7981e-03, 1.9837e-01])\n",
            "Sum of predicted tensor(5.9641)\n",
            "Loss tensor(0.1791)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.9257e-06, 3.9337e-05, 8.7993e-04, 6.9206e-05, 2.0394e-06, 3.2629e-06,\n",
            "        9.9946e-01, 9.9987e-01, 1.8661e-06, 1.4194e-05, 8.2467e-06, 7.1071e-06,\n",
            "        4.0686e-02, 2.4125e-05, 5.9415e-05, 4.7646e-05, 4.2054e-06, 4.1681e-05,\n",
            "        3.5064e-04, 2.3032e-05, 1.7237e-05, 7.4639e-01, 2.1090e-05, 6.7549e-06,\n",
            "        3.2790e-06, 5.1316e-06, 1.1775e-05, 5.1268e-06, 4.5078e-06, 1.3240e-01,\n",
            "        7.4559e-05, 4.3695e-06, 5.4488e-06, 6.1063e-06, 9.9946e-01, 1.8712e-06,\n",
            "        9.8798e-01, 1.4275e-06, 1.7511e-05, 8.0101e-06, 4.9558e-06, 1.7679e-05,\n",
            "        5.9273e-06, 9.9505e-01, 3.0299e-03, 1.2423e-05, 2.5102e-05, 2.8444e-06,\n",
            "        4.5925e-06, 9.9215e-01, 1.3936e-05, 9.9872e-01, 7.8137e-06, 2.9242e-06,\n",
            "        1.5092e-05, 2.2520e-02, 2.7570e-04, 9.4167e-06, 4.5361e-05, 2.7067e-05,\n",
            "        2.9217e-05, 9.9535e-04, 1.3256e-05, 5.3895e-05])\n",
            "Sum of predicted tensor(7.9211)\n",
            "Loss tensor(0.2614)\n",
            "Acc tensor(0.9531)\n",
            "tensor([7.5731e-06, 3.9720e-06, 1.2062e-05, 1.4867e-05, 5.1314e-06, 6.3588e-05,\n",
            "        3.1694e-06, 4.5961e-01, 1.6958e-05, 1.3658e-05, 2.9411e-06, 9.9902e-01,\n",
            "        4.6031e-04, 3.2790e-06, 1.7630e-04, 2.2643e-05, 1.5891e-05, 9.9846e-01,\n",
            "        1.4291e-05, 3.8100e-05, 3.9823e-06, 6.0973e-06, 1.9783e-02, 9.6789e-06,\n",
            "        1.4238e-05, 1.1298e-05, 8.7312e-06, 2.9735e-06, 2.0011e-06, 1.3385e-06,\n",
            "        9.9754e-01, 7.9766e-06, 9.8655e-01, 1.2401e-05, 4.0741e-01, 2.9158e-05,\n",
            "        9.3056e-06, 5.3011e-06, 6.6222e-02, 1.3922e-05, 9.9953e-01, 6.7420e-06,\n",
            "        1.6766e-04, 1.2473e-04, 3.1174e-06, 8.7043e-04, 1.7192e-05, 9.6710e-06,\n",
            "        6.8702e-06, 5.1800e-06, 2.0561e-05, 1.2715e-05, 1.4867e-05, 2.9301e-04,\n",
            "        2.2332e-06, 6.9248e-06, 6.9672e-05, 5.6894e-05, 2.5317e-05, 2.6665e-06,\n",
            "        1.2950e-05, 9.9993e-01, 1.6234e-05, 9.8849e-03])\n",
            "Sum of predicted tensor(6.9467)\n",
            "Loss tensor(0.2570)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9973e-01, 6.3452e-06, 3.7274e-06, 1.0702e-04, 2.0771e-05, 6.5131e-06,\n",
            "        9.9902e-01, 7.5906e-05, 4.5946e-06, 2.9086e-06, 1.2599e-05, 5.6253e-06,\n",
            "        1.9180e-05, 9.9604e-01, 2.3331e-06, 1.8290e-05, 2.5823e-05, 9.9259e-06,\n",
            "        1.1827e-03, 2.0234e-06, 1.5401e-06, 3.3236e-05, 1.6699e-06, 1.2146e-04,\n",
            "        3.8572e-06, 6.9546e-06, 2.5930e-04, 1.5784e-04, 1.7331e-04, 9.8493e-03,\n",
            "        1.1586e-03, 8.5567e-06, 2.4405e-06, 1.5493e-05, 9.9933e-01, 4.7231e-06,\n",
            "        1.4175e-05, 3.2790e-06, 2.1168e-06, 5.6612e-06, 9.9830e-01, 1.3343e-05,\n",
            "        9.9846e-01, 1.6692e-05, 1.1290e-04, 3.8440e-02, 1.6741e-05, 5.3756e-06,\n",
            "        2.4281e-06, 2.0423e-05, 7.6559e-02, 2.5490e-06, 3.2790e-06, 8.2458e-04,\n",
            "        1.5764e-06, 4.0350e-06, 1.0545e-05, 4.7300e-06, 1.8916e-02, 6.8006e-04,\n",
            "        2.1323e-04, 3.8512e-06, 7.8772e-06, 3.2790e-06])\n",
            "Sum of predicted tensor(6.1401)\n",
            "Loss tensor(0.6203)\n",
            "Acc tensor(0.9062)\n",
            "tensor([8.9183e-06, 5.2499e-03, 1.2929e-05, 1.0711e-05, 2.1625e-06, 6.7667e-05,\n",
            "        3.0785e-06, 1.1067e-03, 3.6818e-06, 7.9751e-01, 6.2768e-06, 2.8015e-06,\n",
            "        2.9239e-03, 1.6015e-06, 9.4823e-06, 7.1515e-04, 2.5378e-05, 1.5211e-06,\n",
            "        3.8192e-05, 3.3705e-05, 1.7632e-04, 1.3958e-05, 8.8585e-06, 1.2420e-04,\n",
            "        8.5381e-06, 7.9639e-06, 4.1606e-01, 7.7337e-06, 2.6570e-06, 4.9257e-06,\n",
            "        8.4712e-06, 4.4460e-06, 6.6825e-06, 6.0813e-02, 8.4902e-03, 3.1559e-05,\n",
            "        1.8811e-05, 7.6007e-06, 2.0182e-05, 4.8124e-06, 5.1251e-06, 1.8158e-05,\n",
            "        8.9135e-02, 1.1586e-04, 1.3244e-06, 1.4911e-05, 7.1785e-06, 1.0540e-03,\n",
            "        1.2226e-06, 1.3427e-03, 9.8249e-01, 2.8278e-05, 3.0644e-04, 3.7372e-01,\n",
            "        5.6180e-02, 3.1230e-05, 8.2668e-06, 6.0221e-06, 5.9849e-05, 1.6237e-05,\n",
            "        1.3017e-05, 1.0455e-03, 1.1348e-05, 2.4402e-05])\n",
            "Sum of predicted tensor(2.7992)\n",
            "Loss tensor(0.0942)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.9499e-06, 8.9420e-06, 2.2076e-05, 5.2866e-05, 1.3764e-04, 1.1837e-05,\n",
            "        3.8024e-06, 3.7115e-06, 7.7927e-06, 3.0691e-06, 8.4753e-06, 7.8900e-06,\n",
            "        1.4042e-05, 3.9653e-05, 1.0163e-03, 3.3206e-06, 2.4426e-03, 1.0594e-05,\n",
            "        1.8666e-06, 1.2069e-05, 3.7415e-06, 4.8078e-05, 8.7884e-06, 1.4495e-05,\n",
            "        1.1885e-05, 5.4062e-05, 6.1196e-05, 1.1790e-05, 7.6651e-05, 8.0544e-04,\n",
            "        4.4385e-05, 1.3097e-05, 2.7095e-06, 2.6570e-06, 3.2143e-05, 4.8694e-06,\n",
            "        6.4969e-06, 4.4118e-03, 1.4331e-05, 6.9564e-06, 2.1795e-02, 4.8385e-06,\n",
            "        7.2711e-06, 6.1030e-06, 4.2718e-05, 2.2650e-02, 8.6600e-06, 1.0138e-05,\n",
            "        3.9643e-06, 2.7879e-04, 2.2400e-06, 1.6615e-04, 2.2546e-05, 4.8701e-06,\n",
            "        4.3292e-06, 8.9506e-04, 8.1621e-06, 1.0060e-05, 5.2054e-05, 9.2029e-06,\n",
            "        3.7412e-01, 1.2580e-05, 1.4062e-02, 3.2790e-06])\n",
            "Sum of predicted tensor(0.4436)\n",
            "Loss tensor(0.0679)\n",
            "Acc tensor(0.9844)\n",
            "tensor([7.6418e-06, 1.4447e-05, 3.9530e-05, 1.8708e-05, 8.9609e-06, 4.6385e-03,\n",
            "        1.8825e-03, 9.9426e-01, 6.5234e-06, 3.2790e-06, 9.9434e-02, 9.9558e-01,\n",
            "        2.8129e-06, 1.1569e-05, 3.7196e-06, 2.8199e-03, 1.1393e-05, 6.0189e-06,\n",
            "        1.4867e-05, 7.9811e-05, 1.8274e-05, 8.3360e-05, 1.2015e-04, 1.4762e-02,\n",
            "        8.7215e-06, 7.7150e-05, 3.5218e-06, 9.9950e-01, 6.3028e-04, 4.7312e-05,\n",
            "        1.1285e-05, 5.6845e-06, 1.7331e-04, 4.0688e-06, 2.8171e-04, 2.8444e-06,\n",
            "        5.8361e-05, 3.0536e-06, 3.0202e-06, 1.8219e-04, 5.9358e-06, 2.8640e-06,\n",
            "        8.7278e-06, 6.0149e-01, 2.1641e-05, 4.8327e-06, 8.1353e-06, 2.4647e-04,\n",
            "        1.2777e-03, 9.9539e-01, 1.7739e-05, 1.1306e-05, 6.6285e-05, 9.6137e-06,\n",
            "        1.9160e-05, 8.7038e-06, 9.7952e-05, 1.5738e-05, 1.4232e-01, 7.6570e-06,\n",
            "        2.3457e-05, 1.2411e-04, 1.9670e-05, 7.4739e-06])\n",
            "Sum of predicted tensor(4.8560)\n",
            "Loss tensor(0.1870)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1442e-05, 7.3871e-02, 3.9683e-05, 1.3456e-04, 2.8762e-05, 1.9016e-05,\n",
            "        9.1558e-01, 8.6149e-06, 8.0504e-06, 1.3790e-06, 8.7567e-01, 2.9380e-06,\n",
            "        4.9271e-05, 9.8038e-01, 1.6601e-06, 9.2070e-01, 3.7165e-02, 1.6955e-04,\n",
            "        2.2314e-05, 6.7632e-06, 1.5729e-06, 7.5153e-06, 1.4139e-05, 3.8661e-05,\n",
            "        1.9330e-05, 3.9557e-05, 3.8073e-06, 4.3751e-06, 1.9475e-03, 1.4194e-05,\n",
            "        9.2417e-06, 2.9899e-05, 9.2555e-05, 2.7612e-06, 9.9846e-01, 1.0989e-05,\n",
            "        9.9930e-01, 4.9447e-05, 8.4602e-06, 6.9189e-06, 3.6116e-06, 8.3393e-06,\n",
            "        6.6344e-04, 7.2090e-06, 2.5790e-05, 1.7550e-05, 9.9705e-01, 9.9846e-01,\n",
            "        1.7157e-05, 4.1772e-06, 1.5382e-05, 3.2627e-04, 2.8444e-06, 3.0289e-04,\n",
            "        1.0204e-05, 3.0487e-05, 1.7153e-06, 1.8486e-06, 3.3738e-06, 7.5260e-06,\n",
            "        1.6462e-06, 1.4881e-06, 4.8099e-06, 3.2790e-06])\n",
            "Sum of predicted tensor(7.8009)\n",
            "Loss tensor(0.1437)\n",
            "Acc tensor(0.9531)\n",
            "tensor([5.7574e-06, 5.9152e-02, 9.9977e-01, 4.4572e-06, 4.3145e-06, 2.4118e-02,\n",
            "        6.9751e-06, 1.2868e-05, 8.2632e-01, 1.8726e-03, 2.4001e-06, 3.3300e-04,\n",
            "        5.6089e-06, 1.0795e-05, 2.0565e-06, 6.5346e-06, 1.0181e-05, 3.4626e-01,\n",
            "        5.8247e-06, 2.1362e-06, 4.7091e-06, 5.3983e-06, 2.6391e-06, 1.9807e-04,\n",
            "        2.2507e-05, 1.2789e-04, 2.0789e-05, 5.7782e-05, 1.1801e-02, 1.5236e-06,\n",
            "        6.6929e-06, 3.4939e-05, 9.8583e-05, 3.2128e-06, 4.4102e-02, 1.7811e-06,\n",
            "        5.5325e-06, 4.4858e-06, 8.3263e-06, 4.7991e-06, 9.9961e-01, 3.8698e-05,\n",
            "        2.1340e-05, 4.6658e-04, 5.1915e-05, 1.3843e-05, 6.0731e-01, 1.5887e-06,\n",
            "        1.1345e-05, 1.2440e-05, 4.1132e-06, 1.1850e-05, 5.1158e-06, 3.4351e-05,\n",
            "        3.0252e-04, 7.3841e-06, 2.8222e-05, 3.0090e-05, 9.9985e-01, 3.1455e-05,\n",
            "        3.5583e-06, 1.3354e-06, 4.3852e-06, 3.6880e-06])\n",
            "Sum of predicted tensor(4.9223)\n",
            "Loss tensor(0.0941)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.3744e-05, 1.2083e-05, 3.9459e-02, 1.3315e-04, 2.1187e-06, 1.4121e-02,\n",
            "        2.2174e-04, 1.2200e-05, 2.6079e-06, 1.2160e-03, 3.5244e-06, 7.6291e-06,\n",
            "        2.8675e-03, 7.6112e-06, 8.9835e-06, 9.1786e-06, 1.1818e-06, 5.5108e-04,\n",
            "        5.9683e-06, 1.9152e-05, 4.5451e-05, 3.9649e-06, 3.7301e-06, 4.3469e-06,\n",
            "        1.2400e-05, 1.6384e-03, 5.5388e-06, 1.7702e-05, 4.8281e-06, 2.0225e-06,\n",
            "        2.7777e-06, 4.2454e-06, 5.7609e-06, 1.3456e-04, 2.4044e-04, 1.2507e-05,\n",
            "        1.0674e-05, 5.7616e-05, 6.5458e-06, 1.3240e-05, 3.6810e-06, 1.2176e-05,\n",
            "        2.1061e-06, 1.4505e-06, 1.1661e-05, 1.9938e-01, 3.4035e-06, 2.7383e-06,\n",
            "        1.8239e-02, 4.4246e-06, 3.2047e-06, 1.7786e-04, 3.2796e-05, 1.4909e-01,\n",
            "        2.7778e-06, 5.2574e-06, 7.3930e-06, 7.7036e-06, 7.3363e-06, 1.1889e-05,\n",
            "        1.4702e-05, 5.1814e-05, 9.9900e-01, 9.9769e-01])\n",
            "Sum of predicted tensor(2.4247)\n",
            "Loss tensor(0.2100)\n",
            "Acc tensor(0.9375)\n",
            "tensor([5.4289e-06, 5.0523e-06, 4.4858e-06, 6.0949e-06, 2.0602e-03, 5.2404e-06,\n",
            "        6.3861e-06, 6.4423e-05, 9.9932e-01, 6.1652e-04, 1.2770e-05, 5.2022e-06,\n",
            "        6.9330e-06, 2.7051e-06, 7.6777e-06, 1.6227e-06, 7.3714e-05, 1.2463e-05,\n",
            "        1.2918e-05, 7.1636e-06, 5.1577e-02, 2.5905e-06, 3.7172e-06, 7.0679e-05,\n",
            "        1.0943e-05, 5.2886e-06, 1.8493e-05, 3.5439e-06, 7.5716e-05, 2.2446e-05,\n",
            "        9.8283e-01, 5.3934e-05, 1.5276e-05, 1.5937e-05, 1.5374e-05, 3.7706e-06,\n",
            "        1.2851e-05, 3.7242e-06, 2.7347e-02, 1.2200e-02, 2.3069e-05, 5.9620e-03,\n",
            "        7.6542e-06, 4.2056e-03, 3.3316e-05, 5.9850e-06, 4.5331e-06, 1.0925e-02,\n",
            "        1.1414e-06, 2.0955e-04, 1.1569e-05, 2.5557e-03, 4.6009e-06, 1.6277e-06,\n",
            "        8.7824e-06, 6.8904e-01, 3.4001e-06, 8.6032e-06, 2.5908e-05, 4.3564e-02,\n",
            "        3.2406e-06, 6.9528e-01, 9.9780e-01, 3.2420e-05])\n",
            "Sum of predicted tensor(4.5262)\n",
            "Loss tensor(0.3783)\n",
            "Acc tensor(0.9219)\n",
            "tensor([9.9000e-01, 7.6152e-04, 1.9910e-06, 7.9015e-06, 1.2653e-05, 4.4458e-06,\n",
            "        1.3609e-04, 1.5759e-05, 9.9884e-01, 1.6623e-05, 2.4391e-05, 1.5360e-03,\n",
            "        5.7969e-06, 8.9873e-06, 2.0456e-06, 9.1885e-05, 9.4570e-01, 7.1610e-06,\n",
            "        1.9150e-05, 5.3528e-06, 1.1828e-02, 2.8796e-03, 7.6152e-06, 8.1126e-02,\n",
            "        5.2839e-05, 7.4085e-05, 1.6508e-05, 1.9582e-05, 4.6682e-05, 4.4253e-05,\n",
            "        5.1553e-06, 1.4573e-05, 7.7863e-06, 1.2038e-05, 4.8510e-04, 9.9990e-01,\n",
            "        2.5384e-06, 2.6599e-06, 4.0177e-04, 3.1766e-02, 9.4507e-01, 1.4566e-05,\n",
            "        7.3240e-06, 2.1564e-06, 7.2078e-04, 9.6419e-01, 1.0172e-05, 7.5078e-06,\n",
            "        8.2127e-06, 1.0853e-05, 2.7167e-02, 2.1078e-02, 3.3811e-02, 7.7205e-06,\n",
            "        2.0671e-06, 8.7509e-04, 6.8098e-04, 2.1587e-05, 5.3608e-06, 1.1407e-04,\n",
            "        6.8558e-06, 6.4330e-05, 3.8843e-06, 7.5047e-06])\n",
            "Sum of predicted tensor(6.0598)\n",
            "Loss tensor(0.1601)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.1351e-02, 4.0348e-06, 1.0478e-05, 2.5489e-05, 1.7935e-05, 4.0830e-06,\n",
            "        1.5127e-06, 3.9622e-04, 3.0464e-06, 1.8641e-05, 5.7058e-06, 2.5548e-04,\n",
            "        3.9264e-04, 1.1802e-04, 1.0745e-05, 7.5996e-06, 6.7405e-06, 9.9954e-01,\n",
            "        1.6123e-06, 9.9923e-01, 4.0064e-06, 2.9357e-06, 3.3809e-04, 2.9128e-05,\n",
            "        2.1485e-05, 1.7465e-05, 9.0513e-06, 7.9449e-01, 3.1819e-04, 6.7331e-06,\n",
            "        2.0446e-05, 6.2902e-06, 2.6125e-06, 6.7463e-05, 1.5455e-06, 8.5069e-06,\n",
            "        9.9963e-01, 2.2929e-06, 9.5625e-06, 8.9428e-05, 1.3558e-05, 5.4094e-06,\n",
            "        1.0490e-05, 2.2189e-05, 5.2404e-06, 9.9954e-01, 3.0874e-06, 4.7273e-06,\n",
            "        9.8115e-01, 5.1056e-01, 1.4531e-05, 3.6149e-06, 7.3841e-06, 7.7841e-05,\n",
            "        3.1726e-06, 5.0903e-06, 3.9142e-06, 2.5790e-04, 1.5998e-06, 5.0946e-04,\n",
            "        2.2210e-04, 1.1697e-02, 1.5950e-05, 2.3823e-03])\n",
            "Sum of predicted tensor(6.3130)\n",
            "Loss tensor(0.2146)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.2209e-05, 1.9582e-05, 9.9976e-01, 2.8155e-02, 2.0807e-05, 6.2759e-06,\n",
            "        9.9929e-01, 2.4800e-05, 2.1118e-06, 4.4701e-06, 4.5165e-06, 1.6536e-06,\n",
            "        5.5401e-06, 1.2764e-06, 2.3499e-06, 2.5857e-05, 9.6710e-06, 1.8637e-05,\n",
            "        2.1228e-06, 4.5127e-06, 1.1012e-05, 2.1516e-06, 4.3441e-06, 5.9839e-03,\n",
            "        5.4849e-05, 3.5263e-06, 1.0090e-05, 4.6505e-06, 1.0898e-04, 1.7024e-06,\n",
            "        2.8632e-06, 9.9954e-01, 1.5076e-05, 3.6916e-06, 1.9127e-06, 1.3120e-05,\n",
            "        2.4074e-05, 9.7400e-01, 1.5706e-04, 3.3205e-05, 6.9648e-01, 1.8306e-04,\n",
            "        3.2636e-06, 9.9965e-01, 4.0437e-05, 3.2796e-05, 3.7709e-06, 3.7467e-05,\n",
            "        2.8986e-05, 7.1574e-06, 8.3555e-06, 9.9850e-01, 8.7203e-05, 9.6958e-06,\n",
            "        6.5746e-06, 2.4391e-05, 2.1558e-04, 7.1631e-06, 1.0418e-04, 9.9938e-01,\n",
            "        1.3764e-05, 3.2680e-06, 9.9883e-01, 9.9889e-01])\n",
            "Sum of predicted tensor(9.6999)\n",
            "Loss tensor(0.1213)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.9332e-01, 6.8216e-06, 2.4823e-06, 2.2069e-05, 1.7523e-05, 2.1207e-04,\n",
            "        4.8975e-06, 2.1417e-06, 1.3465e-05, 4.3644e-06, 2.4391e-05, 9.9855e-01,\n",
            "        8.7307e-06, 1.5370e-05, 7.5181e-06, 6.7399e-06, 3.6714e-05, 6.2363e-05,\n",
            "        6.5376e-06, 3.7718e-06, 5.7785e-06, 9.9974e-01, 7.8823e-06, 9.9584e-01,\n",
            "        3.7214e-06, 5.5916e-06, 2.8275e-02, 1.8426e-06, 3.4855e-06, 2.6184e-02,\n",
            "        1.5588e-05, 9.9278e-01, 6.2073e-06, 2.8163e-06, 1.3818e-05, 2.0239e-05,\n",
            "        9.4975e-05, 1.8037e-06, 1.9582e-05, 3.6697e-06, 9.9639e-01, 2.8270e-06,\n",
            "        8.0018e-05, 3.3999e-05, 2.1634e-06, 9.9278e-01, 7.4390e-06, 9.9228e-06,\n",
            "        9.8287e-06, 3.3896e-05, 2.0070e-06, 2.9068e-05, 6.2354e-06, 8.1928e-01,\n",
            "        2.1227e-05, 3.3438e-02, 6.2812e-06, 6.9627e-05, 2.9455e-05, 5.1813e-05,\n",
            "        7.5078e-06, 7.6154e-01, 1.3202e-06, 4.9880e-05])\n",
            "Sum of predicted tensor(8.6392)\n",
            "Loss tensor(0.1953)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.5312e-05, 4.5957e-06, 3.3393e-05, 7.9942e-06, 7.4694e-06, 6.3779e-06,\n",
            "        4.0794e-05, 4.2072e-04, 7.9820e-06, 1.3829e-06, 2.8778e-04, 7.7940e-06,\n",
            "        4.3412e-05, 1.1019e-06, 2.4819e-06, 5.0752e-06, 1.2606e-05, 1.2775e-05,\n",
            "        1.8297e-05, 2.3614e-06, 3.5372e-06, 3.5652e-05, 9.1678e-06, 2.7031e-05,\n",
            "        3.3855e-06, 2.2714e-06, 9.9967e-01, 7.7029e-03, 1.3873e-05, 4.9391e-06,\n",
            "        3.0995e-06, 9.3976e-06, 7.6126e-06, 1.9936e-06, 9.1566e-06, 5.3115e-03,\n",
            "        1.2592e-05, 3.8315e-04, 1.1445e-06, 1.4817e-05, 2.4595e-06, 6.6149e-06,\n",
            "        5.0903e-06, 1.1016e-04, 6.7922e-05, 3.0753e-06, 1.9932e-05, 9.2864e-05,\n",
            "        1.9593e-05, 1.3697e-05, 1.8550e-03, 3.4607e-06, 2.2676e-06, 2.6935e-05,\n",
            "        1.2598e-05, 2.2796e-03, 1.3538e-04, 7.8053e-06, 3.1561e-06, 3.2796e-05,\n",
            "        3.7350e-01, 3.7671e-06, 1.4322e-05, 3.6834e-06])\n",
            "Sum of predicted tensor(1.3924)\n",
            "Loss tensor(0.0835)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.4492e-06, 1.4942e-06, 1.4014e-05, 5.3658e-05, 5.8044e-06, 1.7371e-06,\n",
            "        5.5989e-06, 2.1082e-06, 1.1483e-05, 9.4869e-06, 8.4017e-01, 1.2438e-05,\n",
            "        4.4034e-06, 6.8944e-06, 9.3734e-06, 1.9003e-06, 1.6730e-05, 3.4872e-06,\n",
            "        2.3592e-06, 7.4245e-01, 9.9890e-01, 8.8641e-05, 1.3701e-05, 1.1629e-05,\n",
            "        1.6695e-06, 6.5475e-06, 4.8147e-03, 7.1173e-06, 4.4749e-06, 1.7011e-05,\n",
            "        3.8305e-04, 3.7789e-06, 7.9639e-03, 6.5608e-06, 3.2268e-06, 7.3859e-06,\n",
            "        2.8556e-06, 1.8853e-06, 9.9996e-01, 5.5615e-06, 8.7830e-06, 5.0623e-06,\n",
            "        1.4329e-04, 4.2335e-05, 7.9632e-06, 1.6630e-03, 1.9701e-06, 1.6466e-06,\n",
            "        4.5333e-06, 6.6127e-05, 1.9249e-05, 3.4185e-06, 4.9552e-06, 9.9891e-01,\n",
            "        5.9542e-05, 2.1302e-05, 5.5659e-06, 3.7021e-06, 8.5371e-06, 1.8383e-05,\n",
            "        2.3437e-05, 2.1645e-06, 5.2449e-03, 7.1335e-01])\n",
            "Sum of predicted tensor(5.3146)\n",
            "Loss tensor(0.1271)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.1967e-04, 3.7944e-06, 4.2665e-06, 1.8989e-04, 2.3144e-05, 9.9894e-01,\n",
            "        9.9377e-06, 4.4169e-06, 1.3288e-03, 9.8808e-06, 4.0497e-05, 7.9113e-06,\n",
            "        1.3575e-06, 5.0026e-06, 1.4646e-05, 6.1274e-06, 6.7774e-06, 8.8259e-02,\n",
            "        1.2773e-05, 5.6160e-06, 8.3744e-05, 3.9316e-06, 1.5157e-05, 4.3711e-04,\n",
            "        1.7008e-06, 2.2712e-06, 1.1770e-05, 2.4141e-05, 1.0582e-06, 4.9617e-06,\n",
            "        2.2585e-05, 7.5277e-04, 4.6568e-06, 9.0224e-04, 3.5903e-06, 2.3380e-05,\n",
            "        5.0409e-06, 2.9266e-06, 2.1871e-06, 3.3997e-06, 9.4690e-06, 2.3550e-06,\n",
            "        1.8838e-05, 2.3794e-06, 4.2372e-06, 3.9407e-02, 5.0290e-05, 3.7985e-04,\n",
            "        1.4328e-05, 2.7115e-05, 1.0133e-05, 1.1700e-05, 3.3366e-05, 2.8407e-05,\n",
            "        1.6177e-05, 4.7121e-06, 1.1734e-05, 1.3555e-05, 9.1140e-06, 1.7594e-06,\n",
            "        2.0774e-06, 3.1249e-06, 1.0741e-04, 2.9020e-05])\n",
            "Sum of predicted tensor(1.1317)\n",
            "Loss tensor(0.1761)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.7797e-06, 3.4056e-06, 4.4685e-06, 8.2712e-06, 2.5431e-06, 2.9231e-06,\n",
            "        6.6484e-03, 5.0439e-03, 3.3149e-06, 7.1571e-06, 5.8161e-05, 4.7723e-06,\n",
            "        5.3167e-05, 2.6691e-06, 2.2939e-01, 4.0097e-04, 1.2590e-05, 1.5843e-05,\n",
            "        2.6627e-06, 4.2417e-04, 2.8255e-06, 5.2315e-06, 6.6454e-06, 9.9911e-01,\n",
            "        9.9927e-01, 2.6389e-05, 7.9790e-06, 7.0417e-06, 1.4657e-03, 9.1140e-06,\n",
            "        2.4927e-05, 1.2870e-05, 6.8155e-06, 9.1786e-06, 1.4262e-06, 2.6974e-05,\n",
            "        1.8533e-05, 1.2452e-05, 1.3678e-05, 1.0103e-05, 6.9677e-06, 3.6279e-02,\n",
            "        1.9334e-06, 9.9906e-01, 1.7016e-04, 7.3334e-06, 8.9281e-05, 6.9062e-06,\n",
            "        1.3391e-05, 3.1515e-06, 8.9515e-06, 1.3758e-06, 1.6865e-04, 8.9401e-04,\n",
            "        9.2452e-05, 4.8541e-06, 1.5560e-04, 2.8388e-06, 8.2039e-06, 1.6865e-04,\n",
            "        9.6262e-01, 5.4778e-05, 2.7380e-05, 2.1712e-04])\n",
            "Sum of predicted tensor(4.2422)\n",
            "Loss tensor(0.1075)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.6328e-06, 1.3360e-05, 7.0053e-06, 4.9715e-06, 4.9150e-05, 3.9573e-06,\n",
            "        9.9902e-01, 8.0371e-06, 7.4797e-06, 2.6604e-06, 7.4130e-05, 6.6608e-06,\n",
            "        3.3467e-06, 1.5440e-05, 6.8358e-01, 1.8669e-06, 2.0258e-06, 2.1553e-06,\n",
            "        6.5068e-03, 1.5154e-04, 7.3080e-02, 1.5369e-06, 6.6562e-06, 7.0279e-06,\n",
            "        5.8109e-05, 5.7005e-04, 1.2103e-03, 1.6697e-06, 2.9606e-05, 5.1447e-06,\n",
            "        9.3667e-06, 1.9600e-06, 6.1447e-06, 2.8349e-05, 2.5204e-06, 3.0403e-05,\n",
            "        1.3209e-05, 1.1253e-05, 3.2672e-06, 6.0438e-05, 4.7717e-06, 5.1578e-06,\n",
            "        9.6723e-06, 1.5633e-06, 9.1091e-01, 2.7243e-06, 1.6001e-03, 3.6176e-04,\n",
            "        4.8403e-06, 6.0125e-05, 1.9362e-06, 4.9067e-06, 5.4440e-06, 9.8148e-06,\n",
            "        2.2672e-05, 4.8607e-06, 1.8752e-05, 1.0161e-01, 2.0474e-04, 8.7126e-06,\n",
            "        2.7729e-05, 6.4673e-06, 4.5967e-06, 1.3828e-05])\n",
            "Sum of predicted tensor(2.7795)\n",
            "Loss tensor(0.1734)\n",
            "Acc tensor(0.9844)\n",
            "tensor([2.0067e-04, 4.7377e-06, 3.2149e-06, 7.1266e-06, 1.1926e-05, 1.4920e-06,\n",
            "        3.8863e-06, 1.7012e-05, 2.7822e-04, 2.2611e-05, 6.4175e-06, 1.5464e-06,\n",
            "        1.2202e-05, 1.3435e-05, 3.5975e-06, 4.0604e-06, 9.1966e-05, 3.5177e-05,\n",
            "        1.9971e-04, 9.8302e-01, 1.4636e-04, 9.9877e-01, 9.9935e-01, 3.1135e-06,\n",
            "        1.1591e-02, 1.3539e-05, 2.4370e-06, 1.7310e-05, 4.6681e-02, 6.1584e-06,\n",
            "        9.9945e-01, 4.6829e-06, 8.7680e-06, 4.0374e-04, 5.4631e-06, 1.6423e-05,\n",
            "        4.1744e-06, 6.8653e-06, 1.0618e-03, 1.9589e-05, 3.7389e-04, 9.9607e-01,\n",
            "        5.4637e-06, 4.2113e-06, 1.8726e-05, 3.1677e-05, 3.0480e-05, 1.9701e-06,\n",
            "        9.6662e-06, 9.9887e-01, 1.4054e-03, 6.1672e-06, 4.0064e-06, 1.2578e-05,\n",
            "        6.0675e-05, 1.0992e-01, 6.7502e-03, 9.6344e-03, 1.4080e-05, 2.9446e-05,\n",
            "        4.6685e-06, 9.9582e-06, 2.8345e-04, 9.9899e-01])\n",
            "Sum of predicted tensor(7.1640)\n",
            "Loss tensor(0.4938)\n",
            "Acc tensor(0.9062)\n",
            "tensor([4.4859e-06, 6.7522e-06, 1.8918e-04, 4.1744e-06, 4.2760e-06, 6.7423e-05,\n",
            "        3.8611e-06, 7.8538e-06, 6.3112e-06, 9.9403e-06, 4.7743e-06, 2.1171e-06,\n",
            "        2.0626e-04, 8.4130e-06, 6.7844e-01, 6.3072e-06, 1.0853e-03, 5.1447e-06,\n",
            "        2.1998e-04, 2.0117e-05, 7.5312e-06, 1.2622e-05, 1.0894e-05, 4.8370e-06,\n",
            "        3.3645e-06, 3.2072e-04, 1.4496e-05, 2.3440e-04, 2.2306e-04, 1.6031e-04,\n",
            "        9.0878e-01, 1.1642e-05, 1.4660e-02, 2.9164e-06, 1.1201e-05, 3.2302e-06,\n",
            "        8.4881e-06, 3.0592e-06, 7.5209e-02, 1.4599e-05, 9.0580e-06, 6.8155e-06,\n",
            "        3.3206e-06, 4.6417e-06, 4.3337e-05, 3.6412e-06, 2.0932e-05, 9.8300e-01,\n",
            "        3.0208e-06, 4.6110e-05, 5.2298e-06, 3.4041e-05, 3.3139e-06, 1.0880e-03,\n",
            "        1.3282e-05, 4.1951e-06, 3.8146e-06, 2.7075e-05, 2.7746e-06, 3.2430e-06,\n",
            "        1.3244e-05, 2.6349e-06, 8.5211e-05, 6.0076e-06])\n",
            "Sum of predicted tensor(2.6644)\n",
            "Loss tensor(0.3981)\n",
            "Acc tensor(0.9219)\n",
            "tensor([9.9514e-04, 8.1721e-06, 1.9254e-06, 4.9651e-06, 9.4989e-06, 4.0706e-05,\n",
            "        2.5212e-05, 2.5709e-01, 2.4596e-02, 7.9249e-01, 3.1216e-06, 5.3728e-06,\n",
            "        8.6538e-06, 2.7371e-06, 5.8973e-06, 4.4217e-05, 7.9470e-06, 2.0393e-05,\n",
            "        3.1959e-05, 1.8627e-04, 1.1700e-05, 1.9081e-05, 7.8153e-06, 1.6698e-04,\n",
            "        3.3667e-06, 1.2626e-01, 3.4898e-06, 1.4930e-06, 1.1989e-05, 1.6090e-06,\n",
            "        3.4898e-06, 4.3150e-06, 3.5247e-02, 8.0557e-06, 3.2226e-06, 3.9469e-05,\n",
            "        1.1187e-05, 1.9796e-05, 4.2794e-06, 2.4944e-05, 3.1247e-06, 2.1333e-05,\n",
            "        9.7652e-01, 1.6632e-04, 1.9765e-06, 3.9013e-06, 2.0572e-04, 1.7033e-06,\n",
            "        1.9701e-06, 1.6217e-06, 1.3461e-05, 1.6466e-06, 1.2461e-05, 2.0455e-05,\n",
            "        1.9807e-06, 5.3031e-06, 5.6118e-06, 5.9165e-06, 5.9077e-06, 7.8915e-04,\n",
            "        2.9432e-05, 1.2537e-02, 9.9868e-01, 3.2171e-06])\n",
            "Sum of predicted tensor(3.2265)\n",
            "Loss tensor(0.2844)\n",
            "Acc tensor(0.9531)\n",
            "tensor([2.6199e-05, 1.3616e-05, 1.4748e-06, 3.8902e-06, 4.0212e-06, 1.0556e-05,\n",
            "        7.0920e-06, 5.5835e-06, 1.7267e-06, 1.6752e-05, 6.7727e-05, 1.1139e-05,\n",
            "        1.5843e-05, 3.1019e-06, 7.1663e-05, 3.0961e-06, 1.0766e-05, 8.3506e-06,\n",
            "        3.4918e-02, 3.6360e-03, 1.4655e-06, 6.0841e-05, 3.1815e-06, 1.7960e-05,\n",
            "        6.6279e-05, 1.3277e-05, 3.4489e-06, 2.8669e-06, 3.1531e-06, 3.5054e-06,\n",
            "        4.7082e-06, 5.0602e-06, 2.4295e-05, 9.3511e-05, 1.9297e-03, 1.7693e-06,\n",
            "        8.0947e-06, 1.4484e-05, 2.6482e-05, 1.0321e-05, 9.7651e-01, 5.9645e-06,\n",
            "        2.4643e-06, 2.1861e-05, 3.3307e-06, 2.3354e-05, 2.6295e-06, 9.3370e-05,\n",
            "        2.7704e-06, 2.0688e-05, 5.4202e-06, 1.7345e-03, 2.5141e-04, 3.4921e-06,\n",
            "        3.8568e-05, 5.9362e-06, 2.6945e-05, 3.7878e-06, 1.4118e-05, 2.3492e-05,\n",
            "        5.0189e-06, 8.1242e-06, 4.1790e-02, 1.9701e-06])\n",
            "Sum of predicted tensor(1.0617)\n",
            "Loss tensor(0.1517)\n",
            "Acc tensor(0.9844)\n",
            "tensor([8.7025e-01, 2.8679e-06, 5.7346e-06, 1.5962e-03, 3.4549e-05, 6.7754e-04,\n",
            "        1.3214e-05, 5.2494e-05, 1.9446e-06, 1.2898e-05, 1.9697e-05, 3.9663e-05,\n",
            "        1.4732e-04, 2.8939e-02, 1.3329e-05, 4.2001e-06, 1.2600e-05, 1.3663e-05,\n",
            "        1.6524e-04, 9.5663e-06, 2.3092e-06, 7.9360e-06, 2.4641e-06, 1.0491e-05,\n",
            "        3.0018e-06, 2.2160e-06, 8.1831e-06, 8.3776e-06, 3.7317e-06, 1.4620e-04,\n",
            "        2.8706e-05, 7.3426e-06, 2.2459e-05, 2.4909e-04, 1.9213e-05, 9.9095e-01,\n",
            "        4.1701e-06, 2.6520e-01, 8.5417e-05, 4.9114e-05, 3.8858e-06, 6.2099e-06,\n",
            "        5.3925e-04, 3.7504e-05, 6.7147e-04, 2.4747e-05, 2.5652e-05, 4.9797e-06,\n",
            "        1.2283e-04, 4.5804e-05, 1.6536e-05, 1.2694e-05, 2.8369e-06, 2.9624e-06,\n",
            "        9.9860e-01, 1.1693e-05, 3.5258e-05, 7.9501e-05, 2.3333e-02, 5.8273e-05,\n",
            "        1.0815e-04, 6.2292e-06, 2.8397e-06, 8.2480e-06])\n",
            "Sum of predicted tensor(3.1826)\n",
            "Loss tensor(0.3816)\n",
            "Acc tensor(0.9375)\n",
            "tensor([3.4748e-05, 4.7430e-06, 1.5613e-05, 1.9103e-05, 5.3211e-06, 6.4298e-06,\n",
            "        7.2726e-06, 5.3379e-06, 1.0820e-05, 2.2825e-06, 1.2354e-06, 2.3705e-06,\n",
            "        1.8104e-06, 9.5361e-05, 7.2230e-05, 1.6447e-05, 4.1702e-06, 1.2629e-05,\n",
            "        1.0449e-05, 9.6737e-06, 2.2241e-04, 2.1414e-03, 8.3331e-05, 1.6421e-05,\n",
            "        1.9103e-05, 3.7963e-06, 2.7072e-04, 7.6282e-06, 7.4384e-06, 1.7445e-05,\n",
            "        6.2099e-06, 2.0560e-05, 6.1566e-05, 5.0460e-06, 5.8727e-06, 1.8554e-06,\n",
            "        2.1143e-05, 8.9584e-05, 8.9507e-06, 2.6094e-06, 2.5015e-04, 3.2901e-06,\n",
            "        6.1648e-04, 2.9317e-05, 5.4674e-06, 1.9871e-05, 1.5804e-04, 2.2489e-05,\n",
            "        6.2099e-06, 4.6378e-06, 1.0711e-04, 1.9604e-06, 8.9131e-06, 1.7321e-04,\n",
            "        8.8567e-06, 5.0622e-06, 3.0723e-05, 8.1420e-06, 2.2959e-06, 2.3665e-05,\n",
            "        3.2320e-06, 1.0875e-04, 5.2672e-06, 1.7049e-05])\n",
            "Sum of predicted tensor(0.0050)\n",
            "Loss tensor(7.7721e-05)\n",
            "Acc tensor(1.)\n",
            "tensor([4.8142e-01, 8.2877e-06, 6.5247e-06, 1.8619e-05, 5.3211e-06, 4.7276e-06,\n",
            "        9.9252e-06, 4.9359e-06, 1.3342e-05, 4.8516e-06, 1.3465e-02, 5.3211e-06,\n",
            "        8.7712e-06, 6.0894e-06, 9.9974e-01, 1.7034e-06, 9.7447e-06, 3.2223e-06,\n",
            "        8.0491e-04, 5.5211e-06, 1.1752e-04, 3.1002e-06, 2.4720e-06, 1.9714e-06,\n",
            "        4.1450e-06, 1.4077e-05, 1.2978e-04, 8.1380e-06, 9.3506e-06, 1.4397e-05,\n",
            "        2.2888e-06, 8.4578e-01, 1.9209e-05, 7.7101e-06, 9.7539e-01, 1.5105e-05,\n",
            "        4.6487e-01, 1.4113e-04, 9.9643e-01, 1.4217e-04, 8.9507e-06, 2.3048e-06,\n",
            "        3.5074e-06, 6.5854e-04, 1.4747e-06, 6.5674e-06, 9.3752e-01, 4.7199e-06,\n",
            "        3.0277e-06, 1.2833e-02, 5.6356e-06, 9.4932e-06, 4.2030e-06, 1.3505e-05,\n",
            "        4.5477e-03, 6.7564e-05, 4.3994e-06, 1.2567e-01, 4.4485e-06, 3.2718e-06,\n",
            "        1.0490e-03, 1.7987e-06, 1.5377e-05, 5.3211e-06])\n",
            "Sum of predicted tensor(5.8611)\n",
            "Loss tensor(0.3679)\n",
            "Acc tensor(0.8906)\n",
            "tensor([3.2382e-06, 9.4985e-05, 2.1059e-05, 9.9894e-01, 8.2174e-06, 9.9955e-01,\n",
            "        6.2986e-05, 1.1434e-05, 3.5878e-06, 9.9950e-01, 9.9121e-01, 4.3878e-06,\n",
            "        2.0727e-06, 1.3384e-05, 7.0858e-06, 6.8349e-06, 7.1391e-05, 8.4127e-06,\n",
            "        1.4904e-06, 2.6907e-06, 7.9964e-06, 2.7719e-06, 2.0269e-06, 2.2666e-06,\n",
            "        9.9126e-05, 2.1341e-06, 1.5088e-04, 2.3725e-01, 9.9958e-01, 2.2627e-06,\n",
            "        9.7959e-04, 9.6161e-05, 9.6882e-06, 4.3310e-05, 2.3883e-06, 1.9697e-05,\n",
            "        9.9941e-01, 9.9902e-01, 2.7328e-05, 4.0455e-03, 3.2660e-06, 2.7931e-01,\n",
            "        6.1487e-06, 2.7939e-05, 9.9932e-01, 5.5059e-05, 1.1236e-05, 8.1390e-06,\n",
            "        7.3534e-05, 3.1765e-06, 1.9559e-02, 2.0092e-03, 1.0660e-05, 4.7244e-04,\n",
            "        5.7257e-03, 1.6987e-05, 1.3721e-04, 1.2928e-05, 2.3369e-05, 1.7270e-06,\n",
            "        4.6503e-05, 2.6558e-04, 3.8557e-06, 2.1815e-06])\n",
            "Sum of predicted tensor(8.5374)\n",
            "Loss tensor(0.4916)\n",
            "Acc tensor(0.9375)\n",
            "tensor([7.6410e-06, 2.8453e-06, 2.7648e-06, 1.2311e-02, 2.6219e-06, 2.5166e-06,\n",
            "        1.1948e-05, 1.8897e-05, 1.8010e-05, 3.2813e-06, 2.7492e-02, 3.1104e-03,\n",
            "        2.8627e-06, 1.0407e-04, 5.1342e-05, 3.9433e-05, 5.3932e-06, 9.9783e-01,\n",
            "        2.5139e-06, 1.0484e-05, 3.9942e-06, 1.0390e-05, 6.7331e-06, 6.6058e-06,\n",
            "        1.0470e-05, 6.9760e-05, 4.8295e-06, 4.6214e-06, 3.4615e-03, 1.9246e-05,\n",
            "        6.1472e-06, 2.4738e-06, 1.2545e-05, 5.6954e-06, 9.9820e-01, 1.7141e-06,\n",
            "        1.8954e-05, 1.5693e-05, 4.4661e-02, 9.9699e-01, 2.6477e-06, 2.5039e-05,\n",
            "        1.2011e-05, 3.1765e-06, 1.4990e-03, 4.4960e-01, 2.6717e-06, 5.6049e-05,\n",
            "        5.7894e-04, 9.1074e-05, 2.3139e-06, 1.8349e-06, 9.9928e-01, 9.6313e-06,\n",
            "        3.7077e-05, 2.2478e-05, 2.0012e-06, 2.8121e-06, 6.2541e-06, 4.4038e-06,\n",
            "        5.3211e-06, 4.7687e-06, 4.5266e-06, 7.6699e-06])\n",
            "Sum of predicted tensor(4.5358)\n",
            "Loss tensor(0.1097)\n",
            "Acc tensor(0.9844)\n",
            "tensor([5.6071e-06, 5.2798e-06, 4.5501e-06, 4.4343e-06, 1.2790e-01, 1.7394e-06,\n",
            "        8.8710e-05, 7.6072e-06, 2.3845e-05, 8.2824e-06, 9.9977e-01, 5.3211e-06,\n",
            "        5.2392e-06, 9.9633e-01, 9.8658e-03, 3.8945e-06, 3.6866e-06, 6.8682e-04,\n",
            "        5.6174e-06, 2.8721e-06, 9.9455e-01, 1.5604e-06, 1.1130e-04, 7.5964e-06,\n",
            "        8.8353e-06, 1.1832e-05, 3.2764e-06, 4.0526e-06, 2.4072e-05, 9.6085e-05,\n",
            "        2.8569e-05, 1.3298e-06, 5.5035e-05, 1.4900e-05, 5.4620e-05, 2.8068e-06,\n",
            "        5.3232e-06, 2.6494e-06, 1.9451e-06, 3.9151e-06, 1.8145e-01, 1.5964e-06,\n",
            "        6.2099e-06, 7.8387e-01, 1.8437e-05, 1.1364e-03, 4.3511e-05, 1.1978e-05,\n",
            "        4.4933e-04, 9.0877e-06, 2.9786e-05, 1.3590e-05, 2.6844e-05, 1.3982e-05,\n",
            "        4.5957e-05, 1.8892e-05, 4.8825e-06, 5.4128e-04, 1.6368e-05, 1.5304e-05,\n",
            "        5.5972e-06, 8.6281e-06, 3.1338e-05, 2.5198e-05])\n",
            "Sum of predicted tensor(4.0975)\n",
            "Loss tensor(0.0531)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.8149e-05, 9.9138e-01, 1.6276e-04, 1.3162e-05, 4.7019e-05, 2.8562e-05,\n",
            "        3.4016e-06, 1.0882e-05, 2.0466e-06, 8.3595e-06, 4.6035e-06, 3.0374e-06,\n",
            "        6.1191e-06, 1.6421e-04, 5.7529e-06, 8.9998e-06, 9.9909e-01, 4.1410e-02,\n",
            "        1.8500e-06, 4.4897e-06, 4.8400e-06, 8.9139e-06, 2.4792e-05, 6.2606e-03,\n",
            "        8.4894e-05, 4.1783e-06, 9.9978e-01, 4.8839e-06, 5.2018e-06, 9.9594e-01,\n",
            "        3.2106e-06, 1.5426e-03, 5.3211e-06, 2.2263e-05, 2.4130e-06, 1.8922e-06,\n",
            "        1.1238e-05, 9.9881e-01, 9.9864e-01, 1.2842e-04, 4.6114e-02, 7.6270e-02,\n",
            "        4.4880e-06, 3.3106e-04, 8.6471e-02, 2.7574e-05, 6.1940e-04, 5.6242e-05,\n",
            "        1.6017e-06, 2.3792e-06, 6.0799e-06, 2.4326e-05, 1.5782e-05, 2.2317e-03,\n",
            "        5.4384e-06, 8.3461e-05, 4.6775e-06, 9.6872e-01, 1.2656e-05, 9.9901e-01,\n",
            "        7.3208e-06, 7.0341e-04, 9.9121e-01, 2.7173e-06])\n",
            "Sum of predicted tensor(9.2056)\n",
            "Loss tensor(0.2340)\n",
            "Acc tensor(0.9688)\n",
            "tensor([3.2320e-06, 2.3186e-05, 1.1734e-04, 9.4022e-03, 1.7014e-06, 1.1988e-05,\n",
            "        7.8297e-01, 9.1439e-06, 2.4292e-06, 2.0241e-05, 6.7615e-06, 2.5164e-06,\n",
            "        7.7296e-06, 1.7972e-04, 4.6036e-06, 2.5690e-06, 5.1667e-06, 8.1157e-06,\n",
            "        1.5656e-04, 8.3007e-06, 5.2526e-04, 4.2195e-06, 3.2245e-05, 5.8324e-06,\n",
            "        4.9018e-05, 7.6624e-06, 4.2456e-04, 5.0016e-04, 6.7345e-06, 2.7864e-05,\n",
            "        4.6078e-06, 5.7026e-06, 1.6859e-05, 2.6571e-06, 1.9998e-05, 5.0683e-06,\n",
            "        7.8264e-05, 9.2115e-06, 1.5908e-03, 6.5289e-06, 5.6252e-06, 3.5176e-06,\n",
            "        1.8236e-05, 9.3743e-05, 1.4918e-06, 1.0148e-04, 2.0491e-05, 1.6918e-06,\n",
            "        6.4912e-06, 3.3237e-06, 8.2683e-05, 1.9301e-06, 9.7772e-01, 1.1682e-04,\n",
            "        4.9152e-05, 4.5025e-05, 1.0044e-04, 1.4995e-05, 3.1822e-06, 1.0676e-05,\n",
            "        3.5663e-05, 3.2222e-06, 6.8398e-04, 9.9814e-04])\n",
            "Sum of predicted tensor(1.7764)\n",
            "Loss tensor(0.2820)\n",
            "Acc tensor(0.9531)\n",
            "tensor([3.1468e-04, 2.0472e-05, 1.5319e-04, 6.6183e-03, 9.7909e-06, 2.0927e-06,\n",
            "        2.9162e-06, 3.2300e-06, 4.4567e-04, 9.8796e-01, 5.9722e-06, 1.4487e-05,\n",
            "        4.3302e-04, 3.1554e-04, 3.4221e-06, 4.2257e-06, 1.0540e-05, 8.4054e-06,\n",
            "        2.2866e-06, 8.7712e-03, 2.0620e-06, 1.4661e-04, 8.2689e-06, 2.2957e-06,\n",
            "        8.7668e-06, 4.3121e-01, 2.3865e-06, 6.3192e-06, 2.5897e-04, 5.5510e-06,\n",
            "        3.3685e-04, 2.4903e-05, 2.5717e-04, 1.8895e-06, 2.3292e-06, 5.2683e-06,\n",
            "        3.9738e-05, 4.1042e-06, 8.5369e-04, 8.3007e-06, 7.6893e-06, 6.5366e-06,\n",
            "        4.3991e-06, 3.9246e-05, 1.0094e-05, 1.4239e-05, 9.9766e-01, 6.7194e-06,\n",
            "        3.1299e-06, 8.8422e-06, 3.0165e-05, 2.6134e-05, 1.2758e-04, 1.6471e-05,\n",
            "        1.5400e-05, 1.5981e-05, 5.0072e-05, 2.3549e-05, 2.0400e-05, 8.2528e-03,\n",
            "        6.3538e-03, 7.7931e-05, 3.7728e-05, 1.1279e-04])\n",
            "Sum of predicted tensor(2.4512)\n",
            "Loss tensor(0.2932)\n",
            "Acc tensor(0.9688)\n",
            "tensor([1.5832e-05, 1.1745e-04, 7.7604e-06, 1.3304e-02, 4.8051e-06, 2.9079e-06,\n",
            "        1.2940e-05, 2.9479e-06, 9.9902e-01, 6.3704e-05, 1.1632e-05, 2.9701e-06,\n",
            "        2.5679e-04, 6.4926e-04, 6.9280e-06, 4.6580e-06, 1.9771e-05, 1.5663e-03,\n",
            "        2.4088e-05, 1.6443e-01, 5.9372e-06, 9.6887e-06, 3.1724e-05, 2.6864e-05,\n",
            "        6.1582e-06, 1.1431e-05, 2.4176e-06, 9.9972e-01, 3.0617e-03, 2.7113e-06,\n",
            "        3.4766e-04, 1.0886e-05, 4.8010e-05, 3.6076e-06, 1.0819e-05, 2.0131e-04,\n",
            "        9.9972e-01, 4.6972e-06, 2.3430e-06, 2.0837e-05, 3.3676e-05, 2.6287e-05,\n",
            "        2.7317e-04, 1.0250e-05, 9.9892e-01, 9.9941e-01, 3.4308e-06, 1.4521e-05,\n",
            "        1.0832e-05, 9.9983e-01, 4.4250e-06, 1.3910e-02, 9.1033e-06, 1.2307e-05,\n",
            "        1.1842e-05, 8.8039e-06, 2.5105e-04, 4.6459e-05, 1.6049e-01, 2.3085e-05,\n",
            "        5.8371e-06, 5.8093e-06, 2.8801e-01, 4.8849e-06])\n",
            "Sum of predicted tensor(6.6441)\n",
            "Loss tensor(0.1515)\n",
            "Acc tensor(0.9688)\n",
            "tensor([7.1524e-06, 1.1862e-05, 1.2183e-05, 2.5965e-05, 1.1604e-05, 8.3007e-06,\n",
            "        1.2104e-05, 9.9982e-01, 5.7456e-06, 3.1285e-06, 4.3960e-06, 8.4977e-06,\n",
            "        4.8006e-06, 8.3007e-06, 7.8562e-06, 2.2192e-05, 2.6789e-04, 4.5882e-04,\n",
            "        1.4847e-06, 3.6004e-06, 4.7036e-03, 3.6378e-06, 3.2871e-06, 1.3668e-03,\n",
            "        5.7899e-02, 7.1786e-06, 9.6651e-06, 1.4816e-06, 9.5384e-06, 3.5291e-06,\n",
            "        7.1992e-06, 2.0587e-06, 6.6758e-06, 1.1377e-05, 1.3215e-05, 2.8899e-06,\n",
            "        5.6206e-03, 2.3558e-05, 1.8273e-04, 1.4212e-05, 5.6670e-06, 8.3007e-06,\n",
            "        1.2883e-04, 1.7622e-05, 6.4677e-06, 7.7562e-06, 2.8771e-06, 4.2318e-04,\n",
            "        1.0630e-01, 3.2791e-04, 6.4985e-06, 2.4291e-06, 4.7298e-05, 9.9838e-01,\n",
            "        5.3756e-04, 3.0283e-06, 5.2891e-05, 1.1398e-04, 9.9989e-01, 8.9561e-06,\n",
            "        6.5461e-06, 2.7420e-04, 9.7296e-06, 4.5010e-06])\n",
            "Sum of predicted tensor(3.1772)\n",
            "Loss tensor(0.1199)\n",
            "Acc tensor(0.9688)\n",
            "tensor([9.5737e-02, 6.1969e-03, 1.7053e-04, 3.7233e-06, 2.2866e-06, 8.8611e-06,\n",
            "        2.4655e-05, 2.3408e-03, 9.9936e-01, 2.9697e-06, 4.5920e-06, 6.7014e-06,\n",
            "        8.9916e-06, 2.1084e-04, 4.7879e-06, 2.9788e-05, 1.5772e-05, 1.3232e-05,\n",
            "        1.6228e-05, 5.4412e-05, 9.9977e-01, 1.0280e-04, 1.6680e-06, 5.1196e-06,\n",
            "        3.4408e-05, 2.4030e-05, 1.7924e-04, 1.3745e-05, 1.1229e-05, 1.5370e-06,\n",
            "        2.1915e-06, 4.3110e-06, 1.9323e-06, 8.3007e-06, 1.6787e-05, 1.9320e-05,\n",
            "        2.7513e-06, 6.2805e-06, 1.2630e-05, 4.4771e-05, 8.3007e-06, 1.4170e-05,\n",
            "        1.3215e-05, 5.0979e-06, 8.0644e-06, 9.9136e-06, 3.2190e-04, 7.2609e-05,\n",
            "        3.6349e-02, 8.6005e-06, 6.4082e-06, 4.8050e-06, 5.2161e-06, 4.6078e-06,\n",
            "        2.2144e-05, 9.2994e-06, 1.4959e-05, 6.2772e-06, 1.5100e-04, 3.2190e-04,\n",
            "        1.7760e-06, 1.0017e-02, 2.0187e-05, 8.3007e-06])\n",
            "Sum of predicted tensor(2.1519)\n",
            "Loss tensor(0.2755)\n",
            "Acc tensor(0.9688)\n",
            "tensor([2.6949e-06, 1.2334e-05, 9.6138e-04, 8.0566e-01, 9.9622e-01, 8.9997e-06,\n",
            "        4.2318e-04, 3.8992e-06, 2.6740e-05, 7.6076e-06, 6.7341e-05, 6.5869e-01,\n",
            "        2.0290e-06, 4.5817e-06, 2.4236e-05, 5.2387e-02, 5.5524e-06, 8.1936e-05,\n",
            "        4.3790e-06, 2.8385e-05, 3.5881e-05, 7.8974e-05, 2.3829e-03, 1.9860e-06,\n",
            "        1.9254e-06, 1.5222e-06, 2.5754e-05, 4.0490e-06, 1.9287e-05, 1.6520e-06,\n",
            "        2.4698e-06, 9.6960e-04, 4.9304e-06, 4.1005e-03, 1.8336e-05, 1.6304e-05,\n",
            "        2.0557e-06, 4.9018e-05, 1.0178e-05, 4.4359e-05, 2.7684e-06, 4.2172e-05,\n",
            "        2.4734e-05, 3.5529e-05, 3.2057e-05, 9.3998e-01, 4.1552e-06, 1.9971e-06,\n",
            "        1.0980e-05, 1.9970e-05, 3.1691e-03, 2.0651e-06, 3.6349e-02, 8.3007e-06,\n",
            "        9.5903e-03, 8.8463e-06, 3.8281e-02, 4.2015e-06, 1.9287e-06, 1.9887e-04,\n",
            "        4.7416e-06, 1.5167e-04, 9.9907e-01, 8.6636e-05])\n",
            "Sum of predicted tensor(4.5495)\n",
            "Loss tensor(0.2136)\n",
            "Acc tensor(0.9531)\n",
            "tensor([1.7843e-05, 1.8810e-06, 5.6633e-01, 4.6082e-06, 4.8270e-04, 3.3103e-06,\n",
            "        5.7546e-06, 1.2420e-05, 6.5436e-06, 3.4151e-04, 7.0390e-06, 1.5982e-05,\n",
            "        2.4390e-05, 7.2172e-06, 4.4858e-04, 4.4704e-06, 1.2353e-03, 1.6125e-06,\n",
            "        4.5768e-06, 2.3039e-06, 2.5233e-06, 9.8912e-01, 9.7617e-01, 1.5871e-06,\n",
            "        1.9970e-06, 2.8858e-06, 5.1311e-06, 3.4777e-05, 2.1592e-06, 2.2894e-05,\n",
            "        4.1760e-06, 6.2124e-06, 9.9604e-01, 2.7188e-01, 2.5927e-05, 3.3527e-06,\n",
            "        1.5946e-05, 2.7382e-06, 2.2128e-05, 6.4198e-06, 3.7786e-06, 7.8688e-04,\n",
            "        2.2196e-04, 3.2854e-01, 6.5023e-06, 6.4151e-05, 5.8177e-06, 1.6023e-06,\n",
            "        3.2225e-06, 7.1885e-06, 1.3350e-04, 1.1284e-04, 1.3099e-05, 9.9115e-01,\n",
            "        5.3971e-06, 1.4587e-06, 5.8177e-06, 1.9131e-06, 3.0323e-04, 1.6431e-05,\n",
            "        6.9100e-06, 1.5855e-02, 4.7520e-06, 9.7005e-01])\n",
            "Sum of predicted tensor(6.1096)\n",
            "Loss tensor(0.2453)\n",
            "Acc tensor(0.9062)\n",
            "tensor([1.8992e-06, 2.0273e-05, 1.9987e-06, 3.4149e-05, 8.3823e-06, 5.2908e-06,\n",
            "        1.8543e-05, 9.0881e-06, 2.9308e-05, 1.3733e-05, 2.6151e-05, 3.7368e-05,\n",
            "        3.2965e-01, 1.6052e-06, 9.6777e-01, 9.8584e-01, 1.1588e-05, 9.9245e-01,\n",
            "        8.8760e-02, 4.2930e-03, 8.5702e-06, 1.4356e-06, 5.6929e-06, 1.2152e-06,\n",
            "        1.0696e-03, 9.2425e-06, 5.7579e-06, 2.7152e-05, 3.6109e-06, 3.1904e-05,\n",
            "        6.9145e-05, 8.4902e-05, 3.8859e-06, 1.1114e-05, 9.9614e-01, 2.1140e-05,\n",
            "        3.0292e-05, 7.7967e-06, 5.8818e-06, 3.9033e-06, 4.8065e-06, 7.2731e-05,\n",
            "        6.5023e-06, 5.1736e-06, 3.8081e-06, 4.3494e-03, 3.9074e-06, 8.2149e-06,\n",
            "        1.5923e-02, 1.1433e-03, 1.9117e-05, 2.1432e-05, 9.2425e-06, 6.5023e-06,\n",
            "        1.8821e-05, 1.1969e-02, 4.0826e-05, 2.2002e-05, 4.2012e-04, 5.3181e-04,\n",
            "        1.4778e-02, 2.2133e-03, 2.2759e-05, 1.6529e-06])\n",
            "Sum of predicted tensor(4.4181)\n",
            "Loss tensor(0.3322)\n",
            "Acc tensor(0.9062)\n",
            "tensor([8.9798e-06, 1.2684e-05, 2.6281e-06, 2.0255e-05, 5.4235e-06, 9.6097e-01,\n",
            "        9.9990e-01, 1.5172e-05, 3.3574e-06, 8.1673e-06, 4.9032e-06, 7.0344e-05,\n",
            "        4.2346e-06, 2.6886e-05, 1.5082e-03, 1.2800e-01, 6.1632e-06, 2.2438e-05,\n",
            "        1.8416e-05, 3.9754e-03, 4.6661e-05, 9.1334e-05, 1.4302e-06, 4.4398e-01,\n",
            "        1.5041e-06, 1.5371e-01, 2.4992e-05, 2.6973e-05, 4.8289e-06, 4.7968e-06,\n",
            "        8.3266e-05, 2.8332e-05, 6.4117e-03, 2.7050e-06, 1.1938e-04, 2.3640e-06,\n",
            "        1.0301e-04, 6.7031e-06, 2.3181e-04, 1.6536e-06, 7.6036e-06, 4.5994e-06,\n",
            "        9.8038e-01, 1.5890e-05, 9.8791e-06, 9.9971e-01, 6.8474e-05, 9.9352e-01,\n",
            "        1.4588e-03, 1.4691e-06, 1.7139e-05, 9.2425e-06, 1.3127e-02, 4.1436e-05,\n",
            "        1.3349e-05, 2.0680e-06, 8.2508e-05, 9.9058e-01, 2.7787e-06, 4.0476e-04,\n",
            "        2.0437e-06, 5.2898e-06, 9.2061e-06, 2.6767e-06])\n",
            "Sum of predicted tensor(6.6789)\n",
            "Loss tensor(0.3895)\n",
            "Acc tensor(0.9375)\n",
            "tensor([6.5023e-06, 5.6025e-05, 9.0419e-06, 3.8712e-06, 5.9768e-06, 2.0598e-06,\n",
            "        2.0135e-05, 2.8585e-06, 2.7872e-06, 7.4900e-06, 8.2406e-06, 7.7571e-05,\n",
            "        6.0999e-06, 9.4357e-01, 7.8825e-06, 1.1639e-01, 2.6446e-05, 5.3485e-06,\n",
            "        6.6532e-05, 5.4004e-06, 4.0469e-06, 1.8060e-05, 3.2866e-06, 5.5829e-05,\n",
            "        4.2935e-06, 1.4137e-04, 3.1827e-05, 1.0524e-05, 2.4439e-06, 5.1018e-05,\n",
            "        4.5647e-06, 4.4018e-02, 1.4040e-05, 2.2518e-06, 3.0130e-06, 9.8307e-01,\n",
            "        4.0369e-06, 8.8997e-06, 7.3283e-06, 4.5710e-06, 1.5847e-05, 5.1663e-01,\n",
            "        3.1771e-05, 2.1656e-05, 9.9907e-01, 9.9507e-01, 4.1011e-06, 7.7943e-06,\n",
            "        6.0390e-05, 6.0579e-06, 1.7159e-02, 6.5023e-06, 2.9882e-01, 2.0053e-06,\n",
            "        2.9113e-06, 5.0579e-06, 4.9040e-06, 2.9178e-03, 3.1606e-06, 9.9923e-01,\n",
            "        8.5348e-06, 5.8177e-06, 1.7579e-06, 1.3191e-05])\n",
            "Sum of predicted tensor(5.9168)\n",
            "Loss tensor(0.3272)\n",
            "Acc tensor(0.9375)\n",
            "tensor([9.9985e-01, 6.6896e-06, 6.6802e-06, 7.4236e-04, 1.7251e-05, 9.6424e-06,\n",
            "        1.5167e-05, 4.9313e-05, 3.2314e-05, 4.0309e-02, 2.2642e-06, 9.9077e-03,\n",
            "        5.7729e-05, 5.5716e-06, 5.8791e-05, 1.5639e-03, 6.7318e-06, 1.0450e-02,\n",
            "        9.3186e-06, 1.7262e-05, 9.6078e-01, 6.1092e-05, 9.8962e-01, 7.8296e-06,\n",
            "        3.1729e-05, 2.8934e-05, 3.7022e-03, 2.9219e-05, 7.2781e-05, 2.0074e-05,\n",
            "        3.3702e-06, 1.3004e-04, 4.3435e-05, 1.1116e-04, 6.7236e-06, 5.4948e-05,\n",
            "        1.2588e-04, 9.9104e-01, 2.0695e-06, 5.5643e-06, 2.0228e-06, 3.7448e-04,\n",
            "        8.6144e-05, 1.5529e-04, 7.7992e-03, 2.4240e-06, 1.0086e-05, 4.0950e-06,\n",
            "        8.1382e-04, 2.6190e-05, 3.2198e-06, 8.0022e-06, 3.8799e-06, 1.1229e-05,\n",
            "        6.5023e-06, 1.0693e-01, 1.5211e-05, 6.8027e-06, 9.9990e-01, 1.9970e-06,\n",
            "        1.5088e-05, 2.6970e-06, 2.1501e-06, 5.6939e-06])\n",
            "Sum of predicted tensor(5.1252)\n",
            "Loss tensor(0.3392)\n",
            "Acc tensor(0.9375)\n",
            "tensor([1.3305e-05, 6.1119e-05, 1.9334e-06, 9.0568e-06, 8.5762e-06, 7.4437e-06,\n",
            "        4.1534e-06, 2.8396e-06, 6.9155e-01, 1.2575e-05, 8.9622e-05, 5.4329e-01,\n",
            "        3.2005e-05, 3.0769e-05, 3.3176e-06, 3.4172e-06, 3.6133e-06, 5.2478e-06,\n",
            "        7.7153e-05, 3.7365e-04, 3.4581e-05, 3.0084e-03, 6.3861e-05, 1.7071e-05,\n",
            "        1.1151e-05, 5.6997e-05, 1.1174e-05, 7.1546e-04, 3.3199e-06, 8.3012e-03,\n",
            "        1.0310e-01, 7.6468e-06, 6.4932e-06, 3.5123e-06, 1.4565e-06, 6.1417e-06,\n",
            "        4.0816e-05, 1.0594e-05, 5.1953e-06, 3.1052e-04, 1.5615e-05, 1.9879e-06,\n",
            "        5.7741e-06, 1.1746e-04, 7.3451e-05, 1.0131e-03, 9.3918e-06, 2.5562e-04,\n",
            "        8.8215e-04, 2.9495e-06, 3.9846e-02, 7.7799e-05, 5.7242e-06, 1.3548e-05,\n",
            "        6.5482e-06, 3.2016e-06, 1.2094e-04, 5.2616e-06, 7.2702e-05, 9.6079e-01,\n",
            "        4.7307e-06, 1.8841e-05, 1.8852e-05, 1.1301e-05])\n",
            "Sum of predicted tensor(2.3547)\n",
            "Loss tensor(0.2138)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.7881e-06, 8.3352e-06, 1.0523e-04, 4.9523e-05, 2.9300e-02, 6.9286e-05,\n",
            "        1.4970e-05, 4.5182e-06, 2.6580e-06, 6.1473e-06, 5.6713e-03, 3.7302e-06,\n",
            "        2.4569e-05, 2.2679e-05, 7.7158e-06, 6.6322e-06, 9.9943e-01, 5.5130e-06,\n",
            "        1.6959e-05, 2.5822e-06, 1.4742e-06, 4.7642e-05, 1.0987e-04, 4.0094e-06,\n",
            "        2.2092e-06, 5.7367e-03, 1.0174e-03, 2.4334e-06, 9.9933e-01, 4.2643e-02,\n",
            "        4.3709e-06, 2.1176e-06, 1.3388e-04, 1.2484e-05, 5.6922e-06, 1.3355e-05,\n",
            "        2.5571e-06, 1.4975e-05, 3.6595e-05, 8.3565e-06, 3.3506e-04, 6.1471e-01,\n",
            "        1.1497e-05, 5.6787e-06, 3.1495e-05, 1.0156e-05, 1.3697e-05, 1.4003e-05,\n",
            "        8.1470e-06, 7.7659e-06, 1.7435e-05, 8.3163e-06, 1.8925e-05, 1.8925e-05,\n",
            "        1.7296e-05, 8.5866e-06, 5.4877e-02, 3.4204e-06, 2.8394e-06, 1.5457e-06,\n",
            "        1.9252e-06, 1.0157e-02, 4.9983e-06, 1.9060e-05])\n",
            "Sum of predicted tensor(2.7642)\n",
            "Loss tensor(0.0173)\n",
            "Acc tensor(0.9844)\n",
            "tensor([3.4006e-05, 1.8089e-04, 2.2431e-05, 4.4915e-06, 1.5740e-06, 7.7126e-05,\n",
            "        7.1277e-06, 1.4749e-05, 8.6302e-05, 1.3112e-02, 8.5156e-06, 8.4544e-06,\n",
            "        2.1059e-05, 4.3352e-06, 2.5013e-06, 6.1700e-05, 3.7776e-06, 3.4744e-05,\n",
            "        4.9286e-05, 3.2542e-06, 3.2411e-06, 9.7661e-05, 3.6274e-06, 9.0933e-05,\n",
            "        2.9766e-05, 5.8176e-06, 1.1983e-05, 9.9931e-01, 8.6604e-06, 3.9324e-06,\n",
            "        1.5326e-04, 2.5519e-04, 4.9657e-05, 3.2240e-05, 5.4904e-06, 1.1333e-05,\n",
            "        1.4127e-05, 6.5872e-06, 2.7705e-05, 1.3308e-05, 2.4747e-06, 1.6500e-05,\n",
            "        4.2867e-06, 3.3199e-06, 2.8198e-05, 5.6642e-05, 2.9078e-06, 9.2754e-01,\n",
            "        5.1700e-06, 7.9062e-06, 1.3871e-05, 6.8992e-05, 6.1137e-06, 2.1349e-05,\n",
            "        1.7215e-03, 7.7213e-06, 1.9242e-05, 5.3890e-01, 1.8228e-05, 9.6290e-06,\n",
            "        4.6258e-06, 9.9816e-01, 2.7546e-05, 5.6586e-05])\n",
            "Sum of predicted tensor(3.4806)\n",
            "Loss tensor(0.1977)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.0212e-06, 1.2913e-05, 3.2120e-06, 3.3835e-06, 2.9055e-04, 2.0152e-05,\n",
            "        3.0576e-01, 1.2292e-05, 4.3314e-06, 2.5809e-06, 1.8741e-04, 1.0284e-02,\n",
            "        3.2429e-05, 1.8882e-03, 8.9307e-04, 2.3225e-05, 1.2179e-06, 4.9225e-06,\n",
            "        2.1415e-06, 1.7001e-05, 1.2438e-05, 2.5661e-04, 6.1692e-06, 3.9296e-06,\n",
            "        1.9837e-05, 3.4787e-04, 2.0181e-05, 6.9104e-06, 3.6338e-05, 1.3112e-02,\n",
            "        1.4908e-05, 3.2346e-06, 3.2018e-06, 3.0839e-04, 1.0062e-04, 6.1254e-05,\n",
            "        4.2588e-06, 3.6339e-06, 1.2090e-05, 6.9523e-01, 2.0963e-05, 3.6035e-06,\n",
            "        3.0298e-05, 8.7810e-05, 1.5079e-05, 5.5499e-06, 8.2933e-01, 9.2952e-06,\n",
            "        9.4229e-05, 1.2277e-05, 1.9757e-04, 8.6719e-05, 2.8400e-06, 9.9708e-01,\n",
            "        5.0693e-06, 7.6665e-06, 3.2455e-05, 1.3702e-05, 6.9989e-02, 3.4848e-06,\n",
            "        5.2320e-06, 2.4761e-05, 3.8172e-05, 9.2237e-06])\n",
            "Sum of predicted tensor(2.9261)\n",
            "Loss tensor(0.0535)\n",
            "Acc tensor(0.9688)\n",
            "tensor([6.4999e-06, 9.9372e-05, 3.0625e-05, 5.6268e-06, 1.3310e-05, 2.9890e-06,\n",
            "        5.8825e-05, 1.3648e-05, 7.7066e-06, 4.2013e-06, 6.1904e-06, 3.9254e-02,\n",
            "        1.1718e-04, 1.2242e-05, 2.3962e-05, 9.8740e-05, 8.6063e-06, 4.5810e-05,\n",
            "        1.9821e-04, 3.8785e-04, 4.6959e-05, 3.9915e-03, 1.0746e-04, 6.5170e-06,\n",
            "        2.9933e-06, 5.6784e-06, 7.1421e-02, 1.3061e-01, 3.5078e-06, 1.3660e-05,\n",
            "        6.1867e-06, 4.3248e-05, 9.8286e-01, 6.9618e-06, 2.6644e-06, 1.3019e-05,\n",
            "        7.3267e-06, 1.7853e-05, 5.6415e-05, 1.2896e-05, 4.1591e-06, 7.9849e-06,\n",
            "        6.7442e-05, 7.3758e-05, 3.8355e-05, 2.3271e-06, 3.5210e-05, 9.0186e-05,\n",
            "        3.8751e-06, 1.5976e-05, 3.9904e-06, 7.1622e-06, 2.8509e-06, 6.8441e-06,\n",
            "        5.1931e-06, 8.8771e-06, 1.0056e-04, 3.7998e-05, 4.1516e-05, 4.9974e-05,\n",
            "        2.8408e-05, 6.0544e-04, 6.0064e-06, 1.3986e-04])\n",
            "Sum of predicted tensor(1.2310)\n",
            "Loss tensor(0.2534)\n",
            "Acc tensor(0.9531)\n",
            "tensor([4.3229e-05, 1.2593e-05, 1.0510e-05, 8.1100e-06, 1.4761e-04, 1.7141e-02,\n",
            "        2.3749e-06, 9.9890e-01, 2.7987e-04, 1.0060e-05, 6.3331e-06, 3.5940e-06,\n",
            "        7.1064e-06, 2.1034e-05, 5.1365e-06, 3.3960e-06, 7.1034e-06, 8.8514e-01,\n",
            "        1.8493e-04, 1.0769e-01, 7.5213e-06, 2.7985e-06, 1.2528e-05, 5.4651e-05,\n",
            "        3.2772e-03, 1.6770e-04, 9.9964e-01, 6.7611e-05, 6.0290e-01, 3.5369e-03,\n",
            "        8.5961e-05, 1.0796e-02, 4.7593e-05, 9.9964e-01, 9.5945e-06, 6.0428e-01,\n",
            "        5.5519e-06, 8.7444e-05, 4.1526e-03, 6.4428e-04, 4.8834e-06, 2.2553e-06,\n",
            "        2.9646e-05, 3.7522e-06, 1.9932e-05, 8.4585e-06, 2.4462e-04, 4.5465e-06,\n",
            "        5.6880e-01, 1.0579e-05, 2.6032e-06, 6.8612e-06, 4.1787e-06, 5.1485e-05])\n",
            "Sum of predicted tensor(5.8082)\n",
            "Loss tensor(0.3232)\n",
            "Acc tensor(0.8889)\n",
            "\tTrain Loss: 0.048 | Train Acc: 98.50%\n",
            "\t Val. Loss: 0.234 |  Val. Acc: 94.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy 2"
      ],
      "metadata": {
        "id": "nT1hNQgA6Lr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = './data'\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "train_model(model=model,\n",
        "            optimizer=optimizer,\n",
        "            train_loader=train_iterator,\n",
        "            valid_loader=valid_iterator,\n",
        "            criterion = criterion,\n",
        "            num_epochs = 20,\n",
        "            file_path = SAVE_PATH,\n",
        "            device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUL6mtLR2PNO",
        "outputId": "0c3cd348-ab64-4677-c3fa-5557f1dbb5c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [175/7000], Train Loss: 0.2249, Valid Loss: 0.1568\n",
            "0.15684693338970343\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [1/20], Step [350/7000], Train Loss: 0.1511, Valid Loss: 0.1351\n",
            "0.13509278620282808\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [2/20], Step [525/7000], Train Loss: 0.1071, Valid Loss: 0.1412\n",
            "Epoch [2/20], Step [700/7000], Train Loss: 0.0959, Valid Loss: 0.1175\n",
            "0.11750506459424893\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [3/20], Step [875/7000], Train Loss: 0.0672, Valid Loss: 0.1268\n",
            "Epoch [3/20], Step [1050/7000], Train Loss: 0.0689, Valid Loss: 0.1275\n",
            "Epoch [4/20], Step [1225/7000], Train Loss: 0.0405, Valid Loss: 0.1476\n",
            "Epoch [4/20], Step [1400/7000], Train Loss: 0.0463, Valid Loss: 0.1367\n",
            "Epoch [5/20], Step [1575/7000], Train Loss: 0.0306, Valid Loss: 0.1554\n",
            "Epoch [5/20], Step [1750/7000], Train Loss: 0.0253, Valid Loss: 0.1640\n",
            "Epoch [6/20], Step [1925/7000], Train Loss: 0.0159, Valid Loss: 0.1861\n",
            "Epoch [6/20], Step [2100/7000], Train Loss: 0.0177, Valid Loss: 0.1931\n",
            "Epoch [7/20], Step [2275/7000], Train Loss: 0.0106, Valid Loss: 0.2073\n",
            "Epoch [7/20], Step [2450/7000], Train Loss: 0.0126, Valid Loss: 0.2168\n",
            "Epoch [8/20], Step [2625/7000], Train Loss: 0.0077, Valid Loss: 0.2681\n",
            "Epoch [8/20], Step [2800/7000], Train Loss: 0.0106, Valid Loss: 0.2329\n",
            "Epoch [9/20], Step [2975/7000], Train Loss: 0.0045, Valid Loss: 0.2562\n",
            "Epoch [9/20], Step [3150/7000], Train Loss: 0.0064, Valid Loss: 0.2553\n",
            "Epoch [10/20], Step [3325/7000], Train Loss: 0.0041, Valid Loss: 0.2919\n",
            "Epoch [10/20], Step [3500/7000], Train Loss: 0.0044, Valid Loss: 0.2807\n",
            "Epoch [11/20], Step [3675/7000], Train Loss: 0.0071, Valid Loss: 0.2843\n",
            "Epoch [11/20], Step [3850/7000], Train Loss: 0.0068, Valid Loss: 0.2658\n",
            "Epoch [12/20], Step [4025/7000], Train Loss: 0.0036, Valid Loss: 0.2822\n",
            "Epoch [12/20], Step [4200/7000], Train Loss: 0.0028, Valid Loss: 0.3072\n",
            "Epoch [13/20], Step [4375/7000], Train Loss: 0.0020, Valid Loss: 0.3187\n",
            "Epoch [13/20], Step [4550/7000], Train Loss: 0.0050, Valid Loss: 0.2993\n",
            "Epoch [14/20], Step [4725/7000], Train Loss: 0.0027, Valid Loss: 0.3178\n",
            "Epoch [14/20], Step [4900/7000], Train Loss: 0.0037, Valid Loss: 0.3165\n",
            "Epoch [15/20], Step [5075/7000], Train Loss: 0.0013, Valid Loss: 0.3443\n",
            "Epoch [15/20], Step [5250/7000], Train Loss: 0.0022, Valid Loss: 0.3376\n",
            "Epoch [16/20], Step [5425/7000], Train Loss: 0.0013, Valid Loss: 0.3475\n",
            "Epoch [16/20], Step [5600/7000], Train Loss: 0.0015, Valid Loss: 0.3522\n",
            "Epoch [17/20], Step [5775/7000], Train Loss: 0.0014, Valid Loss: 0.3627\n",
            "Epoch [17/20], Step [5950/7000], Train Loss: 0.0012, Valid Loss: 0.3633\n",
            "Epoch [18/20], Step [6125/7000], Train Loss: 0.0010, Valid Loss: 0.3722\n",
            "Epoch [18/20], Step [6300/7000], Train Loss: 0.0102, Valid Loss: 0.2391\n",
            "Epoch [19/20], Step [6475/7000], Train Loss: 0.0030, Valid Loss: 0.2946\n",
            "Epoch [19/20], Step [6650/7000], Train Loss: 0.0062, Valid Loss: 0.2927\n",
            "Epoch [20/20], Step [6825/7000], Train Loss: 0.0012, Valid Loss: 0.3392\n",
            "Epoch [20/20], Step [7000/7000], Train Loss: 0.0024, Valid Loss: 0.3034\n",
            "[0.22490913101605006, 0.15113367381904808, 0.10711617232965572, 0.09587981392230306, 0.06722255719559533, 0.06887151260195033, 0.04049396553342896, 0.046341607867340956, 0.030563433944813107, 0.025297086795326322, 0.015882813031951496, 0.017667661013879947, 0.01060992616744313, 0.012588464579935784, 0.007656374522505628, 0.010645560214449817, 0.004462051057655896, 0.0063651652678839, 0.0040581515425167575, 0.004382546567430836, 0.007139488911790458, 0.0067795308355042445, 0.0036253824532780396, 0.002782532474759916, 0.0019529684178113322, 0.005009046287632372, 0.002675127655903842, 0.0036541511286226785, 0.001306385933654123, 0.0021990501527346037, 0.0012598286610383575, 0.0015043904793115611, 0.001436829830410196, 0.0011931263728287636, 0.0010446516518530708, 0.01018171932303533, 0.003030092906743188, 0.006174216907675145, 0.0012156694749163373, 0.0023717816071413315] [0.15684693338970343, 0.13509278620282808, 0.14115689746414622, 0.11750506459424893, 0.12676820190623403, 0.12754117558399836, 0.14755874926224352, 0.1367315235051016, 0.15544288728386163, 0.16401561624448124, 0.186068304956619, 0.19314352240025376, 0.20728975365384636, 0.21683414315474997, 0.2681470723739282, 0.23294913480523974, 0.25621029428410114, 0.25529885999159885, 0.29190295333275573, 0.28071362709687186, 0.2842661564858281, 0.2657597458837214, 0.28217344495889846, 0.3071852425686666, 0.31872416102759116, 0.2993164249344894, 0.31784601599270296, 0.3164769884930987, 0.3443355363339227, 0.3375987531557227, 0.3475481753922971, 0.3522304534246208, 0.36271189141098753, 0.36331473540674475, 0.3721729228498331, 0.23906255327650192, 0.2946410032476342, 0.29274523781515505, 0.3391795210712371, 0.30343259912275244] [175, 350, 525, 700, 875, 1050, 1225, 1400, 1575, 1750, 1925, 2100, 2275, 2450, 2625, 2800, 2975, 3150, 3325, 3500, 3675, 3850, 4025, 4200, 4375, 4550, 4725, 4900, 5075, 5250, 5425, 5600, 5775, 5950, 6125, 6300, 6475, 6650, 6825, 7000]\n",
            "Model saved to ==> /metrics.pt\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "best_model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, TEXT.vocab, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)\n",
        "\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=0.00001)\n",
        "\n",
        "load_checkpoint('/model.pt', best_model, optimizer)\n",
        "evaluate_full(best_model, test_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "1a7XL4A12Ive",
        "outputId": "b676f7c5-189a-4a58-b26a-789727e2a635"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /model.pt\n",
            "Test Accuracy:  0.9601419031719532\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7826    0.5607    0.6534       321\n",
            "           0     0.9691    0.9888    0.9789      4471\n",
            "\n",
            "    accuracy                         0.9601      4792\n",
            "   macro avg     0.8759    0.7748    0.8161      4792\n",
            "weighted avg     0.9566    0.9601    0.9571      4792\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}