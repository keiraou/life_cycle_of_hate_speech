{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "marked-western",
      "metadata": {
        "id": "marked-western"
      },
      "source": [
        "\n",
        "Reference:\n",
        "- https://fasttext.cc/docs/en/unsupervised-tutorial.html\n",
        "- https://blog.csdn.net/feilong_csdn/article/details/88655927\n",
        "\n",
        "Deep average network\n",
        "\n",
        "\n",
        "Unlabeled dataset:\n",
        "   -  upsample by few shot learning (upsample)\n",
        "   -  A Siamese neural network upsample\n",
        "   \n",
        "\n",
        "Mainly based on Hw3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "directed-leonard",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "directed-leonard",
        "outputId": "9c5322b0-df58-4e23-98c7-d02a4774f93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "    USE_CUDA=True\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    USE_CUDA=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "constant-photography",
      "metadata": {
        "id": "constant-photography"
      },
      "outputs": [],
      "source": [
        "SEED = 30255 # Specify a seed for reproducability\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "alien-haiti",
      "metadata": {
        "id": "alien-haiti"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data/Twitter/hate_twitter/hate_train.csv')\n",
        "val_df = pd.read_csv('data/Twitter/hate_twitter/hate_val.csv')\n",
        "test_df = pd.read_csv('data/Twitter/hate_twitter/hate_test.csv')\n",
        "\n",
        "\n",
        "# Check and drop na values in clean_tweet column\n",
        "train_df[train_df['clean_tweet'].isnull()]\n",
        "\n",
        "train_df = train_df[train_df['clean_tweet'].notna()]\n",
        "val_df = val_df[val_df['clean_tweet'].notna()]\n",
        "test_df = test_df[test_df['clean_tweet'].notna()]\n",
        "\n",
        "\n",
        "x_train = train_df['clean_tweet']\n",
        "y_train = train_df['label']\n",
        "\n",
        "x_test = test_df['clean_tweet']\n",
        "y_test = test_df['label']\n",
        "\n",
        "x_val = val_df['clean_tweet']\n",
        "y_val = val_df['label']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "starting-reviewer",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "starting-reviewer",
        "outputId": "c117c7b6-a08a-4c27-9787-d299f1d1b056"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    20776\n",
              "1     1574\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_df.groupby('label').size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incident-monaco",
      "metadata": {
        "id": "incident-monaco"
      },
      "source": [
        "## Convert to Pytorch Data Objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "indian-choir",
      "metadata": {
        "id": "indian-choir"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dataclassCNN.py\n",
        "File to create a Custom Data Class and Collate Function for PyTorch.\n",
        "This file is for the CNN model.\n",
        "'''\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "\n",
        "class ProjectDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target_col=None, text_col=None):\n",
        "\n",
        "        # Target first, then Inputs.\n",
        "        self.samples = []\n",
        "        tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "        if not target_col and not text_col:\n",
        "            targets = list(data[0])\n",
        "            inputs = list(data[1])\n",
        "            for idx in range(len(targets)):\n",
        "                text = tokenizer(inputs[idx])\n",
        "                self.samples.append([targets[idx], text])\n",
        "        else:\n",
        "            for _, row in data.iterrows():\n",
        "                text = row[text_col]\n",
        "                text = tokenizer(text)\n",
        "                target = row[target_col]\n",
        "                self.samples.append([target, text])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "unnecessary-andrew",
      "metadata": {
        "id": "unnecessary-andrew"
      },
      "outputs": [],
      "source": [
        "data_obj = ProjectDataset(train_df, 'label', 'clean_tweet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abroad-employee",
      "metadata": {
        "id": "abroad-employee"
      },
      "source": [
        "## Load Pre-trained vectors GloVe "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "acceptable-kernel",
      "metadata": {
        "id": "acceptable-kernel"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "significant-findings",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "significant-findings",
        "outputId": "faa5f64b-b327-4063-9f5a-747bee2bef8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary length is 32820 words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "./.vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:42<00:00, 9423.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of words for a given tweet in the Dataset:\n",
            "21\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "\n",
        "counter_words = Counter()\n",
        "for (label, text) in data_obj:\n",
        "    counter_words.update(text)\n",
        "    \n",
        "vocab_words = vocab(counter_words)\n",
        "vocab_words.set_default_index(0)\n",
        "\n",
        "\n",
        "print('The vocabulary length is {} words'.format(len(vocab_words)))\n",
        "\n",
        "\n",
        "VECTORS_CACHE_DIR = './.vector_cache'\n",
        "\n",
        "\n",
        "from torchtext import vocab\n",
        "\n",
        "pretrained_emb = vocab.GloVe(name='6B',cache=VECTORS_CACHE_DIR)\n",
        "\n",
        "print('Max number of words for a given tweet in the Dataset:')\n",
        "max_len = train_df[\"clean_tweet\"].apply(lambda x: len(x.split())).max()\n",
        "print(max_len)\n",
        "\n",
        "pre_trained_vectors = pretrained_emb.get_vecs_by_tokens(vocab_words.get_itos(),\n",
        "                                                       lower_case_backup=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "original-newark",
      "metadata": {
        "id": "original-newark"
      },
      "outputs": [],
      "source": [
        "def collate_for_cnn(batch, max_len=max_len):\n",
        "    '''\n",
        "    For each batch, develop the appropiate inputs from the models (i.e. embeddings)\n",
        "    '''\n",
        "    input_vector = []\n",
        "    labels = []\n",
        "    for i, (label, tokenized_sent) in enumerate(batch):\n",
        "        \n",
        "        #Append labels \n",
        "        labels.append(label)\n",
        "        \n",
        "        #Get tokenized sentence\n",
        "        diff =  (max_len - len(tokenized_sent))\n",
        "        tokenized_sent += ['<pad>'] * diff\n",
        "        \n",
        "        assert(len(tokenized_sent) == max_len), tokenized_sent\n",
        "        \n",
        "        wordstoidx = [vocab_words[w] for w in tokenized_sent]\n",
        "        input_vector.append(wordstoidx)\n",
        "    \n",
        "    return torch.tensor(labels).to(device), torch.tensor(input_vector).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bizarre-mexico",
      "metadata": {
        "id": "bizarre-mexico"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# W = 1\n",
        "# WINDOW_SIZE = (2 * W + 1)\n",
        "\n",
        "# TAG = 'ud'\n",
        "\n",
        "# def collate_for_cnn(batch, w = W, tag = TAG):\n",
        "    \n",
        "#     ## WRITE YOUR CODE BELOW\n",
        "#     labels = []\n",
        "#     word_idxs = []\n",
        "\n",
        "#     for idx in range(len(batch)):\n",
        "#         one_word_idx = []\n",
        "#         word_lst = batch[idx][0]\n",
        "\n",
        "#         # index of the tag for the center word\n",
        "#         center = w\n",
        "#         center_word_tag = batch[idx][1][center]\n",
        "#         tag_idx = vocab_words[center_word_tag]\n",
        "#         labels.append(tag_idx)\n",
        "\n",
        "#         #  index of each of the words in the example\n",
        "#         for word in word_lst:\n",
        "#             one_word_idx.append(vocab_words[word])\n",
        "        \n",
        "#         word_idxs.append(one_word_idx)\n",
        "\n",
        "#     word_idxs = torch.LongTensor(word_idxs)\n",
        "#     labels = torch.LongTensor(labels)\n",
        "#     # print(word_idxs)\n",
        "#     # print(labels)\n",
        "\n",
        "#     ## WRITE YOUR CODE ABOVE\n",
        "#     # The tensors you return should be placed in the correct device\n",
        "#     # as shown below.\n",
        "#     return labels.to(device), word_idxs.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regional-jumping",
      "metadata": {
        "id": "regional-jumping"
      },
      "source": [
        "## Setting up DataLoader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "normal-cosmetic",
      "metadata": {
        "id": "normal-cosmetic"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def data_loader(train_tuple, val_tuple, test_tuple,\n",
        "                batch_size=30):\n",
        "    \"\"\"Convert train, validation and test sets into Dataloaders\n",
        "    \"\"\"\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = ProjectDataset(train_tuple)\n",
        "    train_dataloader = DataLoader(train_data, shuffle=True,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "    \n",
        "    # Create DataLoader for validation data\n",
        "    valid_data = ProjectDataset(val_tuple)\n",
        "    val_dataloader = DataLoader(valid_data, shuffle=False,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "\n",
        "    # Create DataLoader for test data\n",
        "    test_data = ProjectDataset(test_tuple)\n",
        "    test_dataloader = DataLoader(test_data, shuffle=False,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "narrow-official",
      "metadata": {
        "id": "narrow-official"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = data_loader([y_train, x_train],\n",
        "                                                               [y_val, x_val],\n",
        "                                                               [y_test, x_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "legendary-breast",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "legendary-breast",
        "outputId": "fbeba201-0fed-4425-f5fb-b8461fa9c9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 21])\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataloader:\n",
        "    labels, inputs = i[0], i[1]\n",
        "    print(inputs.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immediate-passage",
      "metadata": {
        "id": "immediate-passage"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "personalized-weekly",
      "metadata": {
        "id": "personalized-weekly"
      },
      "outputs": [],
      "source": [
        "FILTER_SIZES = [3,4,5]\n",
        "N_FILTERS = [100, 100,100]\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = vocab_words['<pad>']\n",
        "num_classes = 2 #0, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "artistic-mason",
      "metadata": {
        "id": "artistic-mason"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_classification(nn.Module):\n",
        "    ''' An 1D Convulational Neural Network for Sentence Classification'''\n",
        "    \n",
        "    def __init__(self, pretrained_embedding=None, freeze_embedding=False,\n",
        "                 vocab_size=None, embed_dim=None,\n",
        "                 filter_sizes=FILTER_SIZES, num_filters=N_FILTERS,\n",
        "                 num_classes=num_classes,pad_id=PAD_IDX,\n",
        "                 dropout=DROPOUT):\n",
        "        \"\"\"\n",
        "        pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
        "                shape (vocab_size, embed_dim)\n",
        "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
        "                vectors. Default: False\n",
        "            vocab_size (int): Need to be specified when not pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (int): Number of filters. Default: 100\n",
        "            n_classes (int): Number of classes. Default: 3\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_classification, self).__init__()\n",
        "        \n",
        "        # 1. Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx= pad_id,\n",
        "                                          )\n",
        "        \n",
        "        # 2. Convolutional Layers (for each filter size --> n-gram)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = self.embed_dim,\n",
        "                                              out_channels = num_filters[i], \n",
        "                                              kernel_size = fs)\n",
        "                                    for i, fs in enumerate(filter_sizes)\n",
        "                                    ])\n",
        "        \n",
        "    \n",
        "        # 3. Fully-connected layer\n",
        "        self.linear = nn.Linear(in_features = np.sum(num_filters), \n",
        "                                out_features = num_classes\n",
        "                               )\n",
        "        \n",
        "        \n",
        "        # Additional Feature\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        x_embedded = self.embedding(inputs)\n",
        "        \n",
        "        x_embedded = x_embedded.permute(0, 2, 1)\n",
        "        \n",
        "        # Apply CNN and ReLU\n",
        "        convs_list = [F.relu(conv(x_embedded)) for conv in self.convs]\n",
        "\n",
        "        # Max pooling.\n",
        "        pooled = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]).squeeze(2)\n",
        "                  for x_conv in convs_list]\n",
        "        \n",
        "        # Concatenate Pool list to feed the fully connected layer\n",
        "        input_fc = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        \n",
        "        # Compute probabilities\n",
        "        predictions_classes = self.linear(input_fc)\n",
        "        \n",
        "        return predictions_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "scientific-kenya",
      "metadata": {
        "id": "scientific-kenya"
      },
      "outputs": [],
      "source": [
        "def train_an_epoch(model, dataloader, loss_function, optimizer):\n",
        "    \n",
        "    model.train() # Sets the module in training mode.\n",
        "    total_loss = 0\n",
        "    \n",
        "    for idx, batch in enumerate(dataloader):\n",
        "        \n",
        "        # Load batch to GPU\n",
        "        labels, inputs = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Zero out any previously calculated gradients\n",
        "        model.zero_grad()\n",
        "        \n",
        "        #Perform a forward pass.\n",
        "        log_probs = model(inputs)\n",
        "        \n",
        "        # Compute loss and accumulate the loss values\n",
        "        loss = loss_function(log_probs, labels.long())\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Perform a backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    return total_loss/len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLG68DJnmc0",
        "outputId": "d7a851ae-f370-4eaf-f043-076dff99a8b2"
      },
      "id": "_hLG68DJnmc0",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Collecting pyDeprecate==0.3.*\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: pyDeprecate, torchmetrics\n",
            "Successfully installed pyDeprecate-0.3.2 torchmetrics-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "median-feature",
      "metadata": {
        "id": "median-feature"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import F1Score, Accuracy, Recall, Precision\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    '''\n",
        "    Evaluate the model on the given data (e.g. validation data or test data).\n",
        "    '''\n",
        "\n",
        "    #As we are now using dropout, we must remember to use model.eval() \n",
        "    #to ensure the dropout is \"turned off\" while evaluating.\n",
        "    model.eval()\n",
        "    \n",
        "    total_accuracy = []\n",
        "    total_loss = []\n",
        "    total_recall = []\n",
        "    total_precision = []\n",
        "    total_f1_score = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            \n",
        "            # Load batch to Device\n",
        "            labels, inputs = batch[0], batch[1]\n",
        "            if USE_CUDA:\n",
        "                labels, inputs = labels.cuda(), inputs.cuda()\n",
        "                \n",
        "            # Obtain probabilities of each class per sentence\n",
        "            output = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_fn(output, labels.long())\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            # Get the predictions\n",
        "            preds = torch.argmax(output, dim=1).flatten()\n",
        "            \n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == labels).sum()/len(preds)\n",
        "            total_accuracy.append(accuracy)\n",
        "\n",
        "            # Calculate the recall score\n",
        "            # print(((preds == labels) & (labels == 1)).sum())\n",
        "            # print((labels == 1).sum())\n",
        "            # print(torch.sum(labels == 1))\n",
        "            # print(torch.sum((preds == labels) & (labels == 1)))\n",
        "            recall = Recall()\n",
        "            recall_score = recall(preds, labels).item()\n",
        "            total_recall.append(recall_score)\n",
        "            # recall = torch.sum((preds == labels) & (labels == 1)) / torch.sum(labels == 1)\n",
        "            # recall = len(((preds == labels) & (labels == 1)).sum()) / len((labels == 1).sum())\n",
        "            total_recall.append(recall_score)\n",
        "\n",
        "            # Calculate the precision score\n",
        "            # precision = torch.sum((preds == labels) & (labels == 1)) / torch.sum(preds == 1)\n",
        "            # precision = len(((preds == labels) & (labels == 1)).sum()) / len((preds == 1).sum())\n",
        "            precision = Precision()\n",
        "            precision_score = precision(preds, labels).item()\n",
        "            total_precision.append(precision_score)\n",
        "\n",
        "            # Calculate the f1 score\n",
        "            # f1_score = (2 * recall * precision) / (recall + precision)\n",
        "            f1 = F1Score(num_classes=2)\n",
        "            f1_score = f1(preds, labels).item()\n",
        "            total_f1_score.append(f1_score)\n",
        "\n",
        "    return np.mean(total_loss), np.mean(total_accuracy), np.mean(total_recall), np.mean(total_precision), np.mean(total_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "relevant-taste",
      "metadata": {
        "id": "relevant-taste"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def train_validate(model, optimizer, train_dataloader, val_dataloader, pretrained_embedding, epochs=20):\n",
        "    \n",
        "    \"\"\"Train the CNN model.\"\"\"\n",
        "    \n",
        "    #Loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Tracking best model\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_recalls = []\n",
        "    val_precisions = []\n",
        "    val_f1_scores = []\n",
        "    best_model = None\n",
        "    \n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Time(secs)':^8} | {'Train Loss':^9}| {'Val Loss':^10}| {'Val Acc':^11} | {'Val Recall':^12}  | {'Val Precision':^13}  | {'Val F1':^14}\")\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        start_time = datetime.datetime.now()\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        \n",
        "        avg_loss_train = train_an_epoch(model, train_dataloader, loss_function, optimizer)\n",
        "        \n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        \n",
        "        val_loss, val_accuracy, val_recall, val_precision, val_f1 = evaluate(model, val_dataloader, loss_function)\n",
        "\n",
        "        # Track the best model\n",
        "        if len(val_losses) == 0 or val_loss < min(val_losses):\n",
        "            best_model = type(model)(pretrained_embedding=pretrained_embedding,\n",
        "                                     vocab_size=len(vocab_words), \n",
        "                                     embed_dim=model.embed_dim,\n",
        "                                     filter_sizes=FILTER_SIZES,\n",
        "                                     num_filters=N_FILTERS, num_classes=num_classes)\n",
        "            best_model.load_state_dict(model.state_dict())\n",
        "            if USE_CUDA:\n",
        "                best_model = best_model.cuda()\n",
        "        \n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        val_recalls.append(val_recall)\n",
        "        val_precisions.append(val_precision)\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        time_diff = (datetime.datetime.now() - start_time).seconds\n",
        "        \n",
        "        print(f\"{epoch_i + 1:^7} |  {time_diff:^8}  | {avg_loss_train:^9.2f} | {val_loss:^10.2f}| {val_accuracy:^11.2f}| {val_recall:^12.2f}| {val_precision:^13.2f} | {val_f1:^14.2f}\")\n",
        "        print('')\n",
        "        \n",
        "    return best_model, val_accuracies, val_recalls, val_precisions, val_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "healthy-latitude",
      "metadata": {
        "id": "healthy-latitude"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "inner-findings",
      "metadata": {
        "id": "inner-findings"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN_classification(pretrained_embedding=None,\n",
        "                 vocab_size=len(vocab_words), embed_dim=400,\n",
        "                        freeze_embedding=False,\n",
        "                        filter_sizes=FILTER_SIZES,\n",
        "                        num_filters=N_FILTERS,\n",
        "                        num_classes=num_classes,\n",
        "                        dropout=0.5).to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), \n",
        "                                 lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "liked-colombia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liked-colombia",
        "outputId": "882d21c8-0c8f-4ed8-adb3-3fb7e4d6e864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  | Time(secs) | Train Loss|  Val Loss |   Val Acc   |  Val Recall   | Val Precision  |     Val F1    \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "   1    |    135     |   0.20    |    0.15   |    0.96    |     0.96    |     0.96      |      0.96     \n",
            "\n",
            "   2    |    144     |   0.09    |    0.16   |    0.96    |     0.96    |     0.96      |      0.96     \n",
            "\n",
            "   3    |    145     |   0.04    |    0.23   |    0.96    |     0.96    |     0.96      |      0.96     \n",
            "\n",
            "   4    |    149     |   0.04    |    0.27   |    0.96    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   5    |    153     |   0.03    |    0.32   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_model, val_accuracies, val_recalls, val_precisions, val_f1_scores = train_validate(cnn_model, optimizer, \n",
        "                                            train_dataloader, val_dataloader, \n",
        "                                            pretrained_embedding=None,\n",
        "                                            epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the best model"
      ],
      "metadata": {
        "id": "R_P9epXecwy9"
      },
      "id": "R_P9epXecwy9"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "lyric-finish",
      "metadata": {
        "id": "lyric-finish"
      },
      "outputs": [],
      "source": [
        "#Loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "test_loss, test_accuracy, test_recall, test_precision, test_f1_score = evaluate(best_model, test_dataloader, loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{test_loss:^10.2f}| {test_accuracy:^11.2f}| {test_recall:^12.2f}| {test_precision:^13.2f} | {test_f1_score:^14.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAx3IgzvdmCC",
        "outputId": "9ebf9305-00de-4c1e-df3e-a93a33ce7d94"
      },
      "id": "AAx3IgzvdmCC",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0.13   |    0.96    |     0.96    |     0.96      |      0.96     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_metrics(model, test_loader, device='cpu'):\n",
        "    print('Evaluate')\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    total_accuracy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            # Load batch to Device\n",
        "            labels, inputs = batch[0], batch[1]\n",
        "            labels, inputs = labels.to(device), inputs.to(device)\n",
        "            # Obtain probabilities of each class per sentence\n",
        "            output = model(inputs)\n",
        "            # Get the predictions\n",
        "            preds = torch.argmax(output, dim=1).flatten()\n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == labels).sum()/len(preds)\n",
        "            total_accuracy.append(accuracy)\n",
        "            y_pred.extend(list(preds))\n",
        "            y_true.extend(list(labels))\n",
        "\n",
        "    acc = 0.0\n",
        "    for i, y in enumerate(y_pred):\n",
        "        if y == y_true[i]:\n",
        "            acc += 1.0\n",
        "\n",
        "    acc = acc / len(y_pred)\n",
        "\n",
        "    print('Test Accuracy: ', acc)\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1, 0], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    ax = plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['1', '0'])\n",
        "    ax.yaxis.set_ticklabels(['1', '0'])\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "rdGatR-Bdl_q"
      },
      "id": "rdGatR-Bdl_q",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(best_model, test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "O4Jvs2yzv2FX",
        "outputId": "b8cd88cc-e3c3-42bb-a694-033b82af8c11"
      },
      "id": "O4Jvs2yzv2FX",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate\n",
            "Test Accuracy:  0.9586811352253757\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8596    0.4579    0.5976       321\n",
            "           0     0.9623    0.9946    0.9782      4471\n",
            "\n",
            "    accuracy                         0.9587      4792\n",
            "   macro avg     0.9110    0.7263    0.7879      4792\n",
            "weighted avg     0.9555    0.9587    0.9527      4792\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "opMFSg_swI2v"
      },
      "id": "opMFSg_swI2v",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}