{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "marked-western",
      "metadata": {
        "id": "marked-western"
      },
      "source": [
        "# CNN\n",
        "\n",
        "CNN model for tweet classification\n",
        "\n",
        "\n",
        "Reference:\n",
        "- Mainly based on Hw3 codes\n",
        "- A Complete Guide to CNN for Sentence Classification with PyTorch[https://chriskhanhtran.github.io/posts/cnn-sentence-classification/]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "directed-leonard",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "directed-leonard",
        "outputId": "628fefea-ec7c-408c-8b2b-4be79fddadd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "    USE_CUDA=True\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    USE_CUDA=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "constant-photography",
      "metadata": {
        "id": "constant-photography"
      },
      "outputs": [],
      "source": [
        "SEED = 30255 # Specify a seed for reproducability\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "alien-haiti",
      "metadata": {
        "id": "alien-haiti"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data/Twitter/hate_twitter/hate_train.csv')\n",
        "val_df = pd.read_csv('data/Twitter/hate_twitter/hate_val.csv')\n",
        "test_df = pd.read_csv('data/Twitter/hate_twitter/hate_test.csv')\n",
        "\n",
        "\n",
        "# Check and drop na values in clean_tweet column\n",
        "train_df[train_df['clean_tweet'].isnull()]\n",
        "\n",
        "train_df = train_df[train_df['clean_tweet'].notna()]\n",
        "val_df = val_df[val_df['clean_tweet'].notna()]\n",
        "test_df = test_df[test_df['clean_tweet'].notna()]\n",
        "\n",
        "\n",
        "x_train = train_df['clean_tweet']\n",
        "y_train = train_df['label']\n",
        "\n",
        "x_test = test_df['clean_tweet']\n",
        "y_test = test_df['label']\n",
        "\n",
        "x_val = val_df['clean_tweet']\n",
        "y_val = val_df['label']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "starting-reviewer",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "starting-reviewer",
        "outputId": "b0c2ceb0-41a1-4bbe-e8d2-fb63faab370e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    13211\n",
              "1     1028\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_df.groupby('label').size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incident-monaco",
      "metadata": {
        "id": "incident-monaco"
      },
      "source": [
        "## Convert to Pytorch Data Objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "indian-choir",
      "metadata": {
        "id": "indian-choir"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dataclassCNN.py\n",
        "File to create a Custom Data Class and Collate Function for PyTorch.\n",
        "This file is for the CNN model.\n",
        "'''\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "\n",
        "class ProjectDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, target_col=None, text_col=None):\n",
        "\n",
        "        # Target first, then Inputs.\n",
        "        self.samples = []\n",
        "        tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "        if not target_col and not text_col:\n",
        "            targets = list(data[0])\n",
        "            inputs = list(data[1])\n",
        "            for idx in range(len(targets)):\n",
        "                text = tokenizer(inputs[idx])\n",
        "                self.samples.append([targets[idx], text])\n",
        "        else:\n",
        "            for _, row in data.iterrows():\n",
        "                text = row[text_col]\n",
        "                text = tokenizer(text)\n",
        "                target = row[target_col]\n",
        "                self.samples.append([target, text])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "unnecessary-andrew",
      "metadata": {
        "id": "unnecessary-andrew"
      },
      "outputs": [],
      "source": [
        "data_obj = ProjectDataset(train_df, 'label', 'clean_tweet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abroad-employee",
      "metadata": {
        "id": "abroad-employee"
      },
      "source": [
        "## Load Pre-trained vectors GloVe "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "acceptable-kernel",
      "metadata": {
        "id": "acceptable-kernel"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "significant-findings",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "significant-findings",
        "outputId": "160435a6-a1b2-4958-f9b4-5cc307ff8b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary length is 24472 words\n",
            "Max number of words for a given tweet in the Dataset:\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "\n",
        "counter_words = Counter()\n",
        "for (label, text) in data_obj:\n",
        "    counter_words.update(text)\n",
        "    \n",
        "vocab_words = vocab(counter_words)\n",
        "vocab_words.set_default_index(0)\n",
        "\n",
        "print('The vocabulary length is {} words'.format(len(vocab_words)))\n",
        "\n",
        "VECTORS_CACHE_DIR = './.vector_cache'\n",
        "\n",
        "from torchtext import vocab\n",
        "\n",
        "pretrained_emb = vocab.GloVe(name='6B',cache=VECTORS_CACHE_DIR)\n",
        "\n",
        "print('Max number of words for a given tweet in the Dataset:')\n",
        "max_len = train_df[\"clean_tweet\"].apply(lambda x: len(x.split())).max()\n",
        "print(max_len)\n",
        "\n",
        "pre_trained_vectors = pretrained_emb.get_vecs_by_tokens(vocab_words.get_itos(),\n",
        "                                                       lower_case_backup=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "original-newark",
      "metadata": {
        "id": "original-newark"
      },
      "outputs": [],
      "source": [
        "def collate_for_cnn(batch, max_len=20):\n",
        "    '''\n",
        "    For each batch, develop the appropiate inputs from the models (i.e. embeddings)\n",
        "    '''\n",
        "    input_vector = []\n",
        "    labels = []\n",
        "    for i, (label, tokenized_sent) in enumerate(batch):\n",
        "        \n",
        "        #Append labels \n",
        "        labels.append(label)\n",
        "        \n",
        "        #Get tokenized sentence\n",
        "        if len(tokenized_sent) >= max_len:\n",
        "            tokenized_sent = tokenized_sent[:20]\n",
        "        else:\n",
        "            diff =  (max_len - len(tokenized_sent))\n",
        "            tokenized_sent += ['<pad>'] * diff\n",
        "                \n",
        "        wordstoidx = [vocab_words[w] for w in tokenized_sent]\n",
        "        input_vector.append(wordstoidx)\n",
        "    \n",
        "    return torch.tensor(labels).to(device), torch.tensor(input_vector).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bizarre-mexico",
      "metadata": {
        "id": "bizarre-mexico"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regional-jumping",
      "metadata": {
        "id": "regional-jumping"
      },
      "source": [
        "## Setting up DataLoader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "normal-cosmetic",
      "metadata": {
        "id": "normal-cosmetic"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def data_loader(train_tuple, val_tuple, test_tuple,\n",
        "                batch_size=20):\n",
        "    \"\"\"Convert train, validation and test sets into Dataloaders\n",
        "    \"\"\"\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = ProjectDataset(train_tuple)\n",
        "    train_dataloader = DataLoader(train_data, shuffle=True,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "    \n",
        "    # Create DataLoader for validation data\n",
        "    valid_data = ProjectDataset(val_tuple)\n",
        "    val_dataloader = DataLoader(valid_data, shuffle=False,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "\n",
        "    # Create DataLoader for test data\n",
        "    test_data = ProjectDataset(test_tuple)\n",
        "    test_dataloader = DataLoader(test_data, shuffle=False,\n",
        "                                  batch_size=batch_size, \n",
        "                                 collate_fn=collate_for_cnn)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "narrow-official",
      "metadata": {
        "id": "narrow-official"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = data_loader([y_train, x_train],\n",
        "                                                               [y_val, x_val],\n",
        "                                                               [y_test, x_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "legendary-breast",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "legendary-breast",
        "outputId": "22d02405-74a1-4fda-d7d6-6dae6dea3f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 20])\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataloader:\n",
        "    labels, inputs = i[0], i[1]\n",
        "    print(inputs.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immediate-passage",
      "metadata": {
        "id": "immediate-passage"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "personalized-weekly",
      "metadata": {
        "id": "personalized-weekly"
      },
      "outputs": [],
      "source": [
        "FILTER_SIZES = [3,4,5]\n",
        "N_FILTERS = [100, 100,100]\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = vocab_words['<pad>']\n",
        "num_classes = 2 #0, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "artistic-mason",
      "metadata": {
        "id": "artistic-mason"
      },
      "outputs": [],
      "source": [
        "# https://github.com/advanced-ml-project/project/blob/main/cnn_development.ipynb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_classification(nn.Module):\n",
        "    ''' An 1D Convulational Neural Network for Sentence Classification'''\n",
        "    \n",
        "    def __init__(self, pretrained_embedding=None, freeze_embedding=False,\n",
        "                 vocab_size=None, embed_dim=None,\n",
        "                 filter_sizes=FILTER_SIZES, num_filters=N_FILTERS,\n",
        "                 num_classes=num_classes,pad_id=PAD_IDX,\n",
        "                 dropout=DROPOUT):\n",
        "        \"\"\"\n",
        "        pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
        "                shape (vocab_size, embed_dim)\n",
        "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
        "                vectors. Default: False\n",
        "            vocab_size (int): Need to be specified when not pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (int): Number of filters. Default: 100\n",
        "            n_classes (int): Number of classes. Default: 3\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_classification, self).__init__()\n",
        "        \n",
        "        # 1. Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx= pad_id,\n",
        "                                          )\n",
        "        \n",
        "        # 2. Convolutional Layers (for each filter size --> n-gram)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = self.embed_dim,\n",
        "                                              out_channels = num_filters[i], \n",
        "                                              kernel_size = fs)\n",
        "                                    for i, fs in enumerate(filter_sizes)\n",
        "                                    ])\n",
        "        \n",
        "    \n",
        "        # 3. Fully-connected layer\n",
        "        self.linear = nn.Linear(in_features = np.sum(num_filters), \n",
        "                                out_features = num_classes\n",
        "                               )\n",
        "        \n",
        "        \n",
        "        # Additional Feature\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        x_embedded = self.embedding(inputs)\n",
        "        \n",
        "        x_embedded = x_embedded.permute(0, 2, 1)\n",
        "        \n",
        "        # Apply CNN and ReLU\n",
        "        convs_list = [F.relu(conv(x_embedded)) for conv in self.convs]\n",
        "\n",
        "        # Max pooling.\n",
        "        pooled = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]).squeeze(2)\n",
        "                  for x_conv in convs_list]\n",
        "        \n",
        "        # Concatenate Pool list to feed the fully connected layer\n",
        "        input_fc = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        \n",
        "        # Compute probabilities\n",
        "        predictions_classes = self.linear(input_fc)\n",
        "        \n",
        "        return predictions_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "scientific-kenya",
      "metadata": {
        "id": "scientific-kenya"
      },
      "outputs": [],
      "source": [
        "def train_an_epoch(model, dataloader, loss_function, optimizer):\n",
        "    \n",
        "    model.train() # Sets the module in training mode.\n",
        "    total_loss = 0\n",
        "    \n",
        "    for idx, batch in enumerate(dataloader):\n",
        "        \n",
        "        # Load batch to GPU\n",
        "        labels, inputs = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Zero out any previously calculated gradients\n",
        "        model.zero_grad()\n",
        "        \n",
        "        #Perform a forward pass.\n",
        "        log_probs = model(inputs)\n",
        "        \n",
        "        # Compute loss and accumulate the loss values\n",
        "        loss = loss_function(log_probs, labels.long())\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Perform a backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    return total_loss/len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLG68DJnmc0",
        "outputId": "545fe6c6-d8bb-447b-fd7c-ddff47590d52"
      },
      "id": "_hLG68DJnmc0",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "median-feature",
      "metadata": {
        "id": "median-feature"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import F1Score, Accuracy, Recall, Precision\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    '''\n",
        "    Evaluate the model on the given data (e.g. validation data or test data).\n",
        "    '''\n",
        "\n",
        "    #As we are now using dropout, we must remember to use model.eval() \n",
        "    #to ensure the dropout is \"turned off\" while evaluating.\n",
        "    model.eval()\n",
        "    \n",
        "    total_accuracy = []\n",
        "    total_loss = []\n",
        "    total_recall = []\n",
        "    total_precision = []\n",
        "    total_f1_score = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            \n",
        "            # Load batch to Device\n",
        "            labels, inputs = batch[0], batch[1]\n",
        "            if USE_CUDA:\n",
        "                labels, inputs = labels.cuda(), inputs.cuda()\n",
        "                \n",
        "            # Obtain probabilities of each class per sentence\n",
        "            output = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_fn(output, labels.long())\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            # Get the predictions\n",
        "            preds = torch.argmax(output, dim=1).flatten()\n",
        "            \n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == labels).sum()/len(preds)\n",
        "            total_accuracy.append(accuracy)\n",
        "\n",
        "            # Calculate the recall score\n",
        "            recall = Recall()\n",
        "            recall_score = recall(preds, labels).item()\n",
        "            total_recall.append(recall_score)\n",
        "            total_recall.append(recall_score)\n",
        "\n",
        "            # Calculate the precision score\n",
        "            precision = Precision()\n",
        "            precision_score = precision(preds, labels).item()\n",
        "            total_precision.append(precision_score)\n",
        "\n",
        "            # Calculate the f1 score\n",
        "            # f1_score = (2 * recall * precision) / (recall + precision)\n",
        "            f1 = F1Score(num_classes=2)\n",
        "            f1_score = f1(preds, labels).item()\n",
        "            total_f1_score.append(f1_score)\n",
        "\n",
        "    return np.mean(total_loss), np.mean(total_accuracy), np.mean(total_recall), np.mean(total_precision), np.mean(total_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "relevant-taste",
      "metadata": {
        "id": "relevant-taste"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def train_validate(model, optimizer, train_dataloader, val_dataloader, pretrained_embedding, epochs=20):\n",
        "    \n",
        "    \"\"\"Train the CNN model.\"\"\"\n",
        "    \n",
        "    #Loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Tracking best model\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_recalls = []\n",
        "    val_precisions = []\n",
        "    val_f1_scores = []\n",
        "    best_model = None\n",
        "    \n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Time(secs)':^8} | {'Train Loss':^9}| {'Val Loss':^10}| {'Val Acc':^11} | {'Val Recall':^12}  | {'Val Precision':^13}  | {'Val F1':^14}\")\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        start_time = datetime.datetime.now()\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        \n",
        "        avg_loss_train = train_an_epoch(model, train_dataloader, loss_function, optimizer)\n",
        "        \n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        \n",
        "        val_loss, val_accuracy, val_recall, val_precision, val_f1 = evaluate(model, val_dataloader, loss_function)\n",
        "\n",
        "        # Track the best model\n",
        "        if len(val_losses) == 0 or val_loss < min(val_losses):\n",
        "            best_model = type(model)(pretrained_embedding=pretrained_embedding,\n",
        "                                     vocab_size=len(vocab_words), \n",
        "                                     embed_dim=model.embed_dim,\n",
        "                                     filter_sizes=FILTER_SIZES,\n",
        "                                     num_filters=N_FILTERS, num_classes=num_classes)\n",
        "            best_model.load_state_dict(model.state_dict())\n",
        "            if USE_CUDA:\n",
        "                best_model = best_model.cuda()\n",
        "        \n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        val_recalls.append(val_recall)\n",
        "        val_precisions.append(val_precision)\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        time_diff = (datetime.datetime.now() - start_time).seconds\n",
        "        \n",
        "        print(f\"{epoch_i + 1:^7} |  {time_diff:^8}  | {avg_loss_train:^9.2f} | {val_loss:^10.2f}| {val_accuracy:^11.2f}| {val_recall:^12.2f}| {val_precision:^13.2f} | {val_f1:^14.2f}\")\n",
        "        print('')\n",
        "        \n",
        "    return best_model, val_accuracies, val_recalls, val_precisions, val_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "healthy-latitude",
      "metadata": {
        "id": "healthy-latitude"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "inner-findings",
      "metadata": {
        "id": "inner-findings"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN_classification(pretrained_embedding=None,\n",
        "                 vocab_size=len(vocab_words), embed_dim=400,\n",
        "                        freeze_embedding=False,\n",
        "                        filter_sizes=FILTER_SIZES,\n",
        "                        num_filters=N_FILTERS,\n",
        "                        num_classes=num_classes,\n",
        "                        dropout=0.5).to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), \n",
        "                                 lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "liked-colombia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liked-colombia",
        "outputId": "c09ff9c5-8818-48ff-84fe-4d7506975f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  | Time(secs) | Train Loss|  Val Loss |   Val Acc   |  Val Recall   | Val Precision  |     Val F1    \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "   1    |    154     |   0.22    |    0.19   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   2    |    147     |   0.09    |    0.19   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   3    |    166     |   0.05    |    0.33   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   4    |    161     |   0.04    |    0.33   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   5    |    159     |   0.03    |    0.37   |    0.94    |     0.94    |     0.94      |      0.94     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_model, val_accuracies, val_recalls, val_precisions, val_f1_scores = train_validate(cnn_model, optimizer, \n",
        "                                            train_dataloader, val_dataloader, \n",
        "                                            pretrained_embedding=None,\n",
        "                                            epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the best model"
      ],
      "metadata": {
        "id": "R_P9epXecwy9"
      },
      "id": "R_P9epXecwy9"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "lyric-finish",
      "metadata": {
        "id": "lyric-finish"
      },
      "outputs": [],
      "source": [
        "#Loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "test_loss, test_accuracy, test_recall, test_precision, test_f1_score = evaluate(best_model, test_dataloader, loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{test_loss:^10.2f}| {test_accuracy:^11.2f}| {test_recall:^12.2f}| {test_precision:^13.2f} | {test_f1_score:^14.2f}\")\n"
      ],
      "metadata": {
        "id": "AAx3IgzvdmCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e668f7-7379-4bb8-cedb-22c957bd6f25"
      },
      "id": "AAx3IgzvdmCC",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0.15   |    0.95    |     0.95    |     0.95      |      0.95     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_metrics(model, test_loader, device='cpu'):\n",
        "    print('Evaluate')\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    total_accuracy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            # Load batch to Device\n",
        "            labels, inputs = batch[0], batch[1]\n",
        "            labels, inputs = labels.to(device), inputs.to(device)\n",
        "            # Obtain probabilities of each class per sentence\n",
        "            output = model(inputs)\n",
        "            # Get the predictions\n",
        "            preds = torch.argmax(output, dim=1).flatten()\n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == labels).sum()/len(preds)\n",
        "            total_accuracy.append(accuracy)\n",
        "            y_pred.extend(list(preds))\n",
        "            y_true.extend(list(labels))\n",
        "\n",
        "    acc = 0.0\n",
        "    for i, y in enumerate(y_pred):\n",
        "        if y == y_true[i]:\n",
        "            acc += 1.0\n",
        "\n",
        "    acc = acc / len(y_pred)\n",
        "\n",
        "    print('Test Accuracy: ', acc)\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1, 0], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    ax = plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['1', '0'])\n",
        "    ax.yaxis.set_ticklabels(['1', '0'])\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "rdGatR-Bdl_q"
      },
      "id": "rdGatR-Bdl_q",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(best_model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "O4Jvs2yzv2FX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "95768795-a84d-4f0a-fda4-f4bb4e514154"
      },
      "id": "O4Jvs2yzv2FX",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate\n",
            "Test Accuracy:  0.9536727879799666\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7004    0.5389    0.6092       321\n",
            "           0     0.9674    0.9834    0.9754      4471\n",
            "\n",
            "    accuracy                         0.9537      4792\n",
            "   macro avg     0.8339    0.7612    0.7923      4792\n",
            "weighted avg     0.9495    0.9537    0.9508      4792\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upsampled dataset"
      ],
      "metadata": {
        "id": "H1lY6p6ZULSW"
      },
      "id": "H1lY6p6ZULSW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling the Minority Class\n",
        "from sklearn.utils import resample\n",
        "\n",
        "train_majority = train_df[train_df.label==0]\n",
        "train_minority = train_df[train_df.label==1]\n",
        "train_minority_upsampled = resample(train_minority, \n",
        "                                 replace=True,    \n",
        "                                 n_samples=len(train_majority),   \n",
        "                                 random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
        "train_upsampled['label'].value_counts()"
      ],
      "metadata": {
        "id": "opMFSg_swI2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5adfbb54-a497-4456-afe9-3f7c524c9116"
      },
      "id": "opMFSg_swI2v",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    13211\n",
              "0    13211\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_upsampled['clean_tweet']\n",
        "y_train = train_upsampled['label']"
      ],
      "metadata": {
        "id": "8n6W3c_TVY5h"
      },
      "id": "8n6W3c_TVY5h",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_obj = ProjectDataset(train_df, 'label', 'clean_tweet')\n"
      ],
      "metadata": {
        "id": "brNAdvZBVbMt"
      },
      "id": "brNAdvZBVbMt",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "\n",
        "counter_words = Counter()\n",
        "for (label, text) in data_obj:\n",
        "    counter_words.update(text)\n",
        "    \n",
        "vocab_words = vocab(counter_words)\n",
        "vocab_words.set_default_index(0)\n",
        "\n",
        "print('The vocabulary length is {} words'.format(len(vocab_words)))\n",
        "\n",
        "VECTORS_CACHE_DIR = './.vector_cache'\n",
        "\n",
        "from torchtext import vocab\n",
        "\n",
        "pretrained_emb = vocab.GloVe(name='6B',cache=VECTORS_CACHE_DIR)\n",
        "\n",
        "print('Max number of words for a given tweet in the Dataset:')\n",
        "max_len = train_df[\"clean_tweet\"].apply(lambda x: len(x.split())).max()\n",
        "print(max_len)\n",
        "\n",
        "pre_trained_vectors = pretrained_emb.get_vecs_by_tokens(vocab_words.get_itos(),\n",
        "                                                       lower_case_backup=True)"
      ],
      "metadata": {
        "id": "eNMZgAbuVbKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dad556b-d75b-4da3-fd78-a7127b259d8f"
      },
      "id": "eNMZgAbuVbKe",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary length is 24472 words\n",
            "Max number of words for a given tweet in the Dataset:\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = data_loader([y_train, x_train],\n",
        "                                                               [y_val, x_val],\n",
        "                                                               [y_test, x_test])"
      ],
      "metadata": {
        "id": "Ng9J9NyIVbIh"
      },
      "id": "Ng9J9NyIVbIh",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RhM71bdcWp-j"
      },
      "id": "RhM71bdcWp-j",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN_classification(pretrained_embedding=None,\n",
        "                 vocab_size=len(vocab_words), embed_dim=400,\n",
        "                        freeze_embedding=False,\n",
        "                        filter_sizes=FILTER_SIZES,\n",
        "                        num_filters=N_FILTERS,\n",
        "                        num_classes=num_classes,\n",
        "                        dropout=0.5).to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), \n",
        "                                 lr=learning_rate)"
      ],
      "metadata": {
        "id": "HsI-PTYfWqFT"
      },
      "id": "HsI-PTYfWqFT",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, val_accuracies, val_recalls, val_precisions, val_f1_scores = train_validate(cnn_model, optimizer, \n",
        "                                            train_dataloader, val_dataloader, \n",
        "                                            pretrained_embedding=None,\n",
        "                                            epochs=5)"
      ],
      "metadata": {
        "id": "CEfDePwEWwQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12c31e6-5fe6-4f52-8ddc-a9d951524f1e"
      },
      "id": "CEfDePwEWwQq",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  | Time(secs) | Train Loss|  Val Loss |   Val Acc   |  Val Recall   | Val Precision  |     Val F1    \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "   1    |    254     |   0.16    |    0.23   |    0.95    |     0.95    |     0.95      |      0.95     \n",
            "\n",
            "   2    |    307     |   0.03    |    0.31   |    0.94    |     0.94    |     0.94      |      0.94     \n",
            "\n",
            "   3    |    317     |   0.03    |    0.48   |    0.94    |     0.94    |     0.94      |      0.94     \n",
            "\n",
            "   4    |    326     |   0.03    |    0.58   |    0.94    |     0.94    |     0.94      |      0.94     \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the best model\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "test_loss, test_accuracy, test_recall, test_precision, test_f1_score = evaluate(best_model, test_dataloader, loss_function)\n",
        "print(f\"{test_loss:^10.2f}| {test_accuracy:^11.2f}| {test_recall:^12.2f}| {test_precision:^13.2f} | {test_f1_score:^14.2f}\")\n"
      ],
      "metadata": {
        "id": "Q0MNU2jVXCtK"
      },
      "id": "Q0MNU2jVXCtK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(best_model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "6-3pS1ZaXCgz"
      },
      "id": "6-3pS1ZaXCgz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}