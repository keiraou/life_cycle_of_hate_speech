{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Take3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F5mcvgvmJw9",
        "outputId": "544ad9e6-3c76-432d-8beb-fdf091792827"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.11.2 in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Requirement already satisfied: torch==1.10.2 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ps8NS81Pl_kA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "\n",
        "glove = GloVe(name=\"twitter.27B\", dim=25)\n",
        "\n",
        "# glove.twitter.27B.25d\n",
        "\n",
        "# glove = GloVe(name='6B')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nn86Sr-guRE",
        "outputId": "ccec7134-c9f6-4ae1-f416-30f19c184988"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [04:46, 5.31MB/s]                            \n",
            "100%|█████████▉| 1193513/1193514 [00:24<00:00, 49621.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "from itertools import combinations\n",
        "from torchtext import vocab\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "if device == 'cpu':\n",
        "    VECTORS_CACHE_DIR = '/Users/sophiamlawer/.vector_cache'\n",
        "    # Please change above to your cache\n",
        "else:\n",
        "    VECTORS_CACHE_DIR = './.vector_cache'\n",
        "    # This is the default cache on Colab. Caching may not work\n",
        "    # as expected on Colab.\n",
        "\n",
        "\n",
        "\n",
        "glove = vocab.GloVe(name=\"twitter.27B\", dim=100,cache=VECTORS_CACHE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkIj_VgMUKWy",
        "outputId": "965ff6f5-1646-4109-9900-18969d951511"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1193513/1193514 [00:52<00:00, 22710.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Read in data\n",
        "import pandas as pd \n",
        "\n",
        "df_train = pd.read_csv(\"hate_upsampled_train.csv\")\n",
        "df_test = pd.read_csv(\"hate_upsampled_test.csv\")\n",
        "df_val = pd.read_csv(\"hate_upsampled_val.csv\")\n",
        "\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "WzAdrfHXi4-s",
        "outputId": "d1d17dbc-f31d-43ff-8e88-57a2ee8382b2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41608, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1     id  label  \\\n",
              "0       44049         15405  15406      0   \n",
              "1       33142          3684   3685      0   \n",
              "2       40964         12093  12094      0   \n",
              "3       53102         25149  25150      0   \n",
              "4       15142         12744  12745      1   \n",
              "\n",
              "                                               tweet  \\\n",
              "0  @user we really are. lmaoo we are obviously re...   \n",
              "1  i think my #hea need a #bandage again ð   #...   \n",
              "2  new phone #xperiaz3+ #xperiaz4 #sony #thebeast...   \n",
              "3    #dinner at my friend's #steak #shop never #d...   \n",
              "4  @user @user @user and sorry if we are ethnical...   \n",
              "\n",
              "                                            hash_tag  \\\n",
              "0                                                 []   \n",
              "1                         ['hea', 'bandage', 'love']   \n",
              "2  ['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...   \n",
              "3  ['dinner', 'steak', 'shop', 'disappoint', 'yum...   \n",
              "4                                                 []   \n",
              "\n",
              "                                         clean_tweet  \\\n",
              "0  really lmaoo obviously related haha wonder cam...   \n",
              "1                        think hea need bandage love   \n",
              "2  new phone xperiaz3 xperiaz4 sony thebeast copp...   \n",
              "3  dinner friends steak shop never disappoint yum...   \n",
              "4  sorry ethnically cleansing east jerusalem beth...   \n",
              "\n",
              "                                     tokenized_tweet  \\\n",
              "0  <user> we really are. lmaoo we are obviously r...   \n",
              "1  i think my <hashtag> hea need a <hashtag> band...   \n",
              "2  new phone <hashtag> xperiaz<number>+ <hashtag>...   \n",
              "3    <hashtag> dinner at my friend's <hashtag> st...   \n",
              "4  <user> <user> <user> and sorry if we are ethni...   \n",
              "\n",
              "                                tokenized_tweet_NLTK  \n",
              "0  @user really lmaoo obviously related haha wond...  \n",
              "1                        think hea need bandage love  \n",
              "2  new phone xperiaz3 xperiaz4 sony thebeast copp...  \n",
              "3  dinner friend's steak shop never disappoint yu...  \n",
              "4  @user @user @user sorry ethnically cleansing e...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66ad5c1d-5a2b-47b2-89e5-3f84bc5f6533\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hash_tag</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tokenized_tweet</th>\n",
              "      <th>tokenized_tweet_NLTK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44049</td>\n",
              "      <td>15405</td>\n",
              "      <td>15406</td>\n",
              "      <td>0</td>\n",
              "      <td>@user we really are. lmaoo we are obviously re...</td>\n",
              "      <td>[]</td>\n",
              "      <td>really lmaoo obviously related haha wonder cam...</td>\n",
              "      <td>&lt;user&gt; we really are. lmaoo we are obviously r...</td>\n",
              "      <td>@user really lmaoo obviously related haha wond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33142</td>\n",
              "      <td>3684</td>\n",
              "      <td>3685</td>\n",
              "      <td>0</td>\n",
              "      <td>i think my #hea need a #bandage again ð   #...</td>\n",
              "      <td>['hea', 'bandage', 'love']</td>\n",
              "      <td>think hea need bandage love</td>\n",
              "      <td>i think my &lt;hashtag&gt; hea need a &lt;hashtag&gt; band...</td>\n",
              "      <td>think hea need bandage love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40964</td>\n",
              "      <td>12093</td>\n",
              "      <td>12094</td>\n",
              "      <td>0</td>\n",
              "      <td>new phone #xperiaz3+ #xperiaz4 #sony #thebeast...</td>\n",
              "      <td>['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...</td>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "      <td>new phone &lt;hashtag&gt; xperiaz&lt;number&gt;+ &lt;hashtag&gt;...</td>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53102</td>\n",
              "      <td>25149</td>\n",
              "      <td>25150</td>\n",
              "      <td>0</td>\n",
              "      <td>#dinner at my friend's #steak #shop never #d...</td>\n",
              "      <td>['dinner', 'steak', 'shop', 'disappoint', 'yum...</td>\n",
              "      <td>dinner friends steak shop never disappoint yum...</td>\n",
              "      <td>&lt;hashtag&gt; dinner at my friend's &lt;hashtag&gt; st...</td>\n",
              "      <td>dinner friend's steak shop never disappoint yu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15142</td>\n",
              "      <td>12744</td>\n",
              "      <td>12745</td>\n",
              "      <td>1</td>\n",
              "      <td>@user @user @user and sorry if we are ethnical...</td>\n",
              "      <td>[]</td>\n",
              "      <td>sorry ethnically cleansing east jerusalem beth...</td>\n",
              "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; and sorry if we are ethni...</td>\n",
              "      <td>@user @user @user sorry ethnically cleansing e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66ad5c1d-5a2b-47b2-89e5-3f84bc5f6533')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-66ad5c1d-5a2b-47b2-89e5-3f84bc5f6533 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-66ad5c1d-5a2b-47b2-89e5-3f84bc5f6533');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train['clean_tweet'].notna()]\n",
        "df_test = df_test[df_test['clean_tweet'].notna()]\n",
        "df_val = df_val[df_val['clean_tweet'].notna()]\n",
        "\n",
        "df_train[[\"clean_tweet\", \"label\"]].to_csv(\"train.csv\")\n",
        "df_test[[\"clean_tweet\", \"label\"]].to_csv(\"test.csv\")\n",
        "df_val[[\"clean_tweet\", \"label\"]].to_csv(\"val.csv\")"
      ],
      "metadata": {
        "id": "qv7yKipzjE-o"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[[\"clean_tweet\", \"label\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cDLaOXmBzb3F",
        "outputId": "e89f43da-0f6d-488a-d958-4e0af44bea80"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             clean_tweet  label\n",
              "0      really lmaoo obviously related haha wonder cam...      0\n",
              "1                            think hea need bandage love      0\n",
              "2      new phone xperiaz3 xperiaz4 sony thebeast copp...      0\n",
              "3      dinner friends steak shop never disappoint yum...      0\n",
              "4      sorry ethnically cleansing east jerusalem beth...      1\n",
              "...                                                  ...    ...\n",
              "41603      invited help700 actions peace povey sept 1625      1\n",
              "41604         might libtard libtard sjw liberal politics      1\n",
              "41605  people arent protesting trump republican wonth...      1\n",
              "41606          ppl playing statedey playing india future      0\n",
              "41607  tonight presents outdeh rootsandchallis nairob...      0\n",
              "\n",
              "[41560 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ad3d31d-f1ca-4c9b-91d7-08f4ec48cd54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>really lmaoo obviously related haha wonder cam...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>think hea need bandage love</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dinner friends steak shop never disappoint yum...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sorry ethnically cleansing east jerusalem beth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41603</th>\n",
              "      <td>invited help700 actions peace povey sept 1625</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41604</th>\n",
              "      <td>might libtard libtard sjw liberal politics</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41605</th>\n",
              "      <td>people arent protesting trump republican wonth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41606</th>\n",
              "      <td>ppl playing statedey playing india future</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41607</th>\n",
              "      <td>tonight presents outdeh rootsandchallis nairob...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41560 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ad3d31d-f1ca-4c9b-91d7-08f4ec48cd54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ad3d31d-f1ca-4c9b-91d7-08f4ec48cd54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ad3d31d-f1ca-4c9b-91d7-08f4ec48cd54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try This Model"
      ],
      "metadata": {
        "id": "9eYmi8ZBneKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "TEXT = Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = LabelField(sequential=False, use_vocab=False, dtype = torch.float,batch_first=True)"
      ],
      "metadata": {
        "id": "PsL5vWt6iq68"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fields = [('label', LABEL), ('clean_tweet',TEXT)]\n",
        "# fields = [('clean_tweet',TEXT)]\n",
        "fields = [(None, None), ('clean_tweet',TEXT),('label', LABEL)]\n"
      ],
      "metadata": {
        "id": "ItV_PAynjAzX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = TabularDataset.splits(path = \"/content/\", train='train.csv',validation='val.csv',\n",
        "                                         test='test.csv', format='csv',\n",
        "                                         fields= fields, skip_header=True)\n",
        "\n",
        "print(vars(train.examples[6]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PvY2OljnhYo",
        "outputId": "f3b9f5cb-07b5-4f69-e030-039161a24d63"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clean_tweet': ['look', 'repug', 'gop', 'pay', 'bigots'], 'label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train,min_freq=3,vectors = \"glove.twitter.27B.25d\")  \n",
        "# LABEL.build_vocab(train)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "# print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "# print(LABEL.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WQ4ToRVjUO7",
        "outputId": "72e8240a-a256-4324-db4d-23337e91a772"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of TEXT vocabulary: 10396\n",
            "[('nt', 3678), ('love', 2114), ('like', 1965), ('trump', 1943), ('day', 1636), ('i', 1609), ('people', 1424), ('libtard', 1392), ('white', 1329), ('new', 1296)]\n",
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7fb3feee4a90>>, {'<unk>': 0, '<pad>': 1, 'nt': 2, 'love': 3, 'like': 4, 'trump': 5, 'day': 6, 'i': 7, 'people': 8, 'libtard': 9, 'white': 10, 'new': 11, 'black': 12, 'happy': 13, 'm': 14, 's': 15, 'do': 16, 'racist': 17, 'one': 18, 'time': 19, 'politics': 20, 'us': 21, 'allahsoil': 22, 'ca': 23, 'get': 24, 'good': 25, 'life': 26, 'today': 27, 'feel': 28, 'might': 29, 'liberal': 30, 'see': 31, 'women': 32, 'hate': 33, 'go': 34, 'retweet': 35, 'sjw': 36, 'want': 37, 're': 38, 'positive': 39, 'obama': 40, 'thankful': 41, 'you': 42, 'would': 43, 'racism': 44, 'listen': 45, 'via': 46, 'got': 47, 'bihday': 48, 'work': 49, 'make': 50, 'take': 51, 'great': 52, 'never': 53, 'think': 54, 'really': 55, 'need': 56, 'back': 57, 'america': 58, 'smile': 59, 'way': 60, '2016': 61, 'man': 62, 'world': 63, 'right': 64, 'know': 65, 'miami': 66, 'stomping': 67, 'fathers': 68, 'thanks': 69, 'stop': 70, 'going': 71, 'girl': 72, 'much': 73, 'first': 74, 'say': 75, 'woman': 76, 'race': 77, 'fun': 78, 'video': 79, 'even': 80, 'that': 81, 've': 82, 'healthy': 83, 'best': 84, 'look': 85, 'still': 86, 'sad': 87, 'weekend': 88, 'men': 89, 'year': 90, 'days': 91, 'comments': 92, 'family': 93, 'ever': 94, 'live': 95, 'ur': 96, 'many': 97, 'pay': 98, 'wait': 99, 'friends': 100, 'music': 101, 'sex': 102, 'news': 103, 'summer': 104, 'next': 105, 'beautiful': 106, 'always': 107, 'nothing': 108, 'friday': 109, 'girls': 110, 'makes': 111, 'bull': 112, 'says': 113, 'well': 114, '2017': 115, 'wo': 116, 'morning': 117, 'come': 118, 'free': 119, 'show': 120, 'let': 121, 'media': 122, 'thank': 123, 'latest': 124, 'last': 125, 'find': 126, 'tampa': 127, 'home': 128, 'blm': 129, 'orlando': 130, 'call': 131, 'things': 132, 'week': 133, 'is': 134, 'keep': 135, 'please': 136, 'cute': 137, 'every': 138, 'fathersday': 139, 'hope': 140, 'believe': 141, 'everyone': 142, 'hatred': 143, 'tomorrow': 144, 'years': 145, 'model': 146, 'real': 147, 'sunday': 148, 'school': 149, 'act': 150, 'brexit': 151, 'guy': 152, 'night': 153, 'god': 154, 'fuck': 155, 'police': 156, 'does': 157, 'hispanic': 158, 'blog': 159, 'help': 160, 'history': 161, 'follow': 162, 'happiness': 163, 'done': 164, 'game': 165, 'bigot': 166, 'usa': 167, 'thing': 168, 'finally': 169, 'better': 170, 'another': 171, 'calgary': 172, 'little': 173, 'made': 174, 'person': 175, 'altright': 176, 'sikh': 177, 'word': 178, 'affirmation': 179, 'someone': 180, 'said': 181, 'temple': 182, 'watch': 183, 'daily': 184, 'book': 185, 'could': 186, 'leadership': 187, 'peace': 188, 'condemns': 189, 'he': 190, 'thought': 191, 'vandalised': 192, 'wso': 193, 'dad': 194, 'amazing': 195, 'did': 196, 'selfie': 197, 'teambts': 198, 'president': 199, 'face': 200, 'd': 201, 'malevote': 202, 'suppoers': 203, 'boricua': 204, 'healing': 205, 'team': 206, 'lost': 207, 'give': 208, 'must': 209, 'saying': 210, 'true': 211, 'porn': 212, 'looking': 213, 'tonight': 214, 'americans': 215, 'also': 216, 'yes': 217, 'emiratis': 218, 'maga': 219, 'wish': 220, 'bad': 221, 'they': 222, 'may': 223, 'altwaystoheal': 224, 'bigotry': 225, 'gold': 226, 'iam': 227, 'pa': 228, 'attack': 229, 'proud': 230, 'enough': 231, 'change': 232, 'end': 233, 'read': 234, 'lol': 235, 'misogyny': 236, 'na': 237, 'factory': 238, 'feeling': 239, 'needs': 240, 'food': 241, 'play': 242, 'city': 243, 'ready': 244, 'sea': 245, 'ignored': 246, 'silver': 247, 'old': 248, 'kkk': 249, 'kids': 250, 'sta': 251, 'big': 252, 'feminismiscancer': 253, 'feminismisterrorism': 254, 'feminismmuktbharat': 255, 'country': 256, 'without': 257, 'job': 258, 'long': 259, 'antiracism': 260, 'christmas': 261, 'guys': 262, 'nice': 263, 'around': 264, 'gets': 265, 'getting': 266, 'use': 267, 'coming': 268, 'funny': 269, 'enjoy': 270, 'everything': 271, 'misogynist': 272, 'friend': 273, 'uk': 274, 'win': 275, 'seashepherd': 276, 'left': 277, 'others': 278, 'kind': 279, 'shit': 280, 'are': 281, 'away': 282, 'blacklivesmatter': 283, 'father': 284, 'tweet': 285, 'yet': 286, 'blessed': 287, 'hard': 288, 'wow': 289, 'buffalo': 290, 'hey': 291, 'power': 292, 'matter': 293, 'already': 294, 'equality': 295, 'open': 296, 'forex': 297, 'miss': 298, 'paladino': 299, 'trumps': 300, 'fear': 301, 'ppl': 302, 'shepherd': 303, 'angry': 304, 'hea': 305, 'holiday': 306, 'truth': 307, 'whites': 308, '2016in4words': 309, 'american': 310, 'oh': 311, 'tell': 312, 'found': 313, 'sun': 314, 'cool': 315, 'business': 316, 'hot': 317, 'muslims': 318, 'baby': 319, 'climb': 320, 'lets': 321, 'money': 322, 'muslim': 323, 'nude': 324, 'rest': 325, 'twitter': 326, 'whatever': 327, 'comes': 328, 'hair': 329, 'bear': 330, 'ignorance': 331, 'looks': 332, 'naked': 333, 'awesome': 334, 'dog': 335, 'notmypresident': 336, 'house': 337, 'feminism': 338, 'children': 339, 'saturday': 340, '10': 341, 'gon': 342, 'lot': 343, 'waiting': 344, 'boy': 345, 'jews': 346, 'words': 347, 'education': 348, 'something': 349, 'strong': 350, 'anything': 351, 'cnn': 352, 'excited': 353, 'leave': 354, 'southafrica': 355, 'beach': 356, 'mad': 357, 'ally': 358, 'course': 359, 'gay': 360, 'israel': 361, 'sexy': 362, 'talking': 363, 'two': 364, 'videos': 365, 'carl': 366, 'check': 367, 'color': 368, 'fans': 369, 'islam': 370, 'try': 371, 'weeks': 372, 'instagood': 373, 'polar': 374, 'watching': 375, 'boycott': 376, 'place': 377, 'reading': 378, 'sick': 379, 'yeah': 380, 'antisemitism': 381, 'grateful': 382, 'rape': 383, 'resist': 384, 'trying': 385, 'wedding': 386, 'japan': 387, 'oil': 388, 'post': 389, 'far': 390, 'mean': 391, 'means': 392, 'playing': 393, 'social': 394, 'blacks': 395, 'quote': 396, 'vote': 397, 'forward': 398, 'gop': 399, 'list': 400, 'soon': 401, 'tcot': 402, 'vs': 403, 'dear': 404, 'fucking': 405, 'heres': 406, 'obamas': 407, 'ass': 408, 'fascism': 409, 'flag': 410, 'remarks': 411, 'she': 412, 'buy': 413, 'rip': 414, 'suppo': 415, 'there': 416, 'calls': 417, 'full': 418, 'newyork': 419, 'point': 420, 'times': 421, 'crazy': 422, 'death': 423, 'motivation': 424, 'problem': 425, 'talk': 426, 'wants': 427, 'gorilla': 428, 'ht': 429, 'pretty': 430, 'shows': 431, 'bc': 432, 'direct': 433, 'making': 434, 'michelle': 435, 'towards': 436, 'wrong': 437, 'join': 438, 'kill': 439, 'dead': 440, 'dominate': 441, 'war': 442, 'anyone': 443, 'islamic': 444, 'putinschoice': 445, 'seems': 446, 'single': 447, 'young': 448, 'become': 449, 'mind': 450, 'ill': 451, 'republican': 452, 'travel': 453, 'wall': 454, 'called': 455, 'forget': 456, 'goes': 457, 'hillary': 458, 'monday': 459, 'stay': 460, 'stupid': 461, 'target': 462, 'theresistance': 463, 'xenophobia': 464, 'christians': 465, 'islamophobia': 466, 'joy': 467, 'together': 468, 'tweets': 469, 'joke': 470, 'mother': 471, 'omg': 472, 'lovely': 473, 'working': 474, 'fascist': 475, 'perfect': 476, 'comment': 477, 'mindset': 478, 'quotes': 479, 'worst': 480, 'african': 481, 'health': 482, 'religion': 483, 'boys': 484, 'followme': 485, 'calling': 486, 'came': 487, 'different': 488, 'discrimination': 489, 'disgusting': 490, 'june': 491, 'message': 492, 'ai': 493, 'body': 494, 'couple': 495, 'hear': 496, 'lives': 497, 'teamsuperjunior': 498, 'inspiration': 499, 'nyc': 500, 'terrorism': 501, 'traitor': 502, 'ago': 503, 'campaign': 504, 'cow': 505, 'hateful': 506, 'lgbt': 507, 'ny': 508, 'protesting': 509, 'son': 510, 'speak': 511, 'fashion': 512, 'leftright': 513, 'polarisation': 514, 'uselections2016': 515, 'asian': 516, 'cold': 517, 'living': 518, 'bit': 519, 'class': 520, 'reality': 521, 'sexist': 522, 'super': 523, 'treason': 524, 'what': 525, 'carlpaladino': 526, 'liberals': 527, 'tbt': 528, 'victims': 529, 'agree': 530, 'beauty': 531, 'denial': 532, 'human': 533, 'photooftheday': 534, 'racing': 535, 'since': 536, 'used': 537, 'close': 538, 'hell': 539, 'london': 540, 'running': 541, 'who': 542, 'bing': 543, 'liar': 544, 'nazi': 545, 'put': 546, 'understand': 547, 'da': 548, 'fought': 549, 'hours': 550, 'justice': 551, 'less': 552, 'remember': 553, 'story': 554, 'wishes': 555, 'actually': 556, 'almost': 557, 'auspol': 558, 'bong': 559, 'fact': 560, 'run': 561, 'aicle': 562, 'care': 563, 'loving': 564, 'share': 565, 'song': 566, 'bbc': 567, 'dumb': 568, 'shot': 569, 'though': 570, 'troll': 571, 'else': 572, 'forever': 573, 'head': 574, 'homophobic': 575, 'self': 576, 'small': 577, 'special': 578, 'stories': 579, 'surprise': 580, 'gone': 581, 'poetry': 582, 'shooting': 583, 'sleep': 584, 'cochair': 585, 'dads': 586, 'fuhered': 587, 'happened': 588, 'impoant': 589, 'lying': 590, 'nazis': 591, 'newyear': 592, 'seriously': 593, 'states': 594, 'wonthey': 595, 'wtf': 596, 'hand': 597, 'maybe': 598, 'moment': 599, 'officers': 600, 'todays': 601, 'voted': 602, 'we': 603, 'went': 604, 'abuse': 605, 'conference': 606, 'half': 607, 'movie': 608, 'systemic': 609, 'welcome': 610, 'delete': 611, 'male': 612, 'past': 613, 'photo': 614, 'season': 615, 'street': 616, 'tv': 617, 'age': 618, 'celebrate': 619, 'guess': 620, 'least': 621, 'reason': 622, 'response': 623, 'stopracism': 624, 'wife': 625, 'yay': 626, 'anymore': 627, 'disease': 628, 'euro2016': 629, 'india': 630, 'lady': 631, 'meet': 632, 'needed': 633, 'south': 634, 'damn': 635, 'fired': 636, 'kid': 637, 'lifestyle': 638, 'nationalist': 639, 'telling': 640, 'woh': 641, 'dream': 642, 'ignorant': 643, 'jokes': 644, 'office': 645, 'pic': 646, 'send': 647, 'smh': 648, 'sorry': 649, 'thoughts': 650, 'wonderful': 651, 'yesterday': 652, 'claims': 653, 'disappointed': 654, 'fat': 655, 'lt3': 656, 'months': 657, 'neverump': 658, 'seeing': 659, 'sessions': 660, 'sign': 661, 'stand': 662, 'success': 663, 'africa': 664, 'cat': 665, 'culture': 666, 'football': 667, 'given': 668, 'ok': 669, 'reflections': 670, 'seen': 671, 'ugly': 672, 'apparently': 673, 'despite': 674, 'idea': 675, 'russia': 676, 'staff': 677, 'tear': 678, '12': 679, 'all': 680, 'ask': 681, 'church': 682, 'content': 683, 'dangerous': 684, 'depression': 685, 'extremism': 686, 'government': 687, 'heard': 688, 'high': 689, 'inequality': 690, 'tired': 691, 'whitepeople': 692, 'y': 693, 'ag': 694, 'anc': 695, 'cause': 696, 'fucked': 697, 'future': 698, 'gender': 699, 'green': 700, 'lumpy': 701, 'million': 702, 'present': 703, 'revolution': 704, 'rights': 705, 'sure': 706, 'thursday': 707, 'violence': 708, 'whole': 709, 'environment': 710, 'loved': 711, 'mood': 712, 'respect': 713, 'saw': 714, 'shop': 715, 'state': 716, '50': 717, 'board': 718, 'brown': 719, 'chick': 720, 'coffee': 721, 'community': 722, 'gt': 723, 'homes': 724, 'late': 725, 'lies': 726, 'marijuana': 727, 'blue': 728, 'child': 729, 'date': 730, 'guns': 731, 'htt': 732, 'mom': 733, 'scum': 734, 'till': 735, 'alone': 736, 'eyes': 737, 'hello': 738, 'order': 739, 'racial': 740, 'absolutely': 741, 'dance': 742, 'decades': 743, 'fly': 744, 'rejected': 745, 'accept': 746, 'adapt': 747, 'bed': 748, 'car': 749, 'fake': 750, 'impression': 751, 'leaving': 752, 'meant': 753, 'month': 754, 'picture': 755, 'policebrutality': 756, 'poor': 757, 'shopping': 758, 'simulator': 759, 'transformation': 760, 'wanna': 761, 'youtube': 762, 'ad': 763, 'australia': 764, 'combat': 765, 'donaldtrump': 766, 'freedom': 767, 'officially': 768, 'preorder': 769, 'sexism': 770, 'tech': 771, 'choose': 772, 'die': 773, 'll': 774, 'official': 775, 'potus': 776, 'prevents': 777, 'safe': 778, 'schools': 779, 'shut': 780, 'speech': 781, 'dark': 782, 'deal': 783, 'fitness': 784, 'fool': 785, 'hour': 786, 'land': 787, 'save': 788, 'teen': 789, 'truly': 790, 'wanted': 791, 'daughter': 792, 'early': 793, 'expect': 794, 'fight': 795, 'knew': 796, 'later': 797, 'pick': 798, 'pm': 799, 'policies': 800, 'using': 801, 'wednesday': 802, 'either': 803, 'europe': 804, 'flowers': 805, 'gift': 806, 'hollywood': 807, 'humanity': 808, 'humanrights': 809, 'industry': 810, 'law': 811, 'pig': 812, 'racists': 813, 'service': 814, 'simulation': 815, 'sweet': 816, 'common': 817, 'evil': 818, 'happening': 819, 'inauguration': 820, 'jewish': 821, 'light': 822, 'lose': 823, 'nigga': 824, 'officer': 825, 'paladinos': 826, 'places': 827, 'pussy': 828, 'science': 829, 'spread': 830, 'total': 831, 'york': 832, '1st': 833, 'criticism': 834, 'group': 835, 'issue': 836, 'jeffsessions': 837, 'males': 838, 'mc': 839, 'reverse': 840, 'trip': 841, 'account': 842, 'anti': 843, 'attention': 844, 'brand': 845, 'complete': 846, 'daddy': 847, 'hu': 848, 'idiot': 849, 'originally': 850, 'photos': 851, 'propaganda': 852, 'anxiety': 853, 'case': 854, 'everyday': 855, 'flight': 856, 'horrible': 857, 'isis': 858, 'main': 859, 'rather': 860, 'resignation': 861, 'system': 862, 'texas': 863, 'ya': 864, 'able': 865, 'americas': 866, 'believes': 867, 'bullying': 868, 'cleveland': 869, 'cologne': 870, 'company': 871, 'deep': 872, 'donald': 873, 'especially': 874, 'fo': 875, 'following': 876, 'fraud': 877, 'ibooks': 878, 'immigrants': 879, 'jan': 880, 'mil': 881, 'newswithed': 882, 'perhaps': 883, 'realize': 884, 'slavery': 885, 'smiles': 886, 'sunshine': 887, 'terrorists': 888, 'tho': 889, 'turn': 890, 'unleashed': 891, 'eat': 892, 'empty': 893, 'excellent': 894, 'fire': 895, 'jew': 896, 'moron': 897, 'profiling': 898, 'stas': 899, 'wonder': 900, 'xxx': 901, 'add': 902, 'assault': 903, 'brother': 904, 'leads': 905, 'line': 906, 'missing': 907, 'phillysuppophilly': 908, 'snapchat': 909, 'toptags': 910, 'update': 911, 'winner': 912, 'yo': 913, '20': 914, 'answer': 915, 'bitch': 916, 'experience': 917, 'fakenews': 918, 'memes': 919, 'plz': 920, 'podcast': 921, 'practice': 922, 'rwnj': 923, 'stuff': 924, 'treat': 925, 'university': 926, 'wake': 927, 'yrs': 928, 'action': 929, 'charles': 930, 'dystopian': 931, 'eah': 932, 'fan': 933, 'goodmorning': 934, 'hearing': 935, 'instead': 936, 'lack': 937, 'michelleobama': 938, 'msnbc': 939, 'national': 940, 'promote': 941, 'skin': 942, 'surprised': 943, 'takes': 944, 'within': 945, 'write': 946, '100': 947, 'can': 948, 'fall': 949, 'loves': 950, 'nasty': 951, 'not': 952, 'opinion': 953, 'parents': 954, 'public': 955, 'putinspuppet': 956, 'relax': 957, 'rude': 958, 'voters': 959, 'womens': 960, '642': 961, 'along': 962, 'aww': 963, 'comedy': 964, 'destroy': 965, 'enemies': 966, 'environmental': 967, 'female': 968, 'florida': 969, 'genocide': 970, 'jeff': 971, 'la': 972, 'laugh': 973, 'p2': 974, 'putin': 975, 'speaking': 976, 'students': 977, 'top': 978, 'tuesday': 979, 'useful': 980, 'vacation': 981, 'whiteprivilege': 982, 'allowed': 983, 'build': 984, 'disney': 985, 'episode': 986, 'favorite': 987, 'gym': 988, 'immigration': 989, 'killed': 990, 'landholding': 991, 'opposition': 992, 'piece': 993, 'political': 994, 'povey': 995, 'question': 996, 'smiling': 997, 'star': 998, 'told': 999, 'trumpsamerica': 1000, 'violent': 1001, 'walking': 1002, 'dare': 1003, 'deaths': 1004, 'dick': 1005, 'diversity': 1006, 'election': 1007, 'hardcore': 1008, 'members': 1009, 'nigger': 1010, 'prison': 1011, 'reaction': 1012, 'shame': 1013, 'space': 1014, 'was': 1015, 'arabs': 1016, 'birds': 1017, 'confirmation': 1018, 'customer': 1019, 'deletetweets': 1020, 'denounce': 1021, 'depressed': 1022, 'event': 1023, 'expected': 1024, 'fail': 1025, 'happen': 1026, 'hold': 1027, 'meeting': 1028, 'mine': 1029, 'notice': 1030, 'park': 1031, 'series': 1032, 'side': 1033, 'stands': 1034, 'tale': 1035, 'thrown': 1036, 'wh': 1037, '30': 1038, 'assholes': 1039, 'balls': 1040, 'control': 1041, 'decided': 1042, 'due': 1043, 'followers': 1044, 'force': 1045, 'form': 1046, 'haha': 1047, 'harassment': 1048, 'hatecrime': 1049, 'market': 1050, 'met': 1051, 'millions': 1052, 'minute': 1053, 'name': 1054, 'outrage': 1055, 'picoftheday': 1056, 'prayfororlando': 1057, 'purpose': 1058, 'rapist': 1059, 'red': 1060, 'sometimes': 1061, 'tgif': 1062, 'three': 1063, 'tickets': 1064, 'afternoon': 1065, 'agenda': 1066, 'ahead': 1067, 'alabama': 1068, 'argument': 1069, 'attacks': 1070, 'bday': 1071, 'blame': 1072, 'bring': 1073, 'caused': 1074, 'claim': 1075, 'correct': 1076, 'cry': 1077, 'families': 1078, 'fang': 1079, 'guilty': 1080, 'impossible': 1081, 'judge': 1082, 'keeps': 1083, 'nervous': 1084, 'outside': 1085, 'problems': 1086, 'sharpens': 1087, 'staing': 1088, 'swiftly': 1089, 'trash': 1090, 'view': 1091, 'visit': 1092, 'aampe': 1093, 'acts': 1094, 'animals': 1095, 'arrived': 1096, 'beginning': 1097, 'blicqer': 1098, 'born': 1099, 'cantwait': 1100, 'cheer': 1101, 'crying': 1102, 'except': 1103, 'expats': 1104, 'extremists': 1105, 'feminist': 1106, 'haters': 1107, 'have': 1108, 'move': 1109, 'murdering': 1110, 'nature': 1111, 'openly': 1112, 'organizations': 1113, 'probably': 1114, 'silence': 1115, 'style': 1116, 'upset': 1117, 'vine': 1118, 'whitegenocide': 1119, '70': 1120, '99c99p': 1121, 'actions': 1122, 'biggest': 1123, 'boyfriend': 1124, 'died': 1125, 'difficult': 1126, 'evening': 1127, 'facebook': 1128, 'instagram': 1129, 'killing': 1130, 'paid': 1131, 'photography': 1132, 'politicians': 1133, 'relevant': 1134, 'staed': 1135, 'statement': 1136, 'term': 1137, 'tyler': 1138, 'united': 1139, 'walk': 1140, 'water': 1141, 'wear': 1142, 'acting': 1143, 'adam': 1144, 'advice': 1145, 'allow': 1146, 'app': 1147, 'bigots': 1148, 'chance': 1149, 'cultureofdevelopment': 1150, 'definition': 1151, 'donkey': 1152, 'games': 1153, 'learn': 1154, 'leftist': 1155, 'none': 1156, 'often': 1157, 'pedophilia': 1158, 'pics': 1159, 'prayers': 1160, 'resistance': 1161, 'revenge': 1162, 'stream': 1163, 'suppoing': 1164, 'teach': 1165, 'took': 1166, 'works': 1167, 'xenophobic': 1168, 'zionism': 1169, 'adultery': 1170, 'alive': 1171, 'bout': 1172, 'clearly': 1173, 'cover': 1174, 'crime': 1175, 'definitely': 1176, 'democrats': 1177, 'eu': 1178, 'eve': 1179, 'fair': 1180, 'feels': 1181, 'gun': 1182, 'husband': 1183, 'israeli': 1184, 'kills': 1185, 'language': 1186, 'longer': 1187, 'lunch': 1188, 'ones': 1189, 'palestinian': 1190, 'pizza': 1191, 'pop': 1192, 'semitic': 1193, 'terms': 1194, 'tlot': 1195, 'ways': 1196, 'wishing': 1197, 'audiblechannels': 1198, 'blonde': 1199, 'bought': 1200, 'dinner': 1201, 'dreams': 1202, 'entire': 1203, 'glad': 1204, 'happens': 1205, 'insults': 1206, 'knows': 1207, 'listening': 1208, 'movement': 1209, 'pathetic': 1210, 'pigs': 1211, 'plans': 1212, 'pos': 1213, 'radicalisation': 1214, 'rain': 1215, 'repo': 1216, 'room': 1217, 'stereotype': 1218, 'triggered': 1219, 'whitesupremacy': 1220, 'workers': 1221, '2nd': 1222, 'abt': 1223, 'apaheid': 1224, 'booked': 1225, 'break': 1226, 'breakfast': 1227, 'conce': 1228, 'evidence': 1229, 'finished': 1230, 'happier': 1231, 'homophobia': 1232, 'imagine': 1233, 'internet': 1234, 'learning': 1235, 'lots': 1236, 'low': 1237, 'lucky': 1238, 'mexico': 1239, 'mtv': 1240, 'non': 1241, 'perry': 1242, 'powerful': 1243, 'privilege': 1244, 'promoting': 1245, 'sense': 1246, 'soul': 1247, 'sunny': 1248, 'ta': 1249, 'tragic': 1250, 'ableism': 1251, 'affirmations': 1252, 'amwriting': 1253, 'anywhere': 1254, 'boob': 1255, 'broken': 1256, 'buddy': 1257, 'cake': 1258, 'canada': 1259, 'cop': 1260, 'council': 1261, 'declared': 1262, 'equal': 1263, 'etc': 1264, 'familiar': 1265, 'feelings': 1266, 'hands': 1267, 'hrs': 1268, 'kentucky': 1269, 'leader': 1270, 'moments': 1271, 'names': 1272, 'reach': 1273, 'sending': 1274, 'spark': 1275, 'st': 1276, 'thankyou': 1277, 'traffic': 1278, 'turned': 1279, 'typical': 1280, 'version': 1281, 'vids': 1282, 'yoga': 1283, 'actor': 1284, 'beer': 1285, 'clinton': 1286, 'club': 1287, 'concept': 1288, 'debate': 1289, 'destroyed': 1290, 'drink': 1291, 'easy': 1292, 'ends': 1293, 'everybody': 1294, 'fantastic': 1295, 'final': 1296, 'fresh': 1297, 'garden': 1298, 'innocent': 1299, 'inside': 1300, 'kevin': 1301, 'levels': 1302, 'passed': 1303, 'period': 1304, 'research': 1305, 'scared': 1306, 'society': 1307, 'totally': 1308, 'training': 1309, 'unbelievable': 1310, 'unpresidented': 1311, 'voice': 1312, '24': 1313, 'blacktwitter': 1314, 'chief': 1315, 'clothes': 1316, 'colonialism': 1317, 'corruption': 1318, 'democracy': 1319, 'familys': 1320, 'gave': 1321, 'hang': 1322, 'hatespeech': 1323, 'heal': 1324, 'humans': 1325, 'iconic': 1326, 'indigenous': 1327, 'iq': 1328, 'jealous': 1329, 'joe': 1330, 'lame': 1331, 'lover': 1332, 'luck': 1333, 'married': 1334, 'mr': 1335, 'pray': 1336, 'protest': 1337, 'prove': 1338, 'reasons': 1339, 'rock': 1340, 'set': 1341, 'steal': 1342, 'tony': 1343, 'airpo': 1344, 'anniversary': 1345, 'arabic': 1346, 'beat': 1347, 'dogs': 1348, 'driver': 1349, 'em': 1350, 'friendship': 1351, 'gbp': 1352, 'heabroken': 1353, 'incest': 1354, 'lie': 1355, 'likely': 1356, 'missed': 1357, 'pain': 1358, 'plan': 1359, 'projection': 1360, 'ramadan': 1361, 'resign': 1362, 'sadly': 1363, 'serve': 1364, 'thinking': 1365, 'treasonoustrump': 1366, 'whilst': 1367, '14': 1368, 'ashamed': 1369, 'bless': 1370, 'chill': 1371, 'chinese': 1372, 'christianity': 1373, 'countdown': 1374, 'deplorable': 1375, 'dude': 1376, 'faced': 1377, 'fast': 1378, 'film': 1379, 'finding': 1380, 'fox': 1381, 'ideology': 1382, 'koreans': 1383, 'military': 1384, 'nation': 1385, 'notmypres': 1386, 'online': 1387, 'rally': 1388, 'rant': 1389, 'realized': 1390, 'rebellion': 1391, 'records': 1392, 'sexualpredator': 1393, 'simple': 1394, 'swastika': 1395, 'tears': 1396, 'thinks': 1397, 'value': 1398, 'attitude': 1399, 'behind': 1400, 'beyond': 1401, 'blatant': 1402, 'brain': 1403, 'california': 1404, 'chain': 1405, 'congrats': 1406, 'demo': 1407, 'dj': 1408, 'effect': 1409, 'ethnocentrism': 1410, 'favourite': 1411, 'flights': 1412, 'goals': 1413, 'helping': 1414, 'hopeful': 1415, 'hrc': 1416, 'huge': 1417, 'illegal': 1418, 'injustice': 1419, 'logic': 1420, 'loser': 1421, 'mark': 1422, 'memories': 1423, 'middle': 1424, 'milo': 1425, 'mma': 1426, 'moving': 1427, 'normalizing': 1428, 'number': 1429, 'phone': 1430, 'press': 1431, 'pure': 1432, 'refugees': 1433, 'reveals': 1434, 'scary': 1435, 'shitty': 1436, 'sho': 1437, 'silent': 1438, 'sing': 1439, 'spend': 1440, 'store': 1441, 'stupidity': 1442, 'survive': 1443, 'taking': 1444, 'wars': 1445, 'bag': 1446, 'bar': 1447, 'based': 1448, 'biher': 1449, 'block': 1450, 'blocked': 1451, 'chicago': 1452, 'critics': 1453, 'cut': 1454, 'door': 1455, 'england': 1456, 'english': 1457, 'exciting': 1458, 'germany': 1459, 'global': 1460, 'greed': 1461, 'happyholidays': 1462, 'hated': 1463, 'hero': 1464, 'hit': 1465, 'holidays': 1466, 'hosts': 1467, 'irish': 1468, 'jobs': 1469, 'local': 1470, 'lt': 1471, 'natural': 1472, 'okay': 1473, 'orange': 1474, 'result': 1475, 'role': 1476, 'rondarousey': 1477, 'shown': 1478, 'step': 1479, 'sucks': 1480, 'test': 1481, 'veiled': 1482, 'watched': 1483, 'animal': 1484, 'antiblackness': 1485, 'bitter': 1486, 'blamed': 1487, 'bye': 1488, 'caught': 1489, 'checked': 1490, 'chosen': 1491, 'college': 1492, 'defending': 1493, 'energy': 1494, 'everywhere': 1495, 'george': 1496, 'gives': 1497, 'hill': 1498, 'hunger': 1499, 'jesus': 1500, 'jo': 1501, 'king': 1502, 'lead': 1503, 'looked': 1504, 'mens': 1505, 'nearly': 1506, 'omits': 1507, 'opening': 1508, 'rapeculture': 1509, 'reached': 1510, 'reminder': 1511, 'remove': 1512, 'rough': 1513, 'secret': 1514, 'shiless': 1515, 'slow': 1516, 'spain': 1517, 'stage': 1518, 'staup': 1519, 'tedtalks': 1520, 'terror': 1521, 'thousands': 1522, 'trumpism': 1523, 'turning': 1524, 'vandals': 1525, 'vicinity': 1526, '17': 1527, '25': 1528, 'among': 1529, 'anybody': 1530, 'celebrating': 1531, 'cou': 1532, 'crisis': 1533, 'democraticpay': 1534, 'deserve': 1535, 'educate': 1536, 'ending': 1537, 'hopes': 1538, 'hypocrite': 1539, 'idiots': 1540, 'ignore': 1541, 'july': 1542, 'lake': 1543, 'largest': 1544, 'laws': 1545, 'mast': 1546, 'mexican': 1547, 'minority': 1548, 'opposed': 1549, 'policy': 1550, 'republicans': 1551, 'rid': 1552, 'road': 1553, 'rules': 1554, 'signs': 1555, 'suicide': 1556, 'tool': 1557, 'tragedy': 1558, 'trending': 1559, 'unless': 1560, '11': 1561, 'citizens': 1562, 'dying': 1563, 'facts': 1564, 'finger': 1565, 'foodie': 1566, 'forgot': 1567, 'fred': 1568, 'goodbye': 1569, 'govt': 1570, 'kinda': 1571, 'legend': 1572, 'makeup': 1573, 'marketing': 1574, 'marriage': 1575, 'mass': 1576, 'mountains': 1577, 'profile': 1578, 'proof': 1579, 'rac': 1580, 'release': 1581, 'russian': 1582, 'sale': 1583, 'spos': 1584, 'standing': 1585, 'victim': 1586, 'vile': 1587, 'worse': 1588, '3d': 1589, 'acceptable': 1590, 'ape': 1591, 'billion': 1592, 'bollywood': 1593, 'china': 1594, 'clean': 1595, 'completely': 1596, 'explain': 1597, 'folks': 1598, 'groups': 1599, 'growing': 1600, 'instamood': 1601, 'jcpenny': 1602, 'joseon': 1603, 'ladies': 1604, 'lawofattraction': 1605, 'member': 1606, 'nobody': 1607, 'palestine': 1608, 'progress': 1609, 'record': 1610, 'ris2016': 1611, 'sunset': 1612, 'terrorist': 1613, 'trumpusa': 1614, 'type': 1615, 'unifying': 1616, '15': 1617, 'afraid': 1618, 'anger': 1619, 'banks': 1620, 'bluntly': 1621, 'boots': 1622, 'choice': 1623, 'classism': 1624, 'congress': 1625, 'conservative': 1626, 'criminal': 1627, 'design': 1628, 'expose': 1629, 'feeding': 1630, 'festival': 1631, 'grow': 1632, 'insane': 1633, 'japanese': 1634, 'major': 1635, 'masses': 1636, 'minorities': 1637, 'narrative': 1638, 'niggers': 1639, 'note': 1640, 'noticed': 1641, 'page3': 1642, 'played': 1643, 'producers': 1644, 'progressive': 1645, 'queen': 1646, 'refugeeswelcome': 1647, 'regarding': 1648, 'renowned': 1649, 'rich': 1650, 'second': 1651, 'slut': 1652, 'station': 1653, 'task': 1654, 'train': 1655, 'vegan': 1656, 'viral': 1657, 'virginia': 1658, '600': 1659, 'ale': 1660, 'asians': 1661, 'atlanta': 1662, 'attacked': 1663, 'babies': 1664, 'blur': 1665, 'breaking': 1666, 'bs': 1667, 'btw': 1668, 'bullshit': 1669, 'cdnpoli': 1670, 'collection': 1671, 'detroit': 1672, 'difference': 1673, 'double': 1674, 'ep': 1675, 'exposed': 1676, 'fit': 1677, 'freemilo': 1678, 'genetics': 1679, 'hide': 1680, 'hillaryclinton': 1681, 'horrifying': 1682, 'including': 1683, 'interracial': 1684, 'jihad': 1685, 'joking': 1686, 'laughing': 1687, 'marx': 1688, 'nations': 1689, 'naughty': 1690, 'normal': 1691, 'occurredjust': 1692, 'oitnb': 1693, 'otherwise': 1694, 'possible': 1695, 'posted': 1696, 'prevail': 1697, 'quick': 1698, 'repoed': 1699, 'ride': 1700, 'scrubs': 1701, 'showing': 1702, 'site': 1703, 'slammed': 1704, 'southern': 1705, 'stereotypes': 1706, 'supremacy': 1707, 'syria': 1708, 'tells': 1709, 'tips': 1710, 'tlc': 1711, 'tried': 1712, 'trust': 1713, 'upon': 1714, 'youth': 1715, 'adult': 1716, 'advanced': 1717, 'agreed': 1718, 'aist': 1719, 'anal': 1720, 'antisemitic': 1721, 'area': 1722, 'arrested': 1723, 'asked': 1724, 'australian': 1725, 'boss': 1726, 'boycottdelta': 1727, 'bully': 1728, 'changes': 1729, 'committed': 1730, 'conviction': 1731, 'cud': 1732, 'delta': 1733, 'dem': 1734, 'dies': 1735, 'dory': 1736, 'draw': 1737, 'gb': 1738, 'has': 1739, 'hates': 1740, 'hating': 1741, 'hole': 1742, 'instalike': 1743, 'kindness': 1744, 'lighttherapy': 1745, 'losing': 1746, 'majority': 1747, 'mt': 1748, 'nicola': 1749, 'nose': 1750, 'patriotwatch': 1751, 'paul': 1752, 'pool': 1753, 'promise': 1754, 'puppy': 1755, 'sounds': 1756, 'tinfoilhat': 1757, 'udtapunjab': 1758, 'west': 1759, 'wet': 1760, 'winning': 1761, 'anyway': 1762, 'ash': 1763, 'average': 1764, 'bailed': 1765, 'begin': 1766, 'bernanke': 1767, 'bet': 1768, 'bitches': 1769, 'bread': 1770, 'busy': 1771, 'calm': 1772, 'changed': 1773, 'civil': 1774, 'classic': 1775, 'confused': 1776, 'continue': 1777, 'countries': 1778, 'documented': 1779, 'dumping': 1780, 'enduring': 1781, 'false': 1782, 'feminists': 1783, 'four': 1784, 'fridayfeeling': 1785, 'garbage': 1786, 'ground': 1787, 'harm': 1788, 'hitler': 1789, 'homophobe': 1790, 'hood': 1791, 'indian': 1792, 'instadaily': 1793, 'intellectual': 1794, 'issues': 1795, 'january': 1796, 'korematsu': 1797, 'learned': 1798, 'massive': 1799, 'minutes': 1800, 'nbafinals': 1801, 'noh': 1802, 'offensive': 1803, 'prejudice': 1804, 'pride': 1805, 'quest': 1806, 'quit': 1807, 'representation': 1808, 'return': 1809, 'socialmedia': 1810, 'teacher': 1811, 'twice': 1812, 'ty': 1813, 'valid': 1814, 'voting': 1815, 'wine': 1816, 'zionazis': 1817, '2017in3words': 1818, '60s': 1819, '75': 1820, 'attracted': 1821, 'biden': 1822, 'blk': 1823, 'blogger': 1824, 'camp': 1825, 'cats': 1826, 'celebration': 1827, 'chase': 1828, 'chat': 1829, 'coal': 1830, 'dictator': 1831, 'dudes': 1832, 'eating': 1833, 'enjoying': 1834, 'excuse': 1835, 'figure': 1836, 'france': 1837, 'golf': 1838, 'horny': 1839, 'how': 1840, 'hungry': 1841, 'kiss': 1842, 'leaves': 1843, 'like4like': 1844, 'literally': 1845, 'loose': 1846, 'loss': 1847, 'mmusimaimane': 1848, 'msm': 1849, 'oops': 1850, 'pc': 1851, 'pink': 1852, 'policing': 1853, 'prince': 1854, 'project': 1855, 'salad': 1856, 'slant': 1857, 'songs': 1858, 'straight': 1859, 'thailand': 1860, 'turns': 1861, 'wa': 1862, 'web': 1863, 'whiteness': 1864, 'wtf2016': 1865, 'xians': 1866, 'xmas': 1867, 'xx': 1868, '13': 1869, 'asshole': 1870, 'available': 1871, 'bears': 1872, 'celebrates': 1873, 'chant': 1874, 'clear': 1875, 'commercial': 1876, 'count': 1877, 'create': 1878, 'dancing': 1879, 'deplorables': 1880, 'doom': 1881, 'emotions': 1882, 'endof2016': 1883, 'faces': 1884, 'folx': 1885, 'front': 1886, 'giant': 1887, 'giving': 1888, 'guide': 1889, 'helps': 1890, 'indians': 1891, 'inspirational': 1892, 'journalism': 1893, 'kaepernick': 1894, 'key': 1895, 'kinky': 1896, 'microaggressions': 1897, 'ordered': 1898, 'orlandoshooting': 1899, 'pisses': 1900, 'proved': 1901, 'punjab': 1902, 'rainbow': 1903, 'rational': 1904, 'relationship': 1905, 'rose': 1906, 'scumbag': 1907, 'shy': 1908, 'sister': 1909, 'source': 1910, 'suck': 1911, 'tedatibm': 1912, 'thread': 1913, 'trial': 1914, 'trumpworld': 1915, 'valuechain': 1916, 'vast': 1917, 'wakeup': 1918, 'waste': 1919, 'woke': 1920, 'adventure': 1921, 'assume': 1922, 'bewitched': 1923, 'bike': 1924, 'cancel': 1925, 'cavs': 1926, 'christmaseve': 1927, 'continues': 1928, 'cuz': 1929, 'david': 1930, 'deadliest': 1931, 'decision': 1932, 'dispose': 1933, 'drunk': 1934, 'east': 1935, 'expanse': 1936, 'goodvibes': 1937, 'graduation': 1938, 'incredible': 1939, 'inmates': 1940, 'inshot': 1941, 'interested': 1942, 'killer': 1943, 'kitchen': 1944, 'known': 1945, 'lifelessons': 1946, 'lonely': 1947, 'lovelife': 1948, 'mental': 1949, 'nba': 1950, 'negligence': 1951, 'neighbor': 1952, 'netflix': 1953, 'offer': 1954, 'ohio': 1955, 'oppounity': 1956, 'ove': 1957, 'paris': 1958, 'popular': 1959, 'poster': 1960, 'preach': 1961, 'presidents': 1962, 'ra': 1963, 'radio': 1964, 'ran': 1965, 'reject': 1966, 'ripchristina': 1967, 'rohingya': 1968, 'rooster': 1969, 'santa': 1970, 'sealed': 1971, 'serious': 1972, 'shocking': 1973, 'shoes': 1974, 'sir': 1975, 'slurs': 1976, 'socially': 1977, 'title': 1978, 'town': 1979, 'vegas': 1980, 'versace': 1981, 'vibes': 1982, 'village': 1983, 'weird': 1984, 'whores': 1985, 'workplace': 1986, 'wud': 1987, 'acab': 1988, 'album': 1989, 'although': 1990, 'arrive': 1991, 'blood': 1992, 'censorship': 1993, 'choices': 1994, 'classes': 1995, 'communist': 1996, 'covering': 1997, 'di': 1998, 'disgraceful': 1999, 'douchebag': 2000, 'empire': 2001, 'exactly': 2002, 'former': 2003, 'funding': 2004, 'goaded': 2005, 'gorgeous': 2006, 'held': 2007, 'launch': 2008, 'lesson': 2009, 'link': 2010, 'messi': 2011, 'monster': 2012, 'mostly': 2013, 'movies': 2014, 'multiple': 2015, 'nails': 2016, 'nofilter': 2017, 'planning': 2018, 'platform': 2019, 'princess': 2020, 'rapacious': 2021, 'religions': 2022, 'repeat': 2023, 'restrained': 2024, 'review': 2025, 'ring': 2026, 'riyadh': 2027, 'rooted': 2028, 'roots': 2029, 'saleh': 2030, 'sees': 2031, 'segregation': 2032, 'shoot': 2033, 'sight': 2034, 'sky': 2035, 'sma': 2036, 'someones': 2037, 'tea': 2038, 'teapay': 2039, 'travelled': 2040, 'weather': 2041, 'womenvote': 2042, 'workout': 2043, 'yummy': 2044, 'adl': 2045, 'analytics': 2046, 'apple': 2047, 'arrest': 2048, 'awful': 2049, 'axed': 2050, 'basketball': 2051, 'becoming': 2052, 'blatantly': 2053, 'bot': 2054, 'brilliant': 2055, 'brought': 2056, 'brutality': 2057, 'colleague': 2058, 'controversial': 2059, 'cretin': 2060, 'cult': 2061, 'dems': 2062, 'development': 2063, 'e32016': 2064, 'effective': 2065, 'effo': 2066, 'epic': 2067, 'ethnicity': 2068, 'ff': 2069, 'fucktrump': 2070, 'hi': 2071, 'hugs': 2072, 'imperialism': 2073, 'imperialsm': 2074, 'interest': 2075, 'leaders': 2076, 'meditation': 2077, 'mo': 2078, 'montana': 2079, 'msg': 2080, 'near': 2081, 'oiler': 2082, 'presidentelect': 2083, 'previous': 2084, 'pulse': 2085, 'questions': 2086, 'racis': 2087, 'radicalism': 2088, 'relationships': 2089, 'relaxing': 2090, 'remark': 2091, 'retail': 2092, 'round': 2093, 'route66': 2094, 'serves': 2095, 'shake': 2096, 'shameful': 2097, 'sickening': 2098, 'sides': 2099, 'snowflakes': 2100, 'socialjustice': 2101, 'sold': 2102, 'sparks': 2103, 'sundown': 2104, 'systemicracism': 2105, 'towns': 2106, 'travellers': 2107, 'tweet4taiji': 2108, 'wearing': 2109, 'worked': 2110, 'wout': 2111, 'aande': 2112, 'adamsaleh': 2113, 'africans': 2114, 'ambassador': 2115, 'animation': 2116, 'behappy': 2117, 'bigoted': 2118, 'britain': 2119, 'broke': 2120, 'canucks': 2121, 'challenge': 2122, 'cheers': 2123, 'commercials': 2124, 'communism': 2125, 'congratulations': 2126, 'considering': 2127, 'coverage': 2128, 'cr7': 2129, 'criminals': 2130, 'cutest': 2131, 'dictionary': 2132, 'dr': 2133, 'drinks': 2134, 'emboldened': 2135, 'empower': 2136, 'entering': 2137, 'entry': 2138, 'equate': 2139, 'exclusive': 2140, 'faith': 2141, 'feed': 2142, 'fighting': 2143, 'flower': 2144, 'gov': 2145, 'gweh': 2146, 'ha': 2147, 'heabreaking': 2148, 'ideas': 2149, 'klan': 2150, 'kwanzaa': 2151, 'lecture': 2152, 'likes': 2153, 'lmao': 2154, 'mcdonalds': 2155, 'meme': 2156, 'merrychristmas': 2157, 'misogynistic': 2158, 'motherfucker': 2159, 'mouth': 2160, 'offered': 2161, 'offers': 2162, 'piss': 2163, 'player': 2164, 'positivity': 2165, 'products': 2166, 'protect': 2167, 'psychological': 2168, 'rapechucktodd': 2169, 'received': 2170, 'singing': 2171, 'slander': 2172, 'smoking': 2173, 'stl': 2174, 'stone': 2175, 'stuck': 2176, 'suit': 2177, 'syndrome': 2178, 'ticket': 2179, 'union': 2180, 'wing': 2181, 'worry': 2182, 'writing': 2183, 'wypipo': 2184, 'address': 2185, 'africanamerican': 2186, 'alleged': 2187, 'alllivesmatter': 2188, 'amjoy': 2189, 'anime': 2190, 'anonymous': 2191, 'antiwhite': 2192, 'apes': 2193, 'appeal': 2194, 'appreciate': 2195, 'argued': 2196, 'arms': 2197, 'asia': 2198, 'basket': 2199, 'belgian': 2200, 'beloy': 2201, 'benefits': 2202, 'beware': 2203, 'bill': 2204, 'bonhoeffer': 2205, 'bored': 2206, 'boundaries': 2207, 'brief': 2208, 'chair': 2209, 'character': 2210, 'conflate': 2211, 'conman': 2212, 'credit': 2213, 'cruel': 2214, 'data': 2215, 'diet': 2216, 'divisive': 2217, 'ego': 2218, 'embarrass': 2219, 'essential': 2220, 'events': 2221, 'experiences': 2222, 'fabulous': 2223, 'felony': 2224, 'foxnews': 2225, 'gaslighting': 2226, 'gif': 2227, 'gratitude': 2228, 'greatest': 2229, 'happynewyear': 2230, 'hashtag': 2231, 'heas': 2232, 'hire': 2233, 'hype': 2234, 'incredibly': 2235, 'intolerant': 2236, 'involved': 2237, 'john': 2238, 'journey': 2239, 'judges': 2240, 'kick': 2241, 'lazy': 2242, 'mars': 2243, 'meccalive': 2244, 'mixed': 2245, 'ncaa': 2246, 'ness': 2247, 'page': 2248, 'pictures': 2249, 'postbrexit': 2250, 'protests': 2251, 'realitycheck': 2252, 'recordeven': 2253, 'remain': 2254, 'roll': 2255, 'seats': 2256, 'sell': 2257, 'sharing': 2258, 'sound': 2259, 'spit': 2260, 'study': 2261, 'successful': 2262, 'supposed': 2263, 'teaching': 2264, 'throw': 2265, 'touch': 2266, 'trophy': 2267, 'tuckercarlson': 2268, 'turmoil': 2269, 'ugh': 2270, 'ukraine': 2271, 'unmasking': 2272, 'varying': 2273, 'wales': 2274, 'weasel': 2275, 'whitewashing': 2276, 'wild': 2277, 'workshop': 2278, 'advocate': 2279, 'asses': 2280, 'bama': 2281, 'bride': 2282, 'bringing': 2283, 'bus': 2284, 'career': 2285, 'catch': 2286, 'chicks': 2287, 'childish': 2288, 'christina': 2289, 'cops': 2290, 'customers': 2291, 'deny': 2292, 'destruction': 2293, 'dismantle': 2294, 'drawing': 2295, 'drive': 2296, 'edinburgh': 2297, 'eh': 2298, 'european': 2299, 'fp': 2300, 'frustrated': 2301, 'geographic': 2302, 'german': 2303, 'governments': 2304, 'greatness': 2305, 'guardian': 2306, 'highlycontested': 2307, 'honestly': 2308, 'hopefully': 2309, 'interview': 2310, 'keeping': 2311, 'manager': 2312, 'modern': 2313, 'newyearseve': 2314, 'nigeria': 2315, 'offical': 2316, 'paying': 2317, 'performance': 2318, 'prez': 2319, 'program': 2320, 'released': 2321, 'repugnant': 2322, 'restaurant': 2323, 'rhetoric': 2324, 'richardspencer': 2325, 'row': 2326, 'saved': 2327, 'sept': 2328, 'shallow': 2329, 'somebody': 2330, 'soros': 2331, 'spring': 2332, 'stillwithher': 2333, 'stlouis': 2334, 'stunning': 2335, 'terrible': 2336, 'terry': 2337, 'thug': 2338, 'til': 2339, 'weapon': 2340, 'were': 2341, 'whitesupremacist': 2342, 'wins': 2343, 'wisdom': 2344, 'yup': 2345, 'yur': 2346, '13th': 2347, '142017': 2348, '911': 2349, 'abusive': 2350, 'aicles': 2351, 'allies': 2352, 'amandanunes': 2353, 'amrica': 2354, 'antiamerican': 2355, 'antisemite': 2356, 'bestie': 2357, 'bluelivesmatter': 2358, 'books': 2359, 'capitalism': 2360, 'ceain': 2361, 'charge': 2362, 'charging': 2363, 'client': 2364, 'clients': 2365, 'coward': 2366, 'crack': 2367, 'craziness': 2368, 'current': 2369, 'danger': 2370, 'de': 2371, 'definitions': 2372, 'e3': 2373, 'emotional': 2374, 'empathy': 2375, 'enteainment': 2376, 'euref': 2377, 'fell': 2378, 'finish': 2379, 'fl': 2380, 'foodporn': 2381, 'gotten': 2382, 'hotel': 2383, 'humpday': 2384, 'hv': 2385, 'icymi': 2386, 'instructed': 2387, 'invited': 2388, 'lebron': 2389, 'manchester': 2390, 'milk': 2391, 'millennials': 2392, 'misogynists': 2393, 'motivate': 2394, 'moved': 2395, 'murdered': 2396, 'neck': 2397, 'netanyahu': 2398, 'newshow': 2399, 'newyear2017': 2400, 'obstruction': 2401, 'overused': 2402, 'painting': 2403, 'passion': 2404, 'peaceful': 2405, 'posts': 2406, 'preordered': 2407, 'priyankachopra': 2408, 'professor': 2409, 'pueorico': 2410, 'quantico': 2411, 'racially': 2412, 'raygun': 2413, 'recent': 2414, 'reinforced': 2415, 'relatives': 2416, 'replies': 2417, 'repoer': 2418, 'request': 2419, 'retard': 2420, 'risk': 2421, 'september': 2422, 'shocked': 2423, 'shootings': 2424, 'sit': 2425, 'sites': 2426, 'snapshot': 2427, 'sourcenation': 2428, 'stem': 2429, 'stronger': 2430, 'student': 2431, 'subjected': 2432, 'suppoer': 2433, 'talent': 2434, 'thependulum': 2435, 'thieves': 2436, 'thin': 2437, 'tour': 2438, 'tweeted': 2439, 'uncle': 2440, 'unfounately': 2441, 'unity': 2442, 'vehicle': 2443, 'website': 2444, 'whitenationalist': 2445, 'winter': 2446, 'yr': 2447, '2000': 2448, '2008': 2449, '21': 2450, 'abe': 2451, 'ads': 2452, 'afp': 2453, 'ageism': 2454, 'alt': 2455, 'badly': 2456, 'barack': 2457, 'biblical': 2458, 'biherism': 2459, 'brokers': 2460, 'building': 2461, 'card': 2462, 'chaplin': 2463, 'conflates': 2464, 'cuomo': 2465, 'democratic': 2466, 'describe': 2467, 'dress': 2468, 'driving': 2469, 'ebay': 2470, 'elect': 2471, 'embrace': 2472, 'eugenics': 2473, 'exist': 2474, 'findingdory': 2475, 'flapping': 2476, 'general': 2477, 'girlfriend': 2478, 'goodnight': 2479, 'headlines': 2480, 'humiliated': 2481, 'i95n': 2482, 'incessant': 2483, 'inclusivity': 2484, 'info': 2485, 'invasion': 2486, 'leakage': 2487, 'littering': 2488, 'location': 2489, 'lord': 2490, 'lyrics': 2491, 'master': 2492, 'mi': 2493, 'michigan': 2494, 'mistake': 2495, 'mustread': 2496, 'myanmar': 2497, 'neutrality': 2498, 'nonsensical': 2499, 'nytimes': 2500, 'occur': 2501, 'outlets': 2502, 'packing': 2503, 'patience': 2504, 'praying': 2505, 'presented': 2506, 'psa': 2507, 'quiet': 2508, 'rates': 2509, 'reveal': 2510, 'rise': 2511, 'rsf': 2512, 'sarcasm': 2513, 'screaming': 2514, 'sedition': 2515, 'shine': 2516, 'speaks': 2517, 'squad': 2518, 'sums': 2519, 'treats': 2520, 'uae': 2521, 'unstoppable': 2522, 'warriors': 2523, 'weak': 2524, 'wrote': 2525, 'yorkers': 2526, 'zero': 2527, '148': 2528, '5th': 2529, 'accusations': 2530, 'added': 2531, 'announces': 2532, 'august': 2533, 'austin': 2534, 'base': 2535, 'bashing': 2536, 'basis': 2537, 'behavior': 2538, 'bestfriend': 2539, 'box': 2540, 'brings': 2541, 'bro': 2542, 'cameron': 2543, 'cc': 2544, 'childhood': 2545, 'chose': 2546, 'classy': 2547, 'clothing': 2548, 'connecting': 2549, 'counting': 2550, 'creative': 2551, 'crowd': 2552, 'cultism': 2553, 'dc': 2554, 'dec': 2555, 'disgrace': 2556, 'disrespect': 2557, 'districts': 2558, 'dollar': 2559, 'easier': 2560, 'economic': 2561, 'educational': 2562, 'eff': 2563, 'elections': 2564, 'erased': 2565, 'establishment': 2566, 'et': 2567, 'exit': 2568, 'fab': 2569, 'fellow': 2570, 'flying': 2571, 'gamedev': 2572, 'graduated': 2573, 'grew': 2574, 'heading': 2575, 'heaven': 2576, 'hypocrisy': 2577, 'ice': 2578, 'inflame': 2579, 'information': 2580, 'inlove': 2581, 'intolerance': 2582, 'island': 2583, 'jerusalem': 2584, 'knowing': 2585, 'level': 2586, 'lifes': 2587, 'losses': 2588, 'macdonald': 2589, 'mall': 2590, 'march': 2591, 'materials': 2592, 'mayor': 2593, 'menorah': 2594, 'mentalhealth': 2595, 'mentioned': 2596, 'mustfall': 2597, 'neighborhoods': 2598, 'nights': 2599, 'nonracist': 2600, 'path': 2601, 'performing': 2602, 'pls': 2603, 'posing': 2604, 'positivevibes': 2605, 'precisely': 2606, 'process': 2607, 'progressives': 2608, 'publicly': 2609, 'racebaiting': 2610, 'racemix': 2611, 'rambling': 2612, 'reprehensible': 2613, 'responded': 2614, 'retro': 2615, 'riddance': 2616, 'sales': 2617, 'scam': 2618, 'scream': 2619, 'settlement': 2620, 'shi': 2621, 'singer': 2622, 'sisters': 2623, 'situation': 2624, 'spot': 2625, 'springst': 2626, 'starbucks': 2627, 'statements': 2628, 'stoppe': 2629, 'taken': 2630, 'tattoo': 2631, 'tax': 2632, 'trade': 2633, 'ufc': 2634, 'uniteblue': 2635, 'universities': 2636, 'upf': 2637, 'uses': 2638, 'warning': 2639, 'yea': 2640, 'zuma': 2641, '08': 2642, '1200': 2643, '95': 2644, 'aboion': 2645, 'activist': 2646, 'affected': 2647, 'andor': 2648, 'announce': 2649, 'apologize': 2650, 'architecture': 2651, 'astrologer': 2652, 'b4': 2653, 'backlash': 2654, 'bae': 2655, 'begins': 2656, 'bhakt': 2657, 'birdsofafeather': 2658, 'blogpost': 2659, 'buliding': 2660, 'butt': 2661, 'caturday': 2662, 'centre': 2663, 'champ': 2664, 'clintons': 2665, 'condescension': 2666, 'couples': 2667, 'damned': 2668, 'dating': 2669, 'despise': 2670, 'doctor': 2671, 'dumbest': 2672, 'dumptrump': 2673, 'endorses': 2674, 'entrepreneur': 2675, 'escaped': 2676, 'fascists': 2677, 'felt': 2678, 'footage': 2679, 'fyi': 2680, 'goodluck': 2681, 'grace': 2682, 'hawaii': 2683, 'helpcovedolphins': 2684, 'hoping': 2685, 'huh': 2686, 'hus': 2687, 'inferior': 2688, 'james': 2689, 'lds': 2690, 'libcrib': 2691, 'lunatic': 2692, 'malema': 2693, 'mansplaining': 2694, 'melancholy': 2695, 'mere': 2696, 'mins': 2697, 'mummersparade': 2698, 'nick': 2699, 'niggas': 2700, 'ocean': 2701, 'ootd': 2702, 'oscar': 2703, 'pastor': 2704, 'perform': 2705, 'piano': 2706, 'plettenburgbay': 2707, 'poopin': 2708, 'potter': 2709, 'props': 2710, 'prosecute': 2711, 'rage': 2712, 'resolution': 2713, 'respond': 2714, 'rnc': 2715, 'shackles': 2716, 'sheets': 2717, 'sitting': 2718, 'slap': 2719, 'soccer': 2720, 'spent': 2721, 'spirit': 2722, 'stones': 2723, 'strength': 2724, 'stress': 2725, 'thegreenpalmcottage': 2726, 'therapy': 2727, 'thiza': 2728, 'thyini': 2729, 'transition': 2730, 'trc': 2731, 'tuned': 2732, 'tyranny': 2733, 'unappetizing': 2734, 'venue': 2735, 'views': 2736, 'westerncape': 2737, '1117': 2738, '14th': 2739, '16': 2740, '700000': 2741, 'according': 2742, 'bacon': 2743, 'ban': 2744, 'banned': 2745, 'battle': 2746, 'bff': 2747, 'bih': 2748, 'blast': 2749, 'bodies': 2750, 'breaks': 2751, 'brothers': 2752, 'cards': 2753, 'carlpaladinoelection': 2754, 'childrens': 2755, 'claimed': 2756, 'cochairman': 2757, 'colinpowell': 2758, 'colour': 2759, 'con': 2760, 'condoleezzarice': 2761, 'condone': 2762, 'controversy': 2763, 'cooking': 2764, 'crap': 2765, 'crashed': 2766, 'crew': 2767, 'defense': 2768, 'defunding': 2769, 'deliberately': 2770, 'discrepancy': 2771, 'dive': 2772, 'donation': 2773, 'dontbuythesun': 2774, 'doublestandards': 2775, 'employees': 2776, 'engage': 2777, 'essentialoils': 2778, 'euro': 2779, 'excouncillor': 2780, 'exists': 2781, 'explains': 2782, 'fed': 2783, 'filled': 2784, 'fixed': 2785, 'fuher': 2786, 'goodtimes': 2787, 'gravity': 2788, 'greece': 2789, 'grimmie': 2790, 'hacketts': 2791, 'heels': 2792, 'horrific': 2793, 'humour': 2794, 'ibiza': 2795, 'idwp': 2796, 'immature': 2797, 'indeed': 2798, 'inspired': 2799, 'inuit': 2800, 'iran': 2801, 'ireland': 2802, 'ivanka': 2803, 'jimmy': 2804, 'laughter': 2805, 'league': 2806, 'lgbtq': 2807, 'lights': 2808, 'liz': 2809, 'logins': 2810, 'loveislove': 2811, 'loveit': 2812, 'mexicans': 2813, 'mouthpiece': 2814, 'mrampmrs': 2815, 'murder': 2816, 'narrow': 2817, 'negative': 2818, 'neighbors': 2819, 'neonazis': 2820, 'nightclub': 2821, 'nsfw': 2822, 'offended': 2823, 'organiser': 2824, 'ouch': 2825, 'p21': 2826, 'performer': 2827, 'planet': 2828, 'politician': 2829, 'queue': 2830, 'radical': 2831, 'rags': 2832, 'repost': 2833, 'represent': 2834, 'resource': 2835, 'results': 2836, 'ripchristinagrimmie': 2837, 'rother': 2838, 'ruin': 2839, 'savor': 2840, 'scapelliti': 2841, 'scrutinize': 2842, 'seconds': 2843, 'secretaryofstate': 2844, 'simply': 2845, 'sociopath': 2846, 'spiced': 2847, 'spoke': 2848, 'status': 2849, 'studio': 2850, 'sturgeon': 2851, 'suspected': 2852, 'taught': 2853, 'tories': 2854, 'toy': 2855, 'treacy': 2856, 'tube': 2857, 'types': 2858, 'unfair': 2859, 'unleashes': 2860, 'usgtgtgt': 2861, 'wasted': 2862, 'wings': 2863, 'wise': 2864, 'xoxo': 2865, '1930s': 2866, 'al': 2867, 'antiislamist': 2868, 'arresting': 2869, 'athletes': 2870, 'attempt': 2871, 'awards': 2872, 'aware': 2873, 'bds': 2874, 'bliss': 2875, 'blunt': 2876, 'bouncingbaby': 2877, 'bridge': 2878, 'cameronstaff': 2879, 'camstaff': 2880, 'cannon': 2881, 'christ': 2882, 'clarity': 2883, 'click': 2884, 'climatechange': 2885, 'clock': 2886, 'closeted': 2887, 'confession': 2888, 'copy': 2889, 'coronial': 2890, 'cowardcops': 2891, 'cracker': 2892, 'creating': 2893, 'cross': 2894, 'cultures': 2895, 'damon': 2896, 'delicious': 2897, 'differently': 2898, 'duck': 2899, 'endorse': 2900, 'everyones': 2901, 'exercise': 2902, 'feet': 2903, 'feliz': 2904, 'fine': 2905, 'freedoms': 2906, 'fully': 2907, 'gate': 2908, 'genius': 2909, 'gig': 2910, 'greenspan': 2911, 'greenspans': 2912, 'growth': 2913, 'grp': 2914, 'hamids': 2915, 'handsome': 2916, 'happiest': 2917, 'harder': 2918, 'holds': 2919, 'horribly': 2920, 'igers': 2921, 'indianapolis': 2922, 'injured': 2923, 'inquest': 2924, 'interesting': 2925, 'issued': 2926, 'italian': 2927, 'jim': 2928, 'journo': 2929, 'kindle': 2930, 'kurds': 2931, 'lede': 2932, 'liked': 2933, 'lmfao': 2934, 'lock': 2935, 'losangeles': 2936, 'mama': 2937, 'marks': 2938, 'melancholymusic': 2939, 'michael': 2940, 'mike': 2941, 'minions': 2942, 'misogny': 2943, 'mysoginist': 2944, 'negro': 2945, 'neilerikson': 2946, 'nuff': 2947, 'numbers': 2948, 'odd': 2949, 'ofc': 2950, 'officerstaff': 2951, 'overwhelmingly': 2952, 'parenting': 2953, 'per': 2954, 'pissedoffitalian': 2955, 'plane': 2956, 'plus': 2957, 'product': 2958, 'profit': 2959, 'proven': 2960, 'pt': 2961, 'quality': 2962, 'quoteoftheday': 2963, 'races': 2964, 'rat': 2965, 'reflect': 2966, 'refuted': 2967, 'regressive': 2968, 'removed': 2969, 'rewrite': 2970, 'salt': 2971, 'sam': 2972, 'seeks': 2973, 'sends': 2974, 'senseless': 2975, 'sequel': 2976, 'session': 2977, 'shill': 2978, 'should': 2979, 'shuckinandjivin': 2980, 'silly': 2981, 'snowball': 2982, 'societies': 2983, 'sooo': 2984, 'spew': 2985, 'spin': 2986, 'stalking': 2987, 'stars': 2988, 'subtle': 2989, 'survey': 2990, 'swimming': 2991, 'taxpayers': 2992, 'threats': 2993, 'timing': 2994, 'topic': 2995, 'toures': 2996, 'traitorump': 2997, 'ultimately': 2998, 'ushistory': 2999, 'wade': 3000, 'whiteguys': 3001, 'whoever': 3002, 'yep': 3003, 'yum': 3004, '1000x': 3005, '1625': 3006, '21st': 3007, '31': 3008, '4th': 3009, 'accepted': 3010, 'affirmativeaction': 3011, 'airingofthegrievances': 3012, 'alligator': 3013, 'amerikkka': 3014, 'april': 3015, 'arrogant': 3016, 'asianmen': 3017, 'attorney': 3018, 'ball': 3019, 'band': 3020, 'barrackobama': 3021, 'beauregard': 3022, 'becomes': 3023, 'birmingham': 3024, 'blackonblack': 3025, 'blackwomen': 3026, 'bonuses': 3027, 'bunch': 3028, 'cam': 3029, 'candide': 3030, 'center': 3031, 'challenged': 3032, 'chaos': 3033, 'charity': 3034, 'cheap': 3035, 'chocolate': 3036, 'christmasadve': 3037, 'cities': 3038, 'climatecrimes': 3039, 'clip': 3040, 'closed': 3041, 'colombia': 3042, 'columbiamd': 3043, 'comedians': 3044, 'complain': 3045, 'condition': 3046, 'cons': 3047, 'consequences': 3048, 'courage': 3049, 'coz': 3050, 'created': 3051, 'crusader': 3052, 'datingapps': 3053, 'dealing': 3054, 'define': 3055, 'dig': 3056, 'discussed': 3057, 'diy': 3058, 'documentary': 3059, 'dogwhistle': 3060, 'drug': 3061, 'dwyane': 3062, 'eac': 3063, 'ecocide': 3064, 'edm': 3065, 'elitist': 3066, 'empowerment': 3067, 'epidemic': 3068, 'evolution': 3069, 'exhausted': 3070, 'extremely': 3071, 'extremist': 3072, 'f4f': 3073, 'fashio': 3074, 'fb': 3075, 'films': 3076, 'finepass': 3077, 'flagday2016': 3078, 'folk': 3079, 'foolin': 3080, 'gamer': 3081, 'gaming': 3082, 'generation': 3083, 'gf': 3084, 'goodday': 3085, 'google': 3086, 'graduates': 3087, 'greentown': 3088, 'help700': 3089, 'henrico': 3090, 'highfashion': 3091, 'historic': 3092, 'hocomd': 3093, 'hocoschools': 3094, 'hoe': 3095, 'impact': 3096, 'impoance': 3097, 'innovative': 3098, 'iphone': 3099, 'irony': 3100, 'istanbul': 3101, 'jeffery': 3102, 'josephjett': 3103, 'kitty': 3104, 'kumbaya': 3105, 'l4l': 3106, 'launched': 3107, 'lee': 3108, 'lifeisgood': 3109, 'lo': 3110, 'lure': 3111, 'magic': 3112, 'makeamericagreatagain': 3113, 'malia': 3114, 'match': 3115, 'multimillion': 3116, 'nationality': 3117, 'nye': 3118, 'paner': 3119, 'philandocastile': 3120, 'plethora': 3121, 'poland': 3122, 'popularvote': 3123, 'porns': 3124, 'pple': 3125, 'princecharles': 3126, 'private': 3127, 'projecting': 3128, 'rapists': 3129, 'requirement': 3130, 'resigned': 3131, 'resolve': 3132, 'runs': 3133, 'safety': 3134, 'sat': 3135, 'saturda': 3136, 'scene': 3137, 'screening': 3138, 'settlements': 3139, 'sexuality': 3140, 'shelvey': 3141, 'showed': 3142, 'slave': 3143, 'smoke': 3144, 'snowflake': 3145, 'societys': 3146, 'spo': 3147, 'stick': 3148, 'takuyakimura': 3149, 'techjunkiejhthe': 3150, 'threatening': 3151, 'treacherous': 3152, 'trumptrain': 3153, 'tupperwear': 3154, 'unforgettable': 3155, 'unpacks': 3156, 'uyanze': 3157, 'values': 3158, 'vps': 3159, 'waiter': 3160, 'waitingf': 3161, 'walls': 3162, 'warm': 3163, 'warns': 3164, 'wednesdaywisdom': 3165, 'wellness': 3166, 'whitefish': 3167, 'wildest': 3168, 'wilsons': 3169, 'womenmalayalam': 3170, 'woodrow': 3171, '247': 3172, '3rd': 3173, 'absurd': 3174, 'access': 3175, 'across': 3176, 'agitator': 3177, 'amazed': 3178, 'annoyed': 3179, 'apps': 3180, 'asks': 3181, 'assumptions': 3182, 'bailey': 3183, 'belongs': 3184, 'benghazi': 3185, 'biased': 3186, 'bills': 3187, 'binds': 3188, 'bjp': 3189, 'blackandwhite': 3190, 'blackpeople': 3191, 'boo': 3192, 'caer': 3193, 'canadian': 3194, 'candidates': 3195, 'cbb': 3196, 'ccot': 3197, 'celtic': 3198, 'cheerleader': 3199, 'christian': 3200, 'circle': 3201, 'classroom': 3202, 'co': 3203, 'coach': 3204, 'colts': 3205, 'commonsense': 3206, 'concur': 3207, 'corrupt': 3208, 'couesy': 3209, 'cox': 3210, 'cream': 3211, 'crude': 3212, 'disabilities': 3213, 'discriminating': 3214, 'ed': 3215, 'educating': 3216, 'emails': 3217, 'employee': 3218, 'enlightenment': 3219, 'ex': 3220, 'exploitation': 3221, 'fa': 3222, 'facepalm': 3223, 'failure': 3224, 'fave': 3225, 'finest': 3226, 'fk': 3227, 'foods': 3228, 'fragility': 3229, 'fries': 3230, 'friyay': 3231, 'funeral': 3232, 'gangs': 3233, 'gtgt': 3234, 'guncontrol': 3235, 'handmade': 3236, 'hilarious': 3237, 'hip': 3238, 'homicides': 3239, 'hospital': 3240, 'illegalguns': 3241, 'illegitimate': 3242, 'image': 3243, 'imwithher': 3244, 'inequity': 3245, 'islamofascist': 3246, 'italy': 3247, 'janeelliot': 3248, 'jessica': 3249, 'journal': 3250, 'justicereform': 3251, 'justsaying': 3252, 'kenya': 3253, 'keys': 3254, 'ki': 3255, 'legal': 3256, 'letstalkabout': 3257, 'lgbti': 3258, 'likeforlike': 3259, 'listened': 3260, 'madness': 3261, 'material': 3262, 'matters': 3263, 'mercy': 3264, 'mon': 3265, 'monetary': 3266, 'moon': 3267, 'morganfreeman': 3268, 'mum': 3269, 'nbavote': 3270, 'oligarch': 3271, 'opkkk': 3272, 'optrump': 3273, 'outrageous': 3274, 'pen': 3275, 'plutocrat': 3276, 'poc': 3277, 'pointed': 3278, 'population': 3279, 'posting': 3280, 'prayer': 3281, 'pres': 3282, 'push': 3283, 'realise': 3284, 'refuses': 3285, 'rep': 3286, 'resent': 3287, 'rule': 3288, 'screen': 3289, 'sent': 3290, 'sh': 3291, 'slip': 3292, 'slur': 3293, 'socialjus': 3294, 'steps': 3295, 'stockholm': 3296, 'sunderland': 3297, 'surrounded': 3298, 'talented': 3299, 'teeth': 3300, 'theatre': 3301, 'thepoliticalmillennial': 3302, 'thomas': 3303, 'threat': 3304, 'toddler': 3305, 'tonights': 3306, 'toronto': 3307, 'touches': 3308, 'ultra': 3309, 'victoria': 3310, 'videogames': 3311, 'willing': 3312, 'window': 3313, 'zionazi': 3314, '1000': 3315, '18': 3316, '63': 3317, 'actors': 3318, 'aidingenemies': 3319, 'air': 3320, 'alexjones': 3321, 'ampor': 3322, 'analysis': 3323, 'andrew': 3324, 'angel': 3325, 'antisemtism': 3326, 'approx': 3327, 'assistants': 3328, 'attempted': 3329, 'aus': 3330, 'basically': 3331, 'bastard': 3332, 'beast': 3333, 'beating': 3334, 'bihdaygirl': 3335, 'billionaire': 3336, 'bomb': 3337, 'broader': 3338, 'burn': 3339, 'burning': 3340, 'businesses': 3341, 'celebrities': 3342, 'childfree': 3343, 'commie': 3344, 'commonwealth': 3345, 'competition': 3346, 'complaint': 3347, 'confessions': 3348, 'convo': 3349, 'coverups': 3350, 'damonsajnani': 3351, 'decide': 3352, 'discuss': 3353, 'disgraced': 3354, 'disrespectful': 3355, 'divide': 3356, 'download': 3357, 'draintheswamp': 3358, 'dreads': 3359, 'economicapahied': 3360, 'emoji': 3361, 'employed': 3362, 'endless': 3363, 'enemy': 3364, 'engagement': 3365, 'extoion17': 3366, 'fasting': 3367, 'featuring': 3368, 'festive': 3369, 'fish': 3370, 'fitfam': 3371, 'foolish': 3372, 'forced': 3373, 'french': 3374, 'glass': 3375, 'gods': 3376, 'governor': 3377, 'hatredgreedamp': 3378, 'hereticfoundation': 3379, 'houston': 3380, 'indiedev': 3381, 'inflict': 3382, 'insecurity': 3383, 'islamaphobia': 3384, 'islamophobes': 3385, 'jackals': 3386, 'jewishsupremacist': 3387, 'joins': 3388, 'justify': 3389, 'kicking': 3390, 'laid': 3391, 'le': 3392, 'leading': 3393, 'legislative': 3394, 'livesbut': 3395, 'logo': 3396, 'lows': 3397, 'machine': 3398, 'maria': 3399, 'marvel': 3400, 'maryland': 3401, 'mdcollege': 3402, 'meaningless': 3403, 'minded': 3404, 'muslimpos': 3405, 'nationalism': 3406, 'neighborhood': 3407, 'neoconfederate': 3408, 'neoliberalism': 3409, 'nominee': 3410, 'nowhere': 3411, 'nursery': 3412, 'nut': 3413, 'obamajarretts': 3414, 'onelove': 3415, 'oppose': 3416, 'oprah': 3417, 'org': 3418, 'original': 3419, 'oust': 3420, 'overlap': 3421, 'papa': 3422, 'pas': 3423, 'pays': 3424, 'plants': 3425, 'plate': 3426, 'players': 3427, 'plays': 3428, 'powerhungrytraitors': 3429, 'price': 3430, 'prioritized': 3431, 'provide': 3432, 'putins': 3433, 'rare': 3434, 'ref': 3435, 'remedies': 3436, 'reminding': 3437, 'researchers': 3438, 'roof': 3439, 'rushlimbaugh': 3440, 'sajnani': 3441, 'salutbut': 3442, 'seller': 3443, 'senate': 3444, 'sexual': 3445, 'sharp': 3446, 'shift': 3447, 'shis': 3448, 'shout': 3449, 'sins': 3450, 'slaver': 3451, 'smth': 3452, 'snow': 3453, 'sob': 3454, 'sohappy': 3455, 'sons': 3456, 'stages': 3457, 'storytelling': 3458, 'summeime': 3459, 'suppression': 3460, 'sushi': 3461, 'sydney': 3462, 'teaches': 3463, 'tennessee': 3464, 'th': 3465, 'throwback': 3466, 'tolerated': 3467, 'topless': 3468, 'tough': 3469, 'track': 3470, 'transsexy': 3471, 'truthfully': 3472, 'ukip': 3473, 'unitednations': 3474, 'useless': 3475, 'victory': 3476, 'weekends': 3477, 'weight': 3478, 'weightloss': 3479, 'westerners': 3480, 'where': 3481, 'woohoo': 3482, 'woow': 3483, 'worried': 3484, 'written': 3485, 'yyc': 3486, '200k': 3487, '2016in4worlds': 3488, '40': 3489, '99c': 3490, 'absolute': 3491, 'accurately': 3492, 'adorable': 3493, 'alapoet': 3494, 'allows': 3495, 'aloha': 3496, 'alternative': 3497, 'altrightcards': 3498, 'amen': 3499, 'amendment': 3500, 'angels': 3501, 'anne': 3502, 'antisemites': 3503, 'anton': 3504, 'application': 3505, 'arm': 3506, 'atsi': 3507, 'attend': 3508, 'australians': 3509, 'balance': 3510, 'barackobama': 3511, 'bastards': 3512, 'bbq': 3513, 'beard': 3514, 'begun': 3515, 'behaving': 3516, 'bestfriends': 3517, 'biases': 3518, 'blacklivesmatters': 3519, 'blind': 3520, 'blog42': 3521, 'bonus': 3522, 'boos': 3523, 'bright': 3524, 'british': 3525, 'busily': 3526, 'buzzfeeds': 3527, 'cab': 3528, 'caoons': 3529, 'cars': 3530, 'cathedral': 3531, 'chelski': 3532, 'christinagrimmie': 3533, 'closer': 3534, 'cock': 3535, 'code': 3536, 'compassion': 3537, 'contempt': 3538, 'cowboy': 3539, 'cows': 3540, 'crow': 3541, 'cup': 3542, 'curse': 3543, 'dancers': 3544, 'darkness': 3545, 'deeper': 3546, 'defines': 3547, 'deleted': 3548, 'delivered': 3549, 'deploying': 3550, 'deseret': 3551, 'details': 3552, 'discovered': 3553, 'donny': 3554, 'drinking': 3555, 'ect': 3556, 'emirati': 3557, 'endthenation': 3558, 'ethnic': 3559, 'euros': 3560, 'everytime': 3561, 'excuses': 3562, 'eye': 3563, 'facebooks': 3564, 'facility': 3565, 'falls': 3566, 'ferguson': 3567, 'five': 3568, 'freedomofspeech': 3569, 'funded': 3570, 'gandhi': 3571, 'genderinequality': 3572, 'georgia': 3573, 'glasses': 3574, 'goal': 3575, 'goverment': 3576, 'grab': 3577, 'greetings': 3578, 'gwot': 3579, 'hammock': 3580, 'happykwanzaa': 3581, 'hiding': 3582, 'higher': 3583, 'humanrightswatch': 3584, 'humor': 3585, 'iamnotabe': 3586, 'ig': 3587, 'images': 3588, 'incite': 3589, 'inse': 3590, 'insensitive': 3591, 'instapic': 3592, 'instrumental': 3593, 'intelligence': 3594, 'interfering': 3595, 'iraq': 3596, 'jcpenney': 3597, 'jissus': 3598, 'karl': 3599, 'karma': 3600, 'killallwhitemen': 3601, 'komo': 3602, 'korea': 3603, 'lapdog': 3604, 'lately': 3605, 'launching': 3606, 'lbgt': 3607, 'leg': 3608, 'lessons': 3609, 'letting': 3610, 'loveyou': 3611, 'loyal': 3612, 'madrid': 3613, 'manhate': 3614, 'manspreading': 3615, 'materia': 3616, 'melanin': 3617, 'melo': 3618, 'merica': 3619, 'mindfulness': 3620, 'mmiw': 3621, 'mmiwg': 3622, 'mormonism': 3623, 'motivated': 3624, 'mud': 3625, 'narcissist': 3626, 'nerd': 3627, 'neutral': 3628, 'nofascist2017': 3629, 'nominate': 3630, 'nunes': 3631, 'obvious': 3632, 'oils': 3633, 'opkillingbaythecove': 3634, 'oppress': 3635, 'organic': 3636, 'oughta': 3637, 'outdated': 3638, 'overcome': 3639, 'packed': 3640, 'pamgelleheracist': 3641, 'pass': 3642, 'patriarchy': 3643, 'pe': 3644, 'peh': 3645, 'perspective': 3646, 'pet': 3647, 'phenomenon': 3648, 'pipe': 3649, 'poem': 3650, 'poke': 3651, 'poray': 3652, 'potential': 3653, 'pressure': 3654, 'prostitutes': 3655, 'proudly': 3656, 'publishing': 3657, 'pulls': 3658, 'quebecs': 3659, 'rapeugee': 3660, 'regas': 3661, 'registered': 3662, 'reputation': 3663, 'requires': 3664, 'rightfully': 3665, 'rima': 3666, 'roads': 3667, 'robe': 3668, 'romance': 3669, 'roses': 3670, 'san': 3671, 'seat': 3672, 'selfhating': 3673, 'sentiments': 3674, 'shades': 3675, 'shall': 3676, 'shooter': 3677, 'simpleton': 3678, 'skate': 3679, 'slaves': 3680, 'smell': 3681, 'solve': 3682, 'somehow': 3683, 'sophisticated': 3684, 'spanish': 3685, 'spending': 3686, 'standards': 3687, 'stoked': 3688, 'tackling': 3689, 'taharrush': 3690, 'taharrushgamea': 3691, 'ten': 3692, 'text': 3693, 'tht': 3694, 'thurs': 3695, 'tiny': 3696, 'treated': 3697, 'trendy': 3698, 'un': 3699, 'unloads': 3700, 'usual': 3701, 'verbalassault': 3702, 'voices': 3703, 'volunteer': 3704, 'voterid': 3705, 'vsco': 3706, 'whene': 3707, 'whether': 3708, 'whitepriv': 3709, 'whitesupremacists': 3710, 'wisconsinmadison': 3711, 'yell': 3712, 'zen': 3713, 'zionist': 3714, '2016in4': 3715, '2a': 3716, '74': 3717, 'aches': 3718, 'activists': 3719, 'aims': 3720, 'allblack': 3721, 'am': 3722, 'ampamp': 3723, 'appalling': 3724, 'aryanbrotherhood': 3725, 'asshat': 3726, 'associated': 3727, 'attorneygeneral': 3728, 'authorities': 3729, 'award': 3730, 'bandying': 3731, 'barcelona': 3732, 'bash': 3733, 'befriending': 3734, 'belief': 3735, 'bernie': 3736, 'binge': 3737, 'bitching': 3738, 'blackface': 3739, 'blessings': 3740, 'boat': 3741, 'bolt': 3742, 'boom': 3743, 'boot': 3744, 'bots': 3745, 'bracelet': 3746, 'bradshaw': 3747, 'bud': 3748, 'buddhism': 3749, 'buddhist': 3750, 'burmese': 3751, 'butthucreepers': 3752, 'buzzing': 3753, 'cafenoirproject': 3754, 'camron': 3755, 'caucasians': 3756, 'censored': 3757, 'chokes': 3758, 'chronic': 3759, 'cityjournal': 3760, 'clown': 3761, 'coldplay': 3762, 'commentsyou': 3763, 'complicity': 3764, 'confidence': 3765, 'conservatives': 3766, 'cooper': 3767, 'cousins': 3768, 'cowboys': 3769, 'crabs': 3770, 'crosscheckingvoter': 3771, 'cunt': 3772, 'defeat': 3773, 'degrading': 3774, 'delayed': 3775, 'delighted': 3776, 'delivery': 3777, 'depressing': 3778, 'derek': 3779, 'desire': 3780, 'detre': 3781, 'developer': 3782, 'diaz': 3783, 'discourse': 3784, 'discussing': 3785, 'disgusted': 3786, 'doplants': 3787, 'dw': 3788, 'edchat': 3789, 'email': 3790, 'emma': 3791, 'emojis': 3792, 'ended': 3793, 'eng': 3794, 'entitled': 3795, 'eurusd': 3796, 'expe': 3797, 'expectations': 3798, 'familytime': 3799, 'famous': 3800, 'fav': 3801, 'filter': 3802, 'filthy': 3803, 'forher': 3804, 'fowoh': 3805, 'friendly': 3806, 'fuel': 3807, 'genuinely': 3808, 'gifts': 3809, 'grave': 3810, 'guitar': 3811, 'hardly': 3812, 'hategroup': 3813, 'heroes': 3814, 'hired': 3815, 'historian': 3816, 'hits': 3817, 'hockey': 3818, 'holding': 3819, 'honest': 3820, 'hoodie': 3821, 'hypocrites': 3822, 'illegalisraelisettlements': 3823, 'immediately': 3824, 'impeachtrump': 3825, 'incident': 3826, 'increased': 3827, 'indignenous': 3828, 'interference': 3829, 'internal': 3830, 'israelisettlements': 3831, 'ivankatrump': 3832, 'jc': 3833, 'jewworldorder': 3834, 'jobsearch': 3835, 'kindledeals': 3836, 'kingconrad': 3837, 'knocks': 3838, 'latina': 3839, 'letter': 3840, 'lettuce': 3841, 'lied': 3842, 'lil': 3843, 'limiting': 3844, 'lines': 3845, 'loa': 3846, 'luv': 3847, 'mans': 3848, 'manus': 3849, 'maters': 3850, 'mediamisogyny': 3851, 'minds': 3852, 'mirror': 3853, 'mps': 3854, 'murderer': 3855, 'nbc': 3856, 'network': 3857, 'nevermypresident': 3858, 'nonwhite': 3859, 'notfunny': 3860, 'np': 3861, 'obesity': 3862, 'omfg': 3863, 'onto': 3864, 'opi': 3865, 'oppressi': 3866, 'originated': 3867, 'oven': 3868, 'parliament': 3869, 'passes': 3870, 'philosopher': 3871, 'plaster': 3872, 'position': 3873, 'pound': 3874, 'prankster': 3875, 'prejudiced': 3876, 'prevent': 3877, 'preying': 3878, 'prisoners': 3879, 'properly': 3880, 'prorapist': 3881, 'protection': 3882, 'proves': 3883, 'pumped': 3884, 'puts': 3885, 'putting': 3886, 'qualified': 3887, 'quickly': 3888, 'quite': 3889, 'racewar': 3890, 'raison': 3891, 'ranting': 3892, 'rants': 3893, 'recently': 3894, 'redacts': 3895, 'reemergence': 3896, 'regularass': 3897, 'retweeted': 3898, 'revealed': 3899, 'rhymes': 3900, 'riding': 3901, 'rohingyas': 3902, 'root': 3903, 'sainsburys': 3904, 'sanders': 3905, 'sands': 3906, 'sara': 3907, 'sarcastic': 3908, 'scalds': 3909, 'scores': 3910, 'scotus': 3911, 'sdgs': 3912, 'search': 3913, 'seeking': 3914, 'selah': 3915, 'shoppers': 3916, 'shyan': 3917, 'sided': 3918, 'sis': 3919, 'size': 3920, 'slavelabor': 3921, 'slices': 3922, 'sm': 3923, 'smileyong': 3924, 'smog': 3925, 'smooth': 3926, 'snap': 3927, 'stance': 3928, 'storm': 3929, 'stripe': 3930, 'sundayfunday': 3931, 'superhero': 3932, 'suppoed': 3933, 'supremecou': 3934, 'suspend': 3935, 'swim': 3936, 'tarnishing': 3937, 'techjunkiejhaltright': 3938, 'thoughj': 3939, 'threatened': 3940, 'thrive': 3941, 'ticking': 3942, 'tolerance': 3943, 'tomlin': 3944, 'traveling': 3945, 'tshi': 3946, 'tune': 3947, 'twisted': 3948, 'unhappy': 3949, 'usher': 3950, 'utterly': 3951, 'vanilla': 3952, 'vanity': 3953, 'veteran': 3954, 'wages': 3955, 'weaponry': 3956, 'weddings': 3957, 'wen': 3958, 'whiterace': 3959, 'wildlife': 3960, 'wohless': 3961, 'writer': 3962, 'wus': 3963, 'yeg': 3964, 'yellow': 3965, '2raise': 3966, '2stand': 3967, '35': 3968, '50s': 3969, '630': 3970, '700': 3971, '80snostalgia': 3972, '90': 3973, 'abetting': 3974, 'abou': 3975, 'actress': 3976, 'adores': 3977, 'advancement': 3978, 'adveisments': 3979, 'advocates': 3980, 'aftr': 3981, 'airlines': 3982, 'alexpascal': 3983, 'allowing': 3984, 'amazon': 3985, 'americad': 3986, 'amy': 3987, 'annoying': 3988, 'antonyelchin': 3989, 'approval': 3990, 'arc': 3991, 'arkansas': 3992, 'arrests': 3993, 'asean': 3994, 'asking': 3995, 'assailed': 3996, 'aungsansuukyi': 3997, 'awake': 3998, 'babe': 3999, 'badges': 4000, 'bank': 4001, 'beaten': 4002, 'bf': 4003, 'bigly': 4004, 'bird': 4005, 'bistro': 4006, 'blowjob': 4007, 'bn': 4008, 'bob': 4009, 'boston': 4010, 'boycottjcpenney': 4011, 'brands': 4012, 'breitba': 4013, 'brighton': 4014, 'brownshi': 4015, 'bubbles': 4016, 'burma': 4017, 'burninhell': 4018, 'bustymilf': 4019, 'calmly': 4020, 'candidate': 4021, 'cares': 4022, 'cease': 4023, 'ceased': 4024, 'celebrity': 4025, 'century': 4026, 'channel': 4027, 'chops': 4028, 'claycountydevelopmentcorp': 4029, 'cmon': 4030, 'collectively': 4031, 'comics': 4032, 'commenting': 4033, 'communists': 4034, 'computer': 4035, 'confederateflag': 4036, 'confident': 4037, 'construction': 4038, 'controlled': 4039, 'conversations': 4040, 'convinced': 4041, 'corporatebs': 4042, 'corrupttrump': 4043, 'cosmeticsbrand': 4044, 'coworker': 4045, 'crooked': 4046, 'culturalenrichment': 4047, 'curriculum': 4048, 'cwt': 4049, 'declining': 4050, 'deeply': 4051, 'demagoguery': 4052, 'democrat': 4053, 'desperate': 4054, 'destructive': 4055, 'didnot': 4056, 'digged': 4057, 'disadvantaged': 4058, 'disavow': 4059, 'disparo': 4060, 'distos': 4061, 'district': 4062, 'dnc': 4063, 'dollars': 4064, 'downtown': 4065, 'drops': 4066, 'drown': 4067, 'drugs': 4068, 'dumbingdownofamerica': 4069, 'dutch': 4070, 'easily': 4071, 'easteuropeans': 4072, 'ebook': 4073, 'eco': 4074, 'emo': 4075, 'empowerd': 4076, 'en': 4077, 'encounter': 4078, 'enjoyable': 4079, 'entitlement': 4080, 'entreteiment': 4081, 'equated': 4082, 'excitement': 4083, 'explanations': 4084, 'extinction': 4085, 'factophobia': 4086, 'failing': 4087, 'familybut': 4088, 'fe': 4089, 'feds': 4090, 'fighters': 4091, 'firstnations': 4092, 'focus': 4093, 'fr': 4094, 'freaking': 4095, 'funday': 4096, 'geek': 4097, 'generations': 4098, 'glee': 4099, 'gollybar': 4100, 'goodes': 4101, 'grass': 4102, 'griffiths': 4103, 'groom': 4104, 'guaranteed': 4105, 'guessing': 4106, 'gutter': 4107, 'hamzayusuf': 4108, 'hannity': 4109, 'hansen': 4110, 'harlem': 4111, 'hat': 4112, 'hater': 4113, 'headed': 4114, 'headlock': 4115, 'herb': 4116, 'highstool': 4117, 'hitlerjugend': 4118, 'honesty': 4119, 'hooked': 4120, 'iah': 4121, 'idiotic': 4122, 'igmilitia': 4123, 'impo': 4124, 'inadequate': 4125, 'inherent': 4126, 'inspire': 4127, 'ironies': 4128, 'jail': 4129, 'jake': 4130, 'joined': 4131, 'jokeoftheday': 4132, 'judged': 4133, 'kajal': 4134, 'khan': 4135, 'kingfisher': 4136, 'kitazato': 4137, 'kitazatoshibasaburo': 4138, 'kitten': 4139, 'kkkblm': 4140, 'ku': 4141, 'lit': 4142, 'loses': 4143, 'loudest': 4144, 'ltlt': 4145, 'magnettherapy': 4146, 'mandate': 4147, 'manifestdestiny': 4148, 'manning': 4149, 'marine': 4150, 'mate': 4151, 'meal': 4152, 'melania': 4153, 'melted': 4154, 'memo': 4155, 'memory': 4156, 'messages': 4157, 'mfs': 4158, 'migitorio': 4159, 'miles': 4160, 'mondaymotivation': 4161, 'mongers': 4162, 'morals': 4163, 'muslimbrotherhood': 4164, 'myoneresolution': 4165, 'named': 4166, 'net': 4167, 'newyears': 4168, 'nobelprize': 4169, 'noise': 4170, 'nomore': 4171, 'noting': 4172, 'notmyprez': 4173, 'nowe': 4174, 'objectify': 4175, 'opkillingbay': 4176, 'oral': 4177, 'ouuh': 4178, 'palladino': 4179, 'pamelaramseytaylor': 4180, 'paradise': 4181, 'patriotism': 4182, 'pauline': 4183, 'penney': 4184, 'pennsylvania': 4185, 'peoples': 4186, 'persecution': 4187, 'pewpewlife': 4188, 'photoshoot': 4189, 'pittsburgh': 4190, 'pivo': 4191, 'plain': 4192, 'plantation': 4193, 'prank': 4194, 'presses': 4195, 'prices': 4196, 'pridemake': 4197, 'priviledge': 4198, 'prolapsed': 4199, 'psycho': 4200, 'pussygrabber': 4201, 'qatar': 4202, 'quietly': 4203, 'react': 4204, 'rector': 4205, 'reflects': 4206, 'rehired': 4207, 'remake': 4208, 'resources': 4209, 'responsibilities': 4210, 'returning': 4211, 'rife': 4212, 'rockineve': 4213, 'rousey': 4214, 'route': 4215, 'rupemurdochs': 4216, 'saudi': 4217, 'scotland': 4218, 'season4': 4219, 'segregate': 4220, 'servant': 4221, 'setting': 4222, 'sexoffender': 4223, 'sexpredator': 4224, 'sexualassult': 4225, 'shock': 4226, 'shoutout': 4227, 'signed': 4228, 'simplistic': 4229, 'skank': 4230, 'skinned': 4231, 'skinny': 4232, 'sleeping': 4233, 'sleepy': 4234, 'sliding': 4235, 'sme': 4236, 'snobs': 4237, 'solidarity': 4238, 'stab': 4239, 'stolen': 4240, 'stops': 4241, 'supermistict': 4242, 'survivor': 4243, 'swear': 4244, 'sweep': 4245, 'tan': 4246, 'taxevader': 4247, 'teenager': 4248, 'teenagers': 4249, 'tetanusimmunization': 4250, 'thecove': 4251, 'thekingsspeech': 4252, 'theview': 4253, 'thirdworldcanada': 4254, 'threaten': 4255, 'tinyfingeredpuppet': 4256, 'tip': 4257, 'tolerate': 4258, 'touring': 4259, 'tpc': 4260, 'trampled': 4261, 'travelling': 4262, 'trompas': 4263, 'trumpas': 4264, 'trumpdarkzone': 4265, 'uglylady': 4266, 'ukraines': 4267, 'uks': 4268, 'unequal': 4269, 'unjust': 4270, 'verbally': 4271, 'volatile': 4272, 'volodymyrviatrovych': 4273, 'voter': 4274, 'whe': 4275, 'whiteaustralianpolicy': 4276, 'whoa': 4277, 'whose': 4278, 'witness': 4279, 'womenissues': 4280, 'womenonly': 4281, 'woodrowwilson': 4282, 'worldcampaign': 4283, 'worship': 4284, 'wwii': 4285, 'yesu': 4286, 'yields': 4287, 'yolo': 4288, 'zilles': 4289, '1pun': 4290, '2016in4word': 4291, '280': 4292, '342': 4293, '360': 4294, '40404': 4295, '83': 4296, 'accent': 4297, 'acne': 4298, 'administration': 4299, 'adves': 4300, 'alcohol': 4301, 'amid': 4302, 'annual': 4303, 'antics': 4304, 'apologies': 4305, 'asap': 4306, 'asylumseekers': 4307, 'attract': 4308, 'aufstehn': 4309, 'aussie': 4310, 'author': 4311, 'azealia': 4312, 'banksjimmy': 4313, 'basic': 4314, 'bdsm': 4315, 'bee': 4316, 'beingassuming': 4317, 'berlin': 4318, 'besties': 4319, 'bestseller': 4320, 'bihcontrol': 4321, 'blogging': 4322, 'bloody': 4323, 'bold': 4324, 'boring': 4325, 'breast': 4326, 'breed': 4327, 'bringbackpage3': 4328, 'buried': 4329, 'buying': 4330, 'bwahahahahaha': 4331, 'camber': 4332, 'cambma': 4333, 'camping': 4334, 'capabilities': 4335, 'catholicchurch': 4336, 'causes': 4337, 'causing': 4338, 'cave': 4339, 'cell': 4340, 'challenging': 4341, 'charges': 4342, 'chomsky': 4343, 'chrismukkah': 4344, 'christmassy': 4345, 'cite': 4346, 'col': 4347, 'colorblind': 4348, 'colors': 4349, 'combined': 4350, 'communities': 4351, 'complained': 4352, 'conflict': 4353, 'congressman': 4354, 'consent': 4355, 'consultant': 4356, 'convince': 4357, 'corruptgop': 4358, 'cost': 4359, 'coulda': 4360, 'cresc': 4361, 'cspan': 4362, 'day1': 4363, 'ddnt': 4364, 'decolonization': 4365, 'decolonizing': 4366, 'decree': 4367, 'dependent': 4368, 'dept': 4369, 'detention': 4370, 'dickhole': 4371, 'directly': 4372, 'disneyland': 4373, 'dividerofanation': 4374, 'dm': 4375, 'donaldduck': 4376, 'doubt': 4377, 'douchelord': 4378, 'drill': 4379, 'drinkdark': 4380, 'drs': 4381, 'dts': 4382, 'echoes': 4383, 'economy': 4384, 'eg': 4385, 'egomaniac': 4386, 'eltham': 4387, 'enabler': 4388, 'entered': 4389, 'escape': 4390, 'essay': 4391, 'everythingisracist': 4392, 'exaggeration': 4393, 'extending': 4394, 'faisal': 4395, 'farmer': 4396, 'feelgood': 4397, 'fix': 4398, 'fliers': 4399, 'flyer': 4400, 'fn': 4401, 'foreign': 4402, 'frnds': 4403, 'fullretard': 4404, 'furore': 4405, 'galery': 4406, 'gbpusd': 4407, 'gegenrechts': 4408, 'genderstereotypes': 4409, 'generationkkk': 4410, 'germans': 4411, 'goodonedean': 4412, 'govegan': 4413, 'grandmother': 4414, 'greek': 4415, 'grou': 4416, 'grows': 4417, 'gu': 4418, 'had': 4419, 'hangs': 4420, 'harvardsq': 4421, 'heat': 4422, 'helpless': 4423, 'hepburn': 4424, 'hid': 4425, 'highest': 4426, 'hillbots': 4427, 'hires': 4428, 'hispters': 4429, 'historicalamnesia': 4430, 'hm': 4431, 'holy': 4432, 'homegrown': 4433, 'horror': 4434, 'howell': 4435, 'humble': 4436, 'hump': 4437, 'icecream': 4438, 'ideals': 4439, 'ie': 4440, 'illustration': 4441, 'impoed': 4442, 'inhumane': 4443, 'innate': 4444, 'innercity': 4445, 'instahappy': 4446, 'integrity': 4447, 'intentions': 4448, 'international': 4449, 'internationallaw': 4450, 'intersectionality': 4451, 'intrumpsamerica': 4452, 'inveed': 4453, 'investigate': 4454, 'iqg': 4455, 'islamichomophobicrapistwho': 4456, 'jackson': 4457, 'jealousy': 4458, 'jewelry': 4459, 'joie': 4460, 'journalist': 4461, 'journalists': 4462, 'jump': 4463, 'kanyewest': 4464, 'kerala': 4465, 'kettled': 4466, 'lactation': 4467, 'lasvegas': 4468, 'latent': 4469, 'laughs': 4470, 'lauren': 4471, 'lazio': 4472, 'legacy': 4473, 'levin': 4474, 'li': 4475, 'liars': 4476, 'lib': 4477, 'libearian': 4478, 'liberalism': 4479, 'lulic': 4480, 'marxist': 4481, 'mascot': 4482, 'massage': 4483, 'mature': 4484, 'mccain': 4485, 'mcmillanjames': 4486, 'meaning': 4487, 'meanwhile': 4488, 'medicine': 4489, 'meditations': 4490, 'meds': 4491, 'merkel': 4492, 'metoo': 4493, 'metrics': 4494, 'mobile': 4495, 'mode': 4496, 'mosque': 4497, 'mp': 4498, 'msdhu': 4499, 'murders': 4500, 'mystery': 4501, 'nationalanthem': 4502, 'nationalbestfriendsday': 4503, 'need2know': 4504, 'neocolonialism': 4505, 'neonazi': 4506, 'neve': 4507, 'newzeeland': 4508, 'nia': 4509, 'noafd': 4510, 'nokkk': 4511, 'nonazis': 4512, 'nonprofitstatus': 4513, 'nonsens': 4514, 'nooltham': 4515, 'nopegida': 4516, 'nostrache': 4517, 'notallmen': 4518, 'notrump': 4519, 'nov': 4520, 'nowplaying': 4521, 'nutshell': 4522, 'nzpol': 4523, 'objective': 4524, 'obviously': 4525, 'occupation': 4526, 'oft': 4527, 'op': 4528, 'outfit': 4529, 'outraged': 4530, 'overwhelming': 4531, 'owes': 4532, 'passing': 4533, 'passionate': 4534, 'peek': 4535, 'pennyloafer': 4536, 'periscope': 4537, 'personalised': 4538, 'pets': 4539, 'photographer': 4540, 'physical': 4541, 'pissing': 4542, 'poems': 4543, 'politically': 4544, 'popefrancis': 4545, 'populism': 4546, 'pornisforlosers': 4547, 'praised': 4548, 'precious': 4549, 'precrime': 4550, 'preprogrammed': 4551, 'profession': 4552, 'propoganda': 4553, 'prospect': 4554, 'protects': 4555, 'protesters': 4556, 'provoking': 4557, 'prowar': 4558, 'ps': 4559, 'purposedly': 4560, 'pushing': 4561, 'putinism': 4562, 'quebec': 4563, 'raceplay': 4564, 'raise': 4565, 'ratings': 4566, 'reagan': 4567, 'recording': 4568, 'recovery': 4569, 'refer': 4570, 'relaxed': 4571, 'reliability': 4572, 'religionofpeace': 4573, 'represents': 4574, 'reproductiverights': 4575, 'repug': 4576, 'required': 4577, 'returnmyvinyl': 4578, 'reversed': 4579, 'revoltingly': 4580, 'rr': 4581, 'samehypocrite': 4582, 'sandyhook': 4583, 'sans': 4584, 'satisfied': 4585, 'saturdayblogshare': 4586, 'scale': 4587, 'scifi': 4588, 'seem': 4589, 'selfies': 4590, 'selfloathing': 4591, 'senad': 4592, 'sexualbullying': 4593, 'shower': 4594, 'sisterinlaw': 4595, 'six': 4596, 'smaimmigration': 4597, 'smells': 4598, 'socialclass': 4599, 'socialized': 4600, 'soundbite': 4601, 'sovereignty': 4602, 'speakers': 4603, 'spiritual': 4604, 'standwithisrael': 4605, 'storyville': 4606, 'strangely': 4607, 'straya': 4608, 'studying': 4609, 'sunbury': 4610, 'sundaymorning': 4611, 'sunglasses': 4612, 'survived': 4613, 'svpol': 4614, 'systems': 4615, 'tables': 4616, 'takin': 4617, 'tamirrice': 4618, 'taragon': 4619, 'taste': 4620, 'taxwriteoff': 4621, 'tb': 4622, 'tendencies': 4623, 'tha': 4624, 'thatmakes': 4625, 'theme': 4626, 'thissuppoing': 4627, 'thou': 4628, 'thx': 4629, 'timwise': 4630, 'titties': 4631, 'tokyo': 4632, 'tornado': 4633, 'tra': 4634, 'trendolizer': 4635, 'trht': 4636, 'tries': 4637, 'twitch': 4638, 'tws': 4639, 'uber': 4640, 'ufc207': 4641, 'um': 4642, 'unamerican': 4643, 'unbornlivesmatter': 4644, 'undermine': 4645, 'unfittobeinpublicoffice': 4646, 'uniting': 4647, 'unprecedented': 4648, 'uppity': 4649, 'upsetting': 4650, 'vaw': 4651, 'venezuela': 4652, 'vk': 4653, 'vocabulary': 4654, 'void': 4655, 'wakeupamericaca': 4656, 'walked': 4657, 'walks': 4658, 'wave': 4659, 'weddingplanning': 4660, 'whiteknight': 4661, 'willoughbys': 4662, 'wishlist': 4663, 'wmuslim': 4664, 'woobietuesday': 4665, 'worldview': 4666, 'yelchin': 4667, 'yousuck': 4668, 'zeroattacks': 4669, '12313': 4670, '200': 4671, '20days': 4672, '24hrs': 4673, '43': 4674, '60minutes': 4675, '952': 4676, 'aap': 4677, 'accused': 4678, 'acted': 4679, 'adventures': 4680, 'affair': 4681, 'africanamericans': 4682, 'ali': 4683, 'alternet': 4684, 'amateur': 4685, 'ambition': 4686, 'ana': 4687, 'anchor': 4688, 'android': 4689, 'announcement': 4690, 'answered': 4691, 'answers': 4692, 'anxious': 4693, 'applauding': 4694, 'applies': 4695, 'assured': 4696, 'aswell': 4697, 'ate': 4698, 'atheist': 4699, 'avoice': 4700, 'bags': 4701, 'baseball': 4702, 'batman': 4703, 'batshit': 4704, 'becomin': 4705, 'beensqueezing': 4706, 'bestfeature': 4707, 'bestoftheweek': 4708, 'bigger': 4709, 'biggotry': 4710, 'bihdays': 4711, 'bikini': 4712, 'bjork': 4713, 'blackvote': 4714, 'bruh': 4715, 'brunch': 4716, 'brunette': 4717, 'bts': 4718, 'cabinet': 4719, 'canadas': 4720, 'cancer': 4721, 'capitol': 4722, 'carolina': 4723, 'carolsbycandlelight': 4724, 'carriefisher': 4725, 'catalunya': 4726, 'cbc2017': 4727, 'challenges': 4728, 'chicken': 4729, 'childlabor': 4730, 'childrape': 4731, 'choking': 4732, 'clouds': 4733, 'codeword': 4734, 'comfo': 4735, 'communitythey': 4736, 'compet': 4737, 'condolences': 4738, 'confined': 4739, 'confirmed': 4740, 'conjob': 4741, 'conormcgregor': 4742, 'cpc': 4743, 'cpcldr': 4744, 'crackers': 4745, 'cramped': 4746, 'craving': 4747, 'crazybengiefbps': 4748, 'crcker': 4749, 'credibility': 4750, 'creepy': 4751, 'critical': 4752, 'crony': 4753, 'crybully': 4754, 'cucumbers': 4755, 'currently': 4756, 'dat': 4757, 'defeated': 4758, 'defend': 4759, 'defile': 4760, 'defineepitome': 4761, 'demand': 4762, 'depoation': 4763, 'deserves': 4764, 'deviant': 4765, 'dipshit': 4766, 'disingenuous': 4767, 'distant': 4768, 'dna': 4769, 'draining': 4770, 'drop': 4771, 'duke': 4772, 'dumbism': 4773, 'duty': 4774, 'dwd': 4775, 'ear': 4776, 'ebonics': 4777, 'edc': 4778, 'editor': 4779, 'eid': 4780, 'electronic': 4781, 'ellen': 4782, 'elxn42': 4783, 'emperor': 4784, 'enabled': 4785, 'encouraging': 4786, 'endlessly': 4787, 'endofthe': 4788, 'engaged': 4789, 'episodes': 4790, 'epitomeofsorrow': 4791, 'equivalent': 4792, 'estate': 4793, 'everyon': 4794, 'exams': 4795, 'excit': 4796, 'excludedmarginalised': 4797, 'exemplifies': 4798, 'experienced': 4799, 'exploring': 4800, 'eyetalian': 4801, 'f1': 4802, 'fabricate': 4803, 'faiths': 4804, 'fck': 4805, 'fearmongering': 4806, 'fibromyalgia': 4807, 'fiends': 4808, 'fill': 4809, 'finland': 4810, 'firstworldproblems': 4811, 'fled': 4812, 'flood': 4813, 'fooled': 4814, 'forest': 4815, 'freeways': 4816, 'fugly': 4817, 'gameofthrones': 4818, 'gazza': 4819, 'genes': 4820, 'gewitter': 4821, 'glitter': 4822, 'globe': 4823, 'goodmood': 4824, 'goodness': 4825, 'goon': 4826, 'goppropaganda': 4827, 'graffiti': 4828, 'grand': 4829, 'grind': 4830, 'gt9': 4831, 'gutted': 4832, 'havin': 4833, 'heel': 4834, 'heights': 4835, 'heroines': 4836, 'hi5': 4837, 'hispanicvote': 4838, 'hoes': 4839, 'hug': 4840, 'hugh': 4841, 'hvg': 4842, 'hypocritical': 4843, 'identity': 4844, 'idk': 4845, 'idol': 4846, 'ik': 4847, 'illustrated': 4848, 'imagemakeovers': 4849, 'indecent': 4850, 'independent': 4851, 'index': 4852, 'indoctrinating': 4853, 'infants': 4854, 'inn': 4855, 'instacool': 4856, 'instagrams': 4857, 'insulting': 4858, 'invented': 4859, 'ios': 4860, 'isisnt': 4861, 'islamicterrorist': 4862, 'israelikpop': 4863, 'itstheendoftheworldand': 4864, 'jabs': 4865, 'jeans': 4866, 'johnrkhoward': 4867, 'joint': 4868, 'jumpedthesh': 4869, 'justified': 4870, 'kneejerk': 4871, 'knitting': 4872, 'kpopdance': 4873, 'labeling': 4874, 'lan': 4875, 'laser': 4876, 'latepost': 4877, 'latinos': 4878, 'laurenduca': 4879, 'leather': 4880, 'leeds': 4881, 'leejasper': 4882, 'lesbian': 4883, 'letsgo': 4884, 'liberallogic': 4885, 'library': 4886, 'littleenglander': 4887, 'log': 4888, 'loteni': 4889, 'lovemylife': 4890, 'lovers': 4891, 'luicalibres': 4892, 'luxury': 4893, 'mac': 4894, 'macys': 4895, 'mafuckas': 4896, 'magical': 4897, 'makeo': 4898, 'malay': 4899, 'malcolm': 4900, 'management': 4901, 'manhattan': 4902, 'marry': 4903, 'massacre': 4904, 'materi': 4905, 'math': 4906, 'mcconnell': 4907, 'mediamy': 4908, 'mediocre': 4909, 'mention': 4910, 'merocrush': 4911, 'merry': 4912, 'micheleobama': 4913, 'microsoft': 4914, 'misogynisttrump': 4915, 'misunderstood': 4916, 'mob': 4917, 'morality': 4918, 'moves': 4919, 'mrminority': 4920, 'mrs': 4921, 'mtvwhiteguys': 4922, 'muir': 4923, 'murray': 4924, 'murrayhaters': 4925, 'namaste': 4926, 'narratives': 4927, 'native': 4928, 'nephew': 4929, 'neveoolate': 4930, 'nevr': 4931, 'niece': 4932, 'niga': 4933, 'nikita': 4934, 'nm': 4935, 'nope': 4936, 'notfit': 4937, 'notracist': 4938, 'nuclear': 4939, 'obamacare': 4940, 'obamanation': 4941, 'ongoing': 4942, 'opkillingbayseashepherd': 4943, 'oppressor': 4944, 'option': 4945, 'ottawa': 4946, 'oveurning': 4947, 'ow': 4948, 'packyourshitobama': 4949, 'pastors': 4950, 'payday': 4951, 'perfumer': 4952, 'persons': 4953, 'phrases': 4954, 'piggy': 4955, 'plannedparenthood': 4956, 'pleasure': 4957, 'pointer': 4958, 'possibility': 4959, 'pre': 4960, 'presidentobama': 4961, 'priceless': 4962, 'prohibition': 4963, 'prosecuted': 4964, 'pub': 4965, 'publicity': 4966, 'puppet': 4967, 'purchase': 4968, 'putinplant': 4969, 'putinsbitch': 4970, 'quoted': 4971, 'racecard': 4972, 'rachelmaddow': 4973, 'racistdeathwish': 4974, 'raging': 4975, 'raising': 4976, 'raw': 4977, 'referendum': 4978, 'referring': 4979, 'refresher': 4980, 'refugee': 4981, 'relation': 4982, 'remains': 4983, 'reparations': 4984, 'reply': 4985, 'repping': 4986, 'resigns': 4987, 'rhashtags': 4988, 'romantic': 4989, 'ruined': 4990, 'sa': 4991, 'sandniggers': 4992, 'satisfy': 4993, 'schwandorfchwandorf': 4994, 'seal': 4995, 'secrets': 4996, 'selected': 4997, 'selflove': 4998, 'sham': 4999, 'sic': 5000, 'signofthetimes': 5001, 'sjwlogic': 5002, 'sjws': 5003, 'slogan': 5004, 'snowpeople': 5005, 'sociopathic': 5006, 'somewhere': 5007, 'sooooo': 5008, 'soundcloud': 5009, 'spanking': 5010, 'specifically': 5011, 'spoty': 5012, 'staging': 5013, 'starkes': 5014, 'statementdoubling': 5015, 'stellar': 5016, 'sticker': 5017, 'stoned': 5018, 'stood': 5019, 'strategy': 5020, 'strawberry': 5021, 'streetfighter': 5022, 'subhuman': 5023, 'suggest': 5024, 'suicidal': 5025, 'summer2016': 5026, 'summit': 5027, 'surface': 5028, 'swamp': 5029, 'sweetpotato': 5030, 'swinging': 5031, 'tag': 5032, 'tamanna': 5033, 'taxpayer': 5034, 'teamcancer': 5035, 'teammichaelpalage': 5036, 'teens': 5037, 'temptation': 5038, 'thanksjackasses': 5039, 'thighhigh': 5040, 'thru': 5041, 'thursdaythoughts': 5042, 'tie': 5043, 'tis': 5044, 'token': 5045, 'tomorrows': 5046, 'topoli': 5047, 'trailer': 5048, 'trees': 5049, 'trough': 5050, 'trumpconaist': 5051, 'trumpleaks': 5052, 'trumpracist': 5053, 'turkey': 5054, 'twonitwittrumpsuppoers': 5055, 'unabated': 5056, 'unleashyourjoy': 5057, 'unlike': 5058, 'upcoming': 5059, 'updates': 5060, 'uskkk': 5061, 'utah': 5062, 'venusexchange': 5063, 'vicious': 5064, 'victimise': 5065, 'vscocam': 5066, 'waged': 5067, 'wakemeupwhen': 5068, 'wanother': 5069, 'warnung': 5070, 'washout': 5071, 'wetter': 5072, 'wetterwarnung': 5073, 'wifebeater': 5074, 'wilfully': 5075, 'wlk': 5076, 'wnt': 5077, 'wog': 5078, 'womenwednesday': 5079, 'wondered': 5080, 'worldwide': 5081, 'wwi': 5082, 'youtuber': 5083, 'zimbabwe': 5084, 'zit': 5085, '11th': 5086, '13thdocumentary': 5087, '14000': 5088, '19': 5089, '1996': 5090, '1gabba': 5091, '230pmet': 5092, '27': 5093, '946': 5094, 'abd': 5095, 'abpoli': 5096, 'accepting': 5097, 'accountable': 5098, 'activities': 5099, 'actual': 5100, 'af': 5101, 'afghanistan': 5102, 'alex': 5103, 'algona': 5104, 'altogether': 5105, 'alway': 5106, 'ameture': 5107, 'ampthey': 5108, 'angryignorantand': 5109, 'animalrights': 5110, 'appearance': 5111, 'aptly': 5112, 'arabian': 5113, 'arebut': 5114, 'argentine': 5115, 'argues': 5116, 'arise': 5117, 'aryans': 5118, 'attempting': 5119, 'avoid': 5120, 'awarded': 5121, 'backhanders': 5122, 'bailreform': 5123, 'bangladeshis': 5124, 'bbuk': 5125, 'bees': 5126, 'beforesex': 5127, 'behaviour': 5128, 'berberian': 5129, 'bethlehem': 5130, 'bio': 5131, 'biological': 5132, 'bitcoin': 5133, 'bla': 5134, 'blackbrown': 5135, 'blacklives': 5136, 'blazing': 5137, 'bloggers': 5138, 'boycottaustralia': 5139, 'boyden': 5140, 'braids': 5141, 'brainsjust': 5142, 'brazilians': 5143, 'breathes': 5144, 'bullies': 5145, 'bush': 5146, 'bustyescos': 5147, 'calendar': 5148, 'camps': 5149, 'canadians': 5150, 'cannabis': 5151, 'caoonish': 5152, 'capetown': 5153, 'careful': 5154, 'caroline': 5155, 'centred': 5156, 'ceo': 5157, 'cheese': 5158, 'childabuse': 5159, 'choosing': 5160, 'chumps': 5161, 'churchs': 5162, 'cinema': 5163, 'civilians': 5164, 'cjreform': 5165, 'claiming': 5166, 'cleansing': 5167, 'coherently': 5168, 'compared': 5169, 'compass': 5170, 'competency': 5171, 'constantly': 5172, 'contract': 5173, 'contradictions': 5174, 'copkiller': 5175, 'copped': 5176, 'countered': 5177, 'countys': 5178, 'cousin': 5179, 'crookedtrump': 5180, 'crosses': 5181, 'cuck': 5182, 'cuddles': 5183, 'culturesways': 5184, 'datesletstalkabout': 5185, 'deathpenalty': 5186, 'definitive': 5187, 'delusional': 5188, 'deming': 5189, 'democrate': 5190, 'demonize': 5191, 'depl': 5192, 'designed': 5193, 'desirehatred': 5194, 'dicks': 5195, 'differences': 5196, 'dignity': 5197, 'disallowd': 5198, 'disenfranchised': 5199, 'dncchair': 5200, 'dolled': 5201, 'domesticviolence': 5202, 'donations': 5203, 'doublestandard': 5204, 'drama': 5205, 'dummies': 5206, 'dylann': 5207, 'earrings': 5208, 'edgy': 5209, 'electrical': 5210, 'emirat': 5211, 'empress': 5212, 'eod': 5213, 'eren': 5214, 'ethnically': 5215, 'etsy': 5216, 'evasion': 5217, 'exam': 5218, 'existence': 5219, 'explanation': 5220, 'exposing': 5221, 'express': 5222, 'expressions': 5223, 'eyesvote': 5224, 'fakenewsmedia': 5225, 'farah': 5226, 'fare': 5227, 'fatty': 5228, 'fea': 5229, 'filth': 5230, 'finals': 5231, 'flattering': 5232, 'fluently': 5233, 'follow4follow': 5234, 'foot': 5235, 'forcing': 5236, 'freethenipple': 5237, 'freshmen': 5238, 'fruit': 5239, 'fuckdonaldtrump': 5240, 'furniture': 5241, 'gains': 5242, 'game7': 5243, 'geared': 5244, 'gel': 5245, 'gen': 5246, 'grabbing': 5247, 'graceless': 5248, 'grandma': 5249, 'grandpa': 5250, 'grenades': 5251, 'guard': 5252, 'gwent': 5253, 'hacking': 5254, 'hardcoresex': 5255, 'hatefilled': 5256, 'hathaway': 5257, 'hawk': 5258, 'healand': 5259, 'heater': 5260, 'hebrews': 5261, 'helm': 5262, 'herbal': 5263, 'hillbilly': 5264, 'hiphop': 5265, 'hitting': 5266, 'hiv': 5267, 'host': 5268, 'household': 5269, 'hubby': 5270, 'hull': 5271, 'hyperviolent': 5272, 'idolized': 5273, 'iefascistdemocracy': 5274, 'ifyourenotwhiteyourenotracist': 5275, 'ifyousawinyouwhatisee': 5276, 'ignorancehate': 5277, 'illegally': 5278, 'imagination': 5279, 'imitations': 5280, 'immigrant': 5281, 'impacts': 5282, 'inc': 5283, 'indie': 5284, 'indiegamedev': 5285, 'indisputably': 5286, 'infection': 5287, 'infidel': 5288, 'infographic': 5289, 'inhumans': 5290, 'intelligent': 5291, 'interacts': 5292, 'intrusively': 5293, 'irritations': 5294, 'islamophobic': 5295, 'israelis': 5296, 'it': 5297, 'italians': 5298, 'jehovas': 5299, 'jemele': 5300, 'jersey': 5301, 'jlaw': 5302, 'joseph': 5303, 'juan': 5304, 'juvenilejustice': 5305, 'karen': 5306, 'keithellisons': 5307, 'keithx': 5308, 'keshi': 5309, 'kicks': 5310, 'kindleunlimited': 5311, 'kingdom': 5312, 'knowledge': 5313, 'koetankauk': 5314, 'ksa': 5315, 'kwanza': 5316, 'leftistlunatics': 5317, 'legislature': 5318, 'lgbtfamilybut': 5319, 'lid': 5320, 'lifecoach': 5321, 'lisa': 5322, 'liverpool': 5323, 'loud': 5324, 'lunatics': 5325, 'lynching': 5326, 'mailbox': 5327, 'marathi': 5328, 'marginalized': 5329, 'marxism': 5330, 'marxs': 5331, 'masterkeyexperience': 5332, 'melbourne': 5333, 'meltingpot': 5334, 'messiah': 5335, 'metaphor': 5336, 'mic': 5337, 'mini': 5338, 'miriam': 5339, 'misandry': 5340, 'mispronouncing': 5341, 'mocked': 5342, 'mommy': 5343, 'moms': 5344, 'mormon': 5345, 'moronsmatter': 5346, 'moto': 5347, 'moveable': 5348, 'msian': 5349, 'munmid': 5350, 'mutual': 5351, 'mylove': 5352, 'nackt': 5353, 'nc': 5354, 'ndn': 5355, 'neighbours': 5356, 'newghostbusters': 5357, 'newyearsevemay': 5358, 'noone': 5359, 'norman': 5360, 'notamericaschoice': 5361, 'notmypresidnet': 5362, 'novel': 5363, 'nword': 5364, 'nyt': 5365, 'offensivememes69': 5366, 'onlywhitechristmas': 5367, 'oppression': 5368, 'outhmm': 5369, 'overexcited': 5370, 'panthers': 5371, 'panties': 5372, 'paper': 5373, 'peeps': 5374, 'pegida': 5375, 'perpetuating': 5376, 'petous': 5377, 'picked': 5378, 'playground': 5379, 'playlist': 5380, 'plenty': 5381, 'points': 5382, 'politicalmillennial': 5383, 'polls': 5384, 'porait': 5385, 'pornstars': 5386, 'pot': 5387, 'praise': 5388, 'preferences': 5389, 'prick': 5390, 'principal': 5391, 'productive': 5392, 'profitable': 5393, 'profiting': 5394, 'proimmigrant': 5395, 'promotions': 5396, 'prompts': 5397, 'pros': 5398, 'prosecutorialmisconduct': 5399, 'proudtobeanamerican': 5400, 'ps4': 5401, 'pussies': 5402, 'racewarchess': 5403, 'radio2': 5404, 'raids': 5405, 'random': 5406, 'rd': 5407, 'regnant': 5408, 'rendition': 5409, 'repulsion': 5410, 'repulsive': 5411, 'resilience': 5412, 'resolutions': 5413, 'responds': 5414, 'reunited': 5415, 'rhonda': 5416, 'richie': 5417, 'riders': 5418, 'rightie': 5419, 'rocks': 5420, 'rotten': 5421, 'rubio': 5422, 'russias': 5423, 'sabotage': 5424, 'saddles': 5425, 'sadness': 5426, 'santaproject': 5427, 'sblaar16': 5428, 'scapegoat': 5429, 'screenshot': 5430, 'sdoh': 5431, 'secretly': 5432, 'selling': 5433, 'semiretarded': 5434, 'sets': 5435, 'seventies': 5436, 'shadowed': 5437, 'shagged': 5438, 'shairi': 5439, 'sheep': 5440, 'shouts': 5441, 'showmeyoureawoman': 5442, 'skies': 5443, 'slapcam': 5444, 'sleeps': 5445, 'slug': 5446, 'slutty': 5447, 'songwords': 5448, 'soo': 5449, 'southsudan': 5450, 'spectrum': 5451, 'spokesperson': 5452, 'ss': 5453, 'stain': 5454, 'staying': 5455, 'staywoke': 5456, 'stockpiles': 5457, 'stopped': 5458, 'stopthewars': 5459, 'striker': 5460, 'struggling': 5461, 'studied': 5462, 'subversive': 5463, 'sulfur': 5464, 'supermariorun': 5465, 'suppos': 5466, 'swap': 5467, 'sympathisers': 5468, 'table': 5469, 'tape': 5470, 'tasar': 5471, 'tens': 5472, 'terraced': 5473, 'testing': 5474, 'tiffany': 5475, 'tl': 5476, 'toward': 5477, 'trapped': 5478, 'trouble': 5479, 'trumphate': 5480, 'trumpuniversity': 5481, 'unacceptable': 5482, 'uncivilised': 5483, 'uneducated': 5484, 'unicef': 5485, 'unsatisfied': 5486, 'upenn': 5487, 'usa2017': 5488, 'usdcad': 5489, 'ux': 5490, 'various': 5491, 'vengeance': 5492, 'verified': 5493, 'vetting': 5494, 'vi': 5495, 'vintage': 5496, 'vip': 5497, 'virgin': 5498, 'voteleave': 5499, 'wahhhh': 5500, 'waited': 5501, 'waking': 5502, 'wang': 5503, 'waronwomen': 5504, 'washington': 5505, 'wee': 5506, 'wellbeing': 5507, 'whaling': 5508, 'whatsoever': 5509, 'whenever': 5510, 'whiners': 5511, 'whiteland': 5512, 'whitepower': 5513, 'whitesboro': 5514, 'whoop': 5515, 'wingers': 5516, 'wit': 5517, 'yoyas': 5518, 'zionists': 5519, '15thcentury': 5520, '2017npr': 5521, '22': 5522, '23': 5523, '49': 5524, '72': 5525, '940pm': 5526, '99cents': 5527, '99p': 5528, 'absence': 5529, 'achilles': 5530, 'actively': 5531, 'activism': 5532, 'adamitv': 5533, 'adveising': 5534, 'afamhist': 5535, 'aka': 5536, 'alleges': 5537, 'amoral': 5538, 'amounts': 5539, 'andrewanglin': 5540, 'angrygaymafia': 5541, 'anna': 5542, 'announced': 5543, 'anothr': 5544, 'antiblack': 5545, 'apa': 5546, 'appendix': 5547, 'applicable': 5548, 'appropriating': 5549, 'assaults': 5550, 'asssssworst': 5551, 'attendants': 5552, 'atwater': 5553, 'audism': 5554, 'authoritarian': 5555, 'averse': 5556, 'background': 5557, 'bake': 5558, 'bald': 5559, 'bambi': 5560, 'bates': 5561, 'bath': 5562, 'bats': 5563, 'baycat': 5564, 'berniebros': 5565, 'betwn': 5566, 'bible': 5567, 'bikinis': 5568, 'bin': 5569, 'bitchesbelike': 5570, 'blasting': 5571, 'bleating': 5572, 'bloated': 5573, 'bloggersblast': 5574, 'blowing': 5575, 'bogard': 5576, 'bogota': 5577, 'bottle': 5578, 'boycotting': 5579, 'bragged': 5580, 'brave': 5581, 'bregret': 5582, 'bridges': 5583, 'britains': 5584, 'broad': 5585, 'buggeroffboris': 5586, 'bureau': 5587, 'busi': 5588, 'cable': 5589, 'candy': 5590, 'caoon': 5591, 'capone': 5592, 'carefully': 5593, 'carolyn': 5594, 'chaer': 5595, 'chapter': 5596, 'charlespaladinos': 5597, 'cheaters': 5598, 'chepstow': 5599, 'chihuahuas': 5600, 'chri': 5601, 'chump': 5602, 'cleaning': 5603, 'climate': 5604, 'clio': 5605, 'cocktails': 5606, 'codified': 5607, 'colluder': 5608, 'colored': 5609, 'colours': 5610, 'comic': 5611, 'commit': 5612, 'comrade': 5613, 'conces': 5614, 'condemn': 5615, 'contender': 5616, 'cook': 5617, 'cookies': 5618, 'costars': 5619, 'cottoned': 5620, 'countryis': 5621, 'crashes': 5622, 'creates': 5623, 'crook': 5624, 'cuba': 5625, 'cutie': 5626, 'cutsie': 5627, 'cycling': 5628, 'cymru': 5629, 'damage': 5630, 'dankmemes': 5631, 'dares': 5632, 'daughters': 5633, 'deafed': 5634, 'deafinprison': 5635, 'deceit': 5636, 'decriminaliz': 5637, 'defenders': 5638, 'delhi': 5639, 'demanding': 5640, 'demonic': 5641, 'demonstrated': 5642, 'denzelwashington': 5643, 'deploraball': 5644, 'destroying': 5645, 'detoxdiet': 5646, 'dictionaries': 5647, 'diff': 5648, 'digits': 5649, 'direction': 5650, 'director': 5651, 'dis': 5652, 'discredit': 5653, 'displaced': 5654, 'djt': 5655, 'doors': 5656, 'draftdodger': 5657, 'drivers': 5658, 'drsuess': 5659, 'eahs': 5660, 'echo': 5661, 'educated': 5662, 'electoralcollege': 5663, 'elects': 5664, 'embrac': 5665, 'empowering': 5666, 'error': 5667, 'espn': 5668, 'eternity': 5669, 'eugenie': 5670, 'eurosceptic': 5671, 'evan': 5672, 'everlord': 5673, 'example': 5674, 'excrement': 5675, 'expecttrumps': 5676, 'extra': 5677, 'fallacy': 5678, 'falling': 5679, 'fam': 5680, 'familiesfamilycircus': 5681, 'faster': 5682, 'fatherhood': 5683, 'fbi': 5684, 'fearlessly': 5685, 'federline': 5686, 'feedback': 5687, 'feigned': 5688, 'females': 5689, 'flagday': 5690, 'flushed': 5691, 'forbidden': 5692, 'foretold': 5693, 'fragile': 5694, 'freed': 5695, 'ft': 5696, 'fundraising': 5697, 'gang': 5698, 'gays': 5699, 'gee': 5700, 'genderequity': 5701, 'genuine': 5702, 'gin': 5703, 'giphy': 5704, 'glasgow': 5705, 'goodlife': 5706, 'goyim': 5707, 'gracemugabe': 5708, 'gracious': 5709, 'grade': 5710, 'greater': 5711, 'grey': 5712, 'gt6': 5713, 'guzzling': 5714, 'h20': 5715, 'hahaha': 5716, 'haim': 5717, 'hamilton': 5718, 'hanging': 5719, 'hanukkah': 5720, 'harbaugh': 5721, 'harmful': 5722, 'hasbeen': 5723, 'hbd': 5724, 'headline': 5725, 'heck': 5726, 'herschlags': 5727, 'hidden': 5728, 'hollaback': 5729, 'homemade': 5730, 'homophobiaenablin': 5731, 'homoresistant': 5732, 'hye': 5733, 'hyena': 5734, 'idiocy': 5735, 'ignorantly': 5736, 'illegals': 5737, 'impressions': 5738, 'inbred': 5739, 'incidents': 5740, 'increase': 5741, 'increasing': 5742, 'indoctrination': 5743, 'infantile': 5744, 'ink': 5745, 'insanity': 5746, 'institutionalized': 5747, 'intermittently': 5748, 'interterrestrial': 5749, 'invest': 5750, 'invisible': 5751, 'iqgt95': 5752, 'irishman': 5753, 'islamicradicals': 5754, 'islamopho': 5755, 'isolationist': 5756, 'jailed': 5757, 'jerodtwin': 5758, 'jgirl': 5759, 'joemixon': 5760, 'jonjo': 5761, 'journos': 5762, 'joyful': 5763, 'joytrain': 5764, 'juice': 5765, 'jumpedtheshark': 5766, 'keepyoureyesontheprizemlk': 5767, 'kicked': 5768, 'kickitout': 5769, 'kimburrell': 5770, 'kindly': 5771, 'kisses': 5772, 'kissmyarse': 5773, 'koolaid': 5774, 'kpop': 5775, 'krakow': 5776, 'laborviolations': 5777, 'labour': 5778, 'lackey': 5779, 'lanasprayberry': 5780, 'latesnews': 5781, 'lawmakers': 5782, 'leaveeu': 5783, 'letsdothis': 5784, 'lg': 5785, 'liberalisme': 5786, 'lipstick': 5787, 'littlemix': 5788, 'liveleap': 5789, 'lobotomy': 5790, 'loosahs': 5791, 'losers': 5792, 'loveyourself': 5793, 'lpc': 5794, 'mail': 5795, 'mailboxpride': 5796, 'malayalees': 5797, 'malign': 5798, 'malikrichmond': 5799, 'marrying': 5800, 'massincarceration': 5801, 'mating': 5802, 'measure': 5803, 'medical': 5804, 'merely': 5805, 'mfer': 5806, 'midtown': 5807, 'milwaukee': 5808, 'minnesotavikings': 5809, 'minoritypresident': 5810, 'miserable': 5811, 'misrepresentation': 5812, 'mogage': 5813, 'monkey': 5814, 'monsters': 5815, 'moonman': 5816, 'morgan': 5817, 'mugabe': 5818, 'multiracial': 5819, 'mylife': 5820, 'naacp': 5821, 'narcissimmisogyny': 5822, 'nashville': 5823, 'neogaf': 5824, 'neverdelta': 5825, 'nohate': 5826, 'nonjews': 5827, 'nowadays': 5828, 'nudist': 5829, 'nudity': 5830, 'nutrition': 5831, 'nye16': 5832, 'oab': 5833, 'observation': 5834, 'obsidian': 5835, 'offences': 5836, 'offering': 5837, 'opponent': 5838, 'opposing': 5839, 'oprollredroll': 5840, 'orangeisthenewblack': 5841, 'orlandonightclubshooting': 5842, 'outdoors': 5843, 'outsider': 5844, 'ovr': 5845, 'owned': 5846, 'pack': 5847, 'paisanhack': 5848, 'pandy': 5849, 'pants': 5850, 'paranormal': 5851, 'pasture': 5852, 'pattismith': 5853, 'peotus': 5854, 'permit': 5855, 'perpetuates': 5856, 'phones': 5857, 'picturebangladeshi': 5858, 'pissed': 5859, 'playa': 5860, 'pleased': 5861, 'poet': 5862, 'precision': 5863, 'premiere': 5864, 'prepare': 5865, 'preparing': 5866, 'presidentpussygrabber': 5867, 'prize': 5868, 'production': 5869, 'professional': 5870, 'proo': 5871, 'propalestine': 5872, 'propey': 5873, 'pubfriction': 5874, 'pulled': 5875, 'pupil': 5876, 'purge': 5877, 'putinpuppet': 5878, 'qualify': 5879, 'questioned': 5880, 'quiz': 5881, 'qusay': 5882, 'racetraitors': 5883, 'racismfacinghis': 5884, 'raining': 5885, 'range': 5886, 'rap': 5887, 'reactionary': 5888, 'readingbuying': 5889, 'reagans': 5890, 'realhousewives': 5891, 'reappraise': 5892, 'recipe': 5893, 'reconciliation': 5894, 'reg': 5895, 'regret': 5896, 'reich': 5897, 'republicantrash': 5898, 'reso': 5899, 'reson': 5900, 'responsible': 5901, 'rethinkdiscipline': 5902, 'returns': 5903, 'rhoa': 5904, 'rn': 5905, 'roadtrip': 5906, 'robinson': 5907, 'rottweilers': 5908, 'routine': 5909, 'royal': 5910, 'rugby': 5911, 'sand': 5912, 'sandy': 5913, 'schedule': 5914, 'schuster': 5915, 'sciencefiction': 5916, 'score': 5917, 'seattle': 5918, 'selection': 5919, 'separate': 5920, 'servants': 5921, 'sexualharassment': 5922, 'sheree': 5923, 'shitler': 5924, 'shocker': 5925, 'simon': 5926, 'sirf': 5927, 'sistersofcolour': 5928, 'slash': 5929, 'smiley': 5930, 'sms': 5931, 'snaps': 5932, 'soft': 5933, 'solutions': 5934, 'souls': 5935, 'southsudankot': 5936, 'spam': 5937, 'spirituality': 5938, 'staer': 5939, 'starspangledbanner': 5940, 'steubenville': 5941, 'stocking': 5942, 'stpp': 5943, 'strange': 5944, 'strawberries': 5945, 'struggles': 5946, 'stuffed': 5947, 'stupidto': 5948, 'sucked': 5949, 'sucklahoma': 5950, 'suggests': 5951, 'suing': 5952, 'surgical': 5953, 'suspended': 5954, 'swag': 5955, 'swampmonster': 5956, 'swastikas': 5957, 'sweden': 5958, 'talkradio': 5959, 'tennis': 5960, 'third': 5961, 'throwbackthursday': 5962, 'tiesto': 5963, 'tinyhands': 5964, 'tits': 5965, 'transphobia': 5966, 'trentmays': 5967, 'tribalism': 5968, 'truthand': 5969, 'tsa': 5970, 'tt': 5971, 'tucker': 5972, 'twitte': 5973, 'tyrant': 5974, 'uday': 5975, 'ultraohodox': 5976, 'understanding': 5977, 'upbeat': 5978, 'usdjpy': 5979, 'uspoli': 5980, 'usually': 5981, 'uttered': 5982, 'validating': 5983, 'vestal': 5984, 'victimhood': 5985, 'vlicobs': 5986, 'vot': 5987, 'wailed': 5988, 'wattpad': 5989, 'waves': 5990, 'wb': 5991, 'webcam': 5992, 'weirdo': 5993, 'wellduh': 5994, 'wells': 5995, 'what2inlieuof': 5996, 'whate': 5997, 'wherford': 5998, 'whiteamerica': 5999, 'whiteisis': 6000, 'whiter': 6001, 'whitesheet': 6002, 'willsmith': 6003, 'wmn': 6004, 'wn': 6005, 'womans': 6006, 'womenrelated': 6007, 'womensissues': 6008, 'womensrights': 6009, 'wordsalad': 6010, 'worksheet': 6011, 'worlds': 6012, 'wwdc2016': 6013, 'wwe': 6014, 'wwwsmallgirlsexcom': 6015, 'wyou': 6016, 'xd': 6017, 'xenophobe': 6018, 'yavin': 6019, 'youngturks': 6020, 'zurich': 6021, '2015': 6022, '2b': 6023, '38billion': 6024, '399': 6025, '48': 6026, '500': 6027, '5wordtrumplethinskin': 6028, '6th': 6029, '8': 6030, 'aberdeen': 6031, 'accident': 6032, 'accounts': 6033, 'acct': 6034, 'achieve': 6035, 'acknowledge': 6036, 'admin': 6037, 'afl': 6038, 'alarm': 6039, 'alcoholic': 6040, 'aleppo': 6041, 'alliesforblacklives': 6042, 'amarinder': 6043, 'americadoesntwantyou': 6044, 'amsterdam': 6045, 'annoy': 6046, 'anslinger': 6047, 'antimorality': 6048, 'appreciation': 6049, 'ark': 6050, 'armpit': 6051, 'ashiq': 6052, 'atherapy': 6053, 'attire': 6054, 'attorneys': 6055, 'autocorrects': 6056, 'avenue': 6057, 'aw': 6058, 'awork': 6059, 'bagdad': 6060, 'bankruptcy': 6061, 'bcot': 6062, 'berlinattack': 6063, 'bestoftheday': 6064, 'bestsellers': 6065, 'betrays': 6066, 'beutiful': 6067, 'beverlywhalingreinstated': 6068, 'biracial': 6069, 'blackman': 6070, 'blueeyesbrowneyes': 6071, 'blues': 6072, 'bodyimage': 6073, 'boycotts': 6074, 'breathe': 6075, 'burka': 6076, 'butterfly': 6077, 'cafe': 6078, 'cambersands': 6079, 'canpoli': 6080, 'cantankerous': 6081, 'carlpaladinos': 6082, 'cast': 6083, 'cbc': 6084, 'cctv': 6085, 'celebrations': 6086, 'cept': 6087, 'championed': 6088, 'chanapa': 6089, 'changing': 6090, 'chauvinist': 6091, 'choked': 6092, 'chris': 6093, 'christia': 6094, 'classifying': 6095, 'clickbait': 6096, 'closely': 6097, 'closing': 6098, 'cm': 6099, 'coast': 6100, 'coldplaywembley': 6101, 'combover': 6102, 'comingsoon': 6103, 'comprehend': 6104, 'confirm': 6105, 'consideration': 6106, 'contact': 6107, 'coopting': 6108, 'cosplay': 6109, 'crackalogic': 6110, 'craft': 6111, 'crazed': 6112, 'creativity': 6113, 'cried': 6114, 'crimes': 6115, 'cringe': 6116, 'culpable': 6117, 'curry': 6118, 'cussed': 6119, 'daddys': 6120, 'dangerwith': 6121, 'danske': 6122, 'darn': 6123, 'deals': 6124, 'debacle': 6125, 'deltaairlines': 6126, 'demoralizing': 6127, 'denied': 6128, 'destiny': 6129, 'disaster': 6130, 'disc': 6131, 'disciples': 6132, 'discoved': 6133, 'dismantling': 6134, 'disneylooney': 6135, 'dissent': 6136, 'divider': 6137, 'dl': 6138, 'domestic': 6139, 'domestics': 6140, 'domnica': 6141, 'donaldtrumps': 6142, 'dublin': 6143, 'duca': 6144, 'dumbfuck': 6145, 'dzange': 6146, 'earlier': 6147, 'educationfest': 6148, 'effects': 6149, 'eggs': 6150, 'egregious': 6151, 'elected': 6152, 'ellicottsquarebuilding': 6153, 'emb': 6154, 'emotion': 6155, 'enable': 6156, 'encounters': 6157, 'endhate': 6158, 'endofcivilization': 6159, 'entrenched': 6160, 'erica': 6161, 'errand': 6162, 'euros2016': 6163, 'everydayheroes': 6164, 'expensive': 6165, 'expes': 6166, 'failed': 6167, 'fails': 6168, 'fakenewsale': 6169, 'fakequity': 6170, 'fargo': 6171, 'farm': 6172, 'feelinggood': 6173, 'fights': 6174, 'finale': 6175, 'finishing': 6176, 'flat': 6177, 'floor': 6178, 'fml': 6179, 'followed': 6180, 'follower': 6181, 'follows': 6182, 'fooh': 6183, 'forecasted': 6184, 'forecasts': 6185, 'freeman': 6186, 'frm': 6187, 'frog': 6188, 'funde': 6189, 'fuuuuuuuuuuuuck': 6190, 'gardening': 6191, 'gaza': 6192, 'gemini': 6193, 'genocidedefender': 6194, 'getreal': 6195, 'gfy': 6196, 'girles': 6197, 'glastonbury': 6198, 'glorious': 6199, 'golden': 6200, 'goodread': 6201, 'grief': 6202, 'gt10': 6203, 'gt4': 6204, 'gtfoh': 6205, 'hags': 6206, 'hairly': 6207, 'handle': 6208, 'hangout': 6209, 'hatefear': 6210, 'hc': 6211, 'heabreak': 6212, 'healthmen': 6213, 'herbalremedies': 6214, 'highlight': 6215, 'hillarylost': 6216, 'hipster': 6217, 'hmmmsounds': 6218, 'homosexuality': 6219, 'hoodiesup': 6220, 'horse': 6221, 'hostility': 6222, 'hotline': 6223, 'hr': 6224, 'hube': 6225, 'huffington': 6226, 'humphrey': 6227, 'ignoring': 6228, 'imo': 6229, 'implication': 6230, 'improve': 6231, 'in2017iwantto': 6232, 'incompitent': 6233, 'inevitable': 6234, 'infinite': 6235, 'influence': 6236, 'insident': 6237, 'inspiring': 6238, 'instilling': 6239, 'intercourse': 6240, 'intervene': 6241, 'itmaybe': 6242, 'itz': 6243, 'jacket': 6244, 'jewellery': 6245, 'jewishjoke': 6246, 'jind': 6247, 'jocox': 6248, 'jolly': 6249, 'juniors': 6250, 'karachi': 6251, 'keyboard': 6252, 'kit': 6253, 'klux': 6254, 'korean': 6255, 'las': 6256, 'latinas': 6257, 'laughable': 6258, 'lifeguards': 6259, 'lineup': 6260, 'lion': 6261, 'lip': 6262, 'loathsome': 6263, 'lockerroomtalk': 6264, 'lolol': 6265, 'longhair': 6266, 'lookin': 6267, 'lostpopularvote': 6268, 'loudly': 6269, 'lush': 6270, 'maga3x': 6271, 'malvern': 6272, 'manipur': 6273, 'manmade': 6274, 'marathon': 6275, 'marchampit': 6276, 'marco': 6277, 'mateen': 6278, 'matt': 6279, 'med': 6280, 'meets': 6281, 'mei': 6282, 'mentalillness': 6283, 'metal': 6284, 'midweek': 6285, 'misogony': 6286, 'missyou': 6287, 'mix': 6288, 'monsoon': 6289, 'mormonis': 6290, 'mothers': 6291, 'moviemad': 6292, 'musictherapy': 6293, 'muthafucka': 6294, 'myth': 6295, 'naming': 6296, 'narcissistic': 6297, 'natalie': 6298, 'naturism': 6299, 'nds': 6300, 'negativity': 6301, 'newmusic': 6302, 'nightout': 6303, 'nofur': 6304, 'norrisgreen': 6305, 'noyulin': 6306, 'nu': 6307, 'obsessed': 6308, 'oldwoman': 6309, 'opendemocracy': 6310, 'opened': 6311, 'ordinary': 6312, 'orientation': 6313, 'overrode': 6314, 'pageant': 6315, 'paint': 6316, 'pamelaramseytaylorpraised': 6317, 'parentage': 6318, 'passengers': 6319, 'payintheusa': 6320, 'peppa': 6321, 'perks': 6322, 'personal': 6323, 'peruvian': 6324, 'piggies': 6325, 'pitch': 6326, 'pizzagate': 6327, 'placed': 6328, 'plymouth': 6329, 'policeman': 6330, 'polishgirl': 6331, 'poweothewomen': 6332, 'pr': 6333, 'predictions': 6334, 'prep': 6335, 'presentation': 6336, 'presidenttrump': 6337, 'presiding': 6338, 'prevalent': 6339, 'preview': 6340, 'primed': 6341, 'print': 6342, 'prints': 6343, 'pro': 6344, 'progresverebel': 6345, 'psychopath': 6346, 'pulsenightclub': 6347, 'puntohost': 6348, 'purchased': 6349, 'racialequity': 6350, 'rad': 6351, 'rainy': 6352, 'randomly': 6353, 'recast': 6354, 'receive': 6355, 'reefers': 6356, 'reel': 6357, 'refers': 6358, 'reminds': 6359, 'remix': 6360, 'remote': 6361, 'repetitive': 6362, 'replacyclay': 6363, 'republicwhitepower': 6364, 'reunion': 6365, 'ridiculous': 6366, 'rightist': 6367, 'robbiewilliams': 6368, 'rolling': 6369, 'saga': 6370, 'saysomething': 6371, 'scars': 6372, 'scenes': 6373, 'scraps': 6374, 'screwed': 6375, 'scurvy': 6376, 'sewage': 6377, 'shitfest': 6378, 'shots': 6379, 'shrouded': 6380, 'sizescuts': 6381, 'sketch': 6382, 'skills': 6383, 'skincare': 6384, 'smaller': 6385, 'socks': 6386, 'soed': 6387, 'soooo': 6388, 'soreloser': 6389, 'sources': 6390, 'spa': 6391, 'speed': 6392, 'square': 6393, 'stadium': 6394, 'starek': 6395, 'starwars': 6396, 'starwarsrogueone': 6397, 'stateampfederal': 6398, 'steak': 6399, 'stopsexism': 6400, 'stoptalking': 6401, 'stupids': 6402, 'substantive': 6403, 'sue': 6404, 'suffer': 6405, 'superb': 6406, 'surprises': 6407, 'sweetie': 6408, 'swing': 6409, 'swingtanzenverboten': 6410, 'symbols': 6411, 'sympathized': 6412, 'talks': 6413, 'tattler': 6414, 'tedinstitute': 6415, 'tefugeeswelcome': 6416, 'tenacity': 6417, 'termed': 6418, 'terrorize': 6419, 'tests': 6420, 'theconjuring2': 6421, 'throwing': 6422, 'thugs': 6423, 'tiger': 6424, 'toonsthemed': 6425, 'toys': 6426, 'trails': 6427, 'tree': 6428, 'trek': 6429, 'tremendous': 6430, 'trolling': 6431, 'tweeting': 6432, 'twins': 6433, 'twitterbot': 6434, 'unchained': 6435, 'uncov': 6436, 'unfit': 6437, 'upload': 6438, 'urdu': 6439, 'urging': 6440, 'users': 6441, 'vedic': 6442, 'vendor': 6443, 'verizon': 6444, 'vid': 6445, 'villy': 6446, 'visiting': 6447, 'vital': 6448, 'vlog': 6449, 'vogue': 6450, 'vulgar': 6451, 'wakeuppeopl3': 6452, 'warcraft': 6453, 'warsforoil': 6454, 'watc': 6455, 'wembley': 6456, 'whining': 6457, 'whitefragility': 6458, 'whiteman': 6459, 'whiteslavers': 6460, 'whiteslavery': 6461, 'whitey': 6462, 'williams': 6463, 'wiunion': 6464, 'womandegrading': 6465, 'woo': 6466, 'woot': 6467, 'wotching': 6468, 'ww': 6469, 'xboxe3': 6470, 'yiannopoulos': 6471, 'yoy': 6472, 'zelda': 6473, 'zone': 6474, '10k': 6475, '123': 6476, '12mill': 6477, '13479': 6478, '201': 6479, '20th': 6480, '26th': 6481, '28': 6482, '2nites': 6483, '4maps': 6484, '53': 6485, '60': 6486, '6yearolds': 6487, '9th': 6488, 'abandoned': 6489, 'abeed': 6490, 'aberrant': 6491, 'abundance': 6492, 'accessories': 6493, 'active': 6494, 'adding': 6495, 'advance': 6496, 'advantage': 6497, 'affiliated': 6498, 'ageoftrump': 6499, 'ahhhh': 6500, 'aists': 6501, 'aleksandr': 6502, 'allbc': 6503, 'allgrownup': 6504, 'amanda': 6505, 'ampfunny': 6506, 'anacoes': 6507, 'anf': 6508, 'antiegalitarianism': 6509, 'antisemetic': 6510, 'antiwhites': 6511, 'archangels': 6512, 'areclueless': 6513, 'armenian': 6514, 'army': 6515, 'arnt': 6516, 'arrests60mill': 6517, 'assets': 6518, 'asslicker': 6519, 'astrology': 6520, 'atm': 6521, 'attractive': 6522, 'audusd': 6523, 'augusta': 6524, 'authority': 6525, 'avitar': 6526, 'babygirl': 6527, 'badal': 6528, 'barely': 6529, 'bedifferent': 6530, 'beginner': 6531, 'bellyup': 6532, 'berating': 6533, 'beside': 6534, 'bestof2016': 6535, 'betrayed': 6536, 'bite': 6537, 'blacksagainsttrump': 6538, 'blackvotehispanicvote': 6539, 'blessampkeep': 6540, 'blessedt': 6541, 'blessing': 6542, 'bones': 6543, 'booking': 6544, 'bookpublishing': 6545, 'bottles': 6546, 'bottom': 6547, 'bound': 6548, 'bozzoli': 6549, 'brag': 6550, 'breath': 6551, 'bringiton': 6552, 'brithday': 6553, 'brussels': 6554, 'bt': 6555, 'burbank': 6556, 'cali': 6557, 'camera': 6558, 'cardiff': 6559, 'cared': 6560, 'carlpalidino': 6561, 'carlpalladino': 6562, 'catcalled': 6563, 'cd': 6564, 'cedm': 6565, 'celebrated': 6566, 'cent': 6567, 'central': 6568, 'champagne': 6569, 'cheeky': 6570, 'chess': 6571, 'chile': 6572, 'chinguas': 6573, 'citizenaction': 6574, 'citizenry': 6575, 'citys': 6576, 'classifieds': 6577, 'coaching': 6578, 'colorful': 6579, 'communismislam': 6580, 'competitiveness': 6581, 'completed': 6582, 'condoning': 6583, 'connected': 6584, 'connection': 6585, 'consider': 6586, 'constrained': 6587, 'consumers': 6588, 'continuous': 6589, 'cora': 6590, 'cornered': 6591, 'corps': 6592, 'correlation': 6593, 'cos': 6594, 'croatia': 6595, 'cruise': 6596, 'cuddle': 6597, 'dadsavailable': 6598, 'daisy': 6599, 'dates': 6600, 'dayoff': 6601, 'deadly': 6602, 'decor': 6603, 'defended': 6604, 'defined': 6605, 'degrees': 6606, 'dese': 6607, 'desse': 6608, 'devil': 6609, 'dey': 6610, 'diagram': 6611, 'dictate': 6612, 'diddo': 6613, 'diisgusted': 6614, 'disappointment': 6615, 'disdain': 6616, 'distance': 6617, 'distinct': 6618, 'doctrine': 6619, 'doggy': 6620, 'dogsarejoy': 6621, 'doublestan': 6622, 'dreamcatcher': 6623, 'dropping': 6624, 'drove': 6625, 'ducks': 6626, 'dugin': 6627, 'dust': 6628, 'economicsanctions': 6629, 'edit': 6630, 'embodied': 6631, 'embraceyourself': 6632, 'emergency': 6633, 'emmastone': 6634, 'engrus': 6635, 'enjoyed': 6636, 'epcot': 6637, 'era': 6638, 'eurgbp': 6639, 'euroconservatism': 6640, 'excite': 6641, 'exhibition': 6642, 'existed': 6643, 'existing': 6644, 'explained': 6645, 'explore': 6646, 'expressed': 6647, 'fairytail': 6648, 'fantasy': 6649, 'faraz': 6650, 'farmers': 6651, 'fate': 6652, 'fault': 6653, 'featured': 6654, 'feb': 6655, 'fightback': 6656, 'firstfamily': 6657, 'flt': 6658, 'footballbut': 6659, 'founder': 6660, 'frances': 6661, 'fridays': 6662, 'gain': 6663, 'galib': 6664, 'garage': 6665, 'garbagepailkids': 6666, 'gates': 6667, 'gator': 6668, 'gators': 6669, 'genealogy': 6670, 'germanys': 6671, 'ghost': 6672, 'goofy': 6673, 'gopro': 6674, 'gr8': 6675, 'grabyourwallet': 6676, 'graduating': 6677, 'granted': 6678, 'grassrootsaction': 6679, 'greatday': 6680, 'gross': 6681, 'guidance': 6682, 'gunviolence': 6683, 'habits': 6684, 'happynewyear2017': 6685, 'harassers': 6686, 'harry': 6687, 'harsh': 6688, 'heed': 6689, 'heeded': 6690, 'heritage': 6691, 'highereducation': 6692, 'hilton': 6693, 'himselffor': 6694, 'homeopathic': 6695, 'honor': 6696, 'horizonalways': 6697, 'hottweets': 6698, 'humiliating': 6699, 'humorous': 6700, 'hustle': 6701, 'hyped': 6702, 'icons': 6703, 'ideal': 6704, 'in': 6705, 'incl': 6706, 'independence': 6707, 'inexistence': 6708, 'informing': 6709, 'inspirechange': 6710, 'instagay': 6711, 'instasize': 6712, 'internalize': 6713, 'iqbal': 6714, 'irl': 6715, 'irrefutable': 6716, 'isr': 6717, 'jacobmarley': 6718, 'jarodtaylor': 6719, 'jerk': 6720, 'ji': 6721, 'jibes': 6722, 'judd': 6723, 'justin': 6724, 'justjustice': 6725, 'ka': 6726, 'katiesikphoto': 6727, 'kelly': 6728, 'kept': 6729, 'kin': 6730, 'kits': 6731, 'label': 6732, 'large': 6733, 'largelyunrepentant': 6734, 'larger': 6735, 'launches': 6736, 'lawyer': 6737, 'leak': 6738, 'lean': 6739, 'leavers': 6740, 'leftard': 6741, 'legitimizing': 6742, 'lehman': 6743, 'lemans24': 6744, 'libtards': 6745, 'limited': 6746, 'lips': 6747, 'lithuania': 6748, 'livelypics': 6749, 'locks': 6750, 'lovebeingalegend': 6751, 'loveher': 6752, 'loveisland': 6753, 'loveknow': 6754, 'lovinglife': 6755, 'lowest': 6756, 'lt33': 6757, 'lunchtime': 6758, 'mam': 6759, 'managed': 6760, 'manga': 6761, 'mannequinchallenge': 6762, 'masjids': 6763, 'matthew': 6764, 'meat': 6765, 'megaupload': 6766, 'mentioning': 6767, 'merge': 6768, 'mess': 6769, 'mgt': 6770, 'miloyiannopoulos': 6771, 'mindful': 6772, 'misconceptions': 6773, 'mm': 6774, 'mock': 6775, 'models': 6776, 'mohsin': 6777, 'moi': 6778, 'mondaymorning': 6779, 'moneyovermorals': 6780, 'mongering': 6781, 'monthly': 6782, 'motivational': 6783, 'mountain': 6784, 'movements': 6785, 'mug': 6786, 'musicvideo': 6787, 'muzzies': 6788, 'nap': 6789, 'narcissism': 6790, 'natt': 6791, 'neo': 6792, 'neverforget': 6793, 'newlook': 6794, 'newyorker': 6795, 'noborders': 6796, 'nofear': 6797, 'nohern': 6798, 'nosurprise': 6799, 'notmypresidenttheresista': 6800, 'november': 6801, 'nowi': 6802, 'oakland': 6803, 'odds': 6804, 'officebearers': 6805, 'opposite': 6806, 'ordination': 6807, 'outta': 6808, 'owner': 6809, 'padded': 6810, 'paies': 6811, 'pair': 6812, 'panda': 6813, 'peacein2017': 6814, 'penis': 6815, 'personally': 6816, 'pharrell': 6817, 'phase': 6818, 'philippines': 6819, 'photographs': 6820, 'pieces': 6821, 'placement': 6822, 'planned': 6823, 'pokemon': 6824, 'policecivilian': 6825, 'politicalcorrectness': 6826, 'polygamist': 6827, 'pops': 6828, 'possibly': 6829, 'prayfoheworld': 6830, 'predator': 6831, 'pregnancy': 6832, 'premier': 6833, 'preparation': 6834, 'presidential': 6835, 'pretending': 6836, 'primary': 6837, 'procreate': 6838, 'producer': 6839, 'profiled': 6840, 'prom': 6841, 'promotion': 6842, 'propose': 6843, 'psychology': 6844, 'publicpolicy': 6845, 'publicserviceannouncement': 6846, 'pulseshooting': 6847, 'punks': 6848, 'puppies': 6849, 'qualifications': 6850, 'queens': 6851, 'raceonly': 6852, 'raghuramrajan': 6853, 'ramadhan': 6854, 'rarethous': 6855, 'rate': 6856, 'rats': 6857, 'rbc': 6858, 'reali': 6859, 'rebeccas': 6860, 'receiving': 6861, 'recognition': 6862, 'reflected': 6863, 'reggae': 6864, 'regime': 6865, 'reinventimpossible': 6866, 'rejects': 6867, 'relations': 6868, 'relaxation': 6869, 'respected': 6870, 'resultant': 6871, 'rg': 6872, 'ridiculously': 6873, 'rifle': 6874, 'rings': 6875, 'river': 6876, 'rob': 6877, 'roughly': 6878, 'roughwaters': 6879, 'runner': 6880, 'russiagate': 6881, 'saddest': 6882, 'sakesstop': 6883, 'salute': 6884, 'sanfrancisco': 6885, 'satire': 6886, 'scatter': 6887, 'scientists': 6888, 'sculpted': 6889, 'seaside': 6890, 'security': 6891, 'selfish': 6892, 'serving': 6893, 'seven': 6894, 'sewer': 6895, 'sf': 6896, 'shameless': 6897, 'shark': 6898, 'shuts': 6899, 'sigh': 6900, 'signing': 6901, 'sketches': 6902, 'skinheads': 6903, 'smith': 6904, 'snatch': 6905, 'socialist': 6906, 'sofa': 6907, 'solved': 6908, 'spaces': 6909, 'standard': 6910, 'stays': 6911, 'stepmom': 6912, 'stereotyping': 6913, 'stocks': 6914, 'stranger': 6915, 'strident': 6916, 'styles': 6917, 'substantially': 6918, 'suffrage': 6919, 'sundays': 6920, 'sunnyday': 6921, 'sunrise': 6922, 'supremacists': 6923, 'surly': 6924, 'surrogate': 6925, 'sway': 6926, 'sweety': 6927, 'tacky': 6928, 'tagsforlikes': 6929, 'tainting': 6930, 'talked': 6931, 'tall': 6932, 'taylor': 6933, 'teaser': 6934, 'techno': 6935, 'tellall': 6936, 'terming': 6937, 'texts': 6938, 'thesexismproject': 6939, 'thinkbigsundaywithmarsha': 6940, 'thrilled': 6941, 'throat': 6942, 'thwaed': 6943, 'tick': 6944, 'tom': 6945, 'tower': 6946, 'triggerwarning': 6947, 'troubles': 6948, 'truck': 6949, 'truckload': 6950, 'truism': 6951, 'trumpinauguration': 6952, 'tunes': 6953, 'tupac': 6954, 'tyruswong': 6955, 'ultranationalism': 6956, 'unfounate': 6957, 'uni': 6958, 'unique': 6959, 'unite': 6960, 'unpatriotic': 6961, 'unreal': 6962, 'unwanted': 6963, 'ups': 6964, 'usshootingsaccidentsabuse': 6965, 'vividsydney': 6966, 'voicing': 6967, 'vomit': 6968, 'votersuppression': 6969, 'vp': 6970, 'waime': 6971, 'wakes': 6972, 'warcraftmovie': 6973, 'warmonger': 6974, 'wasi': 6975, 'wellthese': 6976, 'weneed': 6977, 'wew': 6978, 'wig': 6979, 'witho': 6980, 'womensordination': 6981, 'wooden': 6982, 'worlddanger': 6983, 'year8': 6984, 'yelling': 6985, 'yogi': 6986, 'yong': 6987, '01': 6988, '100k': 6989, '1306': 6990, '1600': 6991, '18th': 6992, '1960': 6993, '2012': 6994, '2016election': 6995, '2016highlights': 6996, '2016i': 6997, '2016release': 6998, '2017fail': 6999, '299': 7000, '2day': 7001, '3': 7002, '4': 7003, '400000': 7004, '8990': 7005, '8th': 7006, '93': 7007, 'ability': 7008, 'ace': 7009, 'admit': 7010, 'adrenaline': 7011, 'affect': 7012, 'ages': 7013, 'alumni': 7014, 'amreading': 7015, 'andysessions': 7016, 'annihilated': 7017, 'appletstag': 7018, 'aquifer': 7019, 'archives': 7020, 'ascot': 7021, 'askingoh': 7022, 'associates': 7023, 'ating': 7024, 'attending': 7025, 'audience': 7026, 'awaiting': 7027, 'ba': 7028, 'baiting': 7029, 'bali': 7030, 'bangkok': 7031, 'barry': 7032, 'bashful': 7033, 'bathroom': 7034, 'beats': 7035, 'became': 7036, 'beers': 7037, 'beings': 7038, 'belgium': 7039, 'bentley': 7040, 'beyou': 7041, 'bk': 7042, 'blacklistin': 7043, 'bleed': 7044, 'blueeyes': 7045, 'bml': 7046, 'boating': 7047, 'boil': 7048, 'bondage': 7049, 'bonding': 7050, 'boost': 7051, 'boycotttigerwoods': 7052, 'bp': 7053, 'bracelets': 7054, 'brazil': 7055, 'bridetobe': 7056, 'bristol': 7057, 'brooklyn': 7058, 'browns': 7059, 'brutal': 7060, 'bubble': 7061, 'bugg': 7062, 'buildings': 7063, 'built': 7064, 'bunny': 7065, 'burger': 7066, 'burnt': 7067, 'canadiangp': 7068, 'cancelled': 7069, 'candid': 7070, 'candles': 7071, 'capitalist': 7072, 'caption': 7073, 'casting': 7074, 'catching': 7075, 'catsoftwitter': 7076, 'cereal': 7077, 'ceremony': 7078, 'champions': 7079, 'cheering': 7080, 'chihuahua': 7081, 'citation': 7082, 'classifies': 7083, 'closest': 7084, 'cloud': 7085, 'clue': 7086, 'cnv': 7087, 'coffin': 7088, 'cognizant': 7089, 'colonisation': 7090, 'colourism': 7091, 'comfy': 7092, 'commutations': 7093, 'complementarian': 7094, 'complementarianism': 7095, 'conaist': 7096, 'condescending': 7097, 'connect': 7098, 'considered': 7099, 'conspiracy': 7100, 'consumer': 7101, 'contain': 7102, 'contest': 7103, 'coolestlifehack': 7104, 'cope': 7105, 'cpi': 7106, 'crafts': 7107, 'creature': 7108, 'cress': 7109, 'cricket': 7110, 'critiquing': 7111, 'cuckold': 7112, 'cuties': 7113, 'dallas': 7114, 'damning': 7115, 'dan': 7116, 'dancer': 7117, 'demeaning': 7118, 'designer': 7119, 'destigmatize': 7120, 'destination': 7121, 'digital': 7122, 'dinosaurs': 7123, 'dissapointed': 7124, 'distinguished': 7125, 'dividing': 7126, 'dkpol': 7127, 'dogsofinstagram': 7128, 'doin': 7129, 'donuldtrump': 7130, 'dope': 7131, 'dordarshan': 7132, 'downwiththetriarchy': 7133, 'dragged': 7134, 'dragon': 7135, 'dressed': 7136, 'drfranciscresswelsing': 7137, 'dropped': 7138, 'duh': 7139, 'dull': 7140, 'editing': 7141, 'editorial': 7142, 'eek': 7143, 'elitists': 7144, 'elses': 7145, 'emtec': 7146, 'enjoylife': 7147, 'evolutions': 7148, 'factsguide': 7149, 'fascinating': 7150, 'fashionblogger': 7151, 'favorites': 7152, 'feliz2017': 7153, 'fest': 7154, 'fetish': 7155, 'filming': 7156, 'fishing': 7157, 'fitbit': 7158, 'fostercare': 7159, 'frank': 7160, 'freedownload': 7161, 'freemasons': 7162, 'frustrating': 7163, 'fucku2016': 7164, 'gallery': 7165, 'gamergirl': 7166, 'gaypride': 7167, 'ghana': 7168, 'ghostbusters': 7169, 'girlpower': 7170, 'giveaway': 7171, 'gladshesgone': 7172, 'glastofest': 7173, 'glow': 7174, 'gm': 7175, 'goin': 7176, 'grad': 7177, 'grandsons': 7178, 'greens': 7179, 'gs': 7180, 'gt2': 7181, 'guarani': 7182, 'guessed': 7183, 'guests': 7184, 'hall': 7185, 'happ': 7186, 'harmony': 7187, 'hav': 7188, 'headache': 7189, 'heavy': 7190, 'highlights': 7191, 'highly': 7192, 'highs': 7193, 'hiking': 7194, 'himhe': 7195, 'historically': 7196, 'holier': 7197, 'homesweethome': 7198, 'homework': 7199, 'honey': 7200, 'honored': 7201, 'hook': 7202, 'however': 7203, 'humping': 7204, 'hurry': 7205, 'idiocracy': 7206, 'iftar': 7207, 'illegalsettlements': 7208, 'iloveyou': 7209, 'immanuel': 7210, 'indonesia': 7211, 'inferioritycomplex': 7212, 'inked': 7213, 'interviews': 7214, 'ios10': 7215, 'islamaphobic': 7216, 'jack': 7217, 'jackass': 7218, 'jazz': 7219, 'jeremyjoseph': 7220, 'jon': 7221, 'judea': 7222, 'jumping': 7223, 'juneteenth': 7224, 'jus': 7225, 'kanye': 7226, 'kathy': 7227, 'kejriwal': 7228, 'kellyann': 7229, 'keynote': 7230, 'kim': 7231, 'km': 7232, 'kscrashcorrectors': 7233, 'lab': 7234, 'ladyboy': 7235, 'landing': 7236, 'landscape': 7237, 'lane': 7238, 'lang': 7239, 'largely': 7240, 'lay': 7241, 'leaked': 7242, 'leb': 7243, 'led': 7244, 'libey': 7245, 'lightroom': 7246, 'lily': 7247, 'linguistics': 7248, 'livelife': 7249, 'loneliness': 7250, 'lovemyjob': 7251, 'macbook': 7252, 'maimane': 7253, 'makeuptransformation': 7254, 'manypols': 7255, 'mario': 7256, 'mat': 7257, 'mcdaniel': 7258, 'mcfadden': 7259, 'measured': 7260, 'mega': 7261, 'menu': 7262, 'messengerjust': 7263, 'messy': 7264, 'mf': 7265, 'mickey': 7266, 'mindsconsole': 7267, 'mistakes': 7268, 'modi': 7269, 'momtips': 7270, 'mornings': 7271, 'motorcycle': 7272, 'mount': 7273, 'mouse': 7274, 'municipalities': 7275, 'musical': 7276, 'mydubai': 7277, 'naboomspruit': 7278, 'nafris': 7279, 'nationalbestfriendday': 7280, 'nationdivider': 7281, 'navy': 7282, 'nd': 7283, 'ndtv': 7284, 'netherlands': 7285, 'newcastle': 7286, 'newstar': 7287, 'newyorkcity': 7288, 'nicole': 7289, 'nigelfarage': 7290, 'noballs': 7291, 'nobdy': 7292, 'nohope': 7293, 'nomandate': 7294, 'nomnom': 7295, 'nottingham': 7296, 'numb': 7297, 'obamalegacy': 7298, 'obstacles': 7299, 'oppounities': 7300, 'oppressed': 7301, 'oppressive': 7302, 'opt': 7303, 'optimistic': 7304, 'orgasmic': 7305, 'orlandohorror': 7306, 'outdoor': 7307, 'overwatch': 7308, 'overweight': 7309, 'owners': 7310, 'pages': 7311, 'pakis': 7312, 'pap': 7313, 'papalbull': 7314, 'parent': 7315, 'parking': 7316, 'peacekeepers': 7317, 'pedophile': 7318, 'pelosi': 7319, 'peru': 7320, 'picking': 7321, 'plant': 7322, 'polygeny': 7323, 'potato': 7324, 'pougal': 7325, 'pounds': 7326, 'pout': 7327, 'prayersfororlando': 7328, 'pretend': 7329, 'prime': 7330, 'problemofwhiteness': 7331, 'projects': 7332, 'proverb': 7333, 'pseudoscience': 7334, 'pussygrabberinchief': 7335, 'quack': 7336, 'quantities': 7337, 'queenat90': 7338, 'quotas': 7339, 'rabid': 7340, 'raciolinguistics': 7341, 'rains': 7342, 'raised': 7343, 'rampant': 7344, 'recognize': 7345, 'recommend': 7346, 'reddit': 7347, 'redhead': 7348, 'reginald': 7349, 'regrann': 7350, 'relate': 7351, 'relates': 7352, 'relied': 7353, 'religious': 7354, 'remainers': 7355, 'remind': 7356, 'restinpeace': 7357, 'restricted': 7358, 'returningcitizens': 7359, 'rome': 7360, 'royals': 7361, 'russianhack': 7362, 'sacrifice': 7363, 'sake': 7364, 'samachar': 7365, 'samaria': 7366, 'samples': 7367, 'samwu': 7368, 'saturdaynight': 7369, 'saturdays': 7370, 'sauce': 7371, 'scotiabank': 7372, 'script': 7373, 'sdlive': 7374, 'sdob': 7375, 'se': 7376, 'searching': 7377, 'seek': 7378, 'seeklearning': 7379, 'selena': 7380, 'selfietime': 7381, 'selfporait': 7382, 'shared': 7383, 'sharks': 7384, 'shattered': 7385, 'shopalyssas': 7386, 'shou': 7387, 'showcase': 7388, 'singapore': 7389, 'skywilliams': 7390, 'slide': 7391, 'slit': 7392, 'smackdown': 7393, 'smacks': 7394, 'smug': 7395, 'sneezy': 7396, 'socialmooc': 7397, 'speechless': 7398, 'spinning': 7399, 'split': 7400, 'spoiled': 7401, 'spoilers': 7402, 'spoilt': 7403, 'spotify': 7404, 'spotlight': 7405, 'squirrel': 7406, 'stable': 7407, 'stafresh': 7408, 'stanley': 7409, 'steph': 7410, 'stephen': 7411, 'straght': 7412, 'streets': 7413, 'suddenly': 7414, 'sukhbir': 7415, 'sup': 7416, 'superior': 7417, 'sweat': 7418, 'swift': 7419, 'syfy': 7420, 'tattoosleeves': 7421, 'teamd': 7422, 'teams': 7423, 'teamwork': 7424, 'technology': 7425, 'temecula': 7426, 'tent': 7427, 'terminology': 7428, 'terrorattack': 7429, 'thatblacklivesmatter': 7430, 'thatzille': 7431, 'theater': 7432, 'thebest': 7433, 'thefuture': 7434, 'thefutureisfemale': 7435, 'theresamay': 7436, 'thiss': 7437, 'thrones': 7438, 'tight': 7439, 'timenow': 7440, 'toast': 7441, 'tommorow': 7442, 'tonyawards': 7443, 'tory': 7444, 'tossing': 7445, 'toured': 7446, 'trading': 7447, 'trafficking': 7448, 'tragedies': 7449, 'trail': 7450, 'trample': 7451, 'travels': 7452, 'treasure': 7453, 'treatment': 7454, 'trend': 7455, 'trends': 7456, 'tribute': 7457, 'truestory': 7458, 'trump2016': 7459, 'trumpocalypse': 7460, 'truths': 7461, 'tuesdaymotivation': 7462, 'underlying': 7463, 'underway': 7464, 'unitedstateschampion': 7465, 'universityofwisconsinmadison': 7466, 'unknown': 7467, 'unwell': 7468, 'updated': 7469, 'upside': 7470, 'uschampion': 7471, 'vancouver': 7472, 'venice': 7473, 'veto': 7474, 'viewed': 7475, 'viewing': 7476, 'vinyl': 7477, 'vinyls': 7478, 'violently': 7479, 'votes': 7480, 'vr': 7481, 'watermelon': 7482, 'weed': 7483, 'weekly': 7484, 'wehate': 7485, 'welleducated': 7486, 'welsing': 7487, 'wenger': 7488, 'whic': 7489, 'whiskey': 7490, 'whiteperson': 7491, 'whiteskinned': 7492, 'william': 7493, 'wilmington': 7494, 'winners': 7495, 'womensmarch': 7496, 'wood': 7497, 'worldoceansday': 7498, 'wrap': 7499, 'wsaying': 7500, 'wwdc': 7501, 'xboxone': 7502, 'xenopbobia': 7503, 'yehtut': 7504, 'yiannopoulosis': 7505, 'younger': 7506, 'zeenews': 7507, 'zoo': 7508, '02': 7509, '10th': 7510, '15th': 7511, '17th': 7512, '2013': 7513, '29': 7514, '2pac': 7515, '42': 7516, '564943': 7517, '6417153640': 7518, '80': 7519, 'a': 7520, 'acoustic': 7521, 'actorslife': 7522, 'addiction': 7523, 'afford': 7524, 'afrofuturism': 7525, 'ah': 7526, 'ahhh': 7527, 'ahole': 7528, 'allah': 7529, 'alligators': 7530, 'alongside': 7531, 'anarchism': 7532, 'anthems': 7533, 'antiaging': 7534, 'antijewish': 7535, 'apologists': 7536, 'apostolis': 7537, 'appeared': 7538, 'apply': 7539, 'appointment': 7540, 'appreciated': 7541, 'appreciating': 7542, 'appropriate': 7543, 'approved': 7544, 'arriving': 7545, 'arsenal': 7546, 'asleep': 7547, 'astonished': 7548, 'atmosphere': 7549, 'attraction': 7550, 'aunt': 7551, 'autumn': 7552, 'avengers': 7553, 'awareness': 7554, 'awww': 7555, 'babys': 7556, 'backed': 7557, 'backtoschool': 7558, 'backyard': 7559, 'badger': 7560, 'balanced': 7561, 'balloons': 7562, 'bankrupt': 7563, 'battlefield': 7564, 'bay': 7565, 'baylor': 7566, 'beachlife': 7567, 'beg': 7568, 'believing': 7569, 'bell': 7570, 'beloved': 7571, 'benidorm': 7572, 'berniesanders': 7573, 'bffs': 7574, 'billy': 7575, 'biopolitics': 7576, 'blanket': 7577, 'blocks': 7578, 'blow': 7579, 'bombs': 7580, 'bond': 7581, 'booty': 7582, 'bother': 7583, 'bowling': 7584, 'brainwashed': 7585, 'breakingnews': 7586, 'brochure': 7587, 'brokenquotes': 7588, 'bron': 7589, 'bros': 7590, 'bubbly': 7591, 'bump': 7592, 'burgers': 7593, 'burst': 7594, 'cage': 7595, 'callin': 7596, 'camden': 7597, 'canadianvalues': 7598, 'capitalstb': 7599, 'captain': 7600, 'caring': 7601, 'carnival': 7602, 'carry': 7603, 'carrying': 7604, 'cases': 7605, 'casual': 7606, 'catdjt': 7607, 'cheating': 7608, 'chelsea': 7609, 'cherry': 7610, 'childrenofcolor': 7611, 'chillin': 7612, 'chip': 7613, 'chips': 7614, 'chronicpain': 7615, 'cliches': 7616, 'coalitionist': 7617, 'cockwomble': 7618, 'coconut': 7619, 'coffe': 7620, 'collage': 7621, 'colleagues': 7622, 'com': 7623, 'combo': 7624, 'communitystandards': 7625, 'commute': 7626, 'companies': 7627, 'compare': 7628, 'compensatory': 7629, 'compilation': 7630, 'conditioning': 7631, 'constitutional': 7632, 'corner': 7633, 'coybig': 7634, 'crash': 7635, 'creator': 7636, 'crossed': 7637, 'custom': 7638, 'cuteness': 7639, 'cyprus': 7640, 'daddysgirl': 7641, 'dairy': 7642, 'daniel': 7643, 'darling': 7644, 'dave': 7645, 'deceive': 7646, 'decent': 7647, 'decisions': 7648, 'dedicated': 7649, 'del': 7650, 'deleteyouraccount': 7651, 'deliver': 7652, 'denim': 7653, 'depth': 7654, 'determination': 7655, 'determined': 7656, 'dev': 7657, 'disabled': 7658, 'discover': 7659, 'display': 7660, 'distress': 7661, 'disturbing': 7662, 'divorce': 7663, 'donthecon': 7664, 'dorm': 7665, 'dorset': 7666, 'doubletree': 7667, 'douchewaffle': 7668, 'drumpf': 7669, 'earn': 7670, 'earned': 7671, 'ecstatic': 7672, 'edition': 7673, 'eevalancaster': 7674, 'eight': 7675, 'elegant': 7676, 'elephants': 7677, 'employment': 7678, 'endorsement': 7679, 'engineer': 7680, 'environmentalist': 7681, 'ergo': 7682, 'esp': 7683, 'eva': 7684, 'exceptional': 7685, 'excess': 7686, 'exci': 7687, 'experiencing': 7688, 'expressing': 7689, 'extraordinaire': 7690, 'eyed': 7691, 'factor': 7692, 'fallen': 7693, 'falseequavalency': 7694, 'falsenarrative': 7695, 'familyfun': 7696, 'fancy': 7697, 'farewell': 7698, 'fascismo': 7699, 'fashionillustration': 7700, 'fatherday': 7701, 'fauxnews': 7702, 'feat': 7703, 'feelin': 7704, 'feelthebern': 7705, 'ffs': 7706, 'field': 7707, 'fifa17': 7708, 'fighter': 7709, 'fightlikeagirl': 7710, 'fingers': 7711, 'fingerscrossed': 7712, 'fitnessaddict': 7713, 'flash': 7714, 'flashbackfriday': 7715, 'flcrashcorrectors': 7716, 'flies': 7717, 'flippant': 7718, 'florence': 7719, 'flow': 7720, 'focused': 7721, 'followback': 7722, 'forgiveness': 7723, 'forgotten': 7724, 'fotiadis': 7725, 'founate': 7726, 'frat': 7727, 'freespirit': 7728, 'fridayfun': 7729, 'fridaynight': 7730, 'frozen': 7731, 'fuckin': 7732, 'fundamentalism': 7733, 'funtimes': 7734, 'gained': 7735, 'gamers': 7736, 'gardens': 7737, 'gary': 7738, 'gear': 7739, 'gentle': 7740, 'getaway': 7741, 'girltime': 7742, 'girly': 7743, 'glamping': 7744, 'glowing': 7745, 'glutenfree': 7746, 'goa': 7747, 'goat': 7748, 'goodbook': 7749, 'goodies': 7750, 'goodtime': 7751, 'gordie': 7752, 'gosh': 7753, 'grandparents': 7754, 'gray': 7755, 'growup': 7756, 'grumpycat': 7757, 'grunge': 7758, 'gt15': 7759, 'gud': 7760, 'guncontrolnow': 7761, 'gypsy': 7762, 'haircut': 7763, 'harvest': 7764, 'hats': 7765, 'hbo': 7766, 'heads': 7767, 'healthandfitness': 7768, 'healthylife': 7769, 'herbalife': 7770, 'heros': 7771, 'hijab': 7772, 'hippie': 7773, 'holly': 7774, 'homosex': 7775, 'hormone': 7776, 'huing': 7777, 'hulk': 7778, 'hurryup': 7779, 'ibiza2016': 7780, 'ignores': 7781, 'illinois': 7782, 'incineration': 7783, 'includes': 7784, 'inconsequen': 7785, 'innovation': 7786, 'insights': 7787, 'insta': 7788, 'instaaoftheday': 7789, 'instafollow': 7790, 'instamoment': 7791, 'institution': 7792, 'insure': 7793, 'intend': 7794, 'interiordesign': 7795, 'internetmarketing': 7796, 'intersectional': 7797, 'introduction': 7798, 'investment': 7799, 'invite': 7800, 'ion': 7801, 'ipad': 7802, 'iswar': 7803, 'items': 7804, 'jam': 7805, 'jamaica': 7806, 'jetlagged': 7807, 'johnny': 7808, 'joining': 7809, 'jp': 7810, 'jr': 7811, 'junior': 7812, 'kh': 7813, 'kharkiv': 7814, 'kharkivgram': 7815, 'kidding': 7816, 'killings': 7817, 'kings': 7818, 'knicks': 7819, 'knocked': 7820, 'kylie': 7821, 'lasses': 7822, 'latergram': 7823, 'laura': 7824, 'lb': 7825, 'legendary': 7826, 'lessismore': 7827, 'lifted': 7828, 'lincoln': 7829, 'lingerie': 7830, 'lived': 7831, 'livemusic': 7832, 'liye': 7833, 'loads': 7834, 'looki': 7835, 'loosing': 7836, 'lounge': 7837, 'lovequotes': 7838, 'lovewins': 7839, 'lucy': 7840, 'lyon': 7841, 'magicrealism': 7842, 'maintain': 7843, 'mallorca': 7844, 'managing': 7845, 'marbs': 7846, 'marseille': 7847, 'mask': 7848, 'masters': 7849, 'matches': 7850, 'max': 7851, 'meetings': 7852, 'megyn': 7853, 'mid': 7854, 'miller': 7855, 'minneapolis': 7856, 'miscegenation': 7857, 'mission': 7858, 'mizzou': 7859, 'modeling': 7860, 'modelling': 7861, 'momma': 7862, 'monthsary': 7863, 'montreal': 7864, 'mounts': 7865, 'movingon': 7866, 'muhammadali': 7867, 'mygirl': 7868, 'myhappyplace': 7869, 'nah': 7870, 'nailed': 7871, 'nano': 7872, 'nationala': 7873, 'natsu': 7874, 'netanyahou': 7875, 'netanyahuspeech': 7876, 'networking': 7877, 'newproject': 7878, 'news24': 7879, 'nightmare': 7880, 'ninja': 7881, 'nite': 7882, 'nj': 7883, 'no1': 7884, 'notes': 7885, 'nothappy': 7886, 'nurses': 7887, 'obsession': 7888, 'occupiers': 7889, 'ohhhh': 7890, 'oj': 7891, 'older': 7892, 'oldest': 7893, 'olds': 7894, 'ole': 7895, 'omar': 7896, 'ooh': 7897, 'opens': 7898, 'oppressorsprivileged': 7899, 'optimism': 7900, 'oregon': 7901, 'overwhelmed': 7902, 'owls': 7903, 'package': 7904, 'pakistan': 7905, 'palestinians': 7906, 'paners': 7907, 'passpo': 7908, 'patiently': 7909, 'patio': 7910, 'patriarchal': 7911, 'paytime': 7912, 'peanut': 7913, 'pearl': 7914, 'perfectly': 7915, 'personality': 7916, 'philip': 7917, 'pie': 7918, 'pissedoff': 7919, 'pit': 7920, 'pixar': 7921, 'plight': 7922, 'pll': 7923, 'po': 7924, 'poll': 7925, 'polo': 7926, 'poman': 7927, 'pose': 7928, 'prayingfororlando': 7929, 'pregnant': 7930, 'premium': 7931, 'presents': 7932, 'prettiest': 7933, 'prevented': 7934, 'prob': 7935, 'productivity': 7936, 'programme': 7937, 'promises': 7938, 'proper': 7939, 'provides': 7940, 'puff': 7941, 'pug': 7942, 'pull': 7943, 'punjabis': 7944, 'qotd': 7945, 'quotestags': 7946, 'quotestagsapp': 7947, 'racebaiter': 7948, 'realestate': 7949, 'realtalk': 7950, 'recognizing': 7951, 'recovered': 7952, 'redcove': 7953, 'reducing': 7954, 'rehearsal': 7955, 'relatable': 7956, 'relationshipgoals': 7957, 'repoingsystem': 7958, 'repostapp': 7959, 'reviews': 7960, 'rice': 7961, 'ris': 7962, 'risks': 7963, 'roles': 7964, 'ronda': 7965, 'royalascot': 7966, 'rss': 7967, 'saddened': 7968, 'salon': 7969, 'sameshitnewyear': 7970, 'sample': 7971, 'sassy': 7972, 'saturdaymorning': 7973, 'saving': 7974, 'scott': 7975, 'seahawks': 7976, 'seed': 7977, 'semantics': 7978, 'semi': 7979, 'senior': 7980, 'sentence': 7981, 'services': 7982, 'setup': 7983, 'shabbatshalom': 7984, 'shady': 7985, 'shakes': 7986, 'shane': 7987, 'shockfactor': 7988, 'shoe': 7989, 'shopthemint': 7990, 'shore': 7991, 'singlelife': 7992, 'skiing': 7993, 'slaveowner': 7994, 'slowly': 7995, 'smaphone': 7996, 'snack': 7997, 'sneak': 7998, 'sniff': 7999, 'sober': 8000, 'solid': 8001, 'solution': 8002, 'songwriter': 8003, 'sorrynotsorry': 8004, 'southampton': 8005, 'speaker': 8006, 'species': 8007, 'spreadlovethischristmas': 8008, 'stirs': 8009, 'stock': 8010, 'stomach': 8011, 'str': 8012, 'stray': 8013, 'stressfree': 8014, 'strips': 8015, 'stuakenyon81': 8016, 'sugar': 8017, 'suppoive': 8018, 'suppose': 8019, 'surely': 8020, 'sustainable': 8021, 'switzerland': 8022, 'taiwan': 8023, 'tds': 8024, 'tenerife': 8025, 'theistic': 8026, 'thevoice': 8027, 'thi': 8028, 'thunder': 8029, 'thursdays': 8030, 'tomhiddleston': 8031, 'torn': 8032, 'tournament': 8033, 'trained': 8034, 'tranny': 8035, 'traveler': 8036, 'trolls': 8037, 'tropical': 8038, 'trumplies': 8039, 'trumpproofamerica': 8040, 'tule': 8041, 'udta': 8042, 'ukraineblog': 8043, 'unintentionally': 8044, 'universe': 8045, 'upgrade': 8046, 'ushers': 8047, 'valley': 8048, 'vietnam': 8049, 'vijay': 8050, 'vscogood': 8051, 'waronchristmas': 8052, 'warren': 8053, 'weakening': 8054, 'wealthy': 8055, 'weapons': 8056, 'weareorlando': 8057, 'weirdos': 8058, 'western': 8059, 'whitefeminism': 8060, 'wholesome': 8061, 'whopped': 8062, 'wid': 8063, 'wide': 8064, 'wil': 8065, 'wimbledon': 8066, 'wisconsin': 8067, 'wohy': 8068, 'wp': 8069, 'wth': 8070, 'xbox': 8071, 'yen': 8072, 'yesssss': 8073, 'youall': 8074, '06': 8075, '0806': 8076, '1': 8077, '101': 8078, '10alltypespos': 8079, '11400': 8080, '1206': 8081, '1500': 8082, '1900': 8083, '1d': 8084, '1k': 8085, '1q': 8086, '2': 8087, '2014': 8088, '2018': 8089, '22nd': 8090, '25th': 8091, '26': 8092, '2k': 8093, '35th': 8094, '37': 8095, '45': 8096, '4o4o4': 8097, '5': 8098, '50islamicinfo': 8099, '6': 8100, '79': 8101, '80s': 8102, '9': 8103, '900': 8104, '90th': 8105, '999': 8106, 'abba': 8107, 'abc': 8108, 'abrahamhicks': 8109, 'academy': 8110, 'acceptance': 8111, 'aching': 8112, 'acquired': 8113, 'adds': 8114, 'adele': 8115, 'adhan': 8116, 'adopt': 8117, 'adultedu': 8118, 'adults': 8119, 'affectionately': 8120, 'aft': 8121, 'aid': 8122, 'aim': 8123, 'aladdin': 8124, 'alaska': 8125, 'albea': 8126, 'albums': 8127, 'aldub11thmonthsary': 8128, 'alhamdulillah': 8129, 'aliens': 8130, 'alright': 8131, 'alternatively': 8132, 'amigos': 8133, 'amodu': 8134, 'amount': 8135, 'anderson': 8136, 'andreas': 8137, 'andy': 8138, 'anon': 8139, 'antiracist': 8140, 'ap': 8141, 'apament': 8142, 'apaments': 8143, 'appear': 8144, 'appearing': 8145, 'appears': 8146, 'ar15': 8147, 'argentina': 8148, 'arrives': 8149, 'aruba': 8150, 'asianladyboy': 8151, 'aside': 8152, 'atk': 8153, 'audio': 8154, 'audition': 8155, 'autism': 8156, 'avgeek': 8157, 'avocado': 8158, 'awasome': 8159, 'baba': 8160, 'backpack': 8161, 'badday': 8162, 'banana': 8163, 'bang': 8164, 'baptized': 8165, 'bargain': 8166, 'barn': 8167, 'bass': 8168, 'battlefield1': 8169, 'bck': 8170, 'beachbody': 8171, 'beauties': 8172, 'beautifulday': 8173, 'beep': 8174, 'beeroclock': 8175, 'beinlife': 8176, 'belated': 8177, 'belfast': 8178, 'belong': 8179, 'besides': 8180, 'bi': 8181, 'bicycle': 8182, 'bigbang': 8183, 'bihdaycake': 8184, 'bihdaykazuki': 8185, 'bilal': 8186, 'bisexual': 8187, 'blackmen': 8188, 'blackperson': 8189, 'blacksupremacist': 8190, 'blaming': 8191, 'blissful': 8192, 'blooms': 8193, 'boarding': 8194, 'bobby': 8195, 'bodrum': 8196, 'bon': 8197, 'booze': 8198, 'bora': 8199, 'bordercollie': 8200, 'bothered': 8201, 'bounce': 8202, 'bowl': 8203, 'boyfriends': 8204, 'brains': 8205, 'breakup': 8206, 'breeds': 8207, 'breeze': 8208, 'bremain': 8209, 'brew': 8210, 'brian': 8211, 'bridesmaid': 8212, 'bridesmaids': 8213, 'brighten': 8214, 'brisbane': 8215, 'broadcast': 8216, 'broadway': 8217, 'budget': 8218, 'bulgaria': 8219, 'bullet': 8220, 'bullied': 8221, 'bum': 8222, 'bury': 8223, 'butterflies': 8224, 'buttholemouth': 8225, 'buttons': 8226, 'cactus': 8227, 'cakes': 8228, 'calories': 8229, 'camper': 8230, 'canceled': 8231, 'cape': 8232, 'cardio': 8233, 'carlos': 8234, 'carol': 8235, 'carpet': 8236, 'carving': 8237, 'cash': 8238, 'cashing': 8239, 'casino': 8240, 'catchup': 8241, 'ceainly': 8242, 'celebritydeath': 8243, 'censor': 8244, 'centralpark': 8245, 'champion': 8246, 'charlotte': 8247, 'cheaper': 8248, 'checkcashing': 8249, 'cheesy': 8250, 'chef': 8251, 'cherish': 8252, 'cheryl': 8253, 'childcare': 8254, 'childs': 8255, 'chilled': 8256, 'chilling': 8257, 'chills': 8258, 'chuffed': 8259, 'clannad': 8260, 'clash': 8261, 'closet': 8262, 'cloudchaser': 8263, 'cloudy': 8264, 'cmtawards': 8265, 'coat': 8266, 'cocktail': 8267, 'coloring': 8268, 'comfoable': 8269, 'comparisons': 8270, 'complaining': 8271, 'comprehensive': 8272, 'concerned': 8273, 'conclusion': 8274, 'conduct': 8275, 'confuse': 8276, 'conjuring': 8277, 'constant': 8278, 'contagious': 8279, 'contented': 8280, 'convention': 8281, 'costs': 8282, 'couch': 8283, 'county': 8284, 'coupon': 8285, 'covers': 8286, 'craftbeer': 8287, 'crapsac': 8288, 'creations': 8289, 'crochet': 8290, 'crown': 8291, 'crush': 8292, 'cube': 8293, 'cure': 8294, 'curly': 8295, 'cursedchild': 8296, 'customerservice': 8297, 'cutting': 8298, 'cycle': 8299, 'czech': 8300, 'daddygavemeamillion': 8301, 'danny': 8302, 'darkest': 8303, 'datenight': 8304, 'davis': 8305, 'dawn': 8306, 'deafening': 8307, 'debut': 8308, 'deciding': 8309, 'deck': 8310, 'decorative': 8311, 'decors': 8312, 'dee': 8313, 'defame': 8314, 'degree': 8315, 'dementeddonny': 8316, 'dental': 8317, 'dentist': 8318, 'depament': 8319, 'deserved': 8320, 'designs': 8321, 'detox': 8322, 'developing': 8323, 'devops': 8324, 'diego': 8325, 'digging': 8326, 'dining': 8327, 'disagrees': 8328, 'disappoint': 8329, 'disappointing': 8330, 'discount': 8331, 'dislike': 8332, 'disneygatorattack': 8333, 'distractions': 8334, 'dogslife': 8335, 'dogsoftwitter': 8336, 'doj': 8337, 'doll': 8338, 'downloaded': 8339, 'drags': 8340, 'drake': 8341, 'drakeandjosh': 8342, 'draymond': 8343, 'dreaming': 8344, 'dresses': 8345, 'dubai': 8346, 'dukhi': 8347, 'easiest': 8348, 'eastcoast': 8349, 'eaten': 8350, 'economicapaheid': 8351, 'ecourse': 8352, 'eddie': 8353, 'egg': 8354, 'eia': 8355, 'elementary': 8356, 'eli': 8357, 'elite': 8358, 'em2016': 8359, 'embarrassing': 8360, 'emptiness': 8361, 'enchanting': 8362, 'endorsed': 8363, 'enjoyment': 8364, 'enroute': 8365, 'enter': 8366, 'enthusiasm': 8367, 'eternal': 8368, 'eureferendum': 8369, 'expand': 8370, 'expecting': 8371, 'exposeracism': 8372, 'eyebrow': 8373, 'facemask': 8374, 'faded': 8375, 'falsely': 8376, 'fathersday2016': 8377, 'fathersdaymessage': 8378, 'faves': 8379, 'favs': 8380, 'feared': 8381, 'fearless': 8382, 'fears': 8383, 'features': 8384, 'february': 8385, 'fees': 8386, 'felling': 8387, 'fetishes': 8388, 'fever': 8389, 'fewer': 8390, 'fg': 8391, 'fianc': 8392, 'figured': 8393, 'filling': 8394, 'financial': 8395, 'finds': 8396, 'fing': 8397, 'finn': 8398, 'firefly': 8399, 'firenze': 8400, 'fireworks': 8401, 'fits': 8402, 'flags': 8403, 'flip': 8404, 'floating': 8405, 'floral': 8406, 'flourishing': 8407, 'fomc': 8408, 'fotokuapp': 8409, 'fouh': 8410, 'fra': 8411, 'freshsta': 8412, 'fridge': 8413, 'fuming': 8414, 'gals': 8415, 'gameshow': 8416, 'gawa': 8417, 'gayboy': 8418, 'gbpjpy': 8419, 'gd': 8420, 'gem': 8421, 'generous': 8422, 'giftideas': 8423, 'giggles': 8424, 'godbless': 8425, 'gossip': 8426, 'grads': 8427, 'greatful': 8428, 'greet': 8429, 'grill': 8430, 'grown': 8431, 'growthhacking': 8432, 'gunman': 8433, 'habit': 8434, 'hairdresser': 8435, 'hairstyle': 8436, 'halfway': 8437, 'hamza': 8438, 'handjob': 8439, 'handwritten': 8440, 'happyfathersday': 8441, 'hardest': 8442, 'hardworking': 8443, 'harmless': 8444, 'harmonious': 8445, 'hashtags': 8446, 'haul': 8447, 'havefun': 8448, 'hawaiian': 8449, 'hd': 8450, 'heaache': 8451, 'healthyliving': 8452, 'heavenly': 8453, 'hehe': 8454, 'helpful': 8455, 'helpme': 8456, 'henderson': 8457, 'highfive': 8458, 'hint': 8459, 'hippy': 8460, 'ho': 8461, 'homedecor': 8462, 'homeless': 8463, 'homies': 8464, 'honeymoon': 8465, 'hongkong': 8466, 'honour': 8467, 'hoo': 8468, 'hoodies': 8469, 'horses': 8470, 'hoshi': 8471, 'hottie': 8472, 'hs': 8473, 'hudson': 8474, 'hyper': 8475, 'ian': 8476, 'ii': 8477, 'il': 8478, 'imessage': 8479, 'imperfections': 8480, 'included': 8481, 'indieauthor': 8482, 'indiegame': 8483, 'indirect': 8484, 'indulge': 8485, 'industrial': 8486, 'infamous': 8487, 'inflation': 8488, 'ing': 8489, 'injuries': 8490, 'inner': 8491, 'innocence': 8492, 'instafashion': 8493, 'instafood': 8494, 'installation': 8495, 'instalove': 8496, 'intense': 8497, 'intent': 8498, 'intermarket': 8499, 'internship': 8500, 'introduce': 8501, 'invited2jive': 8502, 'iron': 8503, 'irrelevant': 8504, 'ita': 8505, 'itunes': 8506, 'jackie': 8507, 'jacksons': 8508, 'jacob': 8509, 'jar': 8510, 'jivemap': 8511, 'jocoxmp': 8512, 'jungle': 8513, 'justinb': 8514, 'kaaba': 8515, 'karaoke': 8516, 'katie': 8517, 'kaye': 8518, 'keith': 8519, 'kent': 8520, 'kg': 8521, 'kickoff': 8522, 'kiev': 8523, 'knees': 8524, 'knock': 8525, 'ko': 8526, 'kro': 8527, 'kylielipkit': 8528, 'landed': 8529, 'lands': 8530, 'laptop': 8531, 'lastday': 8532, 'lasting': 8533, 'latenights': 8534, 'latin': 8535, 'laying': 8536, 'lbj': 8537, 'leaf': 8538, 'legit': 8539, 'legs': 8540, 'lemonade': 8541, 'leo': 8542, 'lifetime': 8543, 'lift': 8544, 'limit': 8545, 'lions': 8546, 'literacy': 8547, 'livestream': 8548, 'lobster': 8549, 'locked': 8550, 'lokiday': 8551, 'lopez': 8552, 'lopezchiro': 8553, 'loserpizza': 8554, 'louis': 8555, 'louisville': 8556, 'lousy': 8557, 'lovehim': 8558, 'loveme': 8559, 'lovin': 8560, 'luckygirl': 8561, 'luis': 8562, 'luke': 8563, 'lyft': 8564, 'maam': 8565, 'maintenance': 8566, 'majesty': 8567, 'maker': 8568, 'malaysia': 8569, 'maneuver': 8570, 'manor': 8571, 'mar': 8572, 'marbella': 8573, 'marble': 8574, 'massachusetts': 8575, 'mater': 8576, 'maternity': 8577, 'mcflurry': 8578, 'mebeforeyou': 8579, 'membership': 8580, 'memorial': 8581, 'menner': 8582, 'mensfashion': 8583, 'menswear': 8584, 'mentally': 8585, 'mentions': 8586, 'mentor': 8587, 'mentors': 8588, 'mercedes': 8589, 'midnight': 8590, 'mighty': 8591, 'migraine': 8592, 'mile': 8593, 'mileycyrus': 8594, 'milf': 8595, 'min': 8596, 'misses': 8597, 'mitb': 8598, 'mk': 8599, 'mojo': 8600, 'mommylife': 8601, 'mondays': 8602, 'moore': 8603, 'moredun': 8604, 'motherhoodunscripted': 8605, 'motto': 8606, 'mourning': 8607, 'ms': 8608, 'mtb': 8609, 'mua': 8610, 'mumbai': 8611, 'munich': 8612, 'muscles': 8613, 'museum': 8614, 'musician': 8615, 'myhea': 8616, 'mytraining': 8617, 'nab': 8618, 'nail': 8619, 'nascar': 8620, 'naturally': 8621, 'necklace': 8622, 'neither': 8623, 'nelsonmandela': 8624, 'newcar': 8625, 'newday': 8626, 'newest': 8627, 'newjersey': 8628, 'newjob': 8629, 'newly': 8630, 'nhs': 8631, 'ni': 8632, 'nightmares': 8633, 'nightshift': 8634, 'nike': 8635, 'nine': 8636, 'nojustice': 8637, 'nonsense': 8638, 'noooo': 8639, 'norfolk': 8640, 'norm': 8641, 'nuts': 8642, 'nxt': 8643, 'nz': 8644, 'nzdusd': 8645, 'objectification': 8646, 'october': 8647, 'officials': 8648, 'offline': 8649, 'ohwell': 8650, 'ojmadeinamerica': 8651, 'oldschool': 8652, 'omw': 8653, 'oooh': 8654, 'opinions': 8655, 'orders': 8656, 'organization': 8657, 'orlandostrong': 8658, 'orlandounited': 8659, 'osaka': 8660, 'otaku': 8661, 'otw': 8662, 'ounce': 8663, 'owe': 8664, 'owl': 8665, 'oxford': 8666, 'pace': 8667, 'pakistani': 8668, 'palace': 8669, 'palette': 8670, 'pamper': 8671, 'pampered': 8672, 'panel': 8673, 'parade': 8674, 'pasty': 8675, 'patch': 8676, 'paths': 8677, 'pb': 8678, 'peaked': 8679, 'pedophiles': 8680, 'pencil': 8681, 'penguin': 8682, 'pepper': 8683, 'perfume': 8684, 'peter': 8685, 'pharrellwilliams': 8686, 'phil': 8687, 'philadelphia': 8688, 'philly': 8689, 'photobooth': 8690, 'phuket': 8691, 'picnic': 8692, 'pinoy': 8693, 'plaza': 8694, 'pllseason7': 8695, 'plot': 8696, 'pokemonsunmoon': 8697, 'pole': 8698, 'poop': 8699, 'positivethinking': 8700, 'postive': 8701, 'potd': 8702, 'prefer': 8703, 'prepared': 8704, 'presence': 8705, 'presenter': 8706, 'principle': 8707, 'probation': 8708, 'probe': 8709, 'probs': 8710, 'professionals': 8711, 'prosecco': 8712, 'prosperity': 8713, 'protein': 8714, 'providing': 8715, 'punjabi': 8716, 'punk': 8717, 'pup': 8718, 'puppylove': 8719, 'purple': 8720, 'qampa': 8721, 'que': 8722, 'rahulgandhi': 8723, 'raid': 8724, 'rainforest': 8725, 'ramadankareem': 8726, 'ramsey': 8727, 'reactions': 8728, 'recorded': 8729, 'recover': 8730, 'recovers': 8731, 'reflection': 8732, 'refs': 8733, 'regardless': 8734, 'related': 8735, 'releases': 8736, 'releasing': 8737, 'relief': 8738, 'remembered': 8739, 'remembering': 8740, 'renee': 8741, 'replace': 8742, 'replaced': 8743, 'representations': 8744, 'requested': 8745, 'residents': 8746, 'restore': 8747, 'retire': 8748, 'retweets': 8749, 'rick': 8750, 'rider': 8751, 'rifles': 8752, 'ripantonyelchin': 8753, 'robin': 8754, 'rockette': 8755, 'rooms': 8756, 'rope': 8757, 'rotterdam': 8758, 'royaltyfreemusic': 8759, 'ryan': 8760, 'sabbath': 8761, 'sabon': 8762, 'samsunggalaxys2': 8763, 'sana': 8764, 'sandiego': 8765, 'sandwich': 8766, 'satanists': 8767, 'saterday': 8768, 'scenario': 8769, 'scenery': 8770, 'scheme': 8771, 'scratch': 8772, 'section': 8773, 'secure': 8774, 'secured': 8775, 'segment': 8776, 'selfharm': 8777, 'selfharming': 8778, 'semester': 8779, 'seniors': 8780, 'sensitive': 8781, 'seoul': 8782, 'serenity': 8783, 'server': 8784, 'sexting': 8785, 'shape': 8786, 'shares': 8787, 'shaved': 8788, 'sheboutit': 8789, 'shining': 8790, 'shld': 8791, 'shooters': 8792, 'shops': 8793, 'shos': 8794, 'shoulder': 8795, 'showtime': 8796, 'shrm16': 8797, 'shuaibu': 8798, 'simples': 8799, 'sitges': 8800, 'sixty': 8801, 'ski': 8802, 'skynews': 8803, 'slice': 8804, 'slim': 8805, 'slimmingworld': 8806, 'smallthings': 8807, 'smash': 8808, 'smdh': 8809, 'smilemore': 8810, 'smilepowerday': 8811, 'smoothie': 8812, 'snd': 8813, 'snug': 8814, 'sole': 8815, 'solo': 8816, 'somerset': 8817, 'sophie': 8818, 'soundtrack': 8819, 'sp': 8820, 'spare': 8821, 'spectacle': 8822, 'speeches': 8823, 'spirited': 8824, 'spoil': 8825, 'spoonie': 8826, 'spotted': 8827, 'ss17': 8828, 'stairs': 8829, 'starring': 8830, 'staypositive': 8831, 'staystrong': 8832, 'staytuned': 8833, 'steals': 8834, 'stella': 8835, 'stepping': 8836, 'stopping': 8837, 'stopthehate': 8838, 'stores': 8839, 'straw': 8840, 'strikes': 8841, 'stroller': 8842, 'studies': 8843, 'stumble': 8844, 'stylish': 8845, 'subject': 8846, 'submitted': 8847, 'subtitles': 8848, 'subway': 8849, 'succeed': 8850, 'suffering': 8851, 'suicides': 8852, 'summers': 8853, 'sundaymood': 8854, 'superficial': 8855, 'superman': 8856, 'supernatural': 8857, 'supremely': 8858, 'sweetdreams': 8859, 'sweethea': 8860, 'sympathy': 8861, 'tacloban': 8862, 'taichi': 8863, 'tarot': 8864, 'tasty': 8865, 'tattoos': 8866, 'taylorswift1989': 8867, 'tbh': 8868, 'teachers': 8869, 'teamfollowback': 8870, 'teddy': 8871, 'tee': 8872, 'tequila': 8873, 'terribly': 8874, 'thalaivaa': 8875, 'thousand': 8876, 'thriving': 8877, 'thumb': 8878, 'tin': 8879, 'tinyplanet': 8880, 'titanic': 8881, 'todo': 8882, 'toe': 8883, 'toes': 8884, 'tools': 8885, 'tops': 8886, 'toptweeters': 8887, 'toxic': 8888, 'tracks': 8889, 'tracy': 8890, 'traditional': 8891, 'transfer': 8892, 'treasures': 8893, 'tribune': 8894, 'trick': 8895, 'trims': 8896, 'triste': 8897, 'truelove': 8898, 'trumpis': 8899, 'tryna': 8900, 'tu': 8901, 'tubbytoons': 8902, 'turnup': 8903, 'twin': 8904, 'udtapunjableaked': 8905, 'ukrunchat': 8906, 'ultimate': 8907, 'unable': 8908, 'unbo': 8909, 'underfire': 8910, 'understands': 8911, 'understatement': 8912, 'uniform': 8913, 'unitedstates': 8914, 'unlimited': 8915, 'unwavering': 8916, 'upgraded': 8917, 'upsideofflorida': 8918, 'usb': 8919, 'user': 8920, 'va': 8921, 'vacay': 8922, 'valentines': 8923, 'valued': 8924, 'van': 8925, 'vegetables': 8926, 'vehicles': 8927, 'venture': 8928, 'vibe': 8929, 'viernes': 8930, 'vino': 8931, 'vocal': 8932, 'vocals': 8933, 'wage': 8934, 'wakow': 8935, 'washing': 8936, 'wcw': 8937, 'weaker': 8938, 'wealth': 8939, 'weddinganniversary': 8940, 'weddingday': 8941, 'weddingdress': 8942, 'wendy': 8943, 'whack': 8944, 'wheels': 8945, 'whisky': 8946, 'whiteprivelage': 8947, 'wi': 8948, 'wilson': 8949, 'winnipeg': 8950, 'winterfashion': 8951, 'witch': 8952, 'wks': 8953, 'womensrighttochoose': 8954, 'workfromhome': 8955, 'worrying': 8956, 'wounded': 8957, 'writerslife': 8958, 'wthe': 8959, 'xo': 8960, 'yah': 8961, 'ye': 8962, 'ynwa': 8963, 'yu': 8964, 'yusuf': 8965, 'zoro': 8966, '03': 8967, '04': 8968, '05': 8969, '061116': 8970, '0612': 8971, '0624': 8972, '07950': 8973, '100000': 8974, '100happydays': 8975, '1299': 8976, '1400': 8977, '1499': 8978, '150': 8979, '1995': 8980, '19th': 8981, '1day': 8982, '1stammendment': 8983, '201617': 8984, '2100': 8985, '25cricket': 8986, '28th': 8987, '29th': 8988, '2days': 8989, '2k16': 8990, '2nite': 8991, '2the': 8992, '2weeks': 8993, '3000': 8994, '30th': 8995, '34': 8996, '36': 8997, '38': 8998, '3worldnews': 8999, '3yr': 9000, '400': 9001, '40th': 9002, '44': 9003, '46': 9004, '47pm': 9005, '4k': 9006, '55': 9007, '57': 9008, '59': 9009, '5k': 9010, '5sos': 9011, '64': 9012, '67': 9013, '6yearswithinfinite': 9014, '73': 9015, '7th': 9016, '84': 9017, '90s': 9018, 'aaa': 9019, 'aaron': 9020, 'ab': 9021, 'aboutlastnight': 9022, 'ac': 9023, 'academia': 9024, 'acc': 9025, 'accidents': 9026, 'accuse': 9027, 'ache': 9028, 'achieved': 9029, 'achievement': 9030, 'addition': 9031, 'addressing': 9032, 'adidas': 9033, 'adopted': 9034, 'adve': 9035, 'affects': 9036, 'afro': 9037, 'agencies': 9038, 'agr': 9039, 'ahh': 9040, 'alarms': 9041, 'alice': 9042, 'alicia': 9043, 'allen': 9044, 'allsmiles': 9045, 'almighty': 9046, 'almostthere': 9047, 'alohafriday': 9048, 'alot': 9049, 'alps': 9050, 'alum': 9051, 'amo': 9052, 'ampblessed': 9053, 'ampfamilies': 9054, 'amused': 9055, 'ancient': 9056, 'angeles': 9057, 'angelic': 9058, 'angus': 9059, 'animalabuse': 9060, 'animaladvocate': 9061, 'animated': 9062, 'annaswelshzoo': 9063, 'annefrank': 9064, 'anniversaries': 9065, 'anorexia': 9066, 'antonio': 9067, 'anytime': 9068, 'anyways': 9069, 'anz': 9070, 'apparel': 9071, 'applications': 9072, 'apt': 9073, 'arab': 9074, 'archery': 9075, 'areas': 9076, 'arena': 9077, 'arepa': 9078, 'argue': 9079, 'arguing': 9080, 'aria': 9081, 'arianagrandedrawing': 9082, 'arizona': 9083, 'asf': 9084, 'aspect': 9085, 'assessment': 9086, 'asylum': 9087, 'athlete': 9088, 'attacking': 9089, 'attends': 9090, 'attic': 9091, 'aug': 9092, 'automatic': 9093, 'awe': 9094, 'awesomeness': 9095, 'awhile': 9096, 'awkward': 9097, 'aworks': 9098, 'aya': 9099, 'aye': 9100, 'az': 9101, 'backward': 9102, 'badass': 9103, 'badminton': 9104, 'baker': 9105, 'baking': 9106, 'balloon': 9107, 'bands': 9108, 'barbiets93': 9109, 'bare': 9110, 'bars': 9111, 'bbcnews': 9112, 'bcoz': 9113, 'bd': 9114, 'bdayspl': 9115, 'beachday': 9116, 'beatz': 9117, 'beaut': 9118, 'bedroom': 9119, 'bedtime': 9120, 'bei': 9121, 'bel': 9122, 'beliefs': 9123, 'believed': 9124, 'belly': 9125, 'ben': 9126, 'bengaluru': 9127, 'bent': 9128, 'bestdad': 9129, 'bestfriendsday': 9130, 'beta': 9131, 'bethechange': 9132, 'betrayal': 9133, 'bias': 9134, 'bieber': 9135, 'bigender': 9136, 'bihdaypresent': 9137, 'bingewatching': 9138, 'bites': 9139, 'bits': 9140, 'bittersweet': 9141, 'blackmaverick12': 9142, 'blend': 9143, 'blocking': 9144, 'blogginggals': 9145, 'blondie': 9146, 'bloom': 9147, 'blowoutsbringhappiness': 9148, 'blows': 9149, 'bluehand': 9150, 'bluesky': 9151, 'bnz': 9152, 'bo3': 9153, 'bobross': 9154, 'boe': 9155, 'bogotadc': 9156, 'bogumday': 9157, 'boise': 9158, 'boj': 9159, 'bone': 9160, 'boogie': 9161, 'bookclub': 9162, 'bookreview': 9163, 'booster': 9164, 'bopanna': 9165, 'bourbon': 9166, 'boxes': 9167, 'braces': 9168, 'branch': 9169, 'branding': 9170, 'brandnew': 9171, 'breathing': 9172, 'breezy': 9173, 'brentwood': 9174, 'bridal': 9175, 'brokenhea': 9176, 'brooks': 9177, 'browning': 9178, 'browser': 9179, 'bucket': 9180, 'builders': 9181, 'buisness': 9182, 'bummer': 9183, 'burns': 9184, 'businessoppounity': 9185, 'bust': 9186, 'butter': 9187, 'button': 9188, 'buttoning': 9189, 'buzz': 9190, 'calico': 9191, 'camgirl': 9192, 'campaigning': 9193, 'campers': 9194, 'campus': 9195, 'canal': 9196, 'cannotwait': 9197, 'cantsleep': 9198, 'cantstopsmiling': 9199, 'canvas': 9200, 'cap': 9201, 'capable': 9202, 'captured': 9203, 'careers': 9204, 'carefree': 9205, 'careless': 9206, 'carmineryderrr': 9207, 'catlover': 9208, 'caucasian': 9209, 'cdn': 9210, 'cecily': 9211, 'cedarrapids': 9212, 'celebs': 9213, 'cellphone': 9214, 'champs': 9215, 'chandigarh': 9216, 'characters': 9217, 'charleston': 9218, 'charlie': 9219, 'charming': 9220, 'charms': 9221, 'cheat': 9222, 'cheated': 9223, 'checking': 9224, 'checkout': 9225, 'cheerful': 9226, 'cheesecake': 9227, 'chennai': 9228, 'chester': 9229, 'chi': 9230, 'chickens': 9231, 'chipotle': 9232, 'choke': 9233, 'cielo': 9234, 'cincinnati': 9235, 'cinemas': 9236, 'cipd': 9237, 'classical': 9238, 'cleaneating': 9239, 'cleanse': 9240, 'cleared': 9241, 'clears': 9242, 'clever': 9243, 'climbing': 9244, 'clinches': 9245, 'clueless': 9246, 'coaches': 9247, 'coc': 9248, 'cocoa': 9249, 'cod': 9250, 'coke': 9251, 'collab': 9252, 'collapses': 9253, 'collect': 9254, 'colorado': 9255, 'colou': 9256, 'colourful': 9257, 'comeonengland': 9258, 'commentary': 9259, 'commerzbank': 9260, 'communal': 9261, 'communication': 9262, 'compete': 9263, 'complaints': 9264, 'complicated': 9265, 'composed': 9266, 'concern': 9267, 'congressional': 9268, 'connecticut': 9269, 'conscious': 9270, 'constitution': 9271, 'cont': 9272, 'contestants': 9273, 'contracts': 9274, 'contribute': 9275, 'conversion': 9276, 'cooke': 9277, 'cooler': 9278, 'copaamerica': 9279, 'core': 9280, 'cork': 9281, 'cornwall': 9282, 'corporate': 9283, 'cosmetics': 9284, 'costume': 9285, 'cotd': 9286, 'cptplanespotter': 9287, 'cracked': 9288, 'cracking': 9289, 'cracks': 9290, 'craftfest': 9291, 'crappy': 9292, 'cries': 9293, 'crowdfunding': 9294, 'crowds': 9295, 'crueltyfree': 9296, 'crystal': 9297, 'crystals': 9298, 'css': 9299, 'cubs': 9300, 'cue': 9301, 'cuisine': 9302, 'cum': 9303, 'cupcakes': 9304, 'cuppa': 9305, 'cups': 9306, 'curb': 9307, 'curiosity': 9308, 'curious': 9309, 'curvy': 9310, 'cushions': 9311, 'customized': 9312, 'cuts': 9313, 'cybersecurity': 9314, 'dachshund': 9315, 'daddies': 9316, 'dadsday': 9317, 'damaged': 9318, 'danbury': 9319, 'dancelife': 9320, 'dances': 9321, 'darker': 9322, 'darkknight': 9323, 'deadpool': 9324, 'dearfellowwhitepeople': 9325, 'dears': 9326, 'decides': 9327, 'decorating': 9328, 'deepest': 9329, 'delegates': 9330, 'demise': 9331, 'den': 9332, 'dennis': 9333, 'derogatory': 9334, 'descending': 9335, 'desses': 9336, 'devastating': 9337, 'diadelpadre': 9338, 'diner': 9339, 'dinosaur': 9340, 'diploma': 9341, 'dipped': 9342, 'disbanded': 9343, 'discharge': 9344, 'discontinued': 9345, 'dish': 9346, 'dishonest': 9347, 'disneys': 9348, 'disorder': 9349, 'distressed': 9350, 'ditch': 9351, 'divulgaoeparceria': 9352, 'dixie': 9353, 'diysos': 9354, 'djlife': 9355, 'dnaquotes': 9356, 'dnt': 9357, 'doc': 9358, 'doggie': 9359, 'doggies': 9360, 'dome': 9361, 'donalds': 9362, 'doodle': 9363, 'dosti': 9364, 'dovish': 9365, 'dowhatyoulove': 9366, 'dozens': 9367, 'dp': 9368, 'draft': 9369, 'draghi': 9370, 'dragonfly': 9371, 'drained': 9372, 'dread': 9373, 'dreamscometrue': 9374, 'dreamteam': 9375, 'dressage': 9376, 'dressing': 9377, 'dri': 9378, 'dribbble': 9379, 'drives': 9380, 'drought': 9381, 'drugaddicts': 9382, 'drum': 9383, 'drums': 9384, 'dumbass': 9385, 'dvbmultimediagroup': 9386, 'dvd': 9387, 'dvr': 9388, 'dye': 9389, 'dynamic': 9390, 'dysfunction': 9391, 'earp': 9392, 'ears': 9393, 'ece': 9394, 'eden': 9395, 'edfringe2016': 9396, 'edits': 9397, 'edmonton': 9398, 'eeek': 9399, 'effos': 9400, 'ek': 9401, 'el': 9402, 'electric': 9403, 'elegy': 9404, 'elements': 9405, 'eliminate': 9406, 'elizabeth': 9407, 'ella': 9408, 'emea': 9409, 'encouragement': 9410, 'energetic': 9411, 'engaging': 9412, 'engerland': 9413, 'engineering': 9414, 'engwal': 9415, 'entirely': 9416, 'entrance': 9417, 'envy': 9418, 'episode13': 9419, 'equally': 9420, 'esco': 9421, 'essex': 9422, 'esteem': 9423, 'euro16': 9424, 'eurocup2016': 9425, 'evans': 9426, 'evenings': 9427, 'everest': 9428, 'everythings': 9429, 'evn': 9430, 'exact': 9431, 'exc': 9432, 'excellence': 9433, 'excitem': 9434, 'execs': 9435, 'expansion': 9436, 'expat': 9437, 'expos': 9438, 'expression': 9439, 'extended': 9440, 'extraordinaryladyspeaks': 9441, 'eyeing': 9442, 'facial': 9443, 'facing': 9444, 'fairy': 9445, 'fallschurch': 9446, 'familia': 9447, 'familytrip': 9448, 'fandom': 9449, 'fangirling': 9450, 'farage': 9451, 'farming': 9452, 'fatherandson': 9453, 'fatherly': 9454, 'fathersdayquotes': 9455, 'favor': 9456, 'fblogger': 9457, 'fd': 9458, 'fearing': 9459, 'feast': 9460, 'fedup': 9461, 'felicidad': 9462, 'fellas': 9463, 'feta': 9464, 'fiestar': 9465, 'filibuster': 9466, 'finance': 9467, 'firefly2016': 9468, 'fireflymusicfestival': 9469, 'firsttime': 9470, 'fishburn': 9471, 'fitting': 9472, 'fixit': 9473, 'flame': 9474, 'flamingo': 9475, 'flightofalifetime': 9476, 'flooding': 9477, 'floridian': 9478, 'flourish': 9479, 'flowerlove': 9480, 'flyers': 9481, 'focusing': 9482, 'followforfollow': 9483, 'followusoninstagram': 9484, 'foodstagram': 9485, 'forces': 9486, 'ford': 9487, 'foreverliving': 9488, 'forgive': 9489, 'forgivers': 9490, 'forums': 9491, 'fpace': 9492, 'frame': 9493, 'francisco': 9494, 'frankly': 9495, 'freddie': 9496, 'freetime': 9497, 'fri': 9498, 'fruits': 9499, 'fuckoff': 9500, 'fucks': 9501, 'fuelled': 9502, 'fulfilled': 9503, 'furbaby': 9504, 'furry': 9505, 'g': 9506, 'gabby': 9507, 'galway': 9508, 'gap': 9509, 'gas': 9510, 'gathering': 9511, 'gazal': 9512, 'gearing': 9513, 'gentleman': 9514, 'gentlemen': 9515, 'geranium': 9516, 'germanyhetalia': 9517, 'gettin': 9518, 'gg': 9519, 'ghosts': 9520, 'giddy': 9521, 'gifted': 9522, 'glam': 9523, 'glamorousiam': 9524, 'glory': 9525, 'glover': 9526, 'gloves': 9527, 'golinuntern': 9528, 'gonetoosoon': 9529, 'goodnews': 9530, 'goodvibesonly': 9531, 'gossips': 9532, 'gotmelike': 9533, 'gplay': 9534, 'gps': 9535, 'graders': 9536, 'grades': 9537, 'grandad': 9538, 'grandfather': 9539, 'grandson': 9540, 'grapes': 9541, 'greatquotes': 9542, 'greg': 9543, 'grieve': 9544, 'grilled': 9545, 'grimm': 9546, 'grimmies': 9547, 'grin': 9548, 'grooming': 9549, 'grounded': 9550, 'grove': 9551, 'growingup': 9552, 'grrr': 9553, 'gtgtgt': 9554, 'gummy': 9555, 'gunna': 9556, 'gunners': 9557, 'guru': 9558, 'hacked': 9559, 'hackney': 9560, 'hackneywick': 9561, 'hai': 9562, 'handbag': 9563, 'hangin': 9564, 'hannah': 9565, 'hap': 9566, 'happend': 9567, 'happiest5k': 9568, 'happines': 9569, 'happycampers': 9570, 'harambe': 9571, 'healthyeating': 9572, 'heavenoneah': 9573, 'hedgehog': 9574, 'hen': 9575, 'henna': 9576, 'henry': 9577, 'herd': 9578, 'hhmatters': 9579, 'hijacked': 9580, 'hike': 9581, 'hindi': 9582, 'historyancient': 9583, 'hmm': 9584, 'holland': 9585, 'homeland': 9586, 'homeon': 9587, 'hometown': 9588, 'hooligans': 9589, 'hospitals': 9590, 'hosted': 9591, 'housewife': 9592, 'housing': 9593, 'html': 9594, 'huaweiceifiedspecialistpresalesaccessnetwork': 9595, 'hung': 9596, 'hunt': 9597, 'hunting': 9598, 'husbands': 9599, 'hutchings': 9600, 'iampossible': 9601, 'iftaar': 9602, 'igersbnw': 9603, 'ikea': 9604, 'illusion': 9605, 'iloveit': 9606, 'ily': 9607, 'imissyou': 9608, 'impressed': 9609, 'impressive': 9610, 'inbox': 9611, 'incense': 9612, 'income': 9613, 'independenceday': 9614, 'inpain': 9615, 'insideout': 9616, 'instadog': 9617, 'instagirl': 9618, 'instantly': 9619, 'instatravel': 9620, 'instatraveling': 9621, 'int': 9622, 'integral': 9623, 'intro': 9624, 'introducing': 9625, 'invade': 9626, 'inventive': 9627, 'inventory': 9628, 'investigation': 9629, 'invitation': 9630, 'inwoo': 9631, 'iphoneonly': 9632, 'ipsy': 9633, 'irate': 9634, 'irresponsible': 9635, 'isaac': 9636, 'islamist': 9637, 'isle': 9638, 'isolated': 9639, 'itsfriday': 9640, 'itworks': 9641, 'iv': 9642, 'ivy': 9643, 'jacksonville': 9644, 'jaguar': 9645, 'jaguarfpace': 9646, 'jaguarstb': 9647, 'jai': 9648, 'jakaa': 9649, 'jeep': 9650, 'jenner': 9651, 'jerry': 9652, 'jet': 9653, 'jk': 9654, 'johnson': 9655, 'jones': 9656, 'jordan': 9657, 'joshua': 9658, 'judgement': 9659, 'judy': 9660, 'juiceplus': 9661, 'jumpsuit': 9662, 'junaricrm': 9663, 'june16': 9664, 'jungkook': 9665, 'jungkookday': 9666, 'justinbieber': 9667, 'justme': 9668, 'kardashian': 9669, 'katiequeue': 9670, 'kayla': 9671, 'ke': 9672, 'keeper': 9673, 'keisha': 9674, 'kennel': 9675, 'kern': 9676, 'kickstaer': 9677, 'killers': 9678, 'kilo': 9679, 'kinds': 9680, 'knee': 9681, 'koran': 9682, 'ks': 9683, 'kudos': 9684, 'ky': 9685, 'kyrie': 9686, 'lads': 9687, 'languages': 9688, 'lap': 9689, 'lasted': 9690, 'lastnight': 9691, 'latino': 9692, 'latvia': 9693, 'laughed': 9694, 'laundry': 9695, 'lawrence': 9696, 'lax': 9697, 'laziness': 9698, 'lbloggers': 9699, 'leagueoflegends': 9700, 'leanin15': 9701, 'legally': 9702, 'leukemia': 9703, 'lfc': 9704, 'liam': 9705, 'lichfield': 9706, 'lifehacks': 9707, 'lifeisbeautiful': 9708, 'lifestyleblogger': 9709, 'lightning': 9710, 'lightweight': 9711, 'likescam': 9712, 'limits': 9713, 'liners': 9714, 'linstagram': 9715, 'linzy': 9716, 'liquor': 9717, 'listento': 9718, 'lists': 9719, 'lite': 9720, 'lnicjustanevilbday': 9721, 'lnp': 9722, 'lobby': 9723, 'lois': 9724, 'lola': 9725, 'lollipop': 9726, 'longweekend': 9727, 'loop': 9728, 'los': 9729, 'loveconquershate': 9730, 'lovecraft': 9731, 'lovefollow': 9732, 'lovelovelove': 9733, 'lovemyfamily': 9734, 'loveofmylife': 9735, 'lower': 9736, 'ludicrous': 9737, 'luggage': 9738, 'maccosmetics': 9739, 'maddow': 9740, 'maialas': 9741, 'maine': 9742, 'makemoney': 9743, 'makemoneyonline': 9744, 'makeover': 9745, 'makeupaddict': 9746, 'makingmemories': 9747, 'malta': 9748, 'mampms': 9749, 'mango': 9750, 'manipulation': 9751, 'mantra': 9752, 'marcus': 9753, 'markets': 9754, 'mary': 9755, 'mattruff': 9756, 'maui': 9757, 'maza': 9758, 'mba': 9759, 'meatloaf': 9760, 'medium': 9761, 'mehn': 9762, 'melee': 9763, 'melissa': 9764, 'melodic': 9765, 'memorable': 9766, 'meow': 9767, 'merch': 9768, 'messing': 9769, 'michaelacoel': 9770, 'midsummer': 9771, 'milestone': 9772, 'mills': 9773, 'mindblown': 9774, 'minecraft': 9775, 'miniature': 9776, 'minnesota': 9777, 'mint': 9778, 'miracle': 9779, 'mirrors': 9780, 'mis': 9781, 'mistress': 9782, 'mlm': 9783, 'mohenjodaro': 9784, 'momentum': 9785, 'momlife': 9786, 'monica': 9787, 'monitor': 9788, 'moose': 9789, 'mor': 9790, 'morocco': 9791, 'motor': 9792, 'mourn': 9793, 'moveon': 9794, 'mrrat395': 9795, 'msgs': 9796, 'mubarak': 9797, 'muffins': 9798, 'multivitamins': 9799, 'mums': 9800, 'muna': 9801, 'muse': 9802, 'musicians': 9803, 'musicislife': 9804, 'myanmars': 9805, 'mybihday': 9806, 'myhappycapture': 9807, 'mykonos': 9808, 'mymood': 9809, 'mytime': 9810, 'naitik': 9811, 'naruto': 9812, 'nationallobsterday': 9813, 'nationalroseday': 9814, 'nationals': 9815, 'natureperfection': 9816, 'nebraska': 9817, 'necessity': 9818, 'needa': 9819, 'neighbour': 9820, 'neil': 9821, 'netflixandchill': 9822, 'nevada': 9823, 'newsong': 9824, 'newspaper': 9825, 'newsta': 9826, 'nfl': 9827, 'nhl': 9828, 'niceday': 9829, 'nicely': 9830, 'nigel': 9831, 'nigth': 9832, 'nikkei': 9833, 'nikon': 9834, 'nintendo': 9835, 'nintendoe3': 9836, 'noble': 9837, 'nohampton': 9838, 'nohcarolina': 9839, 'nomakeup': 9840, 'nomnomnom': 9841, 'nonton': 9842, 'norway': 9843, 'notifications': 9844, 'notokay': 9845, 'nowlinkup': 9846, 'nra': 9847, 'nuanced': 9848, 'nuascannan': 9849, 'nursing': 9850, 'obs': 9851, 'ocd': 9852, 'oclock': 9853, 'ohh': 9854, 'ohhhhh': 9855, 'oitnb4': 9856, 'oitnbchat': 9857, 'oitnbseason4': 9858, 'olathe': 9859, 'om': 9860, 'omaha': 9861, 'omarosas': 9862, 'onion': 9863, 'onpoint': 9864, 'ontheredcarpet': 9865, 'oomf': 9866, 'opener': 9867, 'opera': 9868, 'operating': 9869, 'operation': 9870, 'oppa': 9871, 'options': 9872, 'ordering': 9873, 'organised': 9874, 'organising': 9875, 'organized': 9876, 'oscarpretorious': 9877, 'oslo': 9878, 'outfitoftheday': 9879, 'outlook': 9880, 'overall': 9881, 'overloaded': 9882, 'overnight': 9883, 'oxygen': 9884, 'oysters': 9885, 'pablo': 9886, 'pac': 9887, 'packaging': 9888, 'paddington': 9889, 'paicipate': 9890, 'paicular': 9891, 'paicularly': 9892, 'painful': 9893, 'pairs': 9894, 'paleo': 9895, 'pals': 9896, 'pancakes': 9897, 'panic': 9898, 'paperback': 9899, 'papers': 9900, 'papi': 9901, 'par': 9902, 'para': 9903, 'paranoid': 9904, 'parente': 9905, 'parks': 9906, 'passenger': 9907, 'password': 9908, 'pasta': 9909, 'pastel': 9910, 'patrick': 9911, 'pavillions': 9912, 'paycheck': 9913, 'pearly': 9914, 'peepz': 9915, 'peonies': 9916, 'percent': 9917, 'perfection': 9918, 'permanent': 9919, 'persian': 9920, 'personaldevelopment': 9921, 'persuade': 9922, 'phd': 9923, 'phelps': 9924, 'phew': 9925, 'pickles': 9926, 'picstitch': 9927, 'pilots': 9928, 'pin': 9929, 'pineapple': 9930, 'pint': 9931, 'pinterest': 9932, 'pity': 9933, 'pjs': 9934, 'playoffs': 9935, 'playstation': 9936, 'playtime': 9937, 'png': 9938, 'pnw': 9939, 'pocket': 9940, 'pol': 9941, 'polaroid': 9942, 'poolside': 9943, 'popcorn': 9944, 'popped': 9945, 'poppy': 9946, 'pork': 9947, 'positively': 9948, 'postion': 9949, 'poussey': 9950, 'pov': 9951, 'ppls': 9952, 'prayforchristina': 9953, 'prayforoakland': 9954, 'prepped': 9955, 'presentations': 9956, 'presidency': 9957, 'pressed': 9958, 'preventable': 9959, 'previously': 9960, 'primaries': 9961, 'printed': 9962, 'printing': 9963, 'prior': 9964, 'priority': 9965, 'prizes': 9966, 'probab': 9967, 'procrastinate': 9968, 'prof': 9969, 'prolly': 9970, 'promised': 9971, 'protected': 9972, 'protecting': 9973, 'protestors': 9974, 'proving': 9975, 'psalm': 9976, 'psychic': 9977, 'published': 9978, 'puglife': 9979, 'pugs': 9980, 'pulsenightclubshooting': 9981, 'pune': 9982, 'punished': 9983, 'puppets': 9984, 'pursue': 9985, 'python': 9986, 'qb': 9987, 'quad': 9988, 'queenie': 9989, 'quillen': 9990, 'rainnyday': 9991, 'rainyday': 9992, 'rajan': 9993, 'ramdan': 9994, 'ramon': 9995, 'ramzan': 9996, 'raped': 9997, 'rapflashback': 9998, 'rappers': 9999, 'rationalize': 10000, 'rave': 10001, 'raves': 10002, 'reaches': 10003, 'reaching': 10004, 'reachout': 10005, 'realbeauty': 10006, 'realizing': 10007, 'reassuring': 10008, 'rebel': 10009, 'recipes': 10010, 'reconsider': 10011, 'redneck': 10012, 'refreshed': 10013, 'refuse': 10014, 'regards': 10015, 'region': 10016, 'register': 10017, 'regram': 10018, 'regular': 10019, 'reignofkong': 10020, 'relative': 10021, 'relieve': 10022, 'relieved': 10023, 'remarkable': 10024, 'reminded': 10025, 'remotely': 10026, 'removes': 10027, 'renaissance': 10028, 'renovation': 10029, 'rent': 10030, 'repeating': 10031, 'repoing': 10032, 'repos': 10033, 'reservation': 10034, 'restaurants': 10035, 'retailtherapy': 10036, 'returned': 10037, 'retweeting': 10038, 'revenue': 10039, 'revival': 10040, 'rewarding': 10041, 'rewards': 10042, 'rhythm': 10043, 'rihanna': 10044, 'rising': 10045, 'rite': 10046, 'robbie': 10047, 'robbins': 10048, 'rocking': 10049, 'rocknroll': 10050, 'roger': 10051, 'rom': 10052, 'romania': 10053, 'roosevelt': 10054, 'ross': 10055, 'roy': 10056, 'rssxactaccounts': 10057, 'rub': 10058, 'rul': 10059, 'runhappy': 10060, 'rus': 10061, 'rush': 10062, 'russians': 10063, 'sacramento': 10064, 'safely': 10065, 'sailing': 10066, 'saint': 10067, 'saints': 10068, 'salary': 10069, 'salut': 10070, 'samsung': 10071, 'sandler': 10072, 'satnight': 10073, 'saves': 10074, 'sc': 10075, 'scan': 10076, 'scarf': 10077, 'scenic': 10078, 'scheduled': 10079, 'schnauzer': 10080, 'schoolofrock': 10081, 'scuba': 10082, 'sd': 10083, 'seductive': 10084, 'seemed': 10085, 'selca': 10086, 'select': 10087, 'selenagomez': 10088, 'selfcare': 10089, 'selfhelp': 10090, 'selfrespect': 10091, 'seminar': 10092, 'senator': 10093, 'sequence': 10094, 'served': 10095, 'seth': 10096, 'settle': 10097, 'several': 10098, 'sexest': 10099, 'shakespeare': 10100, 'shakur': 10101, 'shell': 10102, 'ship': 10103, 'shipping': 10104, 'shitshow': 10105, 'shofilm': 10106, 'shoplocal': 10107, 'shopper': 10108, 'siblings': 10109, 'signal': 10110, 'silveragatka': 10111, 'singers': 10112, 'singles': 10113, 'sinking': 10114, 'sisterhood': 10115, 'skill': 10116, 'skilled': 10117, 'skype': 10118, 'sl': 10119, 'sleepless': 10120, 'sleepover': 10121, 'slime': 10122, 'smaccdub': 10123, 'smallbiz': 10124, 'smores': 10125, 'snacks': 10126, 'snails': 10127, 'snake': 10128, 'snapchater': 10129, 'snatched': 10130, 'sno': 10131, 'soap': 10132, 'socialanxiety': 10133, 'socialists': 10134, 'socute': 10135, 'soexcited': 10136, 'someday': 10137, 'somethin': 10138, 'songwriting': 10139, 'sooner': 10140, 'sore': 10141, 'sorrow': 10142, 'sosad': 10143, 'soulmate': 10144, 'soulmates': 10145, 'sounding': 10146, 'soup': 10147, 'southbeach': 10148, 'specialday': 10149, 'spice': 10150, 'spicy': 10151, 'sponsoring': 10152, 'sponsors': 10153, 'spots': 10154, 'spray': 10155, 'sprayed': 10156, 'spreading': 10157, 'spree': 10158, 'springfield': 10159, 'squadgoals': 10160, 'squeaking': 10161, 'squid': 10162, 'stamford': 10163, 'stan': 10164, 'starawards': 10165, 'staups': 10166, 'stayed': 10167, 'stealing': 10168, 'steam': 10169, 'stepdad': 10170, 'sterling': 10171, 'stil': 10172, 'sto': 10173, 'stockmusic': 10174, 'stopgunviolence': 10175, 'strangers': 10176, 'streaming': 10177, 'stressed': 10178, 'strip': 10179, 'stripes': 10180, 'striving': 10181, 'strolling': 10182, 'struggle': 10183, 'stubborn': 10184, 'studios': 10185, 'stunt': 10186, 'stylehausboutique': 10187, 'stylistic': 10188, 'su': 10189, 'subscribe': 10190, 'succubus': 10191, 'suckin': 10192, 'sucking': 10193, 'suffered': 10194, 'suits': 10195, 'sum': 10196, 'sunbathing': 10197, 'sunnies': 10198, 'sunsets': 10199, 'superstar': 10200, 'surf': 10201, 'surgery': 10202, 'surprising': 10203, 'susan': 10204, 'suspect': 10205, 'sweater': 10206, 'sweetest': 10207, 'swimsuit': 10208, 'switch': 10209, 'symptoms': 10210, 'tablet': 10211, 'tacos': 10212, 'tactics': 10213, 'tanhai': 10214, 'tanks': 10215, 'tanned': 10216, 'tapas': 10217, 'tasting': 10218, 'tdl': 10219, 'tease': 10220, 'teletubbiesusa': 10221, 'temporary': 10222, 'tend': 10223, 'teresa': 10224, 'terrific': 10225, 'terrified': 10226, 'testimony': 10227, 'tgifriday': 10228, 'tgifridays': 10229, 'thankfulthursday': 10230, 'thanking': 10231, 'theatres': 10232, 'thefosters': 10233, 'thegoodlife': 10234, 'themusketeers': 10235, 'thesis': 10236, 'thick': 10237, 'thirst': 10238, 'thoughtleaders': 10239, 'thoughtsandprayers': 10240, 'threw': 10241, 'thriller': 10242, 'thrilling': 10243, 'throne': 10244, 'ties': 10245, 'tim': 10246, 'timeline': 10247, 'tinyplanetbuf': 10248, 'titles': 10249, 'tix': 10250, 'tmrw': 10251, 'tn': 10252, 'tnx': 10253, 'toa': 10254, 'todd': 10255, 'togetherness': 10256, 'ton': 10257, 'toned': 10258, 'topics': 10259, 'totellthetruth': 10260, 'touched': 10261, 'tourism': 10262, 'toyota': 10263, 'tradition': 10264, 'traditions': 10265, 'trainers': 10266, 'trainhard': 10267, 'trans': 10268, 'travelgram': 10269, 'travelingram': 10270, 'traveltuesday': 10271, 'treating': 10272, 'triathlon': 10273, 'tricks': 10274, 'triumph': 10275, 'troopingthecolour': 10276, 'trusted': 10277, 'truthful': 10278, 'tules': 10279, 'tumblr': 10280, 'tuning': 10281, 'turner': 10282, 'tutorial': 10283, 'twist': 10284, 'tx': 10285, 'tym': 10286, 'ufeellucky': 10287, 'uh': 10288, 'ukulele': 10289, 'unbreakable': 10290, 'unchanged': 10291, 'unconditional': 10292, 'unexpected': 10293, 'unlikely': 10294, 'unnecessary': 10295, 'unprofessional': 10296, 'unstable': 10297, 'untill': 10298, 'uploaded': 10299, 'uploading': 10300, 'urself': 10301, 'utter': 10302, 'uv': 10303, 'validation': 10304, 'valuable': 10305, 'veggie': 10306, 'veggies': 10307, 'ventura': 10308, 'verona': 10309, 'versatile': 10310, 'verse': 10311, 'vet': 10312, 'veterans': 10313, 'vets': 10314, 'victorious': 10315, 'videoclip': 10316, 'vinci': 10317, 'visa': 10318, 'vitiating': 10319, 'vitorr': 10320, 'viual': 10321, 'vlogger': 10322, 'vm': 10323, 'vodka': 10324, 'voltaire': 10325, 'volunteering': 10326, 'vox': 10327, 'vscodaily': 10328, 'vulnerable': 10329, 'wacky': 10330, 'wakeupamerica': 10331, 'wal': 10332, 'walkies': 10333, 'wallpaper': 10334, 'wallpapers': 10335, 'walt': 10336, 'wanting': 10337, 'wardrobe': 10338, 'warned': 10339, 'warrior': 10340, 'wash': 10341, 'wat': 10342, 'watchthisspace': 10343, 'watering': 10344, 'waving': 10345, 'wayne': 10346, 'wd': 10347, 'webcammodel': 10348, 'webdesign': 10349, 'webster': 10350, 'weddingcountdown': 10351, 'weddingpay': 10352, 'weekender': 10353, 'weighed': 10354, 'welfare': 10355, 'wesley': 10356, 'westpac': 10357, 'whatajoke': 10358, 'wheel': 10359, 'whispers': 10360, 'whistling': 10361, 'wifi': 10362, 'willow': 10363, 'wind': 10364, 'windows10': 10365, 'windy': 10366, 'wives': 10367, 'wknd': 10368, 'wme': 10369, 'wmy': 10370, 'wohooo': 10371, 'wonderfully': 10372, 'wondering': 10373, 'woods': 10374, 'wordpress': 10375, 'worker': 10376, 'workshops': 10377, 'worries': 10378, 'wrapping': 10379, 'wrecked': 10380, 'wrestling': 10381, 'wti': 10382, 'xxxx': 10383, 'yard': 10384, 'yayyyy': 10385, 'yelp': 10386, 'yg': 10387, 'yogalove': 10388, 'youngforever': 10389, 'youuu': 10390, 'yul': 10391, 'zombie': 10392, 'zomg': 10393, 'zoological': 10394, 'zootopia': 10395})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "#set batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load an iterator\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.clean_tweet),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "qG4xwKapjnza"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        self.embedding_dim = embedding_dim\n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=True, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True, \n",
        "                                                            enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        out_forward = output[range(len(output)), text_lengths - 1, :self.embedding_dim]\n",
        "        out_reverse = output[:, 0, self.embedding_dim:]\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "\n",
        "        dense_outputs=self.fc(out_reduced)\n",
        "        # dense_outputs = torch.squeeze(dense_outputs, 1)\n",
        "        #Final activation function\n",
        "        outputs=F.log_softmax(dense_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "VKSGkL1skcgT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_of_vocab = len(TEXT.vocab)\n",
        "embedding_dim = 25\n",
        "num_hidden_nodes = 20\n",
        "num_output_nodes = 1\n",
        "num_layers = 2\n",
        "bidirection = True\n",
        "dropout = 0.2\n",
        "\n",
        "#instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)"
      ],
      "metadata": {
        "id": "3p0YFjHvjzd2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#architecture\n",
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Initialize the pretrained embedding\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4BUZ_a6kkfr",
        "outputId": "3b8ff3d8-579c-43c6-a14f-b3e91aba60b4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(10396, 25)\n",
            "  (lstm): LSTM(25, 20, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 277,381 trainable parameters\n",
            "torch.Size([10396, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    # print(preds)\n",
        "    # print(y)\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "\n",
        "    accuracy = 0.0\n",
        "    for i, y_ in enumerate(preds):\n",
        "      if y_ == y[i]:\n",
        "          accuracy += 1.0   \n",
        "      accuracy = accuracy / len(preds)\n",
        "      #print('Test Accuracy: ', accuracy)\n",
        "              \n",
        "      # print('Classification Report:')\n",
        "      # print(classification_report(y, preds, labels=[1,0], digits=4))\n",
        "      \n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "QxF8RWfYkst1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    #set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        #resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        #retrieve text and no. of words\n",
        "        text, text_lengths = batch.clean_tweet   \n",
        "        \n",
        "        #convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze()  \n",
        "        \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        #compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "sIrwEHCMmi2J"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    iter = 0\n",
        "\n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            iter += 1\n",
        "            #retrieve text and no. of words\n",
        "            text, text_lengths = batch.clean_tweet\n",
        "            \n",
        "            #convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze()\n",
        "            \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            if iter % 100:\n",
        "              print(\"Loss\", loss)\n",
        "              print(\"Acc\", acc)\n",
        "            \n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "THxrq5RomkQz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPbif2BRmol4",
        "outputId": "c174562a-e677-470f-c152-b79432a729b4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(35.9375)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(28.1250)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(60.9375)\n",
            "Acc tensor(0.3906)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(67.1875)\n",
            "Acc tensor(0.3281)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(64.0625)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(32.8125)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(27.2727)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 50.113 | Train Acc: 49.89%\n",
            "\t Val. Loss: 49.168 |  Val. Acc: 50.83%\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(35.9375)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(28.1250)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(60.9375)\n",
            "Acc tensor(0.3906)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(67.1875)\n",
            "Acc tensor(0.3281)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(64.0625)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(32.8125)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(27.2727)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 50.109 | Train Acc: 49.89%\n",
            "\t Val. Loss: 49.168 |  Val. Acc: 50.83%\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(35.9375)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(28.1250)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(60.9375)\n",
            "Acc tensor(0.3906)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(67.1875)\n",
            "Acc tensor(0.3281)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(64.0625)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(32.8125)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(27.2727)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 50.125 | Train Acc: 49.88%\n",
            "\t Val. Loss: 49.168 |  Val. Acc: 50.83%\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(35.9375)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(28.1250)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(60.9375)\n",
            "Acc tensor(0.3906)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(67.1875)\n",
            "Acc tensor(0.3281)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(64.0625)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(32.8125)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(27.2727)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 50.125 | Train Acc: 49.88%\n",
            "\t Val. Loss: 49.168 |  Val. Acc: 50.83%\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(23.4375)\n",
            "Acc tensor(0.7656)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(35.9375)\n",
            "Acc tensor(0.6406)\n",
            "tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(28.1250)\n",
            "Acc tensor(0.7188)\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(26.5625)\n",
            "Acc tensor(0.7344)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(31.2500)\n",
            "Acc tensor(0.6875)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(34.3750)\n",
            "Acc tensor(0.6562)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(60.9375)\n",
            "Acc tensor(0.3906)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(67.1875)\n",
            "Acc tensor(0.3281)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(64.0625)\n",
            "Acc tensor(0.3594)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(46.8750)\n",
            "Acc tensor(0.5312)\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(45.3125)\n",
            "Acc tensor(0.5469)\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(42.1875)\n",
            "Acc tensor(0.5781)\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(48.4375)\n",
            "Acc tensor(0.5156)\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
            "Loss tensor(56.2500)\n",
            "Acc tensor(0.4375)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "Loss tensor(57.8125)\n",
            "Acc tensor(0.4219)\n",
            "tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1.])\n",
            "Loss tensor(51.5625)\n",
            "Acc tensor(0.4844)\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "Loss tensor(54.6875)\n",
            "Acc tensor(0.4531)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
            "Loss tensor(65.6250)\n",
            "Acc tensor(0.3438)\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(62.5000)\n",
            "Acc tensor(0.3750)\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "Loss tensor(59.3750)\n",
            "Acc tensor(0.4062)\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(50.)\n",
            "Acc tensor(0.5000)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Loss tensor(37.5000)\n",
            "Acc tensor(0.6250)\n",
            "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "Loss tensor(53.1250)\n",
            "Acc tensor(0.4688)\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
            "Loss tensor(39.0625)\n",
            "Acc tensor(0.6094)\n",
            "tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(43.7500)\n",
            "Acc tensor(0.5625)\n",
            "tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "Loss tensor(32.8125)\n",
            "Acc tensor(0.6719)\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "Loss tensor(27.2727)\n",
            "Acc tensor(0.7273)\n",
            "\tTrain Loss: 50.105 | Train Acc: 49.90%\n",
            "\t Val. Loss: 49.168 |  Val. Acc: 50.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From last year\n"
      ],
      "metadata": {
        "id": "L2dJClqaX9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_full(model, test_loader, device='cpu'):\n",
        "    '''\n",
        "    Accepts the current best model and evaluates\n",
        "    the test dataset. Printing test accuracy and\n",
        "    an sklearn confusion matrix report.\n",
        "    \n",
        "    Inputs:\n",
        "    - model: PyTorch model object, the current best model.\n",
        "    - test_loader: an iterator with test data.\n",
        "    - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "    \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        Prints test accuracy.\n",
        "        Prints an Accuracy / F1 Report (sklearn)\n",
        "        Prints a Confusion Matrix (sklearn & matplotlib)\n",
        "    '''\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for ((text, text_len),labels) in test_loader:           \n",
        "            labels = labels.to(device)\n",
        "            print(labels)\n",
        "            text = text.to(device)\n",
        "            text_len = text_len.to(device)\n",
        "            output = model(text, text_len)            \n",
        "            output = torch.max(output, axis=1).indices\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "    print(y_pred)\n",
        "    print(y_true)\n",
        "    acc = 0.0\n",
        "    for i, y in enumerate(y_pred):\n",
        "        if y == y_true[i]:\n",
        "            acc += 1.0\n",
        "    \n",
        "    acc = acc / len(y_pred)\n",
        "    \n",
        "    \n",
        "    print('Test Accuracy: ', acc)\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['1','0'])\n",
        "    ax.yaxis.set_ticklabels(['1','0'])\n",
        "    \n",
        "    return None"
      ],
      "metadata": {
        "id": "qJLkmIllJGuT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "    '''\n",
        "    Used in train_model() to save the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - save_path: string, where to save model specs.\n",
        "        - model: a PyTorch model object.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - train_loader: the iterator with training data.\n",
        "        - valid_loss: float, current model loss.\n",
        "        \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's state dictionary to designated\n",
        "        file path as 'model.pt'\n",
        "    '''\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer, device='cpu'):\n",
        "    '''\n",
        "    Used in evaluate() to load the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - load_path: string, where to save model specs.\n",
        "        - model: a PyTorch model object.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "        \n",
        "    Returns: float, the models last validation loss.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Loads the saved state at load_path\n",
        "        into the current model object\n",
        "    '''\n",
        "    if load_path==None:\n",
        "        return\n",
        "\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "    '''\n",
        "    Used in train_model() to save the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - save_path: string, where to save model specs.\n",
        "        - train_loss_list: list of float, current model training losses.\n",
        "        - valid_loss_list: list of float, current model validation losses.\n",
        "        - global_steps_list: list of loss function calculation steps.\n",
        "        \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's loss history designated\n",
        "        file path as 'metrics.pt'\n",
        "    '''\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path, device='cpu'):\n",
        "    '''\n",
        "    Used in evaluate() to load the current best model.\n",
        "    \n",
        "    Inputs:\n",
        "        - load_path: string, where find model specs.\n",
        "        - device: string, 'cpu' or 'cuda' if using google colab.\n",
        "    Returns: tuple (train_loss_list, valid_loss_list, global_steps_list)\n",
        "        - train_loss_list: list of float, current model training losses.\n",
        "        - valid_loss_list: list of float, current model validation losses.\n",
        "        - global_steps_list: list of loss function calculation steps.\n",
        "    '''\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return (state_dict['train_loss_list'], \n",
        "            state_dict['valid_loss_list'], \n",
        "            state_dict['global_steps_list'])\n",
        "        "
      ],
      "metadata": {
        "id": "WdKALlcEAE6K"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                optimizer,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                criterion = nn.CrossEntropyLoss(),\n",
        "                num_epochs = 2,\n",
        "                file_path = './data',\n",
        "                best_valid_loss = float(\"Inf\"),\n",
        "                device='cpu'):\n",
        "    '''\n",
        "    Runs training and evaluation on validation data loop over specified number of epochs.\n",
        "    \n",
        "    Inputs:\n",
        "        - model: a PyTorch model object. In this case, the one from lstm.py.\n",
        "        - optimizer: a PyTorch Optimizer to be used.\n",
        "        - train_loader: the iterator with training data.\n",
        "        - valid_loader: the iterator with validation data.\n",
        "        - criterion: a PyTorch loss function instance.\n",
        "        - num_epochs: int, how many epochs to train over.\n",
        "        - file_path: string, where to save model specs.\n",
        "        - best_valid_loss: float, defaults to infinity to start training from scratch, \n",
        "            but if continuing from a prior run and wish to only save better outcomes,\n",
        "            you can pass the last best model's loss.\n",
        "        - device: string, 'cpu' or 'cuda' if running on google colab.\n",
        "    \n",
        "    Returns: None.\n",
        "    \n",
        "    Other Effects:\n",
        "        - Saves the best model's state dictionary to designated\n",
        "        file path as 'model.pt'\n",
        "        - Saves loss history to designated file path as '/metrics.pt'\n",
        "    '''\n",
        "    \n",
        "    eval_every = len(train_loader) // 2\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for ((text, text_len), labels) in train_loader:           \n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "            text_len = text_len.to(device)\n",
        "            output = model(text, text_len).squeeze()\n",
        "            loss = criterion(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  for ((text, text_len), labels) in valid_loader:\n",
        "                        labels = labels.to(device)\n",
        "                        text = text.to(device)\n",
        "                        text_len = text_len.to(device)\n",
        "                        output = model(text, text_len).squeeze()\n",
        "                        loss = criterion(output, labels)\n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    print(best_valid_loss)\n",
        "                    save_checkpoint('/model.pt', model, optimizer, best_valid_loss)\n",
        "                    save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print(train_loss_list, valid_loss_list, global_steps_list)\n",
        "    save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "    \n",
        "    return None"
      ],
      "metadata": {
        "id": "90YadA77AEk8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "SAVE_PATH = './data'\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "train_model(model=model,\n",
        "            optimizer=optimizer,\n",
        "            train_loader=train_iterator,\n",
        "            valid_loader=valid_iterator,\n",
        "            criterion = criterion,\n",
        "            num_epochs = 5,\n",
        "            file_path = SAVE_PATH,\n",
        "            device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeSf8_9cPc-n",
        "outputId": "74eabd98-4687-437c-b3ae-f5774998bd89"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [325/3250], Train Loss: 49.6490, Valid Loss: 49.1680\n",
            "49.168019485473636\n",
            "Model saved to ==> /model.pt\n",
            "Model saved to ==> /metrics.pt\n",
            "Epoch [1/5], Step [650/3250], Train Loss: 50.5769, Valid Loss: 49.1680\n",
            "Epoch [2/5], Step [975/3250], Train Loss: 50.0913, Valid Loss: 49.1680\n",
            "Epoch [2/5], Step [1300/3250], Train Loss: 50.1346, Valid Loss: 49.1680\n",
            "Epoch [3/5], Step [1625/3250], Train Loss: 50.0962, Valid Loss: 49.1680\n",
            "Epoch [3/5], Step [1950/3250], Train Loss: 50.1298, Valid Loss: 49.1680\n",
            "Epoch [4/5], Step [2275/3250], Train Loss: 49.9712, Valid Loss: 49.1680\n",
            "Epoch [4/5], Step [2600/3250], Train Loss: 50.2388, Valid Loss: 49.1680\n",
            "Epoch [5/5], Step [2925/3250], Train Loss: 50.3269, Valid Loss: 49.1680\n",
            "Epoch [5/5], Step [3250/3250], Train Loss: 49.9151, Valid Loss: 49.1680\n",
            "[49.64903846153846, 50.57692307692308, 50.09134615384615, 50.13461538461539, 50.09615384615385, 50.12980769230769, 49.97115384615385, 50.2387820493258, 50.32692307692308, 49.91506409865159] [49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636, 49.168019485473636] [325, 650, 975, 1300, 1625, 1950, 2275, 2600, 2925, 3250]\n",
            "Model saved to ==> /metrics.pt\n",
            "Finished Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "best_model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
        "                   bidirectional = True, dropout = dropout)\n",
        "\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
        "\n",
        "load_checkpoint('/model.pt', best_model, optimizer)\n",
        "evaluate_full(best_model, test_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TH5Nm9aTRqT0",
        "outputId": "05d121dd-807c-44bb-e5f5-147cd596e54f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /model.pt\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "tensor([1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1.])\n",
            "tensor([0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
            "tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 1.])\n",
            "tensor([0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
            "tensor([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
            "tensor([1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 0., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 1., 1., 0.])\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 0.])\n",
            "tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 0., 0., 1.])\n",
            "tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
            "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0.])\n",
            "tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
            "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
            "tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 1., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 0., 0.])\n",
            "tensor([1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
            "tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 0., 1., 1.])\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0.])\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
            "tensor([1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 1., 1.])\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 0., 1.])\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0.])\n",
            "tensor([0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 1., 1.])\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 1., 0., 1.])\n",
            "tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 1., 0.])\n",
            "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 1.])\n",
            "tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0.])\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 1., 1.])\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0.])\n",
            "tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 1.])\n",
            "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
            "tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1.])\n",
            "tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0.])\n",
            "tensor([1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
            "tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
            "tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
            "tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0.])\n",
            "tensor([1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0.])\n",
            "tensor([0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
            "tensor([1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
            "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0.])\n",
            "tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0.])\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1.])\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "tensor([0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1.])\n",
            "tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0.])\n",
            "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]\n",
            "Test Accuracy:  0.49932659932659934\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.0000    0.0000    0.0000      4461\n",
            "           0     0.4993    1.0000    0.6661      4449\n",
            "\n",
            "    accuracy                         0.4993      8910\n",
            "   macro avg     0.2497    0.5000    0.3330      8910\n",
            "weighted avg     0.2493    0.4993    0.3326      8910\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oERkAKwB_7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LxYmFMzu_6Um"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}