{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocab from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 8)\n",
      "(4794, 8)\n",
      "(4795, 8)\n"
     ]
    }
   ],
   "source": [
    "data_train_df = pd.read_csv('data/Twitter/hate_twitter/train_clean.csv')\n",
    "train, val, test = np.split(data_train_df.sample(frac=1, random_state=8),\\\n",
    "    [int(0.7*len(data_train_df)),int(0.85*len(data_train_df))])\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/Twitter/hate_twitter/hate_train.csv')\n",
    "val.to_csv('data/Twitter/hate_twitter/hate_val.csv')\n",
    "test.to_csv('data/Twitter/hate_twitter/hate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/Twitter/hate_twitter/hate_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg omg omg yay found wonderful price segasaturn throwback',\n",
       " 'payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place',\n",
       " 'trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo',\n",
       " 'turn resignation',\n",
       " 'happy bihday hajime hosogai bihday bihday 30']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = list(train_df[train_df['clean_tweet'].notna()]['clean_tweet'])\n",
    "data_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hash_tag</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>tokenized_tweet_NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>21970</td>\n",
       "      <td>21970</td>\n",
       "      <td>21971</td>\n",
       "      <td>0</td>\n",
       "      <td>@user not to me.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; not to me.</td>\n",
       "      <td>@user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>15494</td>\n",
       "      <td>15494</td>\n",
       "      <td>15495</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user me to!</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; me to!</td>\n",
       "      <td>@user @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>16320</td>\n",
       "      <td>16320</td>\n",
       "      <td>16321</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user ð very</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; ð very</td>\n",
       "      <td>@user @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>5028</td>\n",
       "      <td>5028</td>\n",
       "      <td>5029</td>\n",
       "      <td>0</td>\n",
       "      <td>@user as you should.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; as you should.</td>\n",
       "      <td>@user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5241</th>\n",
       "      <td>15434</td>\n",
       "      <td>15434</td>\n",
       "      <td>15435</td>\n",
       "      <td>0</td>\n",
       "      <td>ðð» that is all....</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ðð» that is all. &lt;repeat&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1     id  label                        tweet  \\\n",
       "875        21970         21970  21971      0           @user not to me.     \n",
       "2122       15494         15494  15495      0         @user @user me to!     \n",
       "3865       16320         16320  16321      0      @user @user ð very     \n",
       "4114        5028          5028   5029      0        @user as you should.    \n",
       "5241       15434         15434  15435      0    ðð» that is all....    \n",
       "\n",
       "     hash_tag clean_tweet                    tokenized_tweet  \\\n",
       "875        []         NaN                <user> not to me.     \n",
       "2122       []         NaN             <user> <user> me to!     \n",
       "3865       []         NaN          <user> <user> ð very     \n",
       "4114       []         NaN             <user> as you should.    \n",
       "5241       []         NaN    ðð» that is all. <repeat>    \n",
       "\n",
       "     tokenized_tweet_NLTK  \n",
       "875                 @user  \n",
       "2122          @user @user  \n",
       "3865          @user @user  \n",
       "4114                @user  \n",
       "5241                  NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funky behavior with bad tweets - do we want to capture some of these?\n",
    "train_df[train_df['clean_tweet'].isna()][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of line 0: omg omg omg yay found wonderful price segasaturn throwback\n",
      "Tokens of line 0: ['omg', 'omg', 'omg', 'yay', 'found', 'wonderful', 'price', 'segasaturn', 'throwback']\n",
      "String of line 1: payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place\n",
      "Tokens of line 1: ['payintheusa', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'place']\n",
      "String of line 2: trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo\n",
      "Tokens of line 2: ['trainhard', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'places', 'lo']\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('data/Twitter/hate_twitter/')  # Modify the path of `data_dir` as needed.\n",
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "\n",
    "# create unigram vocab\n",
    "for i, line in enumerate(data_train):\n",
    "    #print(i)\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    counter.update(tokens)\n",
    "    if i < 3:\n",
    "        print(f\"String of line {i}: {line.strip()}\")\n",
    "        print(f\"Tokens of line {i}: {tokens}\")\n",
    "counter = dict(counter)\n",
    "\n",
    "vocab = {}\n",
    "# Populate the vocabulary with words that appear at least 3 times.\n",
    "for word, freq in counter.items():\n",
    "    if freq < 3 and word not in ['<pad>', '<unk>']:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "output_filepath = data_dir.joinpath('unigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size before frequency filtering: 82684\n",
      "Vocab size after frequency filtering: 15495\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# create bigram vocab\n",
    "for i, line in enumerate(data_train):\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    # tokens = line.split()\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "    counter.update([tokens[i] + \" \" + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "counter = dict(counter)\n",
    "print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "for word, freq in list(counter.items()):\n",
    "    if freq < 3:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = data_dir.joinpath('bigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab, data_dir, feature_field, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Extract and save the vocab and features.\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    splits = ['train','test','val']\n",
    "    #splits = ['train']\n",
    "\n",
    "    gram, mode = feature_name.split('_')\n",
    "    if gram not in ['unigram', 'bigram'] or mode not in ['binary', 'count']:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for split in splits:\n",
    "        datapath = data_dir.joinpath(f'hate_{split}.csv')\n",
    "        print('datapath',datapath)\n",
    "        data_df = pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        features = list(data_df[feature_field])\n",
    "        \n",
    "        sent_lengths = []\n",
    "        values, rows, cols = [], [], []\n",
    "        labels = list(data_df['label'])\n",
    "        print(f\"\\nExtract {gram} {mode} features from {datapath}\")\n",
    "        for i, line in enumerate(features):\n",
    "            if i % 1000 == 1:\n",
    "                print(f\"Processing {i}/{len(features)} row\")\n",
    "            #label = int(line[0])\n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            # tokens = line[1:].strip().split(  )  # Tokenizing differently affects the results.\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "            tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "            if gram.find('bigram') != -1:\n",
    "                tokens.extend(\n",
    "                    [tokens[i] + ' ' + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "            feature = {}\n",
    "            for tk in tokens:\n",
    "                if tk not in vocab:\n",
    "                    continue\n",
    "                if mode == 'binary':\n",
    "                    feature[vocab[tk]] = 1\n",
    "                elif mode == 'count':\n",
    "                    feature[vocab[tk]] = feature.get(vocab[tk], 0) + 1\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            for j in feature:\n",
    "                values.append(feature[j])\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            sent_lengths.append(len(tokens))\n",
    "            #labels.append(label)\n",
    "\n",
    "        features = sparse.csr_matrix((values, (rows, cols)),\n",
    "                                     shape=(len(features), len(vocab)))\n",
    "        print(f\"{split} feature matrix shape: {features.shape}\")\n",
    "        output_feature_filepath = data_dir.joinpath(f'{split}_{gram}_{mode}_features.npz')\n",
    "        sparse.save_npz(output_feature_filepath, features)\n",
    "\n",
    "        np_labels = np.asarray(labels)\n",
    "        print(f\"{split} label array shape: {np_labels.shape}\")\n",
    "        output_label_filepath = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        np.savez(output_label_filepath, np_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/train_clean.csv\n",
      "data df cols Index(['Unnamed: 0', 'id', 'label', 'tweet', 'hash_tag', 'clean_tweet',\n",
      "       'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/train_clean.csv\n",
      "Processing 1/31932 row\n",
      "Processing 1001/31932 row\n",
      "Processing 2001/31932 row\n",
      "Processing 3001/31932 row\n",
      "Processing 4001/31932 row\n",
      "Processing 5001/31932 row\n",
      "Processing 6001/31932 row\n",
      "Processing 7001/31932 row\n",
      "Processing 8001/31932 row\n",
      "Processing 9001/31932 row\n",
      "Processing 10001/31932 row\n",
      "Processing 11001/31932 row\n",
      "Processing 12001/31932 row\n",
      "Processing 13001/31932 row\n",
      "Processing 14001/31932 row\n",
      "Processing 15001/31932 row\n",
      "Processing 16001/31932 row\n",
      "Processing 17001/31932 row\n",
      "Processing 18001/31932 row\n",
      "Processing 19001/31932 row\n",
      "Processing 20001/31932 row\n",
      "Processing 21001/31932 row\n",
      "Processing 22001/31932 row\n",
      "Processing 23001/31932 row\n",
      "Processing 24001/31932 row\n",
      "Processing 25001/31932 row\n",
      "Processing 26001/31932 row\n",
      "Processing 27001/31932 row\n",
      "Processing 28001/31932 row\n",
      "Processing 29001/31932 row\n",
      "Processing 30001/31932 row\n",
      "Processing 31001/31932 row\n",
      "train feature matrix shape: (31932, 7562)\n",
      "train label array shape: (31932,)\n",
      "datapath data/Twitter/hate_twitter/test_clean.csv\n",
      "data df cols Index(['Unnamed: 0', 'id', 'tweet', 'hash_tag', 'clean_tweet',\n",
      "       'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3359'>3360</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/Twitter/hate_twitter/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m vocab_filepath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/Twitter/hate_twitter/unigram_vocab.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m extract_features(vocab\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(vocab_filepath)),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m                  tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m                  feature_field\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclean_tweet\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=6'>7</a>\u001b[0m                  data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m                  feature_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munigram_binary\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb Cell 12'\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(vocab, data_dir, feature_field, tokenizer, feature_name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000004vscode-remote?line=33'>34</a>\u001b[0m sent_lengths \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000004vscode-remote?line=34'>35</a>\u001b[0m values, rows, cols \u001b[39m=\u001b[39m [], [], []\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000004vscode-remote?line=35'>36</a>\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_df[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000004vscode-remote?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mExtract \u001b[39m\u001b[39m{\u001b[39;00mgram\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m features from \u001b[39m\u001b[39m{\u001b[39;00mdatapath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/cleaning_scratchwork.ipynb#ch0000004vscode-remote?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(features):\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3455'>3456</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3456'>3457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3457'>3458</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3458'>3459</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3459'>3460</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3362'>3363</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3364'>3365</a>\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3365'>3366</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/train_clean.csv\n",
      "data df cols Index(['Unnamed: 0', 'id', 'label', 'tweet', 'hash_tag', 'clean_tweet',\n",
      "       'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/train_clean.csv\n",
      "Processing 1/31932 row\n",
      "Processing 1001/31932 row\n",
      "Processing 2001/31932 row\n",
      "Processing 3001/31932 row\n",
      "Processing 4001/31932 row\n",
      "Processing 5001/31932 row\n",
      "Processing 6001/31932 row\n",
      "Processing 7001/31932 row\n",
      "Processing 8001/31932 row\n",
      "Processing 9001/31932 row\n",
      "Processing 10001/31932 row\n",
      "Processing 11001/31932 row\n",
      "Processing 12001/31932 row\n",
      "Processing 13001/31932 row\n",
      "Processing 14001/31932 row\n",
      "Processing 15001/31932 row\n",
      "Processing 16001/31932 row\n",
      "Processing 17001/31932 row\n",
      "Processing 18001/31932 row\n",
      "Processing 19001/31932 row\n",
      "Processing 20001/31932 row\n",
      "Processing 21001/31932 row\n",
      "Processing 22001/31932 row\n",
      "Processing 23001/31932 row\n",
      "Processing 24001/31932 row\n",
      "Processing 25001/31932 row\n",
      "Processing 26001/31932 row\n",
      "Processing 27001/31932 row\n",
      "Processing 28001/31932 row\n",
      "Processing 29001/31932 row\n",
      "Processing 30001/31932 row\n",
      "Processing 31001/31932 row\n",
      "train feature matrix shape: (31932, 9789)\n",
      "train label array shape: (31932,)\n",
      "datapath data/Twitter/hate_twitter/train_clean.csv\n",
      "data df cols Index(['Unnamed: 0', 'id', 'label', 'tweet', 'hash_tag', 'clean_tweet',\n",
      "       'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/train_clean.csv\n",
      "Processing 1/31932 row\n",
      "Processing 1001/31932 row\n",
      "Processing 2001/31932 row\n",
      "Processing 3001/31932 row\n",
      "Processing 4001/31932 row\n",
      "Processing 5001/31932 row\n",
      "Processing 6001/31932 row\n",
      "Processing 7001/31932 row\n",
      "Processing 8001/31932 row\n",
      "Processing 9001/31932 row\n",
      "Processing 10001/31932 row\n",
      "Processing 11001/31932 row\n",
      "Processing 12001/31932 row\n",
      "Processing 13001/31932 row\n",
      "Processing 14001/31932 row\n",
      "Processing 15001/31932 row\n",
      "Processing 16001/31932 row\n",
      "Processing 17001/31932 row\n",
      "Processing 18001/31932 row\n",
      "Processing 19001/31932 row\n",
      "Processing 20001/31932 row\n",
      "Processing 21001/31932 row\n",
      "Processing 22001/31932 row\n",
      "Processing 23001/31932 row\n",
      "Processing 24001/31932 row\n",
      "Processing 25001/31932 row\n",
      "Processing 26001/31932 row\n",
      "Processing 27001/31932 row\n",
      "Processing 28001/31932 row\n",
      "Processing 29001/31932 row\n",
      "Processing 30001/31932 row\n",
      "Processing 31001/31932 row\n",
      "train feature matrix shape: (31932, 9789)\n",
      "train label array shape: (31932,)\n",
      "datapath data/Twitter/hate_twitter/train_clean.csv\n",
      "data df cols Index(['Unnamed: 0', 'id', 'label', 'tweet', 'hash_tag', 'clean_tweet',\n",
      "       'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/train_clean.csv\n",
      "Processing 1/31932 row\n",
      "Processing 1001/31932 row\n",
      "Processing 2001/31932 row\n",
      "Processing 3001/31932 row\n",
      "Processing 4001/31932 row\n",
      "Processing 5001/31932 row\n",
      "Processing 6001/31932 row\n",
      "Processing 7001/31932 row\n",
      "Processing 8001/31932 row\n",
      "Processing 9001/31932 row\n",
      "Processing 10001/31932 row\n",
      "Processing 11001/31932 row\n",
      "Processing 12001/31932 row\n",
      "Processing 13001/31932 row\n",
      "Processing 14001/31932 row\n",
      "Processing 15001/31932 row\n",
      "Processing 16001/31932 row\n",
      "Processing 17001/31932 row\n",
      "Processing 18001/31932 row\n",
      "Processing 19001/31932 row\n",
      "Processing 20001/31932 row\n",
      "Processing 21001/31932 row\n",
      "Processing 22001/31932 row\n",
      "Processing 23001/31932 row\n",
      "Processing 24001/31932 row\n",
      "Processing 25001/31932 row\n",
      "Processing 26001/31932 row\n",
      "Processing 27001/31932 row\n",
      "Processing 28001/31932 row\n",
      "Processing 29001/31932 row\n",
      "Processing 30001/31932 row\n",
      "Processing 31001/31932 row\n",
      "train feature matrix shape: (31932, 9789)\n",
      "train label array shape: (31932,)\n"
     ]
    }
   ],
   "source": [
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_count')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_count')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str,\n",
    "                                     tune: bool = False) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    tune : `bool`, optional\n",
    "        Whether or not to tune the hyperparameters of the regularization strength\n",
    "        of the model of the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    # Implement logistic regression with scikit-learn.\n",
    "    # Print out the accuracy scores on dev and test data.\n",
    "\n",
    "    #splits = ['train', 'test']\n",
    "    splits = ['train']\n",
    "    features, labels = {}, {}\n",
    "\n",
    "    for split in splits:\n",
    "        features_path = data_dir.joinpath(f'{split}_{feature_name}_features.npz')\n",
    "        labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        features[split] = sparse.load_npz(features_path)\n",
    "        labels[split] = np.load(labels_path)['arr_0']\n",
    "    best_dev, best_model = 0, None\n",
    "    if tune:\n",
    "        for c in np.linspace(-5, 5, 11):\n",
    "            clf = LogisticRegression(random_state=42,\n",
    "                                     max_iter=100,\n",
    "                                     fit_intercept=False,\n",
    "                                     C=np.exp2(c))\n",
    "            clf.fit(features['train'], labels['train'])\n",
    "            dev_preds = clf.predict(features['dev'])\n",
    "            dev_accuracy = accuracy_score(labels['dev'], dev_preds)\n",
    "            print(c, dev_accuracy)\n",
    "            if dev_accuracy > best_dev:\n",
    "                best_dev, best_model = dev_accuracy, clf\n",
    "    else:\n",
    "        best_model = LogisticRegression(random_state=42,\n",
    "                                        max_iter=100,\n",
    "                                        fit_intercept=False)\n",
    "        best_model.fit(features['train'], labels['train'])\n",
    "\n",
    "    preds = {\n",
    "        'dev': best_model.predict(features['dev']),\n",
    "        'test': best_model.predict(features['test'])\n",
    "    }\n",
    "    for splt, splt_preds in preds.items():\n",
    "        print(\"{} accuracy: {:.4f}\".format(splt, accuracy_score(labels[splt],\n",
    "                                                                splt_preds)))\n",
    "        print(\"{} macro f1: {:.4f}\".format(\n",
    "            splt, f1_score(labels[splt], splt_preds, average='macro')))\n",
    "\n",
    "    return best_model\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
