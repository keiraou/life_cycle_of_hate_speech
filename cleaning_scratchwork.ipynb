{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocab from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 8)\n",
      "(4794, 8)\n",
      "(4795, 8)\n"
     ]
    }
   ],
   "source": [
    "data_train_df = pd.read_csv('data/Twitter/hate_twitter/train_clean.csv')\n",
    "\n",
    "# using code from https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "train, val, test = np.split(data_train_df.sample(frac=1, random_state=8),\\\n",
    "    [int(0.7*len(data_train_df)),int(0.85*len(data_train_df))])\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/Twitter/hate_twitter/hate_train.csv')\n",
    "val.to_csv('data/Twitter/hate_twitter/hate_val.csv')\n",
    "test.to_csv('data/Twitter/hate_twitter/hate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/Twitter/hate_twitter/hate_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg omg omg yay found wonderful price segasaturn throwback',\n",
       " 'payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place',\n",
       " 'trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo',\n",
       " 'turn resignation',\n",
       " 'happy bihday hajime hosogai bihday bihday 30']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = list(train_df[train_df['clean_tweet'].notna()]['clean_tweet'])\n",
    "data_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hash_tag</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>tokenized_tweet_NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>21970</td>\n",
       "      <td>21970</td>\n",
       "      <td>21971</td>\n",
       "      <td>0</td>\n",
       "      <td>@user not to me.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; not to me.</td>\n",
       "      <td>@user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>15494</td>\n",
       "      <td>15494</td>\n",
       "      <td>15495</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user me to!</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; me to!</td>\n",
       "      <td>@user @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>16320</td>\n",
       "      <td>16320</td>\n",
       "      <td>16321</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user ð very</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; ð very</td>\n",
       "      <td>@user @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>5028</td>\n",
       "      <td>5028</td>\n",
       "      <td>5029</td>\n",
       "      <td>0</td>\n",
       "      <td>@user as you should.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; as you should.</td>\n",
       "      <td>@user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5241</th>\n",
       "      <td>15434</td>\n",
       "      <td>15434</td>\n",
       "      <td>15435</td>\n",
       "      <td>0</td>\n",
       "      <td>ðð» that is all....</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ðð» that is all. &lt;repeat&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1     id  label                        tweet  \\\n",
       "875        21970         21970  21971      0           @user not to me.     \n",
       "2122       15494         15494  15495      0         @user @user me to!     \n",
       "3865       16320         16320  16321      0      @user @user ð very     \n",
       "4114        5028          5028   5029      0        @user as you should.    \n",
       "5241       15434         15434  15435      0    ðð» that is all....    \n",
       "\n",
       "     hash_tag clean_tweet                    tokenized_tweet  \\\n",
       "875        []         NaN                <user> not to me.     \n",
       "2122       []         NaN             <user> <user> me to!     \n",
       "3865       []         NaN          <user> <user> ð very     \n",
       "4114       []         NaN             <user> as you should.    \n",
       "5241       []         NaN    ðð» that is all. <repeat>    \n",
       "\n",
       "     tokenized_tweet_NLTK  \n",
       "875                 @user  \n",
       "2122          @user @user  \n",
       "3865          @user @user  \n",
       "4114                @user  \n",
       "5241                  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funky behavior with bad tweets - do we want to capture some of these?\n",
    "train_df[train_df['clean_tweet'].isna()][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of line 0: omg omg omg yay found wonderful price segasaturn throwback\n",
      "Tokens of line 0: ['omg', 'omg', 'omg', 'yay', 'found', 'wonderful', 'price', 'segasaturn', 'throwback']\n",
      "String of line 1: payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place\n",
      "Tokens of line 1: ['payintheusa', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'place']\n",
      "String of line 2: trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo\n",
      "Tokens of line 2: ['trainhard', 'polar', 'bear', 'climb', 'racing', 'angry', 'polar', 'bear', 'climb', 'racing', 'polar', 'bear', 'living', 'cold', 'places', 'lo']\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('data/Twitter/hate_twitter/')  # Modify the path of `data_dir` as needed.\n",
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "\n",
    "# create unigram vocab\n",
    "tweet_lens  = []\n",
    "for i, line in enumerate(data_train):\n",
    "    #print(i)\n",
    "    tokens = tokenizer.tokenize(line.strip())\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tweet_lens.append(len(tokens))\n",
    "    counter.update(tokens)\n",
    "    if i < 3:\n",
    "        print(f\"String of line {i}: {line.strip()}\")\n",
    "        print(f\"Tokens of line {i}: {tokens}\")\n",
    "counter = dict(counter)\n",
    "\n",
    "vocab = {}\n",
    "# Populate the vocabulary with words that appear at least 3 times.\n",
    "for word, freq in counter.items():\n",
    "    if freq < 3 and word not in ['<pad>', '<unk>']:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "output_filepath = data_dir.joinpath('unigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tweet_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of line 0: omg omg omg yay found wonderful price segasaturn throwback\n",
      "Tokens of line 0: ['omg omg', 'omg omg', 'omg yay', 'yay found', 'found wonderful', 'wonderful price', 'price <unk>', '<unk> throwback']\n",
      "String of line 1: payintheusa polar bear climb racing angry polar bear climb racing polar bear living cold place\n",
      "Tokens of line 1: ['payintheusa polar', 'polar bear', 'bear climb', 'climb racing', 'racing angry', 'angry polar', 'polar bear', 'bear climb', 'climb racing', 'racing polar', 'polar bear', 'bear living', 'living cold', 'cold place']\n",
      "String of line 2: trainhard polar bear climb racing angry polar bear climb racing polar bear living cold places lo\n",
      "Tokens of line 2: ['trainhard polar', 'polar bear', 'bear climb', 'climb racing', 'racing angry', 'angry polar', 'polar bear', 'bear climb', 'climb racing', 'racing polar', 'polar bear', 'bear living', 'living cold', 'cold places', 'places lo']\n",
      "Vocab size before frequency filtering: 82684\n",
      "Vocab size after frequency filtering: 7933\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "counter = Counter()\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# create bigram vocab\n",
    "for i, line in enumerate(data_train):\n",
    "    tokens = tokenizer.tokenize(line.strip())\n",
    "    # tokens = line.split()\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "    bigram_lst = [tokens[i] + \" \" + tokens[i + 1] for i in range(len(tokens) - 1)]\n",
    "    counter.update([tokens[i] + \" \" + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "    if i < 3:\n",
    "        print(f\"String of line {i}: {line.strip()}\")\n",
    "        print(f\"Tokens of line {i}: {bigram_lst}\")\n",
    "counter = dict(counter)\n",
    "print(f\"Vocab size before frequency filtering: {len(counter)}\")\n",
    "\n",
    "vocab = {}\n",
    "for word, freq in list(counter.items()):\n",
    "    if freq < 3:\n",
    "        continue\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size after frequency filtering: {len(vocab)}\")\n",
    "output_filepath = data_dir.joinpath('bigram_vocab.json')\n",
    "json.dump(vocab, open(output_filepath, mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab, data_dir, feature_field, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Extract and save the vocab and features.\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    splits = ['train','test','val']\n",
    "    #splits = ['train']\n",
    "\n",
    "    gram, mode = feature_name.split('_')\n",
    "    if gram not in ['unigram', 'bigram'] or mode not in ['binary', 'count']:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for split in splits:\n",
    "        datapath = data_dir.joinpath(f'hate_{split}.csv')\n",
    "        print('datapath',datapath)\n",
    "        data_df = pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        features = list(data_df[feature_field])\n",
    "        \n",
    "        sent_lengths = []\n",
    "        values, rows, cols = [], [], []\n",
    "        labels = list(data_df['label'])\n",
    "        print(f\"\\nExtract {gram} {mode} features from {datapath}\")\n",
    "        for i, line in enumerate(features):\n",
    "            if i % 1000 == 1:\n",
    "                print(f\"Processing {i}/{len(features)} row\")\n",
    "            #label = int(line[0])\n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            # tokens = line[1:].strip().split(  )  # Tokenizing differently affects the results.\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "            tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "            if gram.find('bigram') != -1:\n",
    "                #print('yes bigram')\n",
    "                tokens.extend(\n",
    "                    [tokens[i] + ' ' + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "            feature = {}\n",
    "            for tk in tokens:\n",
    "                if tk not in vocab:\n",
    "                    continue\n",
    "                elif mode == 'binary':\n",
    "                    feature[vocab[tk]] = 1\n",
    "                elif mode == 'count':\n",
    "                    feature[vocab[tk]] = feature.get(vocab[tk], 0) + 1\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            for j in feature:\n",
    "                values.append(feature[j])\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            sent_lengths.append(len(tokens))\n",
    "            #labels.append(label)\n",
    "\n",
    "        features = sparse.csr_matrix((values, (rows, cols)),\n",
    "                                     shape=(len(features), len(vocab)))\n",
    "        print(f\"{split} feature matrix shape: {features.shape}\")\n",
    "        output_feature_filepath = data_dir.joinpath(f'{split}_{gram}_{mode}_features.npz')\n",
    "        sparse.save_npz(output_feature_filepath, features)\n",
    "\n",
    "        np_labels = np.asarray(labels)\n",
    "        print(f\"{split} label array shape: {np_labels.shape}\")\n",
    "        output_label_filepath = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        np.savez(output_label_filepath, np_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7562)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7562)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7562)\n",
      "val label array shape: (4790,)\n",
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7562)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7562)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7562)\n",
      "val label array shape: (4790,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7933)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7933)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7933)\n",
      "val label array shape: (4790,)\n",
      "datapath data/Twitter/hate_twitter/hate_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_train.csv\n",
      "Processing 1/22350 row\n",
      "Processing 1001/22350 row\n",
      "Processing 2001/22350 row\n",
      "Processing 3001/22350 row\n",
      "Processing 4001/22350 row\n",
      "Processing 5001/22350 row\n",
      "Processing 6001/22350 row\n",
      "Processing 7001/22350 row\n",
      "Processing 8001/22350 row\n",
      "Processing 9001/22350 row\n",
      "Processing 10001/22350 row\n",
      "Processing 11001/22350 row\n",
      "Processing 12001/22350 row\n",
      "Processing 13001/22350 row\n",
      "Processing 14001/22350 row\n",
      "Processing 15001/22350 row\n",
      "Processing 16001/22350 row\n",
      "Processing 17001/22350 row\n",
      "Processing 18001/22350 row\n",
      "Processing 19001/22350 row\n",
      "Processing 20001/22350 row\n",
      "Processing 21001/22350 row\n",
      "Processing 22001/22350 row\n",
      "train feature matrix shape: (22350, 7933)\n",
      "train label array shape: (22350,)\n",
      "datapath data/Twitter/hate_twitter/hate_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_test.csv\n",
      "Processing 1/4792 row\n",
      "Processing 1001/4792 row\n",
      "Processing 2001/4792 row\n",
      "Processing 3001/4792 row\n",
      "Processing 4001/4792 row\n",
      "test feature matrix shape: (4792, 7933)\n",
      "test label array shape: (4792,)\n",
      "datapath data/Twitter/hate_twitter/hate_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_val.csv\n",
      "Processing 1/4790 row\n",
      "Processing 1001/4790 row\n",
      "Processing 2001/4790 row\n",
      "Processing 3001/4790 row\n",
      "Processing 4001/4790 row\n",
      "val feature matrix shape: (4790, 7933)\n",
      "val label array shape: (4790,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/bigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_count')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str,\n",
    "                                     tune: bool = False) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    tune : `bool`, optional\n",
    "        Whether or not to tune the hyperparameters of the regularization strength\n",
    "        of the model of the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    # Implement logistic regression with scikit-learn.\n",
    "    # Print out the accuracy scores on dev and test data.\n",
    "\n",
    "    splits = ['train', 'val','test']\n",
    "    features, labels = {}, {}\n",
    "\n",
    "    for split in splits:\n",
    "        features_path = data_dir.joinpath(f'{split}_{feature_name}_features.npz')\n",
    "        labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "        features[split] = sparse.load_npz(features_path)\n",
    "        labels[split] = np.load(labels_path)['arr_0']\n",
    "    best_dev, best_model = 0, None\n",
    "    if tune:\n",
    "        for c in np.linspace(-5, 5, 11):\n",
    "            clf = LogisticRegression(random_state=42,\n",
    "                                     max_iter=100,\n",
    "                                     fit_intercept=False,\n",
    "                                     C=np.exp2(c))\n",
    "            clf.fit(features['train'], labels['train'])\n",
    "            dev_preds = clf.predict(features['val'])\n",
    "            dev_accuracy = accuracy_score(labels['val'], dev_preds)\n",
    "            print(c, dev_accuracy)\n",
    "            if dev_accuracy > best_dev:\n",
    "                best_dev, best_model = dev_accuracy, clf\n",
    "    else:\n",
    "        best_model = LogisticRegression(random_state=42,\n",
    "                                        max_iter=100,\n",
    "                                        fit_intercept=False)\n",
    "        best_model.fit(features['train'], labels['train'])\n",
    "\n",
    "    preds = {\n",
    "        'val': best_model.predict(features['val']),\n",
    "        'test': best_model.predict(features['test'])\n",
    "    }\n",
    "    for splt, splt_preds in preds.items():\n",
    "        print(\"{} accuracy: {:.4f}\".format(splt, accuracy_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} binary recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} macro recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds, \n",
    "                                                                average=\"macro\"))),\n",
    "        print(\"{} macro f1: {:.4f}\".format(\n",
    "            splt, f1_score(labels[splt], splt_preds, average='macro')))\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm bigram binary different from bigram count\n",
    "split = 'val'\n",
    "feature_name='bigram_binary'\n",
    "features = {}\n",
    "features_path = Path(data_dir).joinpath(f'{split}_{feature_name}_features.npz')\n",
    "#labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "features[split] = sparse.load_npz(features_path)\n",
    "features[split].toarray()[0:5,18:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 4787, 4788, 4789]),\n",
       " array([18, 18, 18, ..., 18, 18, 18]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices where nonzero\n",
    "np.nonzero(features[split].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [7],\n",
       "       [5],\n",
       "       [1],\n",
       "       [6]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm bigram binary different from bigram count\n",
    "split = 'val'\n",
    "feature_name='bigram_count'\n",
    "features = {}\n",
    "features_path = Path(data_dir).joinpath(f'{split}_{feature_name}_features.npz')\n",
    "#labels_path = data_dir.joinpath(f'{split}_labels.npz')\n",
    "features[split] = sparse.load_npz(features_path)\n",
    "features[split].toarray()[0:5,18:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9559\n",
      "val binary recall: 0.5145\n",
      "val macro recall: 0.7523\n",
      "val macro f1: 0.8016\n",
      "test accuracy: 0.9570\n",
      "test binary recall: 0.5296\n",
      "test macro recall: 0.7586\n",
      "test macro f1: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9547\n",
      "val binary recall: 0.5116\n",
      "val macro recall: 0.7503\n",
      "val macro f1: 0.7973\n",
      "test accuracy: 0.9558\n",
      "test binary recall: 0.5265\n",
      "test macro recall: 0.7565\n",
      "test macro f1: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9282\n",
      "val binary recall: 0.0000\n",
      "val macro recall: 0.5000\n",
      "val macro f1: 0.4814\n",
      "test accuracy: 0.9330\n",
      "test binary recall: 0.0000\n",
      "test macro recall: 0.5000\n",
      "test macro f1: 0.4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9282\n",
      "val binary recall: 0.0000\n",
      "val macro recall: 0.5000\n",
      "val macro f1: 0.4814\n",
      "test accuracy: 0.9330\n",
      "test binary recall: 0.0000\n",
      "test macro recall: 0.5000\n",
      "test macro f1: 0.4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show important weights\n",
    "\n",
    "Code heavily modeled on code from Chenhao Tan's Winter 2022 NLP Course at the University of Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_important_weights(weights, words):\n",
    "    \"\"\"\n",
    "    Print important pairs of weights and words.\n",
    "    # Parameters\n",
    "    weights : `Iterable`, required.\n",
    "        Weights from a learned model.\n",
    "    words : `Iterable`, required.\n",
    "        Word types of the vocabulary.  \n",
    "        It must be true that `len(weights) == len(words)`.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "\n",
    "    def print_pairs(pairs):\n",
    "        for weight, word in pairs:\n",
    "            print(\"{: .4f} | {}\".format(weight, word))\n",
    "\n",
    "    assert len(weights) == len(words)\n",
    "    pairs = list(zip(weights, words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
    "    print(\"Most hateful words:\")\n",
    "    print_pairs(pairs[:10])\n",
    "    print(\"\\nLeast hateful words:\")\n",
    "    print_pairs(reversed(pairs[-10:]))\n",
    "\n",
    "    pairs = list(zip(abs(weights), words))\n",
    "    pairs = sorted(pairs, key=lambda x: x[0], reverse=False)\n",
    "    print(\"\\nMost neutral words:\")\n",
    "    print_pairs(pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9559\n",
      "val binary recall: 0.5145\n",
      "val macro recall: 0.7523\n",
      "val macro f1: 0.8016\n",
      "test accuracy: 0.9570\n",
      "test binary recall: 0.5296\n",
      "test macro recall: 0.7586\n",
      "test macro f1: 0.8000\n",
      "\n",
      "Most hateful words:\n",
      " 4.2309 | allahsoil\n",
      " 3.1589 | racism\n",
      " 2.6945 | misogyny\n",
      " 2.6012 | bigot\n",
      " 2.5492 | 2017\n",
      " 2.5025 | white\n",
      " 2.2742 | latest\n",
      " 2.1515 | racist\n",
      " 2.1493 | treason\n",
      " 2.1267 | misogynist\n",
      "\n",
      "Least hateful words:\n",
      "-3.0948 | bihday\n",
      "-2.6052 | day\n",
      "-2.5491 | orlando\n",
      "-2.3904 | positive\n",
      "-2.3258 | healthy\n",
      "-2.2211 | friday\n",
      "-2.1652 | weekend\n",
      "-2.1620 | hardcore\n",
      "-2.1542 | thankful\n",
      "-2.1484 | days\n",
      "\n",
      "Most neutral words:\n",
      " 0.0000 | <pad>\n",
      " 0.0000 | remains\n",
      " 0.0001 | grandad\n",
      " 0.0001 | awasome\n",
      " 0.0001 | inwoo\n",
      " 0.0002 | selfrespect\n",
      " 0.0003 | titanic\n",
      " 0.0003 | lauren\n",
      " 0.0004 | ebay\n",
      " 0.0005 | systems\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "model_trained: LogisticRegression = fit_and_eval_logistic_regression(\n",
    "    feature_name='unigram_binary', data_dir=Path(data_dir), tune=False)\n",
    "weights = model_trained.coef_[0]\n",
    "vocab = json.load(open(vocab_filepath))\n",
    "print(\"\")\n",
    "print_important_weights(weights=weights, words=vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampled Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41608, 8)\n",
      "(8916, 8)\n",
      "(8916, 8)\n"
     ]
    }
   ],
   "source": [
    "upsampled_train_df = pd.read_csv('data/Twitter/hate_twitter/train_upsampled.csv')\n",
    "\n",
    "# using code from https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "train_up, val_up, test_up = np.split(upsampled_train_df.sample(frac=1, random_state=8),\\\n",
    "    [int(0.7*len(upsampled_train_df)),int(0.85*len(upsampled_train_df))])\n",
    "print(train_up.shape)\n",
    "print(val_up.shape)\n",
    "print(test_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hash_tag</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>tokenized_tweet_NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44049</th>\n",
       "      <td>15405</td>\n",
       "      <td>15406</td>\n",
       "      <td>0</td>\n",
       "      <td>@user we really are. lmaoo we are obviously re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>really lmaoo obviously related haha wonder cam...</td>\n",
       "      <td>&lt;user&gt; we really are. lmaoo we are obviously r...</td>\n",
       "      <td>@user really lmaoo obviously related haha wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33142</th>\n",
       "      <td>3684</td>\n",
       "      <td>3685</td>\n",
       "      <td>0</td>\n",
       "      <td>i think my #hea need a #bandage again ð   #...</td>\n",
       "      <td>['hea', 'bandage', 'love']</td>\n",
       "      <td>think hea need bandage love</td>\n",
       "      <td>i think my &lt;hashtag&gt; hea need a &lt;hashtag&gt; band...</td>\n",
       "      <td>think hea need bandage love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40964</th>\n",
       "      <td>12093</td>\n",
       "      <td>12094</td>\n",
       "      <td>0</td>\n",
       "      <td>new phone #xperiaz3+ #xperiaz4 #sony #thebeast...</td>\n",
       "      <td>['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...</td>\n",
       "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
       "      <td>new phone &lt;hashtag&gt; xperiaz&lt;number&gt;+ &lt;hashtag&gt;...</td>\n",
       "      <td>new phone xperiaz3 xperiaz4 sony thebeast copp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53102</th>\n",
       "      <td>25149</td>\n",
       "      <td>25150</td>\n",
       "      <td>0</td>\n",
       "      <td>#dinner at my friend's #steak #shop never #d...</td>\n",
       "      <td>['dinner', 'steak', 'shop', 'disappoint', 'yum...</td>\n",
       "      <td>dinner friends steak shop never disappoint yum...</td>\n",
       "      <td>&lt;hashtag&gt; dinner at my friend's &lt;hashtag&gt; st...</td>\n",
       "      <td>dinner friend's steak shop never disappoint yu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>12744</td>\n",
       "      <td>12745</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user @user and sorry if we are ethnical...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sorry ethnically cleansing east jerusalem beth...</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; and sorry if we are ethni...</td>\n",
       "      <td>@user @user @user sorry ethnically cleansing e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id  label  \\\n",
       "44049       15405  15406      0   \n",
       "33142        3684   3685      0   \n",
       "40964       12093  12094      0   \n",
       "53102       25149  25150      0   \n",
       "15142       12744  12745      1   \n",
       "\n",
       "                                                   tweet  \\\n",
       "44049  @user we really are. lmaoo we are obviously re...   \n",
       "33142  i think my #hea need a #bandage again ð   #...   \n",
       "40964  new phone #xperiaz3+ #xperiaz4 #sony #thebeast...   \n",
       "53102    #dinner at my friend's #steak #shop never #d...   \n",
       "15142  @user @user @user and sorry if we are ethnical...   \n",
       "\n",
       "                                                hash_tag  \\\n",
       "44049                                                 []   \n",
       "33142                         ['hea', 'bandage', 'love']   \n",
       "40964  ['xperiaz3', 'xperiaz4', 'sony', 'thebeast', '...   \n",
       "53102  ['dinner', 'steak', 'shop', 'disappoint', 'yum...   \n",
       "15142                                                 []   \n",
       "\n",
       "                                             clean_tweet  \\\n",
       "44049  really lmaoo obviously related haha wonder cam...   \n",
       "33142                        think hea need bandage love   \n",
       "40964  new phone xperiaz3 xperiaz4 sony thebeast copp...   \n",
       "53102  dinner friends steak shop never disappoint yum...   \n",
       "15142  sorry ethnically cleansing east jerusalem beth...   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "44049  <user> we really are. lmaoo we are obviously r...   \n",
       "33142  i think my <hashtag> hea need a <hashtag> band...   \n",
       "40964  new phone <hashtag> xperiaz<number>+ <hashtag>...   \n",
       "53102    <hashtag> dinner at my friend's <hashtag> st...   \n",
       "15142  <user> <user> <user> and sorry if we are ethni...   \n",
       "\n",
       "                                    tokenized_tweet_NLTK  \n",
       "44049  @user really lmaoo obviously related haha wond...  \n",
       "33142                        think hea need bandage love  \n",
       "40964  new phone xperiaz3 xperiaz4 sony thebeast copp...  \n",
       "53102  dinner friend's steak shop never disappoint yu...  \n",
       "15142  @user @user @user sorry ethnically cleansing e...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_train.csv')\n",
    "val_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_val.csv')\n",
    "test_up.to_csv('data/Twitter/hate_twitter/hate_upsampled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab, data_dir, feature_field, tokenizer, feature_name):\n",
    "    \"\"\"\n",
    "    Extract and save different features based on vocab of the features.\n",
    "    # Parameters\n",
    "    vocab : `dict[str, int]`, required.\n",
    "        A map from the word type to the index of the word.\n",
    "    data_dir : `Path`, required.\n",
    "        Directory of the dataset\n",
    "    tokenizer : `Callable`, required.\n",
    "        Tokenizer with a method `.tokenize` which returns list of tokens.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    # Returns\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Extract and save the vocab and features.\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    splits = ['train','test','val']\n",
    "    #splits = ['train']\n",
    "\n",
    "    gram, mode = feature_name.split('_')\n",
    "    if gram not in ['unigram', 'bigram'] or mode not in ['binary', 'count']:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for split in splits:\n",
    "        datapath = data_dir.joinpath(f'hate_upsampled_{split}.csv')\n",
    "        print('datapath',datapath)\n",
    "        data_df = pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        features = list(data_df[feature_field])\n",
    "        \n",
    "        sent_lengths = []\n",
    "        values, rows, cols = [], [], []\n",
    "        labels = list(data_df['label'])\n",
    "        print(f\"\\nExtract {gram} {mode} features from {datapath}\")\n",
    "        for i, line in enumerate(features):\n",
    "            if i % 1000 == 1:\n",
    "                print(f\"Processing {i}/{len(features)} row\")\n",
    "            #label = int(line[0])\n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            # tokens = line[1:].strip().split(  )  # Tokenizing differently affects the results.\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "            tokens = [t if t in vocab else '<unk>' for t in tokens]\n",
    "            if gram.find('bigram') != -1:\n",
    "                #print('yes bigram')\n",
    "                tokens.extend(\n",
    "                    [tokens[i] + ' ' + tokens[i + 1] for i in range(len(tokens) - 1)])\n",
    "            feature = {}\n",
    "            for tk in tokens:\n",
    "                if tk not in vocab:\n",
    "                    continue\n",
    "                elif mode == 'binary':\n",
    "                    feature[vocab[tk]] = 1\n",
    "                elif mode == 'count':\n",
    "                    feature[vocab[tk]] = feature.get(vocab[tk], 0) + 1\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            for j in feature:\n",
    "                values.append(feature[j])\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "            sent_lengths.append(len(tokens))\n",
    "            #labels.append(label)\n",
    "\n",
    "        features = sparse.csr_matrix((values, (rows, cols)),\n",
    "                                     shape=(len(features), len(vocab)))\n",
    "        print(f\"{split} feature matrix shape: {features.shape}\")\n",
    "        output_feature_filepath = data_dir.joinpath(f'{split}_{gram}_{mode}_upsamp_features.npz')\n",
    "        sparse.save_npz(output_feature_filepath, features)\n",
    "\n",
    "        np_labels = np.asarray(labels)\n",
    "        print(f\"{split} label array shape: {np_labels.shape}\")\n",
    "        output_label_filepath = data_dir.joinpath(f'{split}_upsamp_labels.npz')\n",
    "        np.savez(output_label_filepath, np_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7562)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7562)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram binary features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7562)\n",
      "val label array shape: (8907,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7562)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7562)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract unigram count features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7562)\n",
      "val label array shape: (8907,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='unigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7933)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7933)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram binary features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7933)\n",
      "val label array shape: (8907,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_train.csv\n",
      "Processing 1/41560 row\n",
      "Processing 1001/41560 row\n",
      "Processing 2001/41560 row\n",
      "Processing 3001/41560 row\n",
      "Processing 4001/41560 row\n",
      "Processing 5001/41560 row\n",
      "Processing 6001/41560 row\n",
      "Processing 7001/41560 row\n",
      "Processing 8001/41560 row\n",
      "Processing 9001/41560 row\n",
      "Processing 10001/41560 row\n",
      "Processing 11001/41560 row\n",
      "Processing 12001/41560 row\n",
      "Processing 13001/41560 row\n",
      "Processing 14001/41560 row\n",
      "Processing 15001/41560 row\n",
      "Processing 16001/41560 row\n",
      "Processing 17001/41560 row\n",
      "Processing 18001/41560 row\n",
      "Processing 19001/41560 row\n",
      "Processing 20001/41560 row\n",
      "Processing 21001/41560 row\n",
      "Processing 22001/41560 row\n",
      "Processing 23001/41560 row\n",
      "Processing 24001/41560 row\n",
      "Processing 25001/41560 row\n",
      "Processing 26001/41560 row\n",
      "Processing 27001/41560 row\n",
      "Processing 28001/41560 row\n",
      "Processing 29001/41560 row\n",
      "Processing 30001/41560 row\n",
      "Processing 31001/41560 row\n",
      "Processing 32001/41560 row\n",
      "Processing 33001/41560 row\n",
      "Processing 34001/41560 row\n",
      "Processing 35001/41560 row\n",
      "Processing 36001/41560 row\n",
      "Processing 37001/41560 row\n",
      "Processing 38001/41560 row\n",
      "Processing 39001/41560 row\n",
      "Processing 40001/41560 row\n",
      "Processing 41001/41560 row\n",
      "train feature matrix shape: (41560, 7933)\n",
      "train label array shape: (41560,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_test.csv\n",
      "Processing 1/8910 row\n",
      "Processing 1001/8910 row\n",
      "Processing 2001/8910 row\n",
      "Processing 3001/8910 row\n",
      "Processing 4001/8910 row\n",
      "Processing 5001/8910 row\n",
      "Processing 6001/8910 row\n",
      "Processing 7001/8910 row\n",
      "Processing 8001/8910 row\n",
      "test feature matrix shape: (8910, 7933)\n",
      "test label array shape: (8910,)\n",
      "datapath data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "data df cols Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'label', 'tweet', 'hash_tag',\n",
      "       'clean_tweet', 'tokenized_tweet', 'tokenized_tweet_NLTK'],\n",
      "      dtype='object')\n",
      "\n",
      "Extract bigram count features from data/Twitter/hate_twitter/hate_upsampled_val.csv\n",
      "Processing 1/8907 row\n",
      "Processing 1001/8907 row\n",
      "Processing 2001/8907 row\n",
      "Processing 3001/8907 row\n",
      "Processing 4001/8907 row\n",
      "Processing 5001/8907 row\n",
      "Processing 6001/8907 row\n",
      "Processing 7001/8907 row\n",
      "Processing 8001/8907 row\n",
      "val feature matrix shape: (8907, 7933)\n",
      "val label array shape: (8907,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/bigram_vocab.json\"\n",
    "\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_binary')\n",
    "extract_features(vocab=json.load(open(vocab_filepath)),\n",
    "                 tokenizer=tokenizer,\n",
    "                 feature_field=\"clean_tweet\",\n",
    "                 data_dir=data_dir,\n",
    "                 feature_name='bigram_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_logistic_regression(data_dir: Path,\n",
    "                                     feature_name: str,\n",
    "                                     tune: bool = False) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit and evaluate the logistic regression model using the scikit-learn library.\n",
    "    # Parameters\n",
    "    data_dir : `Path`, required\n",
    "        The data directory.\n",
    "    feature_name : `str`, required.\n",
    "        Name of the feature, such as unigram_binary.\n",
    "    tune : `bool`, optional\n",
    "        Whether or not to tune the hyperparameters of the regularization strength\n",
    "        of the model of the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    # Returns\n",
    "        model_trained: `LogisticRegression`\n",
    "            The object of `LogisticRegression` after it is trained.\n",
    "    \"\"\"\n",
    "    # Implement logistic regression with scikit-learn.\n",
    "    # Print out the accuracy scores on dev and test data.\n",
    "\n",
    "    splits = ['train', 'val','test']\n",
    "    features, labels = {}, {}\n",
    "\n",
    "    for split in splits:\n",
    "        features_path = data_dir.joinpath(f'{split}_{feature_name}_upsamp_features.npz')\n",
    "        labels_path = data_dir.joinpath(f'{split}_upsamp_labels.npz')\n",
    "        features[split] = sparse.load_npz(features_path)\n",
    "        labels[split] = np.load(labels_path)['arr_0']\n",
    "    best_dev, best_model = 0, None\n",
    "    if tune:\n",
    "        for c in np.linspace(-5, 5, 11):\n",
    "            clf = LogisticRegression(random_state=42,\n",
    "                                     max_iter=100,\n",
    "                                     fit_intercept=False,\n",
    "                                     C=np.exp2(c))\n",
    "            clf.fit(features['train'], labels['train'])\n",
    "            dev_preds = clf.predict(features['val'])\n",
    "            dev_accuracy = accuracy_score(labels['val'], dev_preds)\n",
    "            print(c, dev_accuracy)\n",
    "            if dev_accuracy > best_dev:\n",
    "                best_dev, best_model = dev_accuracy, clf\n",
    "    else:\n",
    "        best_model = LogisticRegression(random_state=42,\n",
    "                                        max_iter=200,\n",
    "                                        fit_intercept=False)\n",
    "        best_model.fit(features['train'], labels['train'])\n",
    "\n",
    "    preds = {\n",
    "        'val': best_model.predict(features['val']),\n",
    "        'test': best_model.predict(features['test'])\n",
    "    }\n",
    "    for splt, splt_preds in preds.items():\n",
    "        print(\"{} accuracy: {:.4f}\".format(splt, accuracy_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} binary recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds))),\n",
    "        print(\"{} macro recall: {:.4f}\".format(splt, recall_score(labels[splt],\n",
    "                                                                splt_preds, \n",
    "                                                                average=\"macro\"))),\n",
    "        print(\"{} macro f1: {:.4f}\".format(\n",
    "            splt, f1_score(labels[splt], splt_preds, average='macro')))\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9650\n",
      "val binary recall: 0.9843\n",
      "val macro recall: 0.9652\n",
      "val macro f1: 0.9650\n",
      "test accuracy: 0.9631\n",
      "test binary recall: 0.9854\n",
      "test macro recall: 0.9630\n",
      "test macro f1: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9641\n",
      "val binary recall: 0.9847\n",
      "val macro recall: 0.9644\n",
      "val macro f1: 0.9641\n",
      "test accuracy: 0.9630\n",
      "test binary recall: 0.9868\n",
      "test macro recall: 0.9629\n",
      "test macro f1: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='unigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.4971\n",
      "val binary recall: 0.9966\n",
      "val macro recall: 0.5040\n",
      "val macro f1: 0.3421\n",
      "test accuracy: 0.5035\n",
      "test binary recall: 0.9951\n",
      "test macro recall: 0.5028\n",
      "test macro f1: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_binary',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.4971\n",
      "val binary recall: 0.9966\n",
      "val macro recall: 0.5040\n",
      "val macro f1: 0.3421\n",
      "test accuracy: 0.5035\n",
      "test binary recall: 0.9951\n",
      "test macro recall: 0.5028\n",
      "test macro f1: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_eval_logistic_regression(feature_name='bigram_count',\n",
    "                                 data_dir=Path(data_dir),\n",
    "                                 tune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.9650\n",
      "val binary recall: 0.9843\n",
      "val macro recall: 0.9652\n",
      "val macro f1: 0.9650\n",
      "test accuracy: 0.9631\n",
      "test binary recall: 0.9854\n",
      "test macro recall: 0.9630\n",
      "test macro f1: 0.9631\n",
      "\n",
      "Most hateful words:\n",
      " 5.9076 | allahsoil\n",
      " 4.2857 | racism\n",
      " 4.0006 | bigot\n",
      " 3.4879 | equality\n",
      " 3.4789 | white\n",
      " 3.3869 | blacklivesmatter\n",
      " 3.3394 | neighbors\n",
      " 3.2137 | mc\n",
      " 3.2048 | blatantly\n",
      " 3.2015 | shitty\n",
      "\n",
      "Least hateful words:\n",
      "-3.9929 | bihday\n",
      "-3.2502 | orlando\n",
      "-3.1340 | hardcore\n",
      "-2.8810 | thankful\n",
      "-2.7886 | healthy\n",
      "-2.7294 | friday\n",
      "-2.7153 | getting\n",
      "-2.6452 | day\n",
      "-2.6402 | tomorrow\n",
      "-2.5961 | followers\n",
      "\n",
      "Most neutral words:\n",
      " 0.0000 | <pad>\n",
      " 0.0000 | elder\n",
      " 0.0000 | litter\n",
      " 0.0000 | vaccines\n",
      " 0.0000 | devoted\n",
      " 0.0000 | rooting\n",
      " 0.0000 | minime\n",
      " 0.0000 | mornin\n",
      " 0.0000 | ankara\n",
      " 0.0000 | pl\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/Twitter/hate_twitter/\"\n",
    "vocab_filepath = \"data/Twitter/hate_twitter/unigram_vocab.json\"\n",
    "\n",
    "model_trained: LogisticRegression = fit_and_eval_logistic_regression(\n",
    "    feature_name='unigram_binary', data_dir=Path(data_dir), tune=False)\n",
    "weights = model_trained.coef_[0]\n",
    "vocab = json.load(open(vocab_filepath))\n",
    "print(\"\")\n",
    "print_important_weights(weights=weights, words=vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
