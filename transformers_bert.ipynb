{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying hate tweets using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npodpx/venv/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.optim import SGD, Adam\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, F1Score, Recall\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import time\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "\n",
    "DATA_DIR = \"./data/Twitter/hate_twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **config_kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n",
    "        logger.info(\"Initilazing BaseModel\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #save hyperparameters to checkpoint\n",
    "        self.step_count = 0\n",
    "        self.output_dir = Path(self.hparams.output_dir)\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        self.accuracy = Accuracy()\n",
    "        self.f1 = F1Score()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def _load_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        #print('doing forward step..\\n')\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print('starting training step..\\n')\n",
    "        input = self.batch2input(batch)\n",
    "        #print('input',input)\n",
    "        labels = input['labels']\n",
    "        #print('input labels',labels)\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('train_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('train_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('val_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('val_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('test_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('test_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        #print(\"configuring optimizer..\\n\")\n",
    "        model = self.model\n",
    "        #print('finished model...\\n')\n",
    "        # optimizer = SGD(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        #print('got optimizer',optimizer)\n",
    "\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            self.train_loader = self.get_dataloader(\"train\", self.hparams.train_batch_size, shuffle=True)\n",
    "            print('got fit train data loader..\\n')\n",
    "            #print(self.train_dataloader)\n",
    "            #print(type(self.train_dataloader))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        #print('training data',self.train_dataloader)\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(\"val\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader(\"test\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_generic_args(parser, root_dir) -> None:\n",
    "        parser.add_argument(\n",
    "            \"--max_epochs\",\n",
    "            default=10,\n",
    "            type=int,\n",
    "            help=\"The number of epochs to train your model.\",\n",
    "        )\n",
    "        ############################################################\n",
    "        ## WARNING: set --gpus 0 if you do not have access to GPUS #\n",
    "        ############################################################\n",
    "        parser.add_argument(\n",
    "            \"--gpus\",\n",
    "            default=1,\n",
    "            type=int,\n",
    "            help=\"The number of GPUs allocated for this, it is by default 1. Set to 0 for no GPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--output_dir\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "        )\n",
    "        parser.add_argument(\"--do_train\", action=\"store_true\", default=True, help=\"Whether to run training.\")\n",
    "        parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "        parser.add_argument(\n",
    "            \"--data_dir\",\n",
    "            default=\"./\",\n",
    "            type=str,\n",
    "            help=\"The input data dir. Should contain the training files.\",\n",
    "        )\n",
    "        parser.add_argument(\"--learning_rate\", default=1e-2, type=float, help=\"The initial learning rate for training.\")\n",
    "        parser.add_argument(\"--num_workers\", default=16, type=int, help=\"kwarg passed to DataLoader\")\n",
    "        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=3, type=int)\n",
    "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--feature_field\",default=\"clean_tweet\",type=str)\n",
    "    \n",
    "def generic_train(\n",
    "    model: BaseModel,\n",
    "    args: argparse.Namespace,\n",
    "    early_stopping_callback=False,\n",
    "    extra_callbacks=[],\n",
    "    checkpoint_callback=None,\n",
    "    logging_callback=None,\n",
    "    **extra_train_kwargs\n",
    "):\n",
    "    \n",
    "    # init model\n",
    "    odir = Path(model.hparams.output_dir)\n",
    "    odir.mkdir(exist_ok=True)\n",
    "    log_dir = Path(os.path.join(model.hparams.output_dir, 'logs'))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Tensorboard logger\n",
    "    pl_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        version=\"version_\" + datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\"),\n",
    "        name=\"\",\n",
    "        default_hp_metric=True\n",
    "    )\n",
    "    # add custom checkpoints\n",
    "    ckpt_path = os.path.join(\n",
    "        args.output_dir, pl_logger.version, \"checkpoints\",\n",
    "    )\n",
    "    if checkpoint_callback is None:\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=ckpt_path, filename=\"{epoch}-{val_acc:.2f}\", monitor=\"val_acc\", mode=\"max\", save_top_k=1, verbose=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    train_params = {}\n",
    "\n",
    "    train_params[\"max_epochs\"] = args.max_epochs\n",
    "    #print('defined max_epochs')\n",
    "\n",
    "    if args.gpus > 1:\n",
    "        train_params[\"distributed_backend\"] = \"ddp\"\n",
    "    #print(\n",
    "    #    'feeding in args',args,'of type',\n",
    "    #    type(args)\n",
    "    #)\n",
    "    trainer = pl.Trainer.from_argparse_args(\n",
    "        args=args,\n",
    "        enable_model_summary=False,\n",
    "        callbacks= [checkpoint_callback] + extra_callbacks,\n",
    "        logger=pl_logger,\n",
    "        **train_params,\n",
    "    )\n",
    "    #print('defined trainer.. \\n')\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "        #print('successfully fit model')\n",
    "        # track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
    "        pl_logger.log_hyperparams(params=model.hparams, metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
    "        pl_logger.save()\n",
    "\n",
    "        # save best model to `best_model.ckpt`\n",
    "        target_path = os.path.join(ckpt_path, 'best_model.ckpt')\n",
    "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} to {target_path}.\")\n",
    "        shutil.copy(checkpoint_callback.best_model_path, target_path)\n",
    "\n",
    "    \n",
    "    # Optionally, predict on test set and write to output_dir\n",
    "    if args.do_predict:\n",
    "        #print('trying to predict')\n",
    "        best_model_path = os.path.join(ckpt_path, \"best_model.ckpt\")\n",
    "        model = model.load_from_checkpoint(best_model_path)\n",
    "        return trainer.test(model)\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTwitterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Using dataset to process input text on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 35 # assigned based on length analysis of training set\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        note = []\n",
    "        label, text = int(self.data[index][0]), self.data[index][1]\n",
    "        #print('labels:',label)\n",
    "        #print('text',text)\n",
    "        return text, label\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        texts, labels = list(zip(*batch_data))\n",
    "        # print(text)\n",
    "        encodings = self.tokenizer(list(texts), padding=True, truncation=True, max_length=self.max_len, return_tensors= 'pt')\n",
    "        return (\n",
    "                encodings['input_ids'],\n",
    "                encodings['attention_mask'],\n",
    "                torch.LongTensor(labels).view(-1,1)\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class BERT_PL(BaseModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name)\n",
    "        \n",
    "    def _load_model(self):\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            self.hparams.model_name,\n",
    "            num_labels=2,\n",
    "        )\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name, config=model_config)\n",
    "\n",
    "    def forward(self, **args):\n",
    "        outputs = self.model(**args)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, predicted_labels, []\n",
    "\n",
    "    def get_dataloader(self, type_path, batch_size, shuffle=False):\n",
    "        datapath = os.path.join(self.hparams.data_dir, f\"hate_{type_path}.csv\")\n",
    "        data_df =  pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[self.hparams.feature_field].notna()]\n",
    "        #print('data df cols',data_df.columns)\n",
    "        data_col = data_df['label'].astype('str') + \" \" + data_df['clean_tweet']\n",
    "        data_col = list(data_col) \n",
    "        data = [d.strip().split(\" \", maxsplit=1) for d in data_col] # list of [label, text] pair\n",
    "        #print('data list',data[0:5])\n",
    "        dataset = BERTTwitterDataset(self.tokenizer, data)\n",
    "\n",
    "        logger.info(f\"Loading {type_path} data and labels from {datapath}\")\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=dataset.collate_fn\n",
    "        )\n",
    "        print('succeeded with data loader')\n",
    "        \n",
    "        return data_loader    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        #print('configuring optimizers')\n",
    "        model = self.model\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    def batch2input(self, batch):\n",
    "        return {\"input_ids\": batch[0], \"labels\": batch[2], \"attention_mask\": batch[1]}\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser, root_dir):\n",
    "        #print('got to add_model_specific_args')\n",
    "        parser.add_argument(\n",
    "            \"--model_name\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Pretrained tokenizer name or path\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--optimizer\",\n",
    "            default=\"adam\",\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Whether to use SGD or not\",\n",
    "        )\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/natra/Documents/Education/UChicago/Advanced ML/life_cycle_of_hate_speech'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/22/2022 14:42:27 - INFO - pytorch_lightning.utilities.seed -   Global seed set to 42\n",
      "05/22/2022 14:42:27 - INFO - __main__ -   Initilazing BaseModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got to add_model_specific_args\n",
      "args Namespace(max_epochs=3, gpus=0, output_dir='bert', do_train=True, do_predict=True, seed=42, data_dir='./data/Twitter/hate_twitter/', learning_rate=5e-05, num_workers=16, train_batch_size=32, eval_batch_size=32, feature_field='clean_tweet', model_name='distilbert-base-uncased', optimizer='adam')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "05/22/2022 14:42:29 - INFO - pytorch_lightning.utilities.distributed -   GPU available: True, used: False\n",
      "05/22/2022 14:42:29 - INFO - pytorch_lightning.utilities.distributed -   TPU available: False, using: 0 TPU cores\n",
      "05/22/2022 14:42:29 - INFO - pytorch_lightning.utilities.distributed -   IPU available: False, using: 0 IPUs\n",
      "/home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "05/22/2022 14:42:29 - INFO - __main__ -   Loading train data and labels from ./data/Twitter/hate_twitter/hate_train.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "import time\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "mock_args = f\"--data_dir {DATA_DIR} --output_dir bert --optimizer adam \\\n",
    "--model_name distilbert-base-uncased --learning_rate 0.00005 --max_epochs 3 --do_predict --gpus=0\"# change model_name here\n",
    "\n",
    "# load hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "BaseModel.add_generic_args(parser, os.getcwd())\n",
    "parser = BERT_PL.add_model_specific_args(parser, os.getcwd())\n",
    "args = parser.parse_args(mock_args.split())\n",
    "#print('args',args)\n",
    "# fix random seed to make sure the result is reproducible\n",
    "pl.seed_everything(args.seed)\n",
    "\n",
    "# If output_dir not provided, a folder will be generated in pwd\n",
    "if args.output_dir is None:\n",
    "    args.output_dir = os.path.join(\n",
    "        \"./results\",\n",
    "        f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",\n",
    "    )\n",
    "    os.makedirs(args.output_dir)\n",
    "dict_args = vars(args)\n",
    "model = BERT_PL(**dict_args)\n",
    "trainer = generic_train(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(max_epochs=3, gpus=0, output_dir='bert', do_train=True, do_predict=True, seed=42, data_dir='./data/Twitter/hate_twitter/', learning_rate=5e-05, num_workers=16, train_batch_size=32, eval_batch_size=32, feature_field='clean_tweet', model_name='distilbert-base-uncased', optimizer='adam')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
