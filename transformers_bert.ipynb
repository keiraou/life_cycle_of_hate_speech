{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying hate tweets using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.optim import SGD, Adam\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, F1Score, Recall\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import time\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "\n",
    "DATA_DIR = \"./data/Twitter/hate_twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **config_kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n",
    "        logger.info(\"Initilazing BaseModel\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #save hyperparameters to checkpoint\n",
    "        self.step_count = 0\n",
    "        self.output_dir = Path(self.hparams.output_dir)\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        self.accuracy = Accuracy()\n",
    "        self.f1 = F1Score()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def _load_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('train_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('train_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('val_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('val_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "        self.log('test_f1', self.f1(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "        self.log('test_recall', self.recall(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        # optimizer = SGD(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            self.train_loader = self.get_dataloader(\"train\", self.hparams.train_batch_size, shuffle=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(\"val\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader(\"test\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_generic_args(parser, root_dir) -> None:\n",
    "        parser.add_argument(\n",
    "            \"--max_epochs\",\n",
    "            default=10,\n",
    "            type=int,\n",
    "            help=\"The number of epochs to train your model.\",\n",
    "        )\n",
    "        ############################################################\n",
    "        ## WARNING: set --gpus 0 if you do not have access to GPUS #\n",
    "        ############################################################\n",
    "        parser.add_argument(\n",
    "            \"--gpus\",\n",
    "            default=1,\n",
    "            type=int,\n",
    "            help=\"The number of GPUs allocated for this, it is by default 1. Set to 0 for no GPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--output_dir\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "        )\n",
    "        parser.add_argument(\"--do_train\", action=\"store_true\", default=True, help=\"Whether to run training.\")\n",
    "        parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "        parser.add_argument(\n",
    "            \"--data_dir\",\n",
    "            default=\"./\",\n",
    "            type=str,\n",
    "            help=\"The input data dir. Should contain the training files.\",\n",
    "        )\n",
    "        parser.add_argument(\"--learning_rate\", default=1e-2, type=float, help=\"The initial learning rate for training.\")\n",
    "        parser.add_argument(\"--num_workers\", default=16, type=int, help=\"kwarg passed to DataLoader\")\n",
    "        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=3, type=int)\n",
    "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--feature_field\",default=\"clean_tweet\",type=str)\n",
    "    \n",
    "def generic_train(\n",
    "    model: BaseModel,\n",
    "    args: argparse.Namespace,\n",
    "    early_stopping_callback=False,\n",
    "    extra_callbacks=[],\n",
    "    checkpoint_callback=None,\n",
    "    logging_callback=None,\n",
    "    **extra_train_kwargs\n",
    "):\n",
    "    \n",
    "    # init model\n",
    "    odir = Path(model.hparams.output_dir)\n",
    "    odir.mkdir(exist_ok=True)\n",
    "    log_dir = Path(os.path.join(model.hparams.output_dir, 'logs'))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Tensorboard logger\n",
    "    pl_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        version=\"version_\" + datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\"),\n",
    "        name=\"\",\n",
    "        default_hp_metric=True\n",
    "    )\n",
    "\n",
    "    # add custom checkpoints\n",
    "    ckpt_path = os.path.join(\n",
    "        args.output_dir, pl_logger.version, \"checkpoints\",\n",
    "    )\n",
    "    if checkpoint_callback is None:\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=ckpt_path, filename=\"{epoch}-{val_acc:.2f}\", monitor=\"val_acc\", mode=\"max\", save_top_k=1, verbose=True\n",
    "        )\n",
    "\n",
    "    train_params = {}\n",
    "\n",
    "    train_params[\"max_epochs\"] = args.max_epochs\n",
    "\n",
    "    if args.gpus > 1:\n",
    "        train_params[\"distributed_backend\"] = \"ddp\"\n",
    "\n",
    "    trainer = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        enable_model_summary=False,\n",
    "        callbacks= [checkpoint_callback] + extra_callbacks,\n",
    "        logger=pl_logger,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "        # track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
    "        pl_logger.log_hyperparams(params=model.hparams, metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
    "        pl_logger.save()\n",
    "\n",
    "        # save best model to `best_model.ckpt`\n",
    "        target_path = os.path.join(ckpt_path, 'best_model.ckpt')\n",
    "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} to {target_path}.\")\n",
    "        shutil.copy(checkpoint_callback.best_model_path, target_path)\n",
    "\n",
    "    \n",
    "    # Optionally, predict on test set and write to output_dir\n",
    "    if args.do_predict:\n",
    "        best_model_path = os.path.join(ckpt_path, \"best_model.ckpt\")\n",
    "        model = model.load_from_checkpoint(best_model_path)\n",
    "        return trainer.test(model)\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTwitterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Using dataset to process input text on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 35 # assigned based on length analysis of training set\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        note = []\n",
    "        label, text = int(self.data[index][0]), self.data[index][1]\n",
    "        return text, label\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        texts, labels = list(zip(*batch_data))\n",
    "        # print(text)\n",
    "        encodings = self.tokenizer(list(texts), padding=True, truncation=True, max_length=self.max_len, return_tensors= 'pt')\n",
    "        return (\n",
    "                encodings['input_ids'],\n",
    "                encodings['attention_mask'],\n",
    "                torch.LongTensor(labels).view(-1,1)\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class BERT_PL(BaseModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name)\n",
    "        \n",
    "    def _load_model(self):\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            self.hparams.model_name,\n",
    "            num_labels=2,\n",
    "        )\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name, config=model_config)\n",
    "\n",
    "    def forward(self, **args):\n",
    "        outputs = self.model(**args)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, predicted_labels, []\n",
    "\n",
    "    def get_dataloader(self, type_path, batch_size, shuffle=False):\n",
    "        datapath = os.path.join(self.hparams.data_dir, f\"{type_path}_clean.csv\")\n",
    "        data_df =  pd.read_csv(datapath)\n",
    "        data_df = data_df[data_df[self.hparams.feature_field].notna()]\n",
    "        print('data df cols',data_df.columns)\n",
    "        data_col = data_df['label'].astype(str) + \",\" + data_df[self.hparams.feature_field]\n",
    "        print('data col head',data_col[0:5])\n",
    "        data = list(data_col) # list of [label, text] pair\n",
    "        dataset = BERTTwitterDataset(self.tokenizer, data)\n",
    "\n",
    "        logger.info(f\"Loading {type_path} data and labels from {datapath}\")\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=dataset.collate_fn\n",
    "        )\n",
    "        \n",
    "        return data_loader    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    def batch2input(self, batch):\n",
    "        return {\"input_ids\": batch[0], \"labels\": batch[2], \"attention_mask\": batch[1]}\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser, root_dir):\n",
    "        parser.add_argument(\n",
    "            \"--model_name\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Pretrained tokenizer name or path\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--optimizer\",\n",
    "            default=\"adam\",\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Whether to use SGD or not\",\n",
    "        )\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/natra/Documents/Education/UChicago/Advanced ML/life_cycle_of_hate_speech'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/20/2022 18:48:35 - INFO - pytorch_lightning.utilities.seed -   Global seed set to 42\n",
      "05/20/2022 18:48:35 - INFO - __main__ -   Initilazing BaseModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(max_epochs=3, gpus=1, output_dir='bert', do_train=True, do_predict=True, seed=42, data_dir='./data/Twitter/hate_twitter/', learning_rate=5e-05, num_workers=16, train_batch_size=32, eval_batch_size=32, feature_field='clean_tweets', model_name='distilbert-base-uncased', optimizer='adam')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "05/20/2022 18:48:38 - INFO - pytorch_lightning.utilities.distributed -   GPU available: True, used: True\n",
      "05/20/2022 18:48:38 - INFO - pytorch_lightning.utilities.distributed -   TPU available: False, using: 0 TPU cores\n",
      "05/20/2022 18:48:38 - INFO - pytorch_lightning.utilities.distributed -   IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clean_tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3359'>3360</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean_tweets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=37'>38</a>\u001b[0m     trainer \u001b[39m=\u001b[39m generic_train(model, args)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=41'>42</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb Cell 6'\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=35'>36</a>\u001b[0m dict_args \u001b[39m=\u001b[39m \u001b[39mvars\u001b[39m(args)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=36'>37</a>\u001b[0m model \u001b[39m=\u001b[39m BERT_PL(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdict_args)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000004vscode-remote?line=37'>38</a>\u001b[0m trainer \u001b[39m=\u001b[39m generic_train(model, args)\n",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb Cell 3'\u001b[0m in \u001b[0;36mgeneric_train\u001b[0;34m(model, args, early_stopping_callback, extra_callbacks, checkpoint_callback, logging_callback, **extra_train_kwargs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=162'>163</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer\u001b[39m.\u001b[39mfrom_argparse_args(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=163'>164</a>\u001b[0m     args,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=164'>165</a>\u001b[0m     enable_model_summary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=167'>168</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=168'>169</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=170'>171</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=171'>172</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=172'>173</a>\u001b[0m     \u001b[39m# track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=173'>174</a>\u001b[0m     pl_logger\u001b[39m.\u001b[39mlog_hyperparams(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mhparams, metrics\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mhp_metric\u001b[39m\u001b[39m'\u001b[39m: checkpoint_callback\u001b[39m.\u001b[39mbest_model_score\u001b[39m.\u001b[39mitem()})\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=734'>735</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=735'>736</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=738'>739</a>\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=739'>740</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=740'>741</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=741'>742</a>\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=674'>675</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=675'>676</a>\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=676'>677</a>\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=681'>682</a>\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=683'>684</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=685'>686</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=686'>687</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=774'>775</a>\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=775'>776</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=776'>777</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=778'>779</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=779'>780</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1138\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1135'>1136</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_before_accelerator_backend_setup\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1136'>1137</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39msetup_environment()\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1137'>1138</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_setup_hook()  \u001b[39m# allow user to setup lightning_module in accelerator environment\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1139'>1140</a>\u001b[0m \u001b[39m# check if we should delay restoring checkpoint till later\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1140'>1141</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mrestore_checkpoint_after_pre_dispatch:\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1439\u001b[0m, in \u001b[0;36mTrainer._call_setup_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1436'>1437</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1437'>1438</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39msetup(stage\u001b[39m=\u001b[39mfn)\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1438'>1439</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_hook(\u001b[39m\"\u001b[39;49m\u001b[39msetup\u001b[39;49m\u001b[39m\"\u001b[39;49m, stage\u001b[39m=\u001b[39;49mfn)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1440'>1441</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mpost_setup\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1498'>1499</a>\u001b[0m model_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1499'>1500</a>\u001b[0m \u001b[39mif\u001b[39;00m callable(model_fx):\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1500'>1501</a>\u001b[0m     output \u001b[39m=\u001b[39m model_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1502'>1503</a>\u001b[0m \u001b[39m# *Bad code alert*\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1503'>1504</a>\u001b[0m \u001b[39m# The `Accelerator` mostly calls the `TrainingTypePlugin` but some of those calls are deprecated.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1504'>1505</a>\u001b[0m \u001b[39m# The following logic selectively chooses which hooks are called on each object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1508'>1509</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1509'>1510</a>\u001b[0m \u001b[39m# call the accelerator hook\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1510'>1511</a>\u001b[0m \u001b[39mif\u001b[39;00m hook_name \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mon_train_start\u001b[39m\u001b[39m\"\u001b[39m,) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, hook_name):\n",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb Cell 3'\u001b[0m in \u001b[0;36mBaseModel.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=69'>70</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, stage):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000002vscode-remote?line=71'>72</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dataloader(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mtrain_batch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb Cell 4'\u001b[0m in \u001b[0;36mBERT_PL.get_dataloader\u001b[0;34m(self, type_path, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000003vscode-remote?line=46'>47</a>\u001b[0m datapath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mdata_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtype_path\u001b[39m}\u001b[39;00m\u001b[39m_clean.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000003vscode-remote?line=47'>48</a>\u001b[0m data_df \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39mread_csv(datapath)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000003vscode-remote?line=48'>49</a>\u001b[0m data_df \u001b[39m=\u001b[39m data_df[data_df[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mfeature_field]\u001b[39m.\u001b[39mnotna()]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000003vscode-remote?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata df cols\u001b[39m\u001b[39m'\u001b[39m,data_df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/npodpx/class/adv_ml/life_cycle_of_hate_speech/transformers_bert.ipynb#ch0000003vscode-remote?line=50'>51</a>\u001b[0m data_col \u001b[39m=\u001b[39m data_df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m data_df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mfeature_field]\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3455'>3456</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3456'>3457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3457'>3458</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3458'>3459</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3459'>3460</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3362'>3363</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3364'>3365</a>\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[1;32m   <a href='file:///home/npodpx/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3365'>3366</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean_tweets'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "import time\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    ########################################################\n",
    "    ## TODO: change args if needed according to your files #\n",
    "    ########################################################\n",
    "    mock_args = f\"--data_dir {DATA_DIR} --output_dir bert --optimizer adam \\\n",
    "    --model_name distilbert-base-uncased --learning_rate 0.00005 --max_epochs 3 --do_predict\" # change model_name here\n",
    "\n",
    "    # load hyperparameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    BaseModel.add_generic_args(parser, os.getcwd())\n",
    "    parser = BERT_PL.add_model_specific_args(parser, os.getcwd())\n",
    "    args = parser.parse_args(mock_args.split())\n",
    "    print(args)\n",
    "    # fix random seed to make sure the result is reproducible\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    # If output_dir not provided, a folder will be generated in pwd\n",
    "    if args.output_dir is None:\n",
    "        args.output_dir = os.path.join(\n",
    "            \"./results\",\n",
    "            f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",\n",
    "        )\n",
    "        os.makedirs(args.output_dir)\n",
    "    dict_args = vars(args)\n",
    "    model = BERT_PL(**dict_args)\n",
    "    trainer = generic_train(model, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40f6b8985ae3d3af9736205d555f7ff87522357a9f5bdb6e88eda9160976b228"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
